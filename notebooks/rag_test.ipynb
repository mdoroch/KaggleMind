{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp39-cp39-macosx_14_0_arm64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "\u001b[K     |████████████████████████████████| 470 kB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: openai in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: packaging in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from sentence-transformers) (0.20.2)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: tqdm in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from sentence-transformers) (4.66.1)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.8 MB 19.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.11.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: scipy in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: Pillow in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from sentence-transformers) (10.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from openai) (2.11.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.1.3)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: certifi in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: filelock in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.12.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: jinja2 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: sympy in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2)\n",
      "Collecting huggingface-hub>=0.20.0\n",
      "  Downloading huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
      "\u001b[K     |████████████████████████████████| 515 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 16.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "\u001b[K     |████████████████████████████████| 418 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hf-xet<2.0.0,>=1.1.2\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Installing collected packages: hf-xet, huggingface-hub, tokenizers, safetensors, transformers, sentence-transformers, faiss-cpu\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.2\n",
      "    Uninstalling huggingface-hub-0.20.2:\n",
      "      Successfully uninstalled huggingface-hub-0.20.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.14.1\n",
      "    Uninstalling tokenizers-0.14.1:\n",
      "      Successfully uninstalled tokenizers-0.14.1\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.0\n",
      "    Uninstalling safetensors-0.4.0:\n",
      "      Successfully uninstalled safetensors-0.4.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.34.1\n",
      "    Uninstalling transformers-4.34.1:\n",
      "      Successfully uninstalled transformers-4.34.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "wavmark 0.0.2 requires torch<2.0, but you have torch 2.2.2 which is incompatible.\n",
      "gradio-client 0.8.0 requires websockets<12.0,>=10.0, but you have websockets 15.0.1 which is incompatible.\n",
      "faster-whisper 0.10.0 requires tokenizers<0.16,>=0.13, but you have tokenizers 0.21.2 which is incompatible.\u001b[0m\n",
      "Successfully installed faiss-cpu-1.11.0 hf-xet-1.1.5 huggingface-hub-0.33.2 safetensors-0.5.3 sentence-transformers-5.0.0 tokenizers-0.21.2 transformers-4.53.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/micheldoroch/VSCODE_PROJECTS/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu sentence-transformers openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydantic in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from google-generativeai) (2.11.3)\n",
      "Requirement already satisfied: tqdm in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from google-generativeai) (4.66.1)\n",
      "Collecting google-ai-generativelanguage==0.6.15\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from google-generativeai) (4.14.1)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from google-generativeai) (2.26.1)\n",
      "Collecting google-api-core\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "\u001b[K     |████████████████████████████████| 160 kB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from google-generativeai) (4.25.1)\n",
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.175.0-py3-none-any.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting proto-plus<2.0.0dev,>=1.22.3\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0.0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from google-api-core->google-generativeai) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from google-api-core->google-generativeai) (1.60.0)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.2)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Collecting grpcio<2.0.0,>=1.33.2\n",
      "  Downloading grpcio-1.73.1-cp39-cp39-macosx_11_0_universal2.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-macosx_10_9_universal2.whl (418 kB)\n",
      "\u001b[K     |████████████████████████████████| 418 kB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4)\n",
      "Collecting httplib2<1.0.0,>=0.19.0\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: uritemplate<5,>=3.0.1 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.0.9)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from pydantic->google-generativeai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from pydantic->google-generativeai) (0.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/micheldoroch/VSCODE_PROJECTS/venv/lib/python3.9/site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Installing collected packages: protobuf, proto-plus, grpcio, googleapis-common-protos, httplib2, grpcio-status, google-api-core, google-auth-httplib2, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.1\n",
      "    Uninstalling protobuf-4.25.1:\n",
      "      Successfully uninstalled protobuf-4.25.1\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.60.0\n",
      "    Uninstalling grpcio-1.60.0:\n",
      "      Successfully uninstalled grpcio-1.60.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "faster-whisper 0.10.0 requires tokenizers<0.16,>=0.13, but you have tokenizers 0.21.2 which is incompatible.\u001b[0m\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.25.1 google-api-python-client-2.175.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.73.1 grpcio-status-1.71.2 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.5\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/micheldoroch/VSCODE_PROJECTS/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/raw_dataset/discussion_links_playground_v2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23083e16bc04b32becdf4aefe0b03fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Инициализация модели\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Загрузка данных (список строк — обсуждений)\n",
    "with open(\"../data/raw_dataset/discussion_links_playground.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f) \n",
    "\n",
    "# Подготовка текстов\n",
    "texts = [''.join(item[\"discussion_texts\"]) for item in raw_data]\n",
    "\n",
    "# Векторизация\n",
    "embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Построение FAISS-индекса\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # или IndexIDMap(IndexFlatL2(d)) если нужны ID\n",
    "index.add(embeddings)\n",
    "\n",
    "# Сохранение индекса\n",
    "faiss.write_index(index, \"feature_index.faiss\")\n",
    "\n",
    "# Сохранение записей (можно оставить только нужные поля)\n",
    "with open(\"records.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([{\"text\": text} for text in texts], f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\n",
      "    \"feature\": \"Distance to Key Locations (e.g., hospitals, schools, major employers)\",\n",
      "    \"reasoning\": \"In the California Housing competition, calculating the distance to cities with a population over 500,000 and distance to coastline proved beneficial.  This suggests that distance to relevant landmarks can be a strong predictor. For the Stroke Prediction Dataset, distance to medical facilities, schools (related to childhood stress and family environment), and major employers (socioeconomic factors) could be useful features. This feature attempts to capture the influence of the environment on stroke risk.\"\n",
      "  },\n",
      "  {\n",
      "    \"feature\": \"PCA Coordinates\",\n",
      "    \"reasoning\": \"PCA was used in the California Housing competition to reduce dimensionality and extract relevant information from the coordinate data. Applying PCA to the original features of the stroke data (or potentially engineered location-based features) might reveal underlying patterns and relationships that aren't immediately obvious from the raw data. It simplifies the data representation while preserving variance.\"\n",
      "  },\n",
      "  {\n",
      "    \"feature\": \"Rotated Coordinates (15, 30, 45 degrees)\",\n",
      "    \"reasoning\": \"Similar to PCA, rotating the coordinate system might highlight different aspects of the spatial data. This was successful in the California Housing competition. In the context of the stroke prediction dataset, it is less relevant, so its usefulness is limited. If location data is available or can be inferred, this technique may capture different spatial relationships.\"\n",
      "  },\n",
      "  {\n",
      "    \"feature\": \"Polar Coordinates (r, theta)\",\n",
      "    \"reasoning\": \"Converting latitude and longitude (if present) to polar coordinates (radius and angle) can be helpful for capturing cyclical patterns and distance relationships. This was helpful in the California Housing competition. Might not be particularly useful if the underlying coordinate system is unrelated to the data. If there are clusters of events or demographic variations at certain angles from a 'center,' this feature can be effective.\"\n",
      "  },\n",
      "  {\n",
      "    \"feature\": \"Interaction Terms (e.g., age * hypertension, BMI * smoking_status)\",\n",
      "    \"reasoning\": \"Interaction terms capture the combined effect of two or more variables. For example, the risk of stroke might be significantly higher in older individuals with hypertension than in either group alone.  Creating interaction terms between known risk factors like age, hypertension, heart disease, smoking status, and BMI is a standard technique to improve model performance. It allows the model to identify non-linear relationships.\"\n",
      "  },\n",
      "  {\n",
      "    \"feature\": \"Polynomial Features (e.g., age^2, BMI^2)\",\n",
      "    \"reasoning\": \"Similar to interaction terms, polynomial features can capture non-linear relationships between features and the target variable. For example, the relationship between age and stroke risk might not be linear, and including age^2 might improve the model's ability to capture this curvature. Creating features such as `age^2`, `BMI^2`, etc., might help.\"\n",
      "  },\n",
      "  {\n",
      "    \"feature\": \"Ensemble Weights Optimized with Optuna\",\n",
      "    \"reasoning\": \"Several successful solutions (Employee Attrition and Stroke Prediction) used Optuna to optimize the weights of ensemble models. This suggests that a well-tuned ensemble can significantly improve performance.  Instead of using fixed weights, Optuna can be used to find the optimal blending of different model predictions.\"\n",
      "  },\n",
      "  {\n",
      "    \"feature\": \"Stratified K-Fold Cross-Validation\",\n",
      "    \"reasoning\": \"Brendan Moore's 9th place solution in the Paris Housing competition used Stratified K-Fold cross-validation based on price per square meter. If the target variable (stroke occurrence) is imbalanced, Stratified K-Fold ensures that each fold has a representative distribution of both classes, which leads to more robust and reliable evaluation.\"\n",
      "  },\n",
      "  {\n",
      "    \"feature\": \"Feature Selection / Importance Analysis\",\n",
      "    \"reasoning\": \"Given the success of using Optuna to find weights in the ensembles (Employee Attrition and Stroke Prediction), and using PCA to simplify features (California Housing) it's clear that focusing on the most important and informative features is important.  Implement feature selection techniques (e.g., using feature importances from a tree-based model, or using SelectKBest) to identify the most relevant features and potentially remove noisy or redundant ones. This will improve model generalization and reduce overfitting.\"\n",
      "  },\n",
      "  {\n",
      "    \"feature\": \"Distribution Matching (Ordinal Regression)\",\n",
      "    \"reasoning\": \"In the Wine Quality competition, the distribution of the classes was leveraged to improve predictions. Apply a similar approach if the target variable exhibits a specific distribution. For instance, if the stroke dataset has a limited set of outcome values with known population frequencies, then it may be possible to improve the predictions by forcing the model to produce the same overall distribution.\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Setup SentenceTransformer + FAISS\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "index = faiss.read_index(\"feature_index.faiss\")\n",
    "\n",
    "# Load mapped texts\n",
    "with open(\"records.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    records = json.load(f)\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=\"AIzaSyDsHBPS47qp1m3qmUFsUbZs3f7eFGHbccM\")  # <-- Your API key here\n",
    "\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "llm = genai.GenerativeModel(model_name)\n",
    "\n",
    "def retrieve_relevant_docs(query: str, k: int = 5):\n",
    "    query_embedding = model.encode(query)\n",
    "    if query_embedding.ndim == 1:\n",
    "        query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    results = [records[i][\"text\"] for i in indices[0]]\n",
    "    return results\n",
    "\n",
    "def make_prompt(new_competition_name: str, contexts: list[str]) -> str:\n",
    "    context_block = \"\\n---\\n\".join(contexts)\n",
    "    prompt = f\"\"\"\n",
    "You are a Kaggle expert. Based on the following discussion excerpts from previous similar competitions:\n",
    "\n",
    "{context_block}\n",
    "\n",
    "Please generate a list of features that would likely work well for the new competition titled \"{new_competition_name}\". Provide detailed reasoning for each feature suggestion.\n",
    "\n",
    "Return only the LIST object with description of relevant features, nothing else:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def ask_rag_for_new_competition(new_comp_name: str):\n",
    "\n",
    "    similar_docs = retrieve_relevant_docs(new_comp_name)\n",
    "\n",
    "    prompt = make_prompt(new_comp_name, similar_docs)\n",
    "\n",
    "    answer = generate_answer(prompt)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def generate_answer(prompt: str):\n",
    "    response = llm.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "def ask_rag(query: str):\n",
    "    docs = retrieve_relevant_docs(query)\n",
    "    prompt = make_prompt(query, docs)\n",
    "    answer = generate_answer(prompt)\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    new_competition = \"playground-series-s3e2\"\n",
    "    features_recommendation = ask_rag_for_new_competition(new_competition)\n",
    "    print(features_recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
