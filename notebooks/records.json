[
  {
    "text": "Regression with a Tabular California Housing Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular California Housing Dataset Playground Series - Season 3, Episode 1 Regression with a Tabular California Housing Dataset Overview Data Code Models Discussion Leaderboard Rules ryan ¬∑ 24th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 15 more_vert 24th Place Solution üèÖ - My second ever Kaggle competition üî• 24th place! üèÖ I recently decided to get back into Kaggle competitions, and I‚Äôm very happy with my performance on this one. One year ago, I competed in the Sartorius Cell Instance Segmentation competition and it was one of the most rewarding things I‚Äôve done in the past two years. The amount of information I learned in such a short period of time was astonishing. The same goes for this competition. Prior to this competition I hadn‚Äôt competed in any tabular competitions, and I again learned an incredible amount of information. To that end, I want to thank the Kaggle team for putting together these competitions and thank everyone who competed for making the competition enjoyable for noobs like myself. I would like to especially give a shout out to the following four notebooks and discussions that I continuously referenced throughout the competition, I couldn‚Äôt have done it without these (go give them an upvote): @phongnguyen1 - https://www.kaggle.com/code/phongnguyen1/distance-to-key-locations @dmitryuarov - https://www.kaggle.com/code/dmitryuarov/ps-s3e1-coordinates-key-to-victory @thedevastator - https://www.kaggle.com/competitions/playground-series-s3e1/discussion/376210 @tilii7 - https://www.kaggle.com/competitions/playground-series-s3e1/discussion/376709 My Solution: Quite frankly, I didn‚Äôt do anything groundbreaking. My solution consisted of an ensemble of XGBoost, LightGBM, and CatBoost using a 10 KFold split. I did some light hyperparameter optimization using Optuna for the XGBoost model, though not on the LightGBM or CatBoost model parameters. Feature Engineering: Distance to any California city with over 500,000 population. Encoding trick listed here Distance to coastline features as listed in this discussion PCA coordinates Rotated coordinates (15, 30, 45) Polar coordinates. CV: To compute my CV score, I used an 80/20 split of the training data and excluded the ‚Äúoriginal‚Äù dataset to get more accurate scores. Trusting local CV: After playing around a bit with various models and feature engineering ideas, I decided to trust my CV score and determined that the top public leaderboard scores were either doing some crazy feature engineering or were slightly overfit. Trusting my CV was the right choice as I increased my position 24 spots in the private leaderboard :) This competition will likely be the first of many for me this year, and hopefully you all will be seeing a lot more of me. I aim to play these tabular series until I land a top 3 position in one of them (I‚Äôm coming for you Kaggle merch). You can find my solution notebook here: https://www.kaggle.com/code/ryanirl/ps-s03e01-main-notebook-xgb-lgbm-cb 1 Please sign in to reply to this topic. comment 5 Comments 3 appreciation  comments Hotness Benny Posted 2 years ago arrow_drop_up 4 more_vert Phenomenal! Ramandeep Singh Posted 7 months ago arrow_drop_up 0 more_vert This is a very helpful and informative repo, Can you share more on discovering such features? Appreciation (3) inversion Kaggle Staff Posted 2 years ago arrow_drop_up 1 more_vert Thanks so much for sharing! Charlamagne Posted 2 years ago arrow_drop_up 0 more_vert Thanks for sharing Jahid hossan Posted 2 years ago arrow_drop_up 0 more_vert Outstanding! Thanks for shareing‚Ä¶..Regression with a Tabular California Housing Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular California Housing Dataset Playground Series - Season 3, Episode 1 Regression with a Tabular California Housing Dataset Overview Data Code Models Discussion Leaderboard Rules ryan ¬∑ 24th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 15 more_vert 24th Place Solution üèÖ - My second ever Kaggle competition üî• 24th place! üèÖ I recently decided to get back into Kaggle competitions, and I‚Äôm very happy with my performance on this one. One year ago, I competed in the Sartorius Cell Instance Segmentation competition and it was one of the most rewarding things I‚Äôve done in the past two years. The amount of information I learned in such a short period of time was astonishing. The same goes for this competition. Prior to this competition I hadn‚Äôt competed in any tabular competitions, and I again learned an incredible amount of information. To that end, I want to thank the Kaggle team for putting together these competitions and thank everyone who competed for making the competition enjoyable for noobs like myself. I would like to especially give a shout out to the following four notebooks and discussions that I continuously referenced throughout the competition, I couldn‚Äôt have done it without these (go give them an upvote): @phongnguyen1 - https://www.kaggle.com/code/phongnguyen1/distance-to-key-locations @dmitryuarov - https://www.kaggle.com/code/dmitryuarov/ps-s3e1-coordinates-key-to-victory @thedevastator - https://www.kaggle.com/competitions/playground-series-s3e1/discussion/376210 @tilii7 - https://www.kaggle.com/competitions/playground-series-s3e1/discussion/376709 My Solution: Quite frankly, I didn‚Äôt do anything groundbreaking. My solution consisted of an ensemble of XGBoost, LightGBM, and CatBoost using a 10 KFold split. I did some light hyperparameter optimization using Optuna for the XGBoost model, though not on the LightGBM or CatBoost model parameters. Feature Engineering: Distance to any California city with over 500,000 population. Encoding trick listed here Distance to coastline features as listed in this discussion PCA coordinates Rotated coordinates (15, 30, 45) Polar coordinates. CV: To compute my CV score, I used an 80/20 split of the training data and excluded the ‚Äúoriginal‚Äù dataset to get more accurate scores. Trusting local CV: After playing around a bit with various models and feature engineering ideas, I decided to trust my CV score and determined that the top public leaderboard scores were either doing some crazy feature engineering or were slightly overfit. Trusting my CV was the right choice as I increased my position 24 spots in the private leaderboard :) This competition will likely be the first of many for me this year, and hopefully you all will be seeing a lot more of me. I aim to play these tabular series until I land a top 3 position in one of them (I‚Äôm coming for you Kaggle merch). You can find my solution notebook here: https://www.kaggle.com/code/ryanirl/ps-s03e01-main-notebook-xgb-lgbm-cb 1 Please sign in to reply to this topic. comment 5 Comments 3 appreciation  comments Hotness Benny Posted 2 years ago arrow_drop_up 4 more_vert Phenomenal! Ramandeep Singh Posted 7 months ago arrow_drop_up 0 more_vert This is a very helpful and informative repo, Can you share more on discovering such features? Appreciation (3) inversion Kaggle Staff Posted 2 years ago arrow_drop_up 1 more_vert Thanks so much for sharing! Charlamagne Posted 2 years ago arrow_drop_up 0 more_vert Thanks for sharing Jahid hossan Posted 2 years ago arrow_drop_up 0 more_vert Outstanding! Thanks for shareing‚Ä¶..Regression with a Tabular California Housing Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular California Housing Dataset Playground Series - Season 3, Episode 1 Regression with a Tabular California Housing Dataset Overview Data Code Models Discussion Leaderboard Rules ryan ¬∑ 24th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 15 more_vert 24th Place Solution üèÖ - My second ever Kaggle competition üî• 24th place! üèÖ I recently decided to get back into Kaggle competitions, and I‚Äôm very happy with my performance on this one. One year ago, I competed in the Sartorius Cell Instance Segmentation competition and it was one of the most rewarding things I‚Äôve done in the past two years. The amount of information I learned in such a short period of time was astonishing. The same goes for this competition. Prior to this competition I hadn‚Äôt competed in any tabular competitions, and I again learned an incredible amount of information. To that end, I want to thank the Kaggle team for putting together these competitions and thank everyone who competed for making the competition enjoyable for noobs like myself. I would like to especially give a shout out to the following four notebooks and discussions that I continuously referenced throughout the competition, I couldn‚Äôt have done it without these (go give them an upvote): @phongnguyen1 - https://www.kaggle.com/code/phongnguyen1/distance-to-key-locations @dmitryuarov - https://www.kaggle.com/code/dmitryuarov/ps-s3e1-coordinates-key-to-victory @thedevastator - https://www.kaggle.com/competitions/playground-series-s3e1/discussion/376210 @tilii7 - https://www.kaggle.com/competitions/playground-series-s3e1/discussion/376709 My Solution: Quite frankly, I didn‚Äôt do anything groundbreaking. My solution consisted of an ensemble of XGBoost, LightGBM, and CatBoost using a 10 KFold split. I did some light hyperparameter optimization using Optuna for the XGBoost model, though not on the LightGBM or CatBoost model parameters. Feature Engineering: Distance to any California city with over 500,000 population. Encoding trick listed here Distance to coastline features as listed in this discussion PCA coordinates Rotated coordinates (15, 30, 45) Polar coordinates. CV: To compute my CV score, I used an 80/20 split of the training data and excluded the ‚Äúoriginal‚Äù dataset to get more accurate scores. Trusting local CV: After playing around a bit with various models and feature engineering ideas, I decided to trust my CV score and determined that the top public leaderboard scores were either doing some crazy feature engineering or were slightly overfit. Trusting my CV was the right choice as I increased my position 24 spots in the private leaderboard :) This competition will likely be the first of many for me this year, and hopefully you all will be seeing a lot more of me. I aim to play these tabular series until I land a top 3 position in one of them (I‚Äôm coming for you Kaggle merch). You can find my solution notebook here: https://www.kaggle.com/code/ryanirl/ps-s03e01-main-notebook-xgb-lgbm-cb 1 Please sign in to reply to this topic. comment 5 Comments 3 appreciation  comments Hotness Benny Posted 2 years ago arrow_drop_up 4 more_vert Phenomenal! Ramandeep Singh Posted 7 months ago arrow_drop_up 0 more_vert This is a very helpful and informative repo, Can you share more on discovering such features? Appreciation (3) inversion Kaggle Staff Posted 2 years ago arrow_drop_up 1 more_vert Thanks so much for sharing! Charlamagne Posted 2 years ago arrow_drop_up 0 more_vert Thanks for sharing Jahid hossan Posted 2 years ago arrow_drop_up 0 more_vert Outstanding! Thanks for shareing‚Ä¶.."
  },
  {
    "text": "Binary Classification with a Tabular Stroke Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Stroke Prediction Dataset Playground Series - Season 3, Episode 2 Binary Classification with a Tabular Stroke Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 6th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 5 more_vert 6th Place Ensemble I hope you find my way of searching weights for blending through Optuna interesting. https://www.kaggle.com/code/viktortaran/ps-feb-2023 Please sign in to reply to this topic. comment 4 Comments Hotness The Devastator Posted 2 years ago arrow_drop_up 1 more_vert I think this is a great approach! In general, I like methods that exploit tools in a different way than the way they are supposed to be used. The Devastator. Viktor Taran Topic Author Posted 2 years ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Yes, it is always interesting to do it. Shibata Posted 2 years ago ¬∑ 584th in this Competition arrow_drop_up 1 more_vert Thank you for sharing. Your notebook is so cool that I can read it easily. Viktor Taran Topic Author Posted 2 years ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thank you very much! Too many requests error Too many requestsBinary Classification with a Tabular Stroke Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Stroke Prediction Dataset Playground Series - Season 3, Episode 2 Binary Classification with a Tabular Stroke Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 6th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 5 more_vert 6th Place Ensemble I hope you find my way of searching weights for blending through Optuna interesting. https://www.kaggle.com/code/viktortaran/ps-feb-2023 Please sign in to reply to this topic. comment 4 Comments Hotness The Devastator Posted 2 years ago arrow_drop_up 1 more_vert I think this is a great approach! In general, I like methods that exploit tools in a different way than the way they are supposed to be used. The Devastator. Viktor Taran Topic Author Posted 2 years ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Yes, it is always interesting to do it. Shibata Posted 2 years ago ¬∑ 584th in this Competition arrow_drop_up 1 more_vert Thank you for sharing. Your notebook is so cool that I can read it easily. Viktor Taran Topic Author Posted 2 years ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thank you very much! Too many requests error Too many requestsBinary Classification with a Tabular Stroke Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Stroke Prediction Dataset Playground Series - Season 3, Episode 2 Binary Classification with a Tabular Stroke Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 6th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 5 more_vert 6th Place Ensemble I hope you find my way of searching weights for blending through Optuna interesting. https://www.kaggle.com/code/viktortaran/ps-feb-2023 Please sign in to reply to this topic. comment 4 Comments Hotness The Devastator Posted 2 years ago arrow_drop_up 1 more_vert I think this is a great approach! In general, I like methods that exploit tools in a different way than the way they are supposed to be used. The Devastator. Viktor Taran Topic Author Posted 2 years ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Yes, it is always interesting to do it. Shibata Posted 2 years ago ¬∑ 584th in this Competition arrow_drop_up 1 more_vert Thank you for sharing. Your notebook is so cool that I can read it easily. Viktor Taran Topic Author Posted 2 years ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thank you very much! Too many requests error Too many requests"
  },
  {
    "text": "Binary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 39th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 39th Place Ensemble I 've just applied the code from the previous competition without crucial adjustments and it works greatly here too. Look at my way of using Optuna to find weights. https://www.kaggle.com/code/viktortaran/ps-jan-3-2023 Please sign in to reply to this topic. comment 3 Comments Hotness Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert Congratulations Taran. I had already checked yours optuna  \"PS - Jan-3 2023\" See you on Ep. 4 Matt OP Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert Nice use of Optuna @viktortaran , great result! @kaggleqrdl Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Great stuff.  I used an ensemble as well of diverse models. Too many requests error Too many requestsBinary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 39th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 39th Place Ensemble I 've just applied the code from the previous competition without crucial adjustments and it works greatly here too. Look at my way of using Optuna to find weights. https://www.kaggle.com/code/viktortaran/ps-jan-3-2023 Please sign in to reply to this topic. comment 3 Comments Hotness Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert Congratulations Taran. I had already checked yours optuna  \"PS - Jan-3 2023\" See you on Ep. 4 Matt OP Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert Nice use of Optuna @viktortaran , great result! @kaggleqrdl Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Great stuff.  I used an ensemble as well of diverse models. Too many requests error Too many requestsBinary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 39th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 39th Place Ensemble I 've just applied the code from the previous competition without crucial adjustments and it works greatly here too. Look at my way of using Optuna to find weights. https://www.kaggle.com/code/viktortaran/ps-jan-3-2023 Please sign in to reply to this topic. comment 3 Comments Hotness Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert Congratulations Taran. I had already checked yours optuna  \"PS - Jan-3 2023\" See you on Ep. 4 Matt OP Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert Nice use of Optuna @viktortaran , great result! @kaggleqrdl Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Great stuff.  I used an ensemble as well of diverse models. Too many requests error Too many requestsBinary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 39th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 39th Place Ensemble I 've just applied the code from the previous competition without crucial adjustments and it works greatly here too. Look at my way of using Optuna to find weights. https://www.kaggle.com/code/viktortaran/ps-jan-3-2023 Please sign in to reply to this topic. comment 3 Comments Hotness Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert Congratulations Taran. I had already checked yours optuna  \"PS - Jan-3 2023\" See you on Ep. 4 Matt OP Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert Nice use of Optuna @viktortaran , great result! @kaggleqrdl Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Great stuff.  I used an ensemble as well of diverse models. Too many requests error Too many requestsBinary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 39th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 39th Place Ensemble I 've just applied the code from the previous competition without crucial adjustments and it works greatly here too. Look at my way of using Optuna to find weights. https://www.kaggle.com/code/viktortaran/ps-jan-3-2023 Please sign in to reply to this topic. comment 3 Comments Hotness Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert Congratulations Taran. I had already checked yours optuna  \"PS - Jan-3 2023\" See you on Ep. 4 Matt OP Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert Nice use of Optuna @viktortaran , great result! @kaggleqrdl Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Great stuff.  I used an ensemble as well of diverse models. Too many requests error Too many requestsBinary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 39th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 39th Place Ensemble I 've just applied the code from the previous competition without crucial adjustments and it works greatly here too. Look at my way of using Optuna to find weights. https://www.kaggle.com/code/viktortaran/ps-jan-3-2023 Please sign in to reply to this topic. comment 3 Comments Hotness Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert Congratulations Taran. I had already checked yours optuna  \"PS - Jan-3 2023\" See you on Ep. 4 Matt OP Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert Nice use of Optuna @viktortaran , great result! @kaggleqrdl Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Great stuff.  I used an ensemble as well of diverse models. Too many requests error Too many requests"
  },
  {
    "text": "Binary Classification with a Tabular Credit Card Fraud Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Credit Card Fraud Dataset Playground Series - Season 3, Episode 4 Binary Classification with a Tabular Credit Card Fraud Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 36th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 36th place solution I used the same strategy as it was before. 1) Added the original data because ROC AUC between original and test was lower than even between train and test. 2) Applied dropping for each model seperately based on Permutation Importance with corr sense. 3) Dropped the duplicates. 4) Used optuna to find the best proportion of the weights in the ensemble. The work is here https://www.kaggle.com/code/viktortaran/ps-jan-4-2023 A good article about the real competition is here https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/111284 Please sign in to reply to this topic. comment 9 Comments Hotness The Devastator Posted 2 years ago arrow_drop_up 1 more_vert Nice work! Using optuna to find the best weights is a really good approach. The Devastator. Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 0 more_vert I'm glad to hear this from someone who has extensive experience in data science! Matt OP Posted 2 years ago ¬∑ 54th in this Competition arrow_drop_up 1 more_vert Very nice notebook & result. Great work @viktortaran Ram Jas Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Great hugh achievement @viktortaran . Nicely done .Best of luck for upcoming competition. Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 1 more_vert Thank you very much! paddykb Posted 2 years ago ¬∑ 27th in this Competition arrow_drop_up 1 more_vert Well played Viktor. Really neat notebook. You've settled into a very readable style. Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 1 more_vert @paddykb Thank you so much! I remember several times you helped me improve my code through one competition (TPS october 2022 Rocket League)! Without the wonderful Kaggle community, it's impossible to improve! Ravi Ramakrishnan Posted 2 years ago ¬∑ 176th in this Competition arrow_drop_up 1 more_vert Hello @viktortaran , congrats for the position. Can you please confirm if the links work? It did not work for me, hence asking‚Ä¶ Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 0 more_vert @ravi20076 Thank you. I've rewritten links. Check it up! Too many requests error Too many requestsBinary Classification with a Tabular Credit Card Fraud Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Credit Card Fraud Dataset Playground Series - Season 3, Episode 4 Binary Classification with a Tabular Credit Card Fraud Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 36th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 36th place solution I used the same strategy as it was before. 1) Added the original data because ROC AUC between original and test was lower than even between train and test. 2) Applied dropping for each model seperately based on Permutation Importance with corr sense. 3) Dropped the duplicates. 4) Used optuna to find the best proportion of the weights in the ensemble. The work is here https://www.kaggle.com/code/viktortaran/ps-jan-4-2023 A good article about the real competition is here https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/111284 Please sign in to reply to this topic. comment 9 Comments Hotness The Devastator Posted 2 years ago arrow_drop_up 1 more_vert Nice work! Using optuna to find the best weights is a really good approach. The Devastator. Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 0 more_vert I'm glad to hear this from someone who has extensive experience in data science! Matt OP Posted 2 years ago ¬∑ 54th in this Competition arrow_drop_up 1 more_vert Very nice notebook & result. Great work @viktortaran Ram Jas Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Great hugh achievement @viktortaran . Nicely done .Best of luck for upcoming competition. Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 1 more_vert Thank you very much! paddykb Posted 2 years ago ¬∑ 27th in this Competition arrow_drop_up 1 more_vert Well played Viktor. Really neat notebook. You've settled into a very readable style. Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 1 more_vert @paddykb Thank you so much! I remember several times you helped me improve my code through one competition (TPS october 2022 Rocket League)! Without the wonderful Kaggle community, it's impossible to improve! Ravi Ramakrishnan Posted 2 years ago ¬∑ 176th in this Competition arrow_drop_up 1 more_vert Hello @viktortaran , congrats for the position. Can you please confirm if the links work? It did not work for me, hence asking‚Ä¶ Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 0 more_vert @ravi20076 Thank you. I've rewritten links. Check it up! Too many requests error Too many requestsBinary Classification with a Tabular Credit Card Fraud Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Credit Card Fraud Dataset Playground Series - Season 3, Episode 4 Binary Classification with a Tabular Credit Card Fraud Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 36th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 36th place solution I used the same strategy as it was before. 1) Added the original data because ROC AUC between original and test was lower than even between train and test. 2) Applied dropping for each model seperately based on Permutation Importance with corr sense. 3) Dropped the duplicates. 4) Used optuna to find the best proportion of the weights in the ensemble. The work is here https://www.kaggle.com/code/viktortaran/ps-jan-4-2023 A good article about the real competition is here https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/111284 Please sign in to reply to this topic. comment 9 Comments Hotness The Devastator Posted 2 years ago arrow_drop_up 1 more_vert Nice work! Using optuna to find the best weights is a really good approach. The Devastator. Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 0 more_vert I'm glad to hear this from someone who has extensive experience in data science! Matt OP Posted 2 years ago ¬∑ 54th in this Competition arrow_drop_up 1 more_vert Very nice notebook & result. Great work @viktortaran Ram Jas Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Great hugh achievement @viktortaran . Nicely done .Best of luck for upcoming competition. Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 1 more_vert Thank you very much! paddykb Posted 2 years ago ¬∑ 27th in this Competition arrow_drop_up 1 more_vert Well played Viktor. Really neat notebook. You've settled into a very readable style. Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 1 more_vert @paddykb Thank you so much! I remember several times you helped me improve my code through one competition (TPS october 2022 Rocket League)! Without the wonderful Kaggle community, it's impossible to improve! Ravi Ramakrishnan Posted 2 years ago ¬∑ 176th in this Competition arrow_drop_up 1 more_vert Hello @viktortaran , congrats for the position. Can you please confirm if the links work? It did not work for me, hence asking‚Ä¶ Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 0 more_vert @ravi20076 Thank you. I've rewritten links. Check it up! Too many requests error Too many requests"
  },
  {
    "text": "Ordinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requestsOrdinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules John Mitchell ¬∑ 16th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert 16th place Solution Thanks to the organisers for another challenging and interesting competition. I tried a  number of different things, of which probably the most interesting was to rank the test instances from highest to lowest predicted class, using real numbers rather than simply integers, and then to fit this to the expected distribution based on the {training plus original} sets. In the {train plus original} data, once duplicates are removed, we have quality values as follows: 8: 1.8% 7: 14.8% 6: 38.6% 5: 41.4% 4: 2.9% 3: 0.6% We import a file containing integers in these proportions, the same number of integers as there are in the test set. Then we match the top 1.8% of our wines to category 8, the next 14.8% to category 7, and so forth. I implemented this here . Based on the success other people were having with simple ensembles, I also tried using a simple median of top public solutions here . Ultimately, the generally poorer public LB scores from the idealised matching persuaded me to select two of the median-based submissions, which turned out not to be optimal. In fact, I had even a simple ensemble of two models that would have placed sixth had I known to select it. Please sign in to reply to this topic. comment 0 Comments Hotness Too many requests error Too many requestsOrdinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules John Mitchell ¬∑ 16th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert 16th place Solution Thanks to the organisers for another challenging and interesting competition. I tried a  number of different things, of which probably the most interesting was to rank the test instances from highest to lowest predicted class, using real numbers rather than simply integers, and then to fit this to the expected distribution based on the {training plus original} sets. In the {train plus original} data, once duplicates are removed, we have quality values as follows: 8: 1.8% 7: 14.8% 6: 38.6% 5: 41.4% 4: 2.9% 3: 0.6% We import a file containing integers in these proportions, the same number of integers as there are in the test set. Then we match the top 1.8% of our wines to category 8, the next 14.8% to category 7, and so forth. I implemented this here . Based on the success other people were having with simple ensembles, I also tried using a simple median of top public solutions here . Ultimately, the generally poorer public LB scores from the idealised matching persuaded me to select two of the median-based submissions, which turned out not to be optimal. In fact, I had even a simple ensemble of two models that would have placed sixth had I known to select it. Please sign in to reply to this topic. comment 0 Comments Hotness Too many requests error Too many requestsOrdinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules John Mitchell ¬∑ 16th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert 16th place Solution Thanks to the organisers for another challenging and interesting competition. I tried a  number of different things, of which probably the most interesting was to rank the test instances from highest to lowest predicted class, using real numbers rather than simply integers, and then to fit this to the expected distribution based on the {training plus original} sets. In the {train plus original} data, once duplicates are removed, we have quality values as follows: 8: 1.8% 7: 14.8% 6: 38.6% 5: 41.4% 4: 2.9% 3: 0.6% We import a file containing integers in these proportions, the same number of integers as there are in the test set. Then we match the top 1.8% of our wines to category 8, the next 14.8% to category 7, and so forth. I implemented this here . Based on the success other people were having with simple ensembles, I also tried using a simple median of top public solutions here . Ultimately, the generally poorer public LB scores from the idealised matching persuaded me to select two of the median-based submissions, which turned out not to be optimal. In fact, I had even a simple ensemble of two models that would have placed sixth had I known to select it. Please sign in to reply to this topic. comment 0 Comments Hotness Too many requests error Too many requestsOrdinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules John Mitchell ¬∑ 16th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert 16th place Solution Thanks to the organisers for another challenging and interesting competition. I tried a  number of different things, of which probably the most interesting was to rank the test instances from highest to lowest predicted class, using real numbers rather than simply integers, and then to fit this to the expected distribution based on the {training plus original} sets. In the {train plus original} data, once duplicates are removed, we have quality values as follows: 8: 1.8% 7: 14.8% 6: 38.6% 5: 41.4% 4: 2.9% 3: 0.6% We import a file containing integers in these proportions, the same number of integers as there are in the test set. Then we match the top 1.8% of our wines to category 8, the next 14.8% to category 7, and so forth. I implemented this here . Based on the success other people were having with simple ensembles, I also tried using a simple median of top public solutions here . Ultimately, the generally poorer public LB scores from the idealised matching persuaded me to select two of the median-based submissions, which turned out not to be optimal. In fact, I had even a simple ensemble of two models that would have placed sixth had I known to select it. Please sign in to reply to this topic. comment 0 Comments Hotness Too many requests error Too many requestsOrdinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules John Mitchell ¬∑ 16th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert 16th place Solution Thanks to the organisers for another challenging and interesting competition. I tried a  number of different things, of which probably the most interesting was to rank the test instances from highest to lowest predicted class, using real numbers rather than simply integers, and then to fit this to the expected distribution based on the {training plus original} sets. In the {train plus original} data, once duplicates are removed, we have quality values as follows: 8: 1.8% 7: 14.8% 6: 38.6% 5: 41.4% 4: 2.9% 3: 0.6% We import a file containing integers in these proportions, the same number of integers as there are in the test set. Then we match the top 1.8% of our wines to category 8, the next 14.8% to category 7, and so forth. I implemented this here . Based on the success other people were having with simple ensembles, I also tried using a simple median of top public solutions here . Ultimately, the generally poorer public LB scores from the idealised matching persuaded me to select two of the median-based submissions, which turned out not to be optimal. In fact, I had even a simple ensemble of two models that would have placed sixth had I known to select it. Please sign in to reply to this topic. comment 0 Comments Hotness Too many requests error Too many requestsOrdinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules John Mitchell ¬∑ 16th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert 16th place Solution Thanks to the organisers for another challenging and interesting competition. I tried a  number of different things, of which probably the most interesting was to rank the test instances from highest to lowest predicted class, using real numbers rather than simply integers, and then to fit this to the expected distribution based on the {training plus original} sets. In the {train plus original} data, once duplicates are removed, we have quality values as follows: 8: 1.8% 7: 14.8% 6: 38.6% 5: 41.4% 4: 2.9% 3: 0.6% We import a file containing integers in these proportions, the same number of integers as there are in the test set. Then we match the top 1.8% of our wines to category 8, the next 14.8% to category 7, and so forth. I implemented this here . Based on the success other people were having with simple ensembles, I also tried using a simple median of top public solutions here . Ultimately, the generally poorer public LB scores from the idealised matching persuaded me to select two of the median-based submissions, which turned out not to be optimal. In fact, I had even a simple ensemble of two models that would have placed sixth had I known to select it. Please sign in to reply to this topic. comment 0 Comments Hotness Too many requests error Too many requestsOrdinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules John Mitchell ¬∑ 16th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert 16th place Solution Thanks to the organisers for another challenging and interesting competition. I tried a  number of different things, of which probably the most interesting was to rank the test instances from highest to lowest predicted class, using real numbers rather than simply integers, and then to fit this to the expected distribution based on the {training plus original} sets. In the {train plus original} data, once duplicates are removed, we have quality values as follows: 8: 1.8% 7: 14.8% 6: 38.6% 5: 41.4% 4: 2.9% 3: 0.6% We import a file containing integers in these proportions, the same number of integers as there are in the test set. Then we match the top 1.8% of our wines to category 8, the next 14.8% to category 7, and so forth. I implemented this here . Based on the success other people were having with simple ensembles, I also tried using a simple median of top public solutions here . Ultimately, the generally poorer public LB scores from the idealised matching persuaded me to select two of the median-based submissions, which turned out not to be optimal. In fact, I had even a simple ensemble of two models that would have placed sixth had I known to select it. Please sign in to reply to this topic. comment 0 Comments Hotness Too many requests error Too many requests"
  },
  {
    "text": "Regression with a Tabular Paris Housing Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Paris Housing Price Dataset Playground Series - Season 3, Episode 6 Regression with a Tabular Paris Housing Price Dataset Overview Data Code Models Discussion Leaderboard Rules Brendan Moore ¬∑ 9th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 9th Place Solution My brief write-up of the 9th place solution can be found in the attached link. https://www.kaggle.com/code/brendanmoore14/9th-place-solution A simple automl approach using autogluon, with the original data included within the training loops, but not the cross-validation, ended up being my highest local cross-validation. I used a 5-fold StratifiedKFold split, with the classes being the three groups of price/sqm (~10/sqm, ~100/sqm, and ~1000/sqm). I tried multiclass classification on these three groups, but it ultimately did not improve the modelling. Feature engineering, oversampling of the minority classes (~10/1000/sqm), and undersampling of the majority classes (~100/sqm) were also not helpful in my experimentation to improve the local cross-validation. In the notebook, I have outlined what worked for me and what did not. Unlike some of the other top scores, I was not able to achieve high local cross-validation using a single model. Please sign in to reply to this topic. comment 4 Comments 1 appreciation  comment Hotness Nick Erickson Posted 2 years ago arrow_drop_up 1 more_vert Awesome example of using AutoGluon to get a strong placement, great work! Brian Okechukwu Posted 2 years ago arrow_drop_up 1 more_vert @brendanmoore14 Great work Brendan! Pardeep Singh Posted 2 years ago ¬∑ 268th in this Competition arrow_drop_up 0 more_vert Great work @brendanmoore14 thanks for sharing your approach! Appreciation (1) Junyi Wang Posted 2 years ago arrow_drop_up 1 more_vert Thank you so much Too many requests error Too many requestsRegression with a Tabular Paris Housing Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Paris Housing Price Dataset Playground Series - Season 3, Episode 6 Regression with a Tabular Paris Housing Price Dataset Overview Data Code Models Discussion Leaderboard Rules Brendan Moore ¬∑ 9th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 9th Place Solution My brief write-up of the 9th place solution can be found in the attached link. https://www.kaggle.com/code/brendanmoore14/9th-place-solution A simple automl approach using autogluon, with the original data included within the training loops, but not the cross-validation, ended up being my highest local cross-validation. I used a 5-fold StratifiedKFold split, with the classes being the three groups of price/sqm (~10/sqm, ~100/sqm, and ~1000/sqm). I tried multiclass classification on these three groups, but it ultimately did not improve the modelling. Feature engineering, oversampling of the minority classes (~10/1000/sqm), and undersampling of the majority classes (~100/sqm) were also not helpful in my experimentation to improve the local cross-validation. In the notebook, I have outlined what worked for me and what did not. Unlike some of the other top scores, I was not able to achieve high local cross-validation using a single model. Please sign in to reply to this topic. comment 4 Comments 1 appreciation  comment Hotness Nick Erickson Posted 2 years ago arrow_drop_up 1 more_vert Awesome example of using AutoGluon to get a strong placement, great work! Brian Okechukwu Posted 2 years ago arrow_drop_up 1 more_vert @brendanmoore14 Great work Brendan! Pardeep Singh Posted 2 years ago ¬∑ 268th in this Competition arrow_drop_up 0 more_vert Great work @brendanmoore14 thanks for sharing your approach! Appreciation (1) Junyi Wang Posted 2 years ago arrow_drop_up 1 more_vert Thank you so much Too many requests error Too many requestsRegression with a Tabular Paris Housing Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Paris Housing Price Dataset Playground Series - Season 3, Episode 6 Regression with a Tabular Paris Housing Price Dataset Overview Data Code Models Discussion Leaderboard Rules Brendan Moore ¬∑ 9th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 9th Place Solution My brief write-up of the 9th place solution can be found in the attached link. https://www.kaggle.com/code/brendanmoore14/9th-place-solution A simple automl approach using autogluon, with the original data included within the training loops, but not the cross-validation, ended up being my highest local cross-validation. I used a 5-fold StratifiedKFold split, with the classes being the three groups of price/sqm (~10/sqm, ~100/sqm, and ~1000/sqm). I tried multiclass classification on these three groups, but it ultimately did not improve the modelling. Feature engineering, oversampling of the minority classes (~10/1000/sqm), and undersampling of the majority classes (~100/sqm) were also not helpful in my experimentation to improve the local cross-validation. In the notebook, I have outlined what worked for me and what did not. Unlike some of the other top scores, I was not able to achieve high local cross-validation using a single model. Please sign in to reply to this topic. comment 4 Comments 1 appreciation  comment Hotness Nick Erickson Posted 2 years ago arrow_drop_up 1 more_vert Awesome example of using AutoGluon to get a strong placement, great work! Brian Okechukwu Posted 2 years ago arrow_drop_up 1 more_vert @brendanmoore14 Great work Brendan! Pardeep Singh Posted 2 years ago ¬∑ 268th in this Competition arrow_drop_up 0 more_vert Great work @brendanmoore14 thanks for sharing your approach! Appreciation (1) Junyi Wang Posted 2 years ago arrow_drop_up 1 more_vert Thank you so much Too many requests error Too many requestsRegression with a Tabular Paris Housing Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Paris Housing Price Dataset Playground Series - Season 3, Episode 6 Regression with a Tabular Paris Housing Price Dataset Overview Data Code Models Discussion Leaderboard Rules Brendan Moore ¬∑ 9th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 9th Place Solution My brief write-up of the 9th place solution can be found in the attached link. https://www.kaggle.com/code/brendanmoore14/9th-place-solution A simple automl approach using autogluon, with the original data included within the training loops, but not the cross-validation, ended up being my highest local cross-validation. I used a 5-fold StratifiedKFold split, with the classes being the three groups of price/sqm (~10/sqm, ~100/sqm, and ~1000/sqm). I tried multiclass classification on these three groups, but it ultimately did not improve the modelling. Feature engineering, oversampling of the minority classes (~10/1000/sqm), and undersampling of the majority classes (~100/sqm) were also not helpful in my experimentation to improve the local cross-validation. In the notebook, I have outlined what worked for me and what did not. Unlike some of the other top scores, I was not able to achieve high local cross-validation using a single model. Please sign in to reply to this topic. comment 4 Comments 1 appreciation  comment Hotness Nick Erickson Posted 2 years ago arrow_drop_up 1 more_vert Awesome example of using AutoGluon to get a strong placement, great work! Brian Okechukwu Posted 2 years ago arrow_drop_up 1 more_vert @brendanmoore14 Great work Brendan! Pardeep Singh Posted 2 years ago ¬∑ 268th in this Competition arrow_drop_up 0 more_vert Great work @brendanmoore14 thanks for sharing your approach! Appreciation (1) Junyi Wang Posted 2 years ago arrow_drop_up 1 more_vert Thank you so much Too many requests error Too many requestsRegression with a Tabular Paris Housing Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Paris Housing Price Dataset Playground Series - Season 3, Episode 6 Regression with a Tabular Paris Housing Price Dataset Overview Data Code Models Discussion Leaderboard Rules Brendan Moore ¬∑ 9th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 9th Place Solution My brief write-up of the 9th place solution can be found in the attached link. https://www.kaggle.com/code/brendanmoore14/9th-place-solution A simple automl approach using autogluon, with the original data included within the training loops, but not the cross-validation, ended up being my highest local cross-validation. I used a 5-fold StratifiedKFold split, with the classes being the three groups of price/sqm (~10/sqm, ~100/sqm, and ~1000/sqm). I tried multiclass classification on these three groups, but it ultimately did not improve the modelling. Feature engineering, oversampling of the minority classes (~10/1000/sqm), and undersampling of the majority classes (~100/sqm) were also not helpful in my experimentation to improve the local cross-validation. In the notebook, I have outlined what worked for me and what did not. Unlike some of the other top scores, I was not able to achieve high local cross-validation using a single model. Please sign in to reply to this topic. comment 4 Comments 1 appreciation  comment Hotness Nick Erickson Posted 2 years ago arrow_drop_up 1 more_vert Awesome example of using AutoGluon to get a strong placement, great work! Brian Okechukwu Posted 2 years ago arrow_drop_up 1 more_vert @brendanmoore14 Great work Brendan! Pardeep Singh Posted 2 years ago ¬∑ 268th in this Competition arrow_drop_up 0 more_vert Great work @brendanmoore14 thanks for sharing your approach! Appreciation (1) Junyi Wang Posted 2 years ago arrow_drop_up 1 more_vert Thank you so much Too many requests error Too many requests"
  }
]