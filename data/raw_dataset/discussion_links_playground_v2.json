[
  {
    "competition_slug": "playground-series-s3e1",
    "discussion_links": [
      "/competitions/playground-series-s3e1/discussion/377137",
      "/competitions/playground-series-s3e1/discussion/377179",
      "/competitions/playground-series-s3e1/discussion/377993"
    ],
    "discussion_texts": [
      "Regression with a Tabular California Housing Dataset | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Playground Prediction Competition · 2 years ago Late Submission more_horiz Regression with a Tabular California Housing Dataset Playground Series - Season 3, Episode 1 Regression with a Tabular California Housing Dataset Overview Data Code Models Discussion Leaderboard Rules Kirderf · 1st in this Competition  · Posted 2 years ago arrow_drop_up 68 more_vert That was a surprise! Here is the 1st place solution.... First - thanks for the fun competition, great public solutions and contributions! 👍 This will be a short summary but still as some maybe are interested in what FE/models etc. included to the solution. Didn't had much time for this competition while doing other as well but joined the competitions for trying some new frameworks versions in this specific dataset. I finally picked the AutoGluon framework and its tabular predictor for the task. \"AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data\" - https://arxiv.org/abs/2003.06505 For the FE part I used a public notebook https://www.kaggle.com/code/dmitryuarov/ps-s3e1-coordinates-key-to-victory ,  credit to author! The AG trained solution is an weighted ensemble of many 8 fold trained common architectures as xgb,lgbm,catb,RF,NN etc and it also used bootstrap aggregation and stacking(3 levels for this one). The final local CV score was 0.5006. That's it! 🙂 5 Please sign in to reply to this topic. comment 40 Comments 1 appreciation  comment Hotness Nick Erickson Posted 2 years ago arrow_drop_up 11 more_vert This is awesome! I'm the lead developer of AutoGluon, and it is very rewarding to see it being used to achieve 1st place in a Kaggle competition when combined with smart feature engineering tricks! There is a lot more that can be done with AutoML, and our team intends to keep pushing the boundaries so that it no longer comes as such a surprise that an AutoML system is able to assist Kaggle Grandmasters ;) . If anyone ends up trying AutoGluon and has suggestions on how it can be improved, please let me know! Cyril Bourgeois Posted 2 years ago · 14th in this Competition arrow_drop_up 3 more_vert Hello, for beginning, THANKS for your work!!! Autogluon is a wonderful hyperstructure!  I learned how to use Autogluon 3 months along, it's very difficult to discover strong actuallized exemples. Do you have adresses for learn with simplicity actual strong exemples with latest tabular implementations? 🌈⛲🌈 Can you be more active on Kaggle for show at community optimized and advanced example of Autogluon utilization? ^^🏄‍♂️🏄‍♀️ Nick Erickson Posted 2 years ago arrow_drop_up 2 more_vert @cyrilbourgeois Thank you for your enthusiasm :) We are working on creating a set of examples for highly performant usage of AutoGluon. Currently, I would recommend following my twitter ( https://twitter.com/innixma ) to be updated with the latest examples. We are also planning on creating a public Discord soon for the community to share their code examples, which will be announced on twitter, hopefully in a couple weeks. Regarding my own participation in Kaggle, I lack bandwidth to do this personally, but you can check my Kaggle notebooks for some examples of AutoGluon usage. Kirderf Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert @innixma Thanks, for the comment and for the work with the framework 👍 No surprise for me on the model and its high final ranking as I picked it as one of the best local CV score but more surprised by the 1st place, that was a positive suprise regardless the model choise :) About suggestions, sure I can drop an email about some small suggestions/findings noted along the way while testing the different modules. Nick Erickson Posted 2 years ago arrow_drop_up 0 more_vert Thanks for the detailed suggestions in your email, it is much appreciated! The Devastator Posted 2 years ago arrow_drop_up 4 more_vert Congratulations on a successful and impressive 1st place finish! It is really amazing that an AutoML system came up 1st on a competition.. I don't remember it ever happening before.. The Devastator. Kirderf Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Thanks! The manual FE before the training was important as well for the final score, so native end-to-end automl didn't get the 1st place :) But as \"model/training + FE part 2 + pp\" choice automl got the best score here. Yassir Acharki Posted 2 years ago · 316th in this Competition arrow_drop_up 1 more_vert Thanks for sharing this solution @kirderf Mario Batista Posted 2 years ago · 646th in this Competition arrow_drop_up 1 more_vert Thank you for sharing your work and results. faissal boutaounte Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! Zakir Khan Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! Amazing results! ✨ ksqrt Posted 2 years ago arrow_drop_up 1 more_vert awsome !!!!!!!! Gimnir Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! Balaji Kartheek Posted 2 years ago arrow_drop_up 1 more_vert Congratulations!! Scorpio42 Posted 2 years ago arrow_drop_up 1 more_vert Very interesting and congratulations! suanow Posted 2 years ago arrow_drop_up 1 more_vert Congrats, very interesting results! Cyril Bourgeois Posted 2 years ago · 14th in this Competition arrow_drop_up 1 more_vert Thannnnkksss !!!!! Rôger Andrade Posted 2 years ago arrow_drop_up 1 more_vert Congrats, way to go! Muhammad Waseem Posted 2 years ago arrow_drop_up 1 more_vert Congratulation,  Nice Notebook and great result Mohit Kumar Posted 2 years ago · 523rd in this Competition arrow_drop_up 1 more_vert Great Notebook!! Lots to learn from this notebook.. btw Congratulation!!🎉🎉 Dmitry Uarov Posted 2 years ago · 54th in this Competition arrow_drop_up 1 more_vert Congratulations and thank you! When I saw you in the first place, I thought that you had invented something cosmic for coordinates, but everything turned out to be simpler! 😄 Adam Wurdits Posted 2 years ago · 392nd in this Competition arrow_drop_up 1 more_vert Congratulations and thank you for the write-up! Tyl3rDurd3n Posted 2 years ago · 172nd in this Competition arrow_drop_up 1 more_vert Congratulations! Did you use the data from the original california housing dataset? Vladislav Viryasov Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! Great work and solution! =) Alexander Shumilin Posted 2 years ago · 25th in this Competition arrow_drop_up 1 more_vert Congratulations! Great result! :) Kirderf Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Thanks! :) THIRAPHAT boaty Posted 2 years ago · 355th in this Competition arrow_drop_up 1 more_vert Congratulation on the top first place :)👍🎉🎉 inversion Kaggle Staff Posted 2 years ago arrow_drop_up 2 more_vert Have you tried the new way to link to your write-up from the leaderboard? https://www.kaggle.com/discussions/product-feedback/373153 Kirderf Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert @inversion I didn't think of it, never been in top 5 before ;) as this is the playground series I didn't thought that was mandatory here though. But the information in the post has all needed information, links to the complete notebook for the FE part and autogluon is an automl framework so no need for more description I believe, all is printed in the post to reproduce the solution. Hope it's OK. 👍 inversion Kaggle Staff Posted 2 years ago arrow_drop_up 0 more_vert It's not at all necessary or required, but if you go to your Team page and put the link of your post there, it will put an icon next to your name on the LB, which will make it easier for people to find and upvote. 😃 Kirderf Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert @inversion too easy to understand haha 🙃 Saw the template and had an image of an arxiv paper! The link to this writeup is now added to the team site! 👍🙂 inversion Kaggle Staff Posted 2 years ago arrow_drop_up 0 more_vert @kirderf Thanks!! MichaelP Posted 2 years ago · 45th in this Competition arrow_drop_up 2 more_vert Congratulations! AutoML was a sensible choice in the time constraints. I didn't have time to implement an NN fully, and although I could see stacking was helping with local cv, I only had time to get to 1 level of stacking. An AutoML framework would likely have sped things up. Congratulations again. Kirderf Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thanks, yes it does help and can give a direction, I tried many frameworks and models. As the dataset was small it gave a good window for testing many different approaches, and I followed the local CV. A good dataset and FE is the also the final key, here I followed and tested the work in the public community as the time was limited. I believe with further hyp.tuning and final ensembles even better score could be done. MichaelP Posted 2 years ago · 45th in this Competition arrow_drop_up 0 more_vert Yes I am sure you are right that there would have been more scope for incremental optimisation with more time. But it is also fun to have a short competition where people don't have to make a long time commitment and in the time available some very effective work was done. Eric Vos Posted 2 years ago · 30th in this Competition arrow_drop_up 2 more_vert Congratulations, i was not expecting an AML framework at the first place. Kirderf Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert Thanks! I tried many frameworks and models, automl and not, but followed the local CV and this was the best here. But a good dataset and FE is the key as well, only automl/model doesn't get the 1st place, both are needed. Jose Cáliz Posted 2 years ago arrow_drop_up 2 more_vert Congratulations on the first place!. Can you share what CV strategy did you use and if you augmented dataset with original California Housing Dataset? Kirderf Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert You have all the info in the links I provided in the post :) E.g. AG use its ParallelLocalFoldFittingStrategy and Repeated k-fold Bagging.",
      "Regression with a Tabular California Housing Dataset | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Playground Prediction Competition · 2 years ago Late Submission more_horiz Regression with a Tabular California Housing Dataset Playground Series - Season 3, Episode 1 Regression with a Tabular California Housing Dataset Overview Data Code Models Discussion Leaderboard Rules jcerpent · 2nd in this Competition  · Posted 2 years ago arrow_drop_up 20 more_vert 2nd place solution - brief summary First of all, thanks to the organizers for a fun, short competition. Also thanks to the many notebook contributors in this competition!!!🎉 My final solution is an ensemble of some public contributions with some of my own personal ideas. -I felt like the boosting path was well-explored by many public notebooks and designed to base my boosting approach on these methods; as mentiond, distance using coordinates plays a big part. -I noticed many people added the external data to their dataset and computed their CV score using this data. Since the competition data is an adaptation of the original dataset, I think this is why some CV scores weren't very well aligned. Instead, I ran a CV split on the original data, and added the external data to the training set afterwards. This forces validation on the supplied dataset, better representing the LB score. Ensembling this with methods that split on the full merged data seemed to diversify a lot and improve LB. -Lastly, I also training a NN in keras using keras_tuner on the standard features + coordinate features. Local CV was only 0.59, but this also added significant diversification in the full blend. Model summary can be seen below: Please sign in to reply to this topic. comment 3 Comments Hotness Maik Pflugradt Posted 2 years ago arrow_drop_up 1 more_vert The source code of the top submissions is not published is it? The Devastator Posted 2 years ago arrow_drop_up 1 more_vert Really cool solution, I like how simple it is! The Devastator. wickedx Posted 2 years ago arrow_drop_up 0 more_vert why cool，hhha",
      "Regression with a Tabular California Housing Dataset | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Playground Prediction Competition · 2 years ago Late Submission more_horiz Regression with a Tabular California Housing Dataset Playground Series - Season 3, Episode 1 Regression with a Tabular California Housing Dataset Overview Data Code Models Discussion Leaderboard Rules ryan · 24th in this Competition  · Posted 2 years ago arrow_drop_up 15 more_vert 24th Place Solution 🏅 - My second ever Kaggle competition 🔥 24th place! 🏅 I recently decided to get back into Kaggle competitions, and I’m very happy with my performance on this one. One year ago, I competed in the Sartorius Cell Instance Segmentation competition and it was one of the most rewarding things I’ve done in the past two years. The amount of information I learned in such a short period of time was astonishing. The same goes for this competition. Prior to this competition I hadn’t competed in any tabular competitions, and I again learned an incredible amount of information. To that end, I want to thank the Kaggle team for putting together these competitions and thank everyone who competed for making the competition enjoyable for noobs like myself. I would like to especially give a shout out to the following four notebooks and discussions that I continuously referenced throughout the competition, I couldn’t have done it without these (go give them an upvote): @phongnguyen1 - https://www.kaggle.com/code/phongnguyen1/distance-to-key-locations @dmitryuarov - https://www.kaggle.com/code/dmitryuarov/ps-s3e1-coordinates-key-to-victory @thedevastator - https://www.kaggle.com/competitions/playground-series-s3e1/discussion/376210 @tilii7 - https://www.kaggle.com/competitions/playground-series-s3e1/discussion/376709 My Solution: Quite frankly, I didn’t do anything groundbreaking. My solution consisted of an ensemble of XGBoost, LightGBM, and CatBoost using a 10 KFold split. I did some light hyperparameter optimization using Optuna for the XGBoost model, though not on the LightGBM or CatBoost model parameters. Feature Engineering: Distance to any California city with over 500,000 population. Encoding trick listed here Distance to coastline features as listed in this discussion PCA coordinates Rotated coordinates (15, 30, 45) Polar coordinates. CV: To compute my CV score, I used an 80/20 split of the training data and excluded the “original” dataset to get more accurate scores. Trusting local CV: After playing around a bit with various models and feature engineering ideas, I decided to trust my CV score and determined that the top public leaderboard scores were either doing some crazy feature engineering or were slightly overfit. Trusting my CV was the right choice as I increased my position 24 spots in the private leaderboard :) This competition will likely be the first of many for me this year, and hopefully you all will be seeing a lot more of me. I aim to play these tabular series until I land a top 3 position in one of them (I’m coming for you Kaggle merch). You can find my solution notebook here: https://www.kaggle.com/code/ryanirl/ps-s03e01-main-notebook-xgb-lgbm-cb 1 Please sign in to reply to this topic. comment 5 Comments 3 appreciation  comments Hotness Benny Posted 2 years ago arrow_drop_up 4 more_vert Phenomenal! Ramandeep Singh Posted 7 months ago arrow_drop_up 0 more_vert This is a very helpful and informative repo, Can you share more on discovering such features? Appreciation (3) inversion Kaggle Staff Posted 2 years ago arrow_drop_up 1 more_vert Thanks so much for sharing! Charlamagne Posted 2 years ago arrow_drop_up 0 more_vert Thanks for sharing Jahid hossan Posted 2 years ago arrow_drop_up 0 more_vert Outstanding! Thanks for shareing….."
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 1 NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook . The dataset for this competition (both train and test) was generated from a deep learning model trained on the California Housing Dataset . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 6.48 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 6.48 MB sample_submission.csv test.csv train.csv 3 files 21 columns ",
    "data_description": "Regression with a Tabular California Housing Dataset | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Playground Prediction Competition · 2 years ago Late Submission more_horiz Regression with a Tabular California Housing Dataset Playground Series - Season 3, Episode 1 Regression with a Tabular California Housing Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Jan 3, 2023 Close Jan 10, 2023 Description link keyboard_arrow_up NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook . Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same—to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! To start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = 1 N ∑ i = 1 N ( y i − y ^ i ) 2 where y ^ i is the predicted value and y i is the original value for each instance i . Submission File For each id in the test set, you must predict the value for the target MedHouseVal . The file should contain a header and have the following format: id ,MedHouseVal 37137 , 2 . 01 37138 , 0 . 92 37139 , 1 . 11 etc . content_copy Timeline link keyboard_arrow_up Start Date - January 3, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  January 9, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Regression with a Tabular California Housing Dataset. https://kaggle.com/competitions/playground-series-s3e1, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 1,618 Entrants 702 Participants 689 Teams 5,163 Submissions Tags Regression Tabular Housing Beginner Root Mean Squared Error Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  }
]