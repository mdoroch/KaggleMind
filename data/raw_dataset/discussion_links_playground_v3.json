[
  {
    "competition_slug": "playground-series-s3e1",
    "discussion_links": [
      "/competitions/playground-series-s3e1/discussion/377137",
      "/competitions/playground-series-s3e1/discussion/377179",
      "/competitions/playground-series-s3e1/discussion/377993"
    ],
    "discussion_texts": [
      "Regression with a Tabular California Housing Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular California Housing Dataset Playground Series - Season 3, Episode 1 Regression with a Tabular California Housing Dataset Overview Data Code Models Discussion Leaderboard Rules Kirderf ¬∑ 1st in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 68 more_vert That was a surprise! Here is the 1st place solution.... First - thanks for the fun competition, great public solutions and contributions! üëç This will be a short summary but still as some maybe are interested in what FE/models etc. included to the solution. Didn't had much time for this competition while doing other as well but joined the competitions for trying some new frameworks versions in this specific dataset. I finally picked the AutoGluon framework and its tabular predictor for the task. \"AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data\" - https://arxiv.org/abs/2003.06505 For the FE part I used a public notebook https://www.kaggle.com/code/dmitryuarov/ps-s3e1-coordinates-key-to-victory ,  credit to author! The AG trained solution is an weighted ensemble of many 8 fold trained common architectures as xgb,lgbm,catb,RF,NN etc and it also used bootstrap aggregation and stacking(3 levels for this one). The final local CV score was 0.5006. That's it! üôÇ 5 Please sign in to reply to this topic. comment 40 Comments 1 appreciation  comment Hotness Nick Erickson Posted 2 years ago arrow_drop_up 11 more_vert This is awesome! I'm the lead developer of AutoGluon, and it is very rewarding to see it being used to achieve 1st place in a Kaggle competition when combined with smart feature engineering tricks! There is a lot more that can be done with AutoML, and our team intends to keep pushing the boundaries so that it no longer comes as such a surprise that an AutoML system is able to assist Kaggle Grandmasters ;) . If anyone ends up trying AutoGluon and has suggestions on how it can be improved, please let me know! Cyril Bourgeois Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 3 more_vert Hello, for beginning, THANKS for your work!!! Autogluon is a wonderful hyperstructure!  I learned how to use Autogluon 3 months along, it's very difficult to discover strong actuallized exemples. Do you have adresses for learn with simplicity actual strong exemples with latest tabular implementations? üåà‚õ≤üåà Can you be more active on Kaggle for show at community optimized and advanced example of Autogluon utilization? ^^üèÑ‚Äç‚ôÇÔ∏èüèÑ‚Äç‚ôÄÔ∏è Nick Erickson Posted 2 years ago arrow_drop_up 2 more_vert @cyrilbourgeois Thank you for your enthusiasm :) We are working on creating a set of examples for highly performant usage of AutoGluon. Currently, I would recommend following my twitter ( https://twitter.com/innixma ) to be updated with the latest examples. We are also planning on creating a public Discord soon for the community to share their code examples, which will be announced on twitter, hopefully in a couple weeks. Regarding my own participation in Kaggle, I lack bandwidth to do this personally, but you can check my Kaggle notebooks for some examples of AutoGluon usage. Kirderf Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert @innixma Thanks, for the comment and for the work with the framework üëç No surprise for me on the model and its high final ranking as I picked it as one of the best local CV score but more surprised by the 1st place, that was a positive suprise regardless the model choise :) About suggestions, sure I can drop an email about some small suggestions/findings noted along the way while testing the different modules. Nick Erickson Posted 2 years ago arrow_drop_up 0 more_vert Thanks for the detailed suggestions in your email, it is much appreciated! The Devastator Posted 2 years ago arrow_drop_up 4 more_vert Congratulations on a successful and impressive 1st place finish! It is really amazing that an AutoML system came up 1st on a competition.. I don't remember it ever happening before.. The Devastator. Kirderf Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks! The manual FE before the training was important as well for the final score, so native end-to-end automl didn't get the 1st place :) But as \"model/training + FE part 2 + pp\" choice automl got the best score here. Yassir Acharki Posted 2 years ago ¬∑ 316th in this Competition arrow_drop_up 1 more_vert Thanks for sharing this solution @kirderf Mario Batista Posted 2 years ago ¬∑ 646th in this Competition arrow_drop_up 1 more_vert Thank you for sharing your work and results. faissal boutaounte Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! Zakir Khan Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! Amazing results! ‚ú® ksqrt Posted 2 years ago arrow_drop_up 1 more_vert awsome !!!!!!!! Gimnir Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! Balaji Kartheek Posted 2 years ago arrow_drop_up 1 more_vert Congratulations!! Scorpio42 Posted 2 years ago arrow_drop_up 1 more_vert Very interesting and congratulations! suanow Posted 2 years ago arrow_drop_up 1 more_vert Congrats, very interesting results! Cyril Bourgeois Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert Thannnnkksss !!!!! R√¥ger Andrade Posted 2 years ago arrow_drop_up 1 more_vert Congrats, way to go! Muhammad Waseem Posted 2 years ago arrow_drop_up 1 more_vert Congratulation,  Nice Notebook and great result Mohit Kumar Posted 2 years ago ¬∑ 523rd in this Competition arrow_drop_up 1 more_vert Great Notebook!! Lots to learn from this notebook.. btw Congratulation!!üéâüéâ Dmitry Uarov Posted 2 years ago ¬∑ 54th in this Competition arrow_drop_up 1 more_vert Congratulations and thank you! When I saw you in the first place, I thought that you had invented something cosmic for coordinates, but everything turned out to be simpler! üòÑ Adam Wurdits Posted 2 years ago ¬∑ 392nd in this Competition arrow_drop_up 1 more_vert Congratulations and thank you for the write-up! Tyl3rDurd3n Posted 2 years ago ¬∑ 172nd in this Competition arrow_drop_up 1 more_vert Congratulations! Did you use the data from the original california housing dataset? Vladislav Viryasov Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! Great work and solution! =) Alexander Shumilin Posted 2 years ago ¬∑ 25th in this Competition arrow_drop_up 1 more_vert Congratulations! Great result! :) Kirderf Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks! :) THIRAPHAT boaty Posted 2 years ago ¬∑ 355th in this Competition arrow_drop_up 1 more_vert Congratulation on the top first place :)üëçüéâüéâ inversion Kaggle Staff Posted 2 years ago arrow_drop_up 2 more_vert Have you tried the new way to link to your write-up from the leaderboard? https://www.kaggle.com/discussions/product-feedback/373153 Kirderf Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert @inversion I didn't think of it, never been in top 5 before ;) as this is the playground series I didn't thought that was mandatory here though. But the information in the post has all needed information, links to the complete notebook for the FE part and autogluon is an automl framework so no need for more description I believe, all is printed in the post to reproduce the solution. Hope it's OK. üëç inversion Kaggle Staff Posted 2 years ago arrow_drop_up 0 more_vert It's not at all necessary or required, but if you go to your Team page and put the link of your post there, it will put an icon next to your name on the LB, which will make it easier for people to find and upvote. üòÉ Kirderf Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert @inversion too easy to understand haha üôÉ Saw the template and had an image of an arxiv paper! The link to this writeup is now added to the team site! üëçüôÇ inversion Kaggle Staff Posted 2 years ago arrow_drop_up 0 more_vert @kirderf Thanks!! MichaelP Posted 2 years ago ¬∑ 45th in this Competition arrow_drop_up 2 more_vert Congratulations! AutoML was a sensible choice in the time constraints. I didn't have time to implement an NN fully, and although I could see stacking was helping with local cv, I only had time to get to 1 level of stacking. An AutoML framework would likely have sped things up. Congratulations again. Kirderf Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks, yes it does help and can give a direction, I tried many frameworks and models. As the dataset was small it gave a good window for testing many different approaches, and I followed the local CV. A good dataset and FE is the also the final key, here I followed and tested the work in the public community as the time was limited. I believe with further hyp.tuning and final ensembles even better score could be done. MichaelP Posted 2 years ago ¬∑ 45th in this Competition arrow_drop_up 0 more_vert Yes I am sure you are right that there would have been more scope for incremental optimisation with more time. But it is also fun to have a short competition where people don't have to make a long time commitment and in the time available some very effective work was done. Eric Vos Posted 2 years ago ¬∑ 30th in this Competition arrow_drop_up 2 more_vert Congratulations, i was not expecting an AML framework at the first place. Kirderf Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Thanks! I tried many frameworks and models, automl and not, but followed the local CV and this was the best here. But a good dataset and FE is the key as well, only automl/model doesn't get the 1st place, both are needed. Jose C√°liz Posted 2 years ago arrow_drop_up 2 more_vert Congratulations on the first place!. Can you share what CV strategy did you use and if you augmented dataset with original California Housing Dataset? Kirderf Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert You have all the info in the links I provided in the post :) E.g. AG use its ParallelLocalFoldFittingStrategy and Repeated k-fold Bagging.",
      "Regression with a Tabular California Housing Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular California Housing Dataset Playground Series - Season 3, Episode 1 Regression with a Tabular California Housing Dataset Overview Data Code Models Discussion Leaderboard Rules jcerpent ¬∑ 2nd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 20 more_vert 2nd place solution - brief summary First of all, thanks to the organizers for a fun, short competition. Also thanks to the many notebook contributors in this competition!!!üéâ My final solution is an ensemble of some public contributions with some of my own personal ideas. -I felt like the boosting path was well-explored by many public notebooks and designed to base my boosting approach on these methods; as mentiond, distance using coordinates plays a big part. -I noticed many people added the external data to their dataset and computed their CV score using this data. Since the competition data is an adaptation of the original dataset, I think this is why some CV scores weren't very well aligned. Instead, I ran a CV split on the original data, and added the external data to the training set afterwards. This forces validation on the supplied dataset, better representing the LB score. Ensembling this with methods that split on the full merged data seemed to diversify a lot and improve LB. -Lastly, I also training a NN in keras using keras_tuner on the standard features + coordinate features. Local CV was only 0.59, but this also added significant diversification in the full blend. Model summary can be seen below: Please sign in to reply to this topic. comment 3 Comments Hotness Maik Pflugradt Posted 2 years ago arrow_drop_up 1 more_vert The source code of the top submissions is not published is it? The Devastator Posted 2 years ago arrow_drop_up 1 more_vert Really cool solution, I like how simple it is! The Devastator. wickedx Posted 2 years ago arrow_drop_up 0 more_vert why coolÔºåhhha",
      "Regression with a Tabular California Housing Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular California Housing Dataset Playground Series - Season 3, Episode 1 Regression with a Tabular California Housing Dataset Overview Data Code Models Discussion Leaderboard Rules ryan ¬∑ 24th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 15 more_vert 24th Place Solution üèÖ - My second ever Kaggle competition üî• 24th place! üèÖ I recently decided to get back into Kaggle competitions, and I‚Äôm very happy with my performance on this one. One year ago, I competed in the Sartorius Cell Instance Segmentation competition and it was one of the most rewarding things I‚Äôve done in the past two years. The amount of information I learned in such a short period of time was astonishing. The same goes for this competition. Prior to this competition I hadn‚Äôt competed in any tabular competitions, and I again learned an incredible amount of information. To that end, I want to thank the Kaggle team for putting together these competitions and thank everyone who competed for making the competition enjoyable for noobs like myself. I would like to especially give a shout out to the following four notebooks and discussions that I continuously referenced throughout the competition, I couldn‚Äôt have done it without these (go give them an upvote): @phongnguyen1 - https://www.kaggle.com/code/phongnguyen1/distance-to-key-locations @dmitryuarov - https://www.kaggle.com/code/dmitryuarov/ps-s3e1-coordinates-key-to-victory @thedevastator - https://www.kaggle.com/competitions/playground-series-s3e1/discussion/376210 @tilii7 - https://www.kaggle.com/competitions/playground-series-s3e1/discussion/376709 My Solution: Quite frankly, I didn‚Äôt do anything groundbreaking. My solution consisted of an ensemble of XGBoost, LightGBM, and CatBoost using a 10 KFold split. I did some light hyperparameter optimization using Optuna for the XGBoost model, though not on the LightGBM or CatBoost model parameters. Feature Engineering: Distance to any California city with over 500,000 population. Encoding trick listed here Distance to coastline features as listed in this discussion PCA coordinates Rotated coordinates (15, 30, 45) Polar coordinates. CV: To compute my CV score, I used an 80/20 split of the training data and excluded the ‚Äúoriginal‚Äù dataset to get more accurate scores. Trusting local CV: After playing around a bit with various models and feature engineering ideas, I decided to trust my CV score and determined that the top public leaderboard scores were either doing some crazy feature engineering or were slightly overfit. Trusting my CV was the right choice as I increased my position 24 spots in the private leaderboard :) This competition will likely be the first of many for me this year, and hopefully you all will be seeing a lot more of me. I aim to play these tabular series until I land a top 3 position in one of them (I‚Äôm coming for you Kaggle merch). You can find my solution notebook here: https://www.kaggle.com/code/ryanirl/ps-s03e01-main-notebook-xgb-lgbm-cb 1 Please sign in to reply to this topic. comment 5 Comments 3 appreciation  comments Hotness Benny Posted 2 years ago arrow_drop_up 4 more_vert Phenomenal! Ramandeep Singh Posted 7 months ago arrow_drop_up 0 more_vert This is a very helpful and informative repo, Can you share more on discovering such features? Appreciation (3) inversion Kaggle Staff Posted 2 years ago arrow_drop_up 1 more_vert Thanks so much for sharing! Charlamagne Posted 2 years ago arrow_drop_up 0 more_vert Thanks for sharing Jahid hossan Posted 2 years ago arrow_drop_up 0 more_vert Outstanding! Thanks for shareing‚Ä¶.."
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 1 NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook . The dataset for this competition (both train and test) was generated from a deep learning model trained on the California Housing Dataset . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 6.48 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 6.48 MB sample_submission.csv test.csv train.csv 3 files 21 columns ",
    "data_description": "Regression with a Tabular California Housing Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular California Housing Dataset Playground Series - Season 3, Episode 1 Regression with a Tabular California Housing Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Jan 3, 2023 Close Jan 10, 2023 Description link keyboard_arrow_up NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook . Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same‚Äîto give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! To start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = ‚àö 1 N N ‚àë i = 1 ( y i ‚àí ÀÜ y i ) 2 where ÀÜ y i is the predicted value and y i is the original value for each instance i . Submission File For each id in the test set, you must predict the value for the target MedHouseVal . The file should contain a header and have the following format: id ,MedHouseVal 37137 , 2 . 01 37138 , 0 . 92 37139 , 1 . 11 etc . content_copy Timeline link keyboard_arrow_up Start Date - January 3, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  January 9, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Regression with a Tabular California Housing Dataset. https://kaggle.com/competitions/playground-series-s3e1, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 1,618 Entrants 702 Participants 689 Teams 5,163 Submissions Tags Regression Tabular Housing Beginner Root Mean Squared Error Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e2",
    "discussion_links": [
      "/competitions/playground-series-s3e2/discussion/378780",
      "/competitions/playground-series-s3e2/discussion/378866",
      "/competitions/playground-series-s3e2/discussion/381377"
    ],
    "discussion_texts": [
      "Binary Classification with a Tabular Stroke Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Stroke Prediction Dataset Playground Series - Season 3, Episode 2 Binary Classification with a Tabular Stroke Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules Jose C√°liz ¬∑ 5th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 46 more_vert 5th Place Solution Hi all, It seems there was quite a good shake up given that dataset was highly imbalanced and AUC can vary a lot depending the number of samples. I realized there was a good difference between by OOF AUC and the leaderboard so I decided to trust only my CV (10 StratifiedKfold). Tricks that worked Fill unknown category form smoking status as never smoked . The ituition was given on my EDA where you can see that unknown class has the lowest probability of stroke. Fill other class from gender as male . I spotted a boost on CV when filling that record in synthetic dataset. I didn't probe the leaderboard to validate this on test. Ensemble using gradient descent and ranking the predictions. Concat original stroke dataset and use StratifiedKfold where validation only has synthetic data. Feature selection using RecursiveFeatureElimanation. Additional features I tried: def generate_features ( df ):\n    df[ 'age/bmi' ] = df.age / df.bmi\n    df[ 'age*bmi' ] = df.age * df.bmi\n    df[ 'bmi/prime' ] = df.bmi / 25 df[ 'obesity' ] = df.avg_glucose_level * df.bmi / 1000 df[ 'blood_heart' ]= df.hypertension*df.heart_disease return df content_copy Things that didn't work Use forward selection taken from this notebook . This was my second submission and scored 0.89941 on private leaderboard. I think It didn't worked because the final ensemble was only composed of XGBoost models while my best submission has a wide variety of models. MeanEncoder, WoEEncoder and CountFrequency encoder. Neither of those provided better solutions that OneHotEncoder. Final Ensemble: My final ensemble is composed of several models: LogisticRegression with RFE, l2, and liblinear solver. LogisticRegression with RFE, no regularization, lbfgs solver. LightGBM no RFE, no Feature Engineering. Another LightGBM with early stopping and monitoring logloss (yes, logloss no AUC). A Catboost model inspired in this notebook by @dmitryuarov . I made some modifications to make sure the OOF AUC was similar to the mean AUC by fold. A tuned XGBoost with feature engineering. (best single model) See the code and results replica Here And that's all. Many congratulations to the winners, looking forward to the next playground competitions. Please sign in to reply to this topic. comment 22 Comments 1 appreciation  comment Hotness Tilii Posted 2 years ago ¬∑ 254th in this Competition arrow_drop_up 2 more_vert Great job! That was some intuition in deciding to rename the unknown labels. Jose C√°liz Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Thanks @tilii7 , you also gave me great ideas during the discussions. Eishkaran Singh Posted 2 years ago ¬∑ 51st in this Competition arrow_drop_up -1 more_vert Nice its informative but instead of XGBoost try and use Catboost which will increase the accuracy according to me @jcaliz Jose C√°liz Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Hi @eishkaran , XGBoost was crowned as the best single model on my iterations but I also used a Catboost. The latter gave me some troubles because OOF AUC was way lower than mean val AUC so the tuning process took longer. Fatih Emir Guler Posted 2 years ago arrow_drop_up 0 more_vert Thanks a lot! I used your recommendations on feature engineering and it helped a lot for my term project! GGopinathan Posted 2 years ago ¬∑ 287th in this Competition arrow_drop_up 0 more_vert Congrats and thanks for the write-up. I like your ideas for #1 and #2. Can you tell us a bit more about #3? For feature selection, I also tried Gender * Hypertension * Heart Disease but it did not help that much but there were other bmi & age combinations that seemed to help. Jose C√°liz Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Hi @ggopinathan , you can find the weights of your ensembles using any gradient descent method with scipy.minimize . here is an implementation so you can take a look. Just a small caveat is that AUC is not a convex function so any method that involves the Hessian may converge in few iterations. I used Nelder-mead in this competition. The Devastator Posted 2 years ago arrow_drop_up 0 more_vert Your tricks and strategies that you decided to implement were very interesting! The Devastator. Vaidehi Savaliya Posted 2 years ago arrow_drop_up 0 more_vert good job‚Ä¶ moth Posted 2 years ago arrow_drop_up 0 more_vert Well done @jcaliz ! Good to report also what didnt work, which is often left out Jose C√°liz Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Oh, @alejopaullier such an honor, I love your notebooks. Samuel Cortinhas Posted 2 years ago ¬∑ 309th in this Competition arrow_drop_up 0 more_vert Congrats Jose! Jose C√°liz Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Thank you Sam, I missed you in this competition. Ilya Posted 2 years ago ¬∑ 319th in this Competition arrow_drop_up 0 more_vert Good job! Many thanks for the insights! I liked the provided notebook a lot, especially because you explain the reasoning behind your code. Shiva Kant Mishra Posted 2 years ago arrow_drop_up 0 more_vert @jcaliz , Thanks for sharing your valuable advice as well as sharing your solution, It would be definitely a add in knowledge specially beginners like me.üôÇüôÇ Sumanta Basak Posted 2 years ago ¬∑ 325th in this Competition arrow_drop_up 0 more_vert Great, congratulations. Is it possible to share any 2/3 codes from the above, so that others can learn? Jose C√°liz Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Sure, check the last version of my EDA . I added the code I used to train my best XGBoost model, and the steps carried along for feature engineering. The results are an exact replica :) Vladislav Viryasov Posted 2 years ago ¬∑ 111th in this Competition arrow_drop_up 0 more_vert Congratulations. Good solution! Gaurav Malik Posted 2 years ago ¬∑ 415th in this Competition arrow_drop_up 0 more_vert Congratulations, very informative, btw, How did you decide to assign weights ? Jose C√°liz Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert I did it using scipy and OOF predictions. Take a look at this notebook This comment has been deleted. Appreciation (1) inversion Kaggle Staff Posted 2 years ago arrow_drop_up 0 more_vert Nice writeup, thanks!",
      "Binary Classification with a Tabular Stroke Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Stroke Prediction Dataset Playground Series - Season 3, Episode 2 Binary Classification with a Tabular Stroke Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 6th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 5 more_vert 6th Place Ensemble I hope you find my way of searching weights for blending through Optuna interesting. https://www.kaggle.com/code/viktortaran/ps-feb-2023 Please sign in to reply to this topic. comment 4 Comments Hotness The Devastator Posted 2 years ago arrow_drop_up 1 more_vert I think this is a great approach! In general, I like methods that exploit tools in a different way than the way they are supposed to be used. The Devastator. Viktor Taran Topic Author Posted 2 years ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Yes, it is always interesting to do it. Shibata Posted 2 years ago ¬∑ 584th in this Competition arrow_drop_up 1 more_vert Thank you for sharing. Your notebook is so cool that I can read it easily. Viktor Taran Topic Author Posted 2 years ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thank you very much!",
      "Binary Classification with a Tabular Stroke Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Stroke Prediction Dataset Playground Series - Season 3, Episode 2 Binary Classification with a Tabular Stroke Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules k0takahashi ¬∑ 8th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 5 more_vert 8th Place Solution It was my first competition.üôÇ I'm not sure if this will be helpful or not, but let me share my notebook. Notebook : https://www.kaggle.com/code/k0takahashi/ps-s3e2-2023-stroke-prediction-8th-place Please sign in to reply to this topic. comment 0 Comments Hotness"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 2 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Stroke Prediction Dataset . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 1.88 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 1.88 MB sample_submission.csv test.csv train.csv 3 files 25 columns ",
    "data_description": "Binary Classification with a Tabular Stroke Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Stroke Prediction Dataset Playground Series - Season 3, Episode 2 Binary Classification with a Tabular Stroke Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Jan 10, 2023 Close Jan 17, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same‚Äîto give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! To start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each id in the test set, you must predict the probability for the target variable stroke . The file should contain a header and have the following format: id ,stroke 15304 , 0 . 23 15305 , 0 . 55 15306 , 0 . 98 etc . content_copy Timeline link keyboard_arrow_up Start Date - January 10, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  January 16, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Binary Classification with a Tabular Stroke Prediction Dataset. https://kaggle.com/competitions/playground-series-s3e2, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 1,689 Entrants 785 Participants 770 Teams 6,254 Submissions Tags Binary Classification Tabular Health Conditions Beginner Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e3",
    "discussion_links": [
      "/competitions/playground-series-s3e3/discussion/380920",
      "/competitions/playground-series-s3e3/discussion/381052",
      "/competitions/playground-series-s3e3/discussion/379347",
      "/competitions/playground-series-s3e3/discussion/380757",
      "/competitions/playground-series-s3e3/discussion/380748",
      "/competitions/playground-series-s3e3/discussion/380738"
    ],
    "discussion_texts": [
      "Binary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules Bill Cruise ¬∑ 1st in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 35 more_vert 1st place. That was unexpected... I hadn't even selected any submissions because I wasn't expecting to do well as I watched my public LB score slip. üòÑ Many thanks to Khawaja Abaid , whose notebook Starting Strong - XGBoost + LightGBM + CatBoost was the basis of my own . Please go upvote Khawaja's notebook if you haven't already. My only big change was to add some feature engineering before training the same models. I had discussed it previously in Adding Risk Factors , but here's the final FE code from the winning version: df[ 'MonthlyIncome/Age' ] = df[ 'MonthlyIncome' ] / df[ 'Age' ]\n\ndf[ \"Age_risk\" ] = (df[ \"Age\" ] < 34 ).astype( int )\ndf[ \"HourlyRate_risk\" ] = (df[ \"HourlyRate\" ] < 60 ).astype( int )\ndf[ \"Distance_risk\" ] = (df[ \"DistanceFromHome\" ] >= 20 ).astype( int )\ndf[ \"YearsAtCo_risk\" ] = (df[ \"YearsAtCompany\" ] < 4 ).astype( int )\n\ndf[ 'NumCompaniesWorked' ] = df[ 'NumCompaniesWorked' ].replace( 0 , 1 )\ndf[ 'AverageTenure' ] = df[ \"TotalWorkingYears\" ] / df[ \"NumCompaniesWorked\" ] # df['YearsAboveAvgTenure'] = df['YearsAtCompany'] - df['AverageTenure'] df[ 'JobHopper' ] = ((df[ \"NumCompaniesWorked\" ] > 2 ) & (df[ \"AverageTenure\" ] < 2.0 )).astype( int )\n\ndf[ \"AttritionRisk\" ] = df[ \"Age_risk\" ] + df[ \"HourlyRate_risk\" ] + df[ \"Distance_risk\" ] + df[ \"YearsAtCo_risk\" ] + df[ 'JobHopper' ] content_copy Please sign in to reply to this topic. comment 14 Comments Hotness Khawaja Abaid Posted 2 years ago ¬∑ 50th in this Competition arrow_drop_up 10 more_vert HAHAHAHA OMG NO WAYYYYY!! Many many congratulations, @bcruise , you totally deserve it. ‚ù§Ô∏è And it's all you for coming up with these genius FE ideas, my notebook does nothing but combines three simple models. üòÖ ngl, I can't stop laughing üòÇüòÇüòÇ and i'm feeling so honored to be mentioned in a competition winner's post! Chris Deotte Posted 2 years ago arrow_drop_up 1 more_vert Congratulations Bill. Well done! Pavel Smirnov Posted 2 years ago ¬∑ 394th in this Competition arrow_drop_up 1 more_vert genius in simplicity üëç KirkDCO Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Congratulations @bcruise !!  It seems that simplicity is key in the Playground these days. Bill Cruise Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thank you. It does look like simpler models (and a little bit of luck) give an advantage in this series so far. I think that's a good thing when the competition only lasts a week. Mohit Kumar Posted 2 years ago ¬∑ 215th in this Competition arrow_drop_up 2 more_vert congrats @bcruise . Your methods used are simply great. Well deserved win!üí™ Matt OP Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 2 more_vert Haha, I felt the same way as I watched my public LB score slip. Sometimes it's pretty difficult to ignore the public LB. Congratulations on 1st place @bcruise and nice work on the feature engineering! Jenifar Posted 2 years ago arrow_drop_up 0 more_vert Really, wonderful‚Ä¶. Ilia Gradina Posted 2 years ago ¬∑ 537th in this Competition arrow_drop_up 0 more_vert Congratulations) Aravilli  Atchuta Ram Posted 2 years ago arrow_drop_up 0 more_vert Congratulations Bill. Well done!!! Paul Bailey Posted 2 years ago arrow_drop_up 0 more_vert Very well done! Priyanka Palshetkar Posted 2 years ago arrow_drop_up 0 more_vert Wow! Many congratulations Bill :D saeusa Posted 2 years ago ¬∑ 53rd in this Competition arrow_drop_up 0 more_vert Great feature engineering! This comment has been deleted.",
      "Binary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules KirkDCO ¬∑ 8th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 12 more_vert Playground S03E03 8th Place Solution Thanks to all the competitors for another week of informative discussions!  Thanks also to the Kaggle organizers for another interesting episode of the Playground Series.  I was able to garner an 8th place position using my XGBoost + focused CV approach from Episode 2 ( notebook and discussion ) The notebook I used for this competition can be found here Influences The main influence on this week's competition was the fantastic EDA notebook by @craigmthomas .  I studied this one very closely and made a lot of decisions from this EDA work. Give that notebook an upvote! Modeling As I did in Episode 2, used tried a variety of techniques (XGBoost, CatBoost, Neural Net, Logistic Regression), and XGBoost consistently gave me the best CVs.  Others came close, but never quite surpassed XGBoost. I used the same CV strategy as before in that I did 10-fold cross-validation and only used the synthetic dataset for calculation of AUC in each fold.   I added the original dataset (using the whole dataset this time around) to the training set within each fold, but did not measure performance on the original set in any way.  I believe this was a huge factor in this competition particularly given the very unbalanced nature of the target class and the small dataset size.  There were too many opportunities for overtraining to occur. Feature Engineering A big difference for this competition was the way I approached feature engineering.  The data consisted of a number of different types of data, and I approached each type independently. Winsorization Others found that there were some data points with overly extreme values.  I chose to reset those extreme values to the maximum value for that particular feature.  Nothing unique here - there are lots of notebooks where others did the same.  There were only 2 Winsorizations, but it did take away their outlier status. Label Encoding One variable, BusinessTravel was an ordinal variable with text categories, Non-Travel , Travel_Rarely , Travel_Frequently .  These 3 categories have a logical order that can be converted to numerical values [0, 1, 2].  I anticipated high levels of BusinessTravel would lead to Attrition, and wanted to maintain the ordinal nature of the variable, rather than converting it to multiple one-hot-encoded files. Ordinal Variables There were a number of ordinal variables that were coded numerically.  I left these in their original form as I thought the relative levels could be important, and one-hot encoding would have increased the total number of features substantially. One-hot Encoding There were 6 categorical features for which I used one-hot encoding.  I was concerned about the number of additional columns this would create, even when using sparse one-hot encoding, and looked at the various features (in a quick and dirty notebook ).  For this figure, I took all the models for which an average CV of 0.8 or greater was found and calculated variable importances - 100s of models.  The boxplots show those importances (y-axis) by each variable. JobRole in particular looks like there is high attrition in sales-related roles and lower attrition in other roles - see the far right of the boxplot.  I tried changing this to Sales and Non-sales categories, but it didn't seem to help.  For min 8th place submission, I left all the one-hot encoded columns in place.  I also used this to drop a few columns, but my results didn't improve. Center and Scale For the continuous variables, I also centered and scaled them, probably more out of habit.  I'm sure this doesn't matter for XGBoost. TPOT One very interesting result came from using TPOT , an autoML tool.  I ran TPOT multiple times with small, short simulations just to get an idea of what kinds of models it found.  About 90% of the time XGBoost or a GradientBoostingClassifer was the top classifier. Please sign in to reply to this topic. comment 6 Comments Hotness Satoshi_S Posted 2 years ago ¬∑ 38th in this Competition arrow_drop_up 1 more_vert A straightforward explanation for how you challenged this competition. Thanks for taking the time to write this post. Ravi Ramakrishnan Posted 2 years ago ¬∑ 146th in this Competition arrow_drop_up 1 more_vert I appreciate your consistency in getting great ranks in the current season @kirkdco , keep up the great work and good luck with episode 4! Viktor Taran Posted 2 years ago ¬∑ 39th in this Competition arrow_drop_up 1 more_vert Congratulations! You are the best! Craig Thomas Posted 2 years ago ¬∑ 54th in this Competition arrow_drop_up 2 more_vert @kirkdco thanks for the shoutout - I'm happy the EDA helped you out! Thanks for sharing your solution. KirkDCO Topic Author Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 2 more_vert Your EDA not only helped out, it was foundational.  It was so thorough, I didn't look at any others.  Impressive work!! Matt OP Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 2 more_vert Very nice approach @kirkdco . Thank you for taking the time to share the details of your solution.",
      "Binary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules @kaggleqrdl ¬∑ 12th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 19 more_vert Using pseudo labels to predict fitness / overfitting I wanted to share a technique I've been working on.  It's something that can be used for this episode, and every episode really, especially if the CV <-> LB score is unreliable Note that 20% leaderboards such as we're getting in the playgrounds can suffer from this. The idea is to use predicted labels to reverse score back onto train as a way to measure the fitness of your model. I tried with novoenzymes here and have encountered some success (warning, long thread) - https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction/discussion/378527 I also tried with the last playground, and found some success.  I got perfect spearman / rank correlation between the scores on train versus the scores on the private LB for a number of subs I pulled from the public notebooks. But critically, when I scored the winning sub I got roc auc of \"0.9023200244502575\" on train, which was more than all the other subs I tried.   As folks may already know, that sub only scored \"0.87945\" on the public LB. Here's the code if you'd like to try it yourself - test = pd.read_csv( \"/kaggle/input/playground-series-s3e2/test.csv\" )\ntrain = pd.read_csv( \"/kaggle/input/playground-series-s3e2/train.csv\" )\nsub =pd.read_csv( \"/kaggle/input/winpg1/submisson_16.csv\" ) #<- Winning sub traind = pd.get_dummies(train)\ntest[ 'stroke' ] = sub[ 'stroke' ] import xgboost\ntestd = pd.get_dummies(test)\nmodel = xgboost.XGBRegressor()\nX = testd.drop([ \"id\" , 'stroke' ], axis= 1 )\nmodel.fit(X, testd[ 'stroke' ]) import sklearn.metrics\nsklearn.metrics.roc_auc_score(traind[ 'stroke' ], model.predict(traind.drop([ \"id\" , \"stroke\" ], axis= 1 ))) 0.9023200244502575 content_copy I've attached a copy of the winning sub to this post. This is something I suspect that could be used broadly across all kaggle contests when selecting final submission candidates, and in fact could be used as a way to measure overfitting / fitness itself in perhaps any situation (well, with some tuning of course :) In code competitions, the idea there might be to use oof labels predicting against in fold labels and cross validating your score that way. As I have time, I'm going to go back over some competitions that suffered shakeup and see if this could have been used.  My guess is that it may be somewhat reliant on relatively good scores in accuracy.  Ie, without accuracy the results might be more noisy and less useful. It'd be interesting to see if something like this could also be used for training, eg selecting epochs / forward feature selection / hill climbing based on not just validation loss but also some kind of semi supervised learning coherence. To be clear, I'm not suggesting here to use pseudo labels for training.  That works for certain things, but also frequently leads to overfitting.  You still need an underlying set of models that are compelling. submisson_16.csv Please sign in to reply to this topic. comment 17 Comments Hotness Maher el Ouahabi Posted 2 years ago ¬∑ 447th in this Competition arrow_drop_up 1 more_vert Wow, it's a brilliant idea @kaggleqrdl . I'm going to try it out and give feedback here paddykb Posted 2 years ago ¬∑ 233rd in this Competition arrow_drop_up 1 more_vert It is a fascinating idea. I'd like to see a simulation study: Bootstrap randomly splitting the training set 50:50 and measure how well this technique recovers the correct AUC  for the held-out 50%. This is not just sour grapes because of the high disagreement with my own CVs :D @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Not at all, bring it on.  I'm sorta curious myself if this will work.   See above for the simulation I'm fairly sure there will be plenty of caveats to consider.  It's very likely not a silver bullet, but perhaps all else being equal, it can be a tie breaker. paddykb Posted 2 years ago ¬∑ 233rd in this Competition arrow_drop_up 1 more_vert One small gotcha. When converting predictions to ranks, this technique outputs a poorer \"fitness\". @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 0 more_vert Yeah, that would definitely be the case.  You'd lose quite a bit of information when you ranked the data for the backwards regression purposes. In the novoenzymes competition, we used ranking in order to normalize between different models which had been optimized to predict relative thermostability (ie, spearman correlation). I'm guessing you're using rank in the same way?  To blend models which aren't outputing the same range probability but yet are still relatively correct? It's a great observation, and I think helps explain why it might work and when it wouldn't. If I had to guess, a big reason it works here (if it does!) is the high accuracy of the models and roc auc metric. Chris Deotte Posted 2 years ago arrow_drop_up 0 more_vert Congratulations @kaggleqrdl achieving 12th place! @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert I hope this isn't your way of avoiding my tag in godaddy :p  It'd be great to get a real competition going, even if it's a conservative one. @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 0 more_vert One thing I will say about this technique if you go to a coarse enough level, the correlation (pearson/spearman) is nearly always perfect. Ashvanth s Posted 2 years ago ¬∑ 75th in this Competition arrow_drop_up 0 more_vert I'm trying to understand the idea over here and I'm quite confused , can you elaborate a bit more or probably explain in simple terms as to what is the intuition behind this ? Thanks! @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 0 more_vert That's a bit vague I'm afraid..  what are you confused about exactly? Ashvanth s Posted 2 years ago ¬∑ 75th in this Competition arrow_drop_up 0 more_vert Using pseudo labels to predict the fitness , the core idea of the discussion post @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 0 more_vert Perhaps start with trying some of the code examples and notebooks linked to.  Let me know if you have any specific questions. @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 0 more_vert Well, I hesitate to rank the public notebooks as they're all so close to each other using the PL metric. I can say it'll probably be around 0.85-0.88 will be the highest notebooks I suspect. @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Very belated, I know =), but still interesting.  Mostly my reluctance in posting pre expiry was the fact that the signal was conflicting so much with the public LB, which made me somewhat wary.  Will be interesting trying this out in the comps ahead. Notebook Private Score Public Score Pseudo Label Catboost _94646csv 0.89876 0.94646 0.877278, 0.863261 PS S03E03 EDA 16 models test 0.94_94849.csv 0.89458 0.94849 0.868473, 0.863069 PS S3 E3 Ensemble model WOE encoding optuna_95098.csv 0.88948 0.95098 0.852109, 0.855588 PS S3 E3 LightAutoML WOE encoding_9518.csv 0.88807 0.951750. 0.857335, 0.851201 PS-3-3 LogisticRegression Tuning_95020.csv 0.88723 0.9502 0.847268, 0.854264, PS_S3E3 WoE CatBoost_94740.csv 0.8887 0.9474 0.869272, 0.863254, PS S3 E3 Keras NN_94942.csv 0.88161 0.94942 0.86133, 0.857932 Employee Attrition Prediction with DNN_94786.csv 0.86057 0.94786 0.845751, 0.859914 There's a near 0 correlation between the public and private scores here, but when you mingle the PL scores it goes up as far as 0.83.  Depends on how much you weight the public scores.  Note this is the same as the previous comp. fwiw, the approach I used was using the techniques described in this thread.  Some fine tuning still required though. @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 0 more_vert I was reading a thesis on SSL, and I thought this  paragraph was very compelling Why Semi-Supervised Learning? This thesis focuses on semi-supervised learning, the setting in which we have limited access to labeled training data. Semi-supervised learning is of great interest because it is often easy to collect large amounts of data but difficult or extremely expensive to label this data. Unfortunately, in practice, most people deal with this issue by discarding their unlabeled data and working only with a small labeled subset. The following toy example shows that simply discarding unlabeled data\nis dangerous, because unlabeled data can change our notion of what a\n‚Äúgood‚Äù solution looks like. https://www.math.harvard.edu/media/Melas-Kyriazi-Thesis.pdf @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 0 more_vert I created a notebook showing how this works with the original IBM dataset https://www.kaggle.com/kaggleqrdl/example-using-pl-metric There is a pearson correlation of .96 and .59 spearman between the private leaderboard score and PL metric I believe this gets better as the models get more accurate.    For this simulation I used somewhat deficient models (missing key columns, like the overtime one). Of course, I could be totally wrong.  Will be interesting to see at the end of the week.  :) @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 0 more_vert If you can somehow figure out the indicies for the private leaderboard versus the public leaderboard via probing, you can get an even higher pearson/spearman.   This is what I get when I fit on Xpriv instead of Xrest (which is pub + priv): pearson correlation between priv score and PL metric  ( 0.9796281518498053 , 7.352636470671848e-07 )\nspearman correlation between priv score and PL metric  SpearmanrResult(correlation= 0.7439024390243902 , pvalue= 0.013630256213711524 ) content_copy Even if you can't, this does indicate that the PL metric is providing a relatively valuable signal of sorts.  And I suspect you don't even have to have 100% accuracy on the private indicies, 80% or 90% will likely juice the metric's accuracy a fair amount, though probably better to err on the side of inclusion.  eg:  100% private indicies + 10% public may be better than 90% private. It's also worth noting if you just use the last 5 columns in the simulation, correlation is greatly enhanced This follows from the thesis that model accuracy improves the accuracy of the metric. I suspect you may want to also do some of the feature engineering that folks are doing as well instead of my basic get_dummies @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 0 more_vert I haven't really dug into this comp, but fwiw when I run this on the highest notebook here (0.95098) https://www.kaggle.com/code/alexandershumilin/ps-s3-e3-lightautoml/notebook I get a slightly lower pseudo label score than this note book here, which scores much less on the public LB (0.89) https://www.kaggle.com/code/oscarm524/ps-s3-ep3-eda-modeling To be fair though, I'm just using the same code above I used for the stroke dataset, which may be completely inappropriate.  It was just a blind copy/paste.  Probably to get more accuracy you'd have to use a more sophisticated model. That said .. these models are scoring slightly higher - https://www.kaggle.com/code/alexandershumilin/ps-s3-e3-ensemble-model and https://www.kaggle.com/code/kirillka95/ps-s03e03-eda-16-models-test-0-94 All in about the 0.84-0.86  range. This is the best I found so far https://www.kaggle.com/code/dmitryuarov/ps-s3e3-eda-random-forest-of-gbts/data , but several of them are in the 0.86 range. While the model I'm using isn't very sophisticated, it is regressing on the outputted probabilities, which is potentially more accurate than the forward predictions which are classifying on binary labels, at least from a fit/predict perspective.  If the metric was accuracy, for example, it might not work as well. That's it for now, but at the end of the week I'll score the best notebooks and see where we stand. This comment has been deleted. Javier Carnero Posted 2 years ago ¬∑ 128th in this Competition arrow_drop_up 0 more_vert Hi @kaggleqrdl , very interesting post, thanks for sharing! I am curious how the technique performed on winning notebooks, did you manage to test them? @kaggleqrdl Topic Author Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 0 more_vert Just the above post.  I don't think folks posted their notebooks. This comment has been deleted.",
      "Binary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules Matt OP ¬∑ 14th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 25 more_vert 14th Place Solution Hello all, it was another very fun competition! I have a decent amount to go through in my solution so I'll just get right to it. Link to full notebook: here Encoding We had a decent amount of categorical features so there were many different valid approaches that I observed. Here is what I used: LabelEncoder () on [ \"Gender\" , \"OverTime\" , \"MaritalStatus\" , \"PerformanceRating\" ] OneHotEncoder () on [ \"Department\" , \"BusinessTravel\" ] LeaveOneOutEncoder (sigma = 0.05 ) on [ \"EducationField\" , \"JobRole\" ] content_copy Outliers There were a couple values in the train set that could be potentially disruptive for building models. Here was my strategy for dealing with them: train .at [527, \"Education\" ] = 5 train .at [1535, \"JobLevel\" ] = 5 train .at [1398, \"DailyRate\" ] = train [ \"DailyRate\" ] .median () content_copy Feature Engineering @snnclsr had a great idea in the first episode of season 3 to add a feature to denote if the data is generated or not : train[ \"is_generated\" ] = 1 test[ \"is_generated\" ] = 1 original[ \"is_generated\" ] = 0 content_copy I ended up using this feature because we were once again working with synthetic data and it gave a small boost in CV score. @craigmthomas also had a great idea in the second episode of season 3 to use Number of Risk Factors as a Feature . It took a good amount of time but I went through all of features and looked closely at the ratio of Attrition . I experimented with a bunch of different subsets but this setup ended up improving CV the most: def feature_risk_factors(df):\n    df[ \"risk_factors\" ] = df[[ \"RelationshipSatisfaction\" , \"MonthlyIncome\" , \"BusinessTravel\" , \"Department\" , \"EducationField\" , \"Education\" , \"JobInvolvement\" , \"JobSatisfaction\" , \"RelationshipSatisfaction\" , \"StockOptionLevel\" , \"TrainingTimesLastYear\" , \"WorkLifeBalance\" , \"OverTime\" ]].apply(\n        lambda x: \\ 0 + ( 1 if x.MonthlyIncome < 3000 else 0 ) + \\ ( 1 if x.BusinessTravel == \"Travel_Frequently\" else 0 ) + \\ ( 1 if x.Department == \"Human Resources\" else 0 ) + \\ ( 1 if x.EducationField in [ \"Human Resources\" , \"Marketing\" ] else 0 ) + \\ ( 1 if x.Education == 1 else 0 ) + \\ ( 1 if x.JobInvolvement == 1 else 0 ) + \\ ( 1 if x.JobSatisfaction == 1 else 0 ) + \\ ( 1 if x.StockOptionLevel == 0 else 0 ) + \\ ( 1 if x.TrainingTimesLastYear == 0 else 0 ) + \\ ( 1 if x.WorkLifeBalance == 1 else 0 ) + \\ ( 1 if x.OverTime == 1 else 0 ),\n        axis = 1 ) return df content_copy This feature actually ended up having the most feature importance by far for CatBoost & XGBoost. Strangely, LGBM only had this feature as 13th most important. Alright alright, I didn't just use other people's feature engineering ideas. Here are the features I personally engineered: def feature_engineering(df):\n    df[ \"Dedication\" ] = df[ \"YearsAtCompany\" ] + df[ \"YearsInCurrentRole\" ] + df[ \"TotalWorkingYears\" ]\n    df[ \"JobSkill\" ] = df[ \"JobInvolvement\" ] * df[ \"JobLevel\" ]\n    df[ \"Satisfaction\" ] = df[ \"EnvironmentSatisfaction\" ] * df[ \"RelationshipSatisfaction\" ]\n    df[ \"MonthlyRateIncome\" ] = df[ \"MonthlyIncome\" ] * df[ \"MonthlyRate\" ]\n    df[ \"HourlyDailyRate\" ] = df[ \"HourlyRate\" ] * df[ \"DailyRate\" ]\n    return df content_copy Pretty basic interaction features, not much to comment on here other than using intuition and trial & error. Models & Validation @kirkdco (who placed 1st in episode 2) had an excellent idea to not include the original data that the competition data was generated from in cross validation splits. I used this technique with 10 fold StratifiedKFold. I also used a basic GridSearch to find optimal hyper parameters for all 3 models. Random note: max_depth = 1 for CatBoost surprisingly worked the best. final blended submission was: cat_preds * 0.55 + xgb_preds * 0.25 + lgbm_preds * 0.2 However cat_preds * 0.34 + xgb_preds * 0.33 + lgbm_preds * 0.33 ended up being just slightly better (0.00004+). Bonus @tilii7 posted this topic in episode 1: A colorful reminder to always ensemble your predictions He explained that in some cases it helps to plot a cumulative distribution function of multiple models and visualize the differences directly. So that's exactly what I did! Thanks again to everyone who participated and shared their ideas! See you in episode 4! Beginner Intermediate Advanced Classification Binary Classification Please sign in to reply to this topic. comment 6 Comments Hotness aci.patlican Posted 2 years ago ¬∑ 490th in this Competition arrow_drop_up 1 more_vert Hi Matt, 2 questions: 1) how did you do feature selection? 2) the weights for the blended model, how did you decide? Thanks Matt OP Topic Author Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 2 more_vert Hi @acipatlican I used feature importance to remove a couple of very unimportant features. Also the interaction features helped to reduce the total amount of features. I plotted a cumulative distribution function of all 3 models (seen above) and checked the Pearson correlation and Kolmogorov-Smirnov statistic between each model. I suggest you take a look at this excellent topic by @tilii7 The Good, the Bad and the Blended aci.patlican Posted 2 years ago ¬∑ 490th in this Competition arrow_drop_up 0 more_vert 1) Ok, about the interaction features. 2 cases: Let'say First.Case: I have two continuous features (col1,col2) that are correlated (spearman) with 0.91 and they are at our top-10 important features according to our base model. Do you create an interaction interaction_col1_col2 and drop two of them? Second.Case: I have two continuous features (col3,col4) that are correlated (spearman) 0.98 and they are at out top[10:20] important features according to our base model. Do you create an interaction interaction_col3_col4 and drop two of them? I would like to learn how to consider such cases correctly. Thanks 2) Thank you for the topic üëç Matt OP Topic Author Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 0 more_vert There is really no definitive answer I can give for both cases. Interaction features are usually derived from possessing domain knowledge of the topic and/or using intuition. For features that are very highly correlated (let's say 0.98 from your second case) I would first try training the model with just col3 and then train the model again with just col4. Compare CV scores, then perhaps try the interaction feature col3-col4 if you feel that it could potentially squeeze out more information that you could feed to the model. KirkDCO Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Well done, @mattop !  Great discussion on feature encoding and engineering. I'm super excited that my CV approach helped.  üëç Matt OP Topic Author Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert Thank you @kirkdco , congratulations on another top 10. That CV approach really seems to do the trick with these synthetic datasets. KirkDCO Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert It does indeed.  I'll write up my approach shortly, but it is essentially the same as the last episode with a small amount of extra feature engineering.  We'll see how it does on truly unbalanced data in episode 4. Matt OP Topic Author Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert It will be exciting to see how it performs in episode 4, looking forward to it! Also, I am interested to see your solution post. Minh T. Nguyen Posted 2 years ago arrow_drop_up 0 more_vert @mattop Nice feature engineering on Dedication, JobSkill, and Satisfaction. Ebc Posted 2 years ago arrow_drop_up 0 more_vert Hi Matt, thanks for sharing. Can you expand a bit on how you decided to encode differently the various categorical variables? (e.g. what criteria did you use? did you compare more strategies? was this an organized/systematic comparison or just trial and error?). Many thanks again!",
      "Binary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules Satoshi_S ¬∑ 38th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert My S3E3  226 - >> 38 This episode frustrated me a lot since I couldn't improve my public leaderboard score from day 2, but it ended up giving me a good result in the private leaderboard. I just cleaned up the data and gave only one new feature \"MonthlyIncome/Age\" as I posted on the discussion . My model was very simple. I used the competition and original data for training. model = CatBoostClassifier(verbose= 0 ,n_estimators= 500 )\npredictions_cat = make_predictions(full, 5 ,model)\ncat =[np.mean(a) for a in zip (*predictions_cat)] content_copy I have a note on how I developed my notebook at the end.  If you are interested, please have a look at my notebook . The best score was from version 8 of this notebook. Hope this helps someone here, and see you soon in the next episode.üëç Please sign in to reply to this topic. comment 4 Comments Hotness Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert Congratulations. See you on Ep. 4. I've already been there on your \"Employee Attrition Prediction\". Satoshi_S Topic Author Posted 2 years ago ¬∑ 38th in this Competition arrow_drop_up 1 more_vert Thanks, Marilia. Your help means a lot. See you in the ep4. Matt OP Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 2 more_vert I couldn't improve my public leaderboard score from day 2 onwards either. I have found it really beneficial to try to ignore the public LB and just focus on improving your local CV. Even more so since the public LB was calculated with only 20% of the test data. Satoshi_S Topic Author Posted 2 years ago ¬∑ 38th in this Competition arrow_drop_up 1 more_vert Yeah, it looks like a simple model tends to get a higher score in this competition. Does your cv score help you to choose your best submission? My submission with the best cv score wasn't my best. Anyways, congratulations that you ended in 14th place. It's impressive. Matt OP Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert Thank you @satoshiss , I really appreciate it. I really trusted my CV in this comp and yes it did help me choose by best submission. A big part of this was excluding the original data that the competition data was generated from in cross validation splits! Satoshi_S Topic Author Posted 2 years ago ¬∑ 38th in this Competition arrow_drop_up 1 more_vert That is awesome. I will have a look at your cross-validation strategy in detail later. See you in the next episode!!",
      "Binary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 39th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 39th Place Ensemble I 've just applied the code from the previous competition without crucial adjustments and it works greatly here too. Look at my way of using Optuna to find weights. https://www.kaggle.com/code/viktortaran/ps-jan-3-2023 Please sign in to reply to this topic. comment 3 Comments Hotness Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert Congratulations Taran. I had already checked yours optuna  \"PS - Jan-3 2023\" See you on Ep. 4 Matt OP Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert Nice use of Optuna @viktortaran , great result! @kaggleqrdl Posted 2 years ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Great stuff.  I used an ensemble as well of diverse models."
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 3 The dataset for this competition (both train and test) was generated from a deep learning model trained on a Employee Attrition . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 455.63 kB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 455.63 kB sample_submission.csv test.csv train.csv 3 files 71 columns ",
    "data_description": "Binary Classification with a Tabular Employee Attrition Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Employee Attrition Dataset Playground Series - Season 3, Episode 3 Binary Classification with a Tabular Employee Attrition Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Jan 17, 2023 Close Jan 24, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same‚Äîto give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! To start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each EmployeeNumber in the test set, you must predict the probability for the target variable Attrition . The file should contain a header and have the following format: EmployeeNumber ,Attrition 1677 , 0 . 78 1678 , 0 . 34 1679 , 0 . 55 etc . content_copy Timeline link keyboard_arrow_up Start Date - January 17, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  January 23, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Binary Classification with a Tabular Employee Attrition Dataset. https://kaggle.com/competitions/playground-series-s3e3, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 1,577 Entrants 676 Participants 665 Teams 5,397 Submissions Tags Tabular Beginner Binary Classification Business Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e4",
    "discussion_links": [
      "/competitions/playground-series-s3e4/discussion/382539",
      "/competitions/playground-series-s3e4/discussion/382447",
      "/competitions/playground-series-s3e4/discussion/382493"
    ],
    "discussion_texts": [
      "Binary Classification with a Tabular Credit Card Fraud Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Credit Card Fraud Dataset Playground Series - Season 3, Episode 4 Binary Classification with a Tabular Credit Card Fraud Dataset Overview Data Code Models Discussion Leaderboard Rules Melkozerov Leonid ¬∑ 10th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 16 more_vert 10th place solution HI folks, a bit surprised to jump up, here is what I did: Featrues: Used both competition and original datasets Transformed Time column to Hour and Day Taken division features from this beautifull notebook. Dropped Id and Time . Model Catboost with custom Focal loss (link: https://github.com/rahowa/catboost_focal_loss ) Cross validation Based on this well-written notebook : 10-fold StratifiedKFold Predictions calculated by the model trained on each split and then avereged. Optimisation Optuna with TPESampler, it worked painfully slow, so only about 50 iterations. Hyperparameters optimised: depth , learning_rate , l2_leaf_reg , subsample , min_data_in_leaf and also  gamma parameter for focal loss. Please upvote people whos work was used! This result might be pure luck, or these series inherently does not require overcomplication. Thanks! Please sign in to reply to this topic. comment 5 Comments 1 appreciation  comment Hotness Oliver Hennh√∂fer Posted 2 years ago ¬∑ 105th in this Competition arrow_drop_up 1 more_vert Thanks for sharing (especially the repo) and congratulations! Want to add this resource for Focal Loss with LightGBM. Might help when the dataset gets larger, although CatBoost/XGBoost seemed to work slightly better for the last playground competitions. The Devastator Posted 2 years ago arrow_drop_up 2 more_vert Wow! Thanks for the catboost with focal loss! And congrats! The Devastator. Diego Garrocho Posted 2 years ago arrow_drop_up 0 more_vert This explanation is excellent! It helped me understand different was to refine my results with things i had not considered. Thanks for sharing. Prathmesh Desai Posted 2 years ago arrow_drop_up 0 more_vert This is some great work. Thanks for sharing üòâ Congrats Appreciation (1) Muhammad Risqi Firdaus Posted 2 years ago arrow_drop_up 0 more_vert Wow thanks for sharing",
      "Binary Classification with a Tabular Credit Card Fraud Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Credit Card Fraud Dataset Playground Series - Season 3, Episode 4 Binary Classification with a Tabular Credit Card Fraud Dataset Overview Data Code Models Discussion Leaderboard Rules Hardy Xu ¬∑ 30th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 28 more_vert 30th Place Solution This comp was an interesting one, and for me the hardest of the Playground Series this month. The addition of the time component to the data, combined with how the train and test sets were split by time, made me more unsure of how to best build a local CV schema and unsure of its reliability. In the end, I used sklearn's TimeSeriesSplit for CV, and at the very least it seemed fairly reliable for optimizing hyperparameters. My final solution was pretty simple, just an average of 50 XGBoost models with the same hyperparameters, trained using Stratified KFold with 5 folds and 10 repeats. I ended up using the entire train dataset plus the instances of fraud from the original Credit Card Fraud data. There were 2 things that I think helped my score a little. First, I added transformed versions of the V features created by subtracting the daily average for that feature . This was inspired by @paddykb 's comment about how many features behaved differently between day 1 and day 2, and upon further observation, it seemed to that the general up-down trend of the features throughout a day was similar between day 1 and day 2, and the main difference was that this trend was shifted up or down between days. So I figured that subtracting out the daily average might help uncover some more signal. This did not improve my Public LB score, but it gave me a boost of +.0015 on the Private LB score. The second addition was inspired by @siukeitin 's comment about using the V features to identify customers. Out of curiosity, I ran a check to see if grouping the train data by any 2 V features would result in pure groups of all fraud or all non-fraud, and I found that the combination of V14 and V21 did just that. This gave me 466 rows in the test set that had a (V14, V21) that also existed in the train set, and based on the train data, I predicted 0 for those rows. This boosted both my Public and Private LB scores by +.0004 . Sadly, it was my decision to include the instances of fraud from the original data that prevented me from placing higher, and I suspect that anyone who scored .83 or above probably did it without any original data. Had I excluded the original data, I would've scored .8333, enough for 1st place. With all of the well-documented differences between the original and synthetic data this time around, my instinct was telling me to not use the original data, but it was hard to go against what my CV and the Public LB were telling me. A good lesson for the next one. Please sign in to reply to this topic. comment 5 Comments Hotness Matt OP Posted 2 years ago ¬∑ 54th in this Competition arrow_drop_up 1 more_vert I concur that this was the hardest of the Playground Series this month. Very nice work with the feature engineering @hardyxu52 . I was also skeptical about adding the original data to the training data which is why I only used original.query(\"Class == 1 and Time > 120580\") . snats Posted 2 years ago ¬∑ 487th in this Competition arrow_drop_up 0 more_vert Hi! Your solution seems really interesting. Do you have any resources so that we could check the ensemble process? Minh T. Nguyen Posted 2 years ago arrow_drop_up 0 more_vert @hardyxu52 Ohhh XGBoost and Stratified Sampling sound like a good combo. Thanks for the tips! Adeel Zafar Posted 2 years ago ¬∑ 379th in this Competition arrow_drop_up 0 more_vert Great insights. Congrats Oscar Aguilar Posted 2 years ago ¬∑ 76th in this Competition arrow_drop_up 0 more_vert Interesting. Good job on feature engineeringüëç I also tried TimeSeriesSplit for CV but it didn't help in the LB score.",
      "Binary Classification with a Tabular Credit Card Fraud Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Credit Card Fraud Dataset Playground Series - Season 3, Episode 4 Binary Classification with a Tabular Credit Card Fraud Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 36th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 36th place solution I used the same strategy as it was before. 1) Added the original data because ROC AUC between original and test was lower than even between train and test. 2) Applied dropping for each model seperately based on Permutation Importance with corr sense. 3) Dropped the duplicates. 4) Used optuna to find the best proportion of the weights in the ensemble. The work is here https://www.kaggle.com/code/viktortaran/ps-jan-4-2023 A good article about the real competition is here https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/111284 Please sign in to reply to this topic. comment 9 Comments Hotness The Devastator Posted 2 years ago arrow_drop_up 1 more_vert Nice work! Using optuna to find the best weights is a really good approach. The Devastator. Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 0 more_vert I'm glad to hear this from someone who has extensive experience in data science! Matt OP Posted 2 years ago ¬∑ 54th in this Competition arrow_drop_up 1 more_vert Very nice notebook & result. Great work @viktortaran Ram Jas Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Great hugh achievement @viktortaran . Nicely done .Best of luck for upcoming competition. Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 1 more_vert Thank you very much! paddykb Posted 2 years ago ¬∑ 27th in this Competition arrow_drop_up 1 more_vert Well played Viktor. Really neat notebook. You've settled into a very readable style. Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 1 more_vert @paddykb Thank you so much! I remember several times you helped me improve my code through one competition (TPS october 2022 Rocket League)! Without the wonderful Kaggle community, it's impossible to improve! Ravi Ramakrishnan Posted 2 years ago ¬∑ 176th in this Competition arrow_drop_up 1 more_vert Hello @viktortaran , congrats for the position. Can you please confirm if the links work? It did not work for me, hence asking‚Ä¶ Viktor Taran Topic Author Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 0 more_vert @ravi20076 Thank you. I've rewritten links. Check it up!"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 4 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Credit Card Fraud Detection . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note, this base dataset for this competition was much larger than previous Tabular Tuesdays datasets, and thus may contain more artifacts than the last three competitions. 3 files 198.09 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 198.09 MB sample_submission.csv test.csv train.csv 3 files 65 columns ",
    "data_description": "Binary Classification with a Tabular Credit Card Fraud Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Credit Card Fraud Dataset Playground Series - Season 3, Episode 4 Binary Classification with a Tabular Credit Card Fraud Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Jan 24, 2023 Close Jan 31, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same‚Äîto give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! To start the year with some fun, January will be the month of Tabular Tuesday. We're launching four week-long tabular competitions, with each starting Tuesday 00:00 UTC. These will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each id in the test set, you must predict the value for the target Class . The file should contain a header and have the following format: id ,Class 341588 , 0 . 23 341589 , 0 . 92 341590 , 0 . 02 etc . content_copy Timeline link keyboard_arrow_up Start Date - January 24, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  January 30, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Binary Classification with a Tabular Credit Card Fraud Dataset. https://kaggle.com/competitions/playground-series-s3e4, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 1,621 Entrants 667 Participants 641 Teams 4,919 Submissions Tags Tabular Binary Classification Beginner Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e5",
    "discussion_links": [
      "/competitions/playground-series-s3e5/discussion/387882",
      "/competitions/playground-series-s3e5/discussion/388011",
      "/competitions/playground-series-s3e5/discussion/386683",
      "/competitions/playground-series-s3e5/discussion/386645",
      "/competitions/playground-series-s3e5/discussion/386657",
      "/competitions/playground-series-s3e5/discussion/386627",
      "/competitions/playground-series-s3e5/discussion/386745",
      "/competitions/playground-series-s3e5/discussion/386789"
    ],
    "discussion_texts": [
      "Ordinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules Heitor Rapela Medeiros ¬∑ 1st in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 40 more_vert [1st place solution] Single model (RAPIDS XGBoost) Hello everyone! First, I would like to thank the Kaggle team for the competition. I have been learning a lot in these competitions, which is helping me sharpen my skills! Unlike other solutions I saw in the Discussion for this competition, I did not use an ensemble. My solution was quite simple but effective (single xgboost model). I worked hard on the first two days of the competition to build a good baseline with a good CV. Link to the notebook üî•üî• Enjoy  :) Additional Data ‚ùå Original Whine or just the competition dataset? For me, this point was hard to decide. During the competition, I saw myself going down on the public leaderboard, but in the end, I decided not to use the additional dataset because it was overfitting a lot on my local CV. Training and Validation (CV) ‚úîÔ∏è For the CV, I used StratifiedKFold due to the imbalance, and I tuned the K based on some submissions to get the public score and check with my local CV. I started with K=5, but in the end, I saw that K=10 was more reliable with my experiments, then K=10 was my final hparam. Feature Engineering  (FE) ‚ùå I tried different FE and the ones available in some public notebooks, but my local CV was doing worse, so I removed it. In the end, my final model did not have a special FE. Model (RAPIDS XGBoost - GPU) üî•üî•üî• I always start with some standard models, like lgbm, xgboost, or catboost. For this competition, I wanted to have the best model as soon as possible because I wanted to iterate fast, so I started with the ** RAPIDS XGBoost**. For training, I used Kaggle GPUs (Thanks!). (xgb objective 'objective': 'reg:squarederror', 'tree_method': 'gpu_hist', early_stopping_rounds=50, 'num_boost_round': 1000). For the test set, I used the ntree_limit=model.best_iteration. The others hparams I will provide the others when I have time to clean the code and release the notebook! :) Regression Optimise Class Cutoff üíØ I would like to thank the discussions and public code available that I used to build my solution. One that I remember was the Regression_OptimiseClassCutoff from @paddykb and https://www.kaggle.com/competitions/playground-series-s3e5/discussion/382525 from @abhishek (discussion: @carlmcbrideellis ) (I used the class OptimizedRounder doing my hyperparameter tuning). As the data distribution shift on my studies was not that high between train/test, I fit for every fold the optimizerRound on the validation predictions to get the cutoff for the test set. After the division by the number of folds, the predictions were float, so I used .round().astype(int) before submitting. Hyperparameters Tuning (Optuna) ‚úîÔ∏è For tuning the model, I used Optuna (where I played a lot with the ranges of the hparams and the number of trials). The metric that I was optimizing was cohen_kappa_score(weights='quadratic') on my train oof after the cutoff. Credits üôèüôå I would like to give credit to other competitors that worked really hard on the competition and the ones that shared a lot of content on the notebooks and discussion. Sorry if I forgot to give credit to someone, but I was not looking anymore at the competition since I was upset about the people using the external data and performing better on the public LB üòÜ Code Release ‚ú® I am planning to release the code when I have some time to clean it and prepare the notebook. That's it right now. TL;DR üí• - RAPIDS:XGBOOST + SKFold10 + Optuna + Regression Class Cutoff (I trusted my local CV to select the two final models). Please sign in to reply to this topic. comment 12 Comments Hotness Girijesh Posted 2 years ago arrow_drop_up 3 more_vert Congratulations @rapela , Great job. Heitor Rapela Medeiros Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks, it's good to see that you liked it, @igirijesh ! :) Andrey Posted 2 years ago arrow_drop_up 1 more_vert As the data distribution shift on my studies was not that high between train/test, I fit for every fold the optimizerRound on the validation predictions to get the cutoff for the test set. How did you estimate the data shift? What would be your course of action if you noticed a significant drift? (I used the class OptimizedRounder doing my hyperparameter tuning) Do you mean that you did not cross-validate OptimizedRounder / cutoff thresholds, but simply fitted them to entire training set? (First two questions still stand. Thanks!) Heitor Rapela Medeiros Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Hello, thanks for the questions! How did you estimate the data shift? What would be your course of action if you noticed a significant drift? I did not use any metric to calculate the shift between domains, I did it empirically looking on the trending between public leaderboard and my oof. Do you mean that you did not cross-validate OptimizedRounder / cutoff thresholds, but simply fitted them to entire training set? No, the OptimizedRounder was used inside my hyperparameter tuning function, so the optimization used the cutoff thresholds based on the cv`s to get the performance for each trial, e.g. for each fold one cutoff on valid, then calculate the performance on the rounded values. Pardeep Singh Posted 2 years ago ¬∑ 152nd in this Competition arrow_drop_up 1 more_vert Congrats on winning this competition @rapela . Kudos for sharing such a well-structured post üéâ Heitor Rapela Medeiros Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks! üòÑüëç Sahil Tayade Posted 2 years ago arrow_drop_up 1 more_vert This Post was super organized, I was able to learn a lot from it. Thanks! Heitor Rapela Medeiros Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks for your comment! It is really important for me to see that you liked the organization. :) Minh T. Nguyen Posted 2 years ago arrow_drop_up 1 more_vert @rapela Looking forward to see your notebook! Great analysis Heitor Rapela Medeiros Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Hello, thanks for your message! :) I updated the post with the link to the notebook Shiva Kant Mishra Posted 2 years ago ¬∑ 692nd in this Competition arrow_drop_up 1 more_vert Thanks for sharing the solution @rapela . Heitor Rapela Medeiros Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks @shiv7080",
      "Ordinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules NHopeT ¬∑ 2nd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 18 more_vert 2nd Place Solution Hi Everyone!  üòé First, I appriciate the Kaggle team for the competition. I was suprized that I got 2nd plaice in private score board only a month and a half after I started participating in the competition. With work and raising a family, I don't have much time.The two weeks available for this competition allowed me to think carefully and create a model. I learned a lot through this competition.„ÄÄArigatogozaimasu. ÊÑüË¨ùüôè First things first, I learned about Quadratic Kappa Metrix from this note book . Thanks a lot. 1.EDA (Data Visualization) Here is the note book . I tried to visualize the data first in order to examine the data distribution. For me, it also serves as an exercise in drawing graphs. 1)Heat Map I draw heat maps of train and test data in order to check feature correlations and  to compare  similarity in training and test data. I am not a scientist so I did not know what the correlation between the data meant. Although I found that the heatmaps of the train data and the test data show the same trend. This confirms that this data is valid to be treated for machine learning. 2)Box Plot Boxplots were used to visualize the distribution of the data. Outliers After plotting, we noticed a few outliers. We thought it would be better to exclude outliers in order to create a model with better accuracy.  I have posted a discussion about handling way of outliers. I tried filtering outliers using the Isolation Fores based on @Carl McBride Ellis ' comment. I tried training on data with outliers removed. I spent a lot of time on it, but the scores were worse, so finally, I decided not to remove the outliers. üò≠ Quality Value Frequency From observation of the data, the data distribution of the train data and the test data seemed to be almost identical. This led me to believe that the score would be better if the frequency of quality values for the test data were predicted to match the frequency of quality values for the train data. The calculated code is this. 2.Model I have only tried xgboost. In previous playground series, I have tried various models. But at this time, instead of trying different models, I focused on finding a threshold that assigns the model's expected probability value to the quality value, which makes the quadratic weigthed kappa score optimal. 1)Simple xgboost Here is the notbook . I first tried to simply choose the quality value with the largest predictive probability. The result of this model, public score was 0.49599. As expected, this was not so good. I also tested the train data with outliers removed and found a score of 0.44529. From this result, I determined that the removal of outliers did not have a positive impact on the model. 2)xgboost (weighting the model's predicted results) Here is the notebook . Next, I tried to weight the model's predicted results so that the distribution of predicted quality values for the test data would be similar to the distribution of quality values for the train data. To optimize the weighting, we used the minmize function of scipy optimize. Here is the code. As it turned out, this was a bad outcome. Forcing the predicted results to be assigned to 3, 4, and 8 made the rating values even worse. The score dropped to 0.39527.  This made me realize that I could not win this competition by forcing myself to predict good wines. 3)xgboost(optimize threshold) Here is the notebook . The next step I tried was to convert the predicted value into an expected quality value, and then determine a certain threshold value for this expected value to be the quality value. The threshold for optimizing qudrautic weighted kappa was looked for using the minimize function in Spicy optimize. The code is this. This worked amazingly well.üëå The score went up to 0.544. The results of this projection did not include 3,4,8. I had my doubts. I am not able to predict good wine. So I posted a discussion . 3.Hyperparameter Tuning I did it manually. I decided by moving some parameters manually.„ÄÄ The score went up a bit to 0.55713. 4.Cross-Validation For cross-validation, I tried n_splits= 12, 5, 2 with StratifiedKFold. Hyperparameters were not tuned to the number of divisions. I did not do this because it was time consuming. Socres were n=12: 0.53736  n=5: 0.54088 n=2:0.54472 . Reducing n_splits seemed to increase the score. So I trained the model with all train data without CV. But this was scored down to 0.4819. As these result, the model trained by n_splits= 2 got 2nd place !! üéâ Here is the 2nd place solution code . 5.Summary I think the winning factor was to find the threshold that optimizes the qudrautic weighted kappa. I came in second in the competition, but I was concerned about my inability to predict good taste wines.üôÑ Thanks again to all the competitors and to the organizers at Kaggle for another fun round in the Playground Series. üòÑ Tabular Beginner XGBoost Please sign in to reply to this topic. comment 1 Comment Hotness Pardeep Singh Posted 2 years ago ¬∑ 152nd in this Competition arrow_drop_up 1 more_vert Congrats @nhopet on getting 2nd position in this competition. Great work summarizing your approach in this post üí•",
      "Ordinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules Gilles Vandewiele ¬∑ 3rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 27 more_vert Third place solution: mode ensemble of 6 public notebooks. Closing time! Very surprised to have survived the shake-up. As I hinted in a previous thread. . A simple ensembling strategy takes a high public LB spot, and now it appears this strategy also takes a high private LB spot ;). I used the following notebooks: 1) PS-3-5 Keras NN Model by @martynovandrey 2) PS-S3E5 using Polars by @kotrying 3) PS s3e5 - NN & LightAutoML & Thresholding, Oh my! by @paddykb 4) SR with inequalities -> simple model by @jano123 5) S3E5 - 0.624 XGB/LGB + Kmeans & PCA by @usedpython 6) PS s3e5 - Repeated SR with inequalities by @paddykb I then used a simple weighted mode on these submissions, in which I added 6) two times to the list (so double weight). The code is below and I published a notebook with this code (although LB scores are somewhat different due to version mismatches) import pandas as pd import numpy as np import glob from sklearn.utils.extmath import weighted_mode\n\nsubs = [] for i, file in enumerate ( sorted (glob.glob( 'subs/*' ))):\n    sub_df = pd.read_csv(file) print (sub_df.head( 5 ))\n    sub_df = sub_df.rename(columns={ 'quality' : f'quality_ {i} ' , 'id' : 'Id' })\n    sub_df = sub_df.sort_values( 'Id' )\n    sub_df = sub_df.set_index( 'Id' , drop= True )\n    subs.append(sub_df) # Add sub6.csv twice to the list for double weight dup_sub = subs[ 5 ]\ndup_sub = dup_sub.rename(columns={ 'quality_5' : 'quality_6' })\nsubs.append(dup_sub)\n\nall_sub_df = pd.concat(subs, axis= 1 ) # Manually make these class weight with small epsilon multiplied by index in counts class_weights = { 3 : 1.01 , 4 : 1.03 , 5 : 1.06 , 6 : 1.05 , 7 : 1.04 , 8 : 1.02 } def my_weighted_mode ( x ):\n    values = x.values\n    weights = [class_weights[i] for i in values] return weighted_mode(values, weights)[ 0 ][ 0 ]\n\nsub = subs[ 0 ].copy()\nsub = sub.drop(columns=[ 'quality_0' ])\nsub[ 'quality' ] = all_sub_df[[ f'quality_ {i} ' for i in range ( len (subs))]].apply(my_weighted_mode, axis= 1 ).astype( int )\nsub content_copy It turns out that the weighted mode might be a bit of an overkill here and scipy.stats.mode has the same results. I selected the second-best scoring sub on the private LB as one of my final subs. One of my subs could have been #1, but that is always easy to say in hindsight! I am happy to make the top-3 and get a piece of exclusive Kaggle swag :). Please sign in to reply to this topic. comment 9 Comments 1 appreciation  comment Hotness Pardeep Singh Posted 2 years ago ¬∑ 152nd in this Competition arrow_drop_up 1 more_vert Congrats on getting to 3rd place in this competition and GM level @group16 üôåüî• Fahad dalwai Posted 2 years ago arrow_drop_up 1 more_vert Very efficient and easy technique. Grouping tied class values into the one that occurs the most is pretty smart if you ask me! Congrats on the GM title! Gilles Vandewiele Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thank you Fahad! PengFei Chen Posted 2 years ago ¬∑ 103rd in this Competition arrow_drop_up 1 more_vert Very simple but efficient work. I also used the same thought. but i just used 3 model to ensemble, so the result was not good. I would like to ask how the value of class_weights is determined. thanks Gilles Vandewiele Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert I based the class_weights on the value counts of the training set (assuming distributions would be similar in train and test). In the case of a tie, the more common class will be picked. PengFei Chen Posted 2 years ago ¬∑ 103rd in this Competition arrow_drop_up 1 more_vert thanks for your answer. i learned a lot from your notebook. John Mitchell Posted 2 years ago ¬∑ 16th in this Competition arrow_drop_up 0 more_vert Interesting and essentially simple solution - albeit with some added extras. Are the weights designed to pull the final classifications towards the central categories and reduce the likelihood of large quadratic penalties? Gilles Vandewiele Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Interesting and essentially simple solution - albeit with some added extras. Are the weights designed to pull the final classifications towards the central categories and reduce the likelihood of large quadratic penalties? Thank you John. They are meant to pull predictions, in case of ties, to the majority classes. E.g. if majority vote has tie between class 5 and class 1, but class 5 occurs more in training set, then pick 5 Ravi Ramakrishnan Posted 2 years ago ¬∑ 102nd in this Competition arrow_drop_up 2 more_vert Hearty congratulations for your grandmaster title and the competition too @group16 Gilles Vandewiele Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 3 more_vert Thank you so much @ravi20076 I don't put as much weight on the discussion GM as I do on my comp master (and preferably comp Grand-Master some day), but I am over the moon to have a yellow circle around my name!! This comment has been deleted. Appreciation (1) Girijesh Posted 2 years ago arrow_drop_up 2 more_vert Thanks very precise, thanks @group16",
      "Ordinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules Craig Thomas ¬∑ 4th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 48 more_vert 4th Place Solution Hi Everyone! Thanks again to all the competitors and to the organizers at Kaggle for another fun round in the Playground Series. As we all somewhat expected, there was another big shakeup when the private LB was revealed. This is somewhat unsurprising, given that: 1) There were 3 quality classes that were quite rare in the dataset (I'll refer to them as rare cases below) - 3, 4 and 8; and, 2) We had a metric of quadratic weighted kappa that heavily penalized ratings the more that they drift apart from each other. What these two conditions meant is that small differences in classifier performance could have a large impact on the overall public LB scores. As pointed out by @ambrosm in the discussion thread Is this competition a lottery? moving a single quality rating one step closer to the rating it should be has a huge impact on the quadratic weighted kappa metric. And of course, this means the opposite is true too - moving it one step further away also has a huge negative impact. This means being conservative and staying close to the center of the ratings scale most of the time is better than taking a chance by making more predictions at the extreme ends of the scale, given that most of the data was clustered in the middle of the scale. In other words, while the latter approach may hit the correct rating of 3 or 8 some of the time, the hit that it takes from being in greater disagreement more often for misclassifications of items that were supposed to be near the center simply outweighs the benefit of being right once or twice for the rare cases. More on that in a bit. For this competition I built a total of 1,466 models that included first level models such as CatBoost, XGBoost, LightGBM, various neural network configurations, and second level stacks that used simple ridge regressors and stochastic gradient descent. My EDA discovered that regressor models worked better than classifiers, and that using a simple optimized rounding strategy as initially implemented in the notebook by @paddykb for determining class threshold was paramount. My EDA also uncovered the fact that simple feature engineering based on correlation worked best. Overall, my best model was a ridge regression based on a stack of 25 first level models types such as XGBoost, LightGBM, and CatBoost with differently tuned hyper parameters. Each first level model was built using a mixture of the competition and original data, and used a minimal amount of feature engineering (more on that below). This model had the lowest gap between my own local metrics and the public LB, and both the public LB and my local metrics trended upwards (in other words, there was no evidence that I was overfitting the public LB or local metrics as well). Some of the learning elements that I was interested in for this competition played a key role in my data exploration and final model selection. For this round of the Playground series, I was interested in several different aspects of the dataset that we were using, and concentrated on four problem areas: 1) The validity of mixing original + competition data. 2) The usefulness of engineered features. 3) The utility of stacking models. 4) The impact of rare case misclassifications. Mixing Original + Competition Data For the first item - mixing original + competition data, I used adversarial validation as a rough metric as to whether we should mix them (there were several others who did this as well - @kdmitrie posted a discussion thread very early on). My own adversarial validation exploration showed that the AUC ROC score of a classifier trained to detect differences between the two sets was 0.6321 (with duplicates removed - shoutout to @mattop for the duplicate discovery). This suggested that there were detectable differences between the datasets, but not so great as we saw in previous competitions. As others had demonstrated in their notebooks, the best way to account for the differences is to train using the mix of datasets, but only produce cross-validated metrics that use the competition data. In this case, I found that the mixture of datasets had better local CV metrics than the competition data alone. Engineered Features There were many suggestions on how various features should be combined (a very interesting suggestion came from @phongnguyen1 where they asked ChatGPT to suggest features ). For my approach, I concentrated mainly on engineering that combined features based on Spearman Correlation such that when we quantized the engineered feature, we could essentially \"spread out\" the quality rankings so that at most 2 or 3 of them appear in each discrete bin, rather than seeing 4 or more appear in the same one. A good example of this was with the density feature. When density is quantized, and ratings are counted and placed into each discrete bin, you end up with: If however, you divide density by alcohol - the features share a fairly strong negative correlation - you end up spreading out the quality ratings so that fewer of them occur together in each discrete bin: This spreading out of the quality ratings theoretically gives an edge to gradient boosting tree approaches. Overall, I empirically tested my engineered features in my EDA . The only two sets of engineered features which really provided lift were those that explored variations on acidity and density, as you can see below: Stacking Models The previous episodes in this year's Playground Series were very surprising to me. Model stacks or ensembles appeared to perform much worse than simple, single models. This is surprising in that most other competitions I've taken part in have proven over and over that model stacks are far superior, and are usually the safer bet when it is time for final submissions. Given that we had a new metric to optimize, I decided to revisit stacks. In nearly all instances, my model stacks provided much better performance when compared to the single model - except for some of my CatBoost models. Some of the CatBoost models had local metrics that slightly outperformed the stacks, but once again, the inversion between my local metrics and the public LB had me dismiss those models as being overfit. Ironically, one of my dismissed CatBoost models with the best local metrics landed a private LB score of 0.60201 - a first place standing. Once again however, I conclude that there is no way that I would have chosen that model based on the metrics I observed - I would have needed to gamble on my second submission option, which I had reserved for another experiment regarding rare case misclassifications (see next section below). Rare Case Misclassifications As I mentioned above, rare cases were key to the competition, and to how we were to trust (or not trust) our performance based on public LB scores. Moving a rating one spot higher or lower resulted in massive shifts in the leaderboard. Given that such massive shifts were possible, I stopped placing so much importance on public LB standing as to where I was actually going to stand in the final reveal, and instead used it as a general barometer. In other words, my local metrics had to trend in the same direction as the public LB. As part of my machine learning framework, I made sure to generate a confusion matrix with each model, so I could observe what \"good\" models were doing, and what \"bad\" models were doing. In every case, my best models were conservative - they tended to make predictions that were clustered around ratings 5, 6, and 7. The best models however, showed evidence that while they still stuck predictions for class 3 and 4 on to clases 5 and 6, there were more predictions closer to their repsective class than other models. To see what I'm talking about, you can see the confusion matrix below for my top performing model: As you can see, while it never predicts a single example as being class 4, it puts the bulk of the actual class 4 samples very close to it. The same thing happens with class 8 - while it never actually labels anything with class 8, it puts the bulk of them close to class 8. We can compare that with my second \"gamble\" submission - a neural network that actually makes predictions of class 4 and 8: As you can see, it correctly classifies 5 samples as belonging to class 4, but in doing so, misclassifies 34 examples that are actually class 5. Because the quadratic weighted kappa penalizes so heavily when we have disagreements, we take a sizeable hit in our score. I gambled on this model even though my metrics didn't support it as a contender because it was such a large departure in approach from my other submission (i.e. it was a single model, neural net, made predictions of classes 4 and 8, and had at least somewhat decent performance metrics). Conclusions Once again, another interesting competition that forced us to really examine the metric at play and how different folds of data (public vs private LB) can have an impact on our final standing. The quadratic weighted kappa, combined with rare cases, made for an interesting competition. Please sign in to reply to this topic. comment 11 Comments Hotness Andrey Posted 2 years ago arrow_drop_up 1 more_vert Did you use any tools for tracking experiments? Like MLFlow, WandB or Lotus 1-2-3? I imagine you stored those 1,466 models along with their predictions on fixed splits? Thanks for sharing your thoughts and writing them up so nicely, @craigmthomas ! Craig Thomas Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert @andreyvm good questions. For this competition, I tracked all results using neptune.ai - an online experiment tracking system. It's one I started using a few years ago and is very good at letting me tag, sort, and filter experimental results, although WandB, MLFlow, or ClearML would also be able to track results as well, and can be deployed locally. I built a custom framework that abstracted all the details of model building away. All predictions across all experiments were made on the same set of splits of the data, otherwise the stack would overfit. Girijesh Posted 2 years ago arrow_drop_up 1 more_vert Congrats @craigmthomas Samuel Cortinhas Posted 2 years ago arrow_drop_up 1 more_vert Congratulations @craigmthomas ! Great result. Matt OP Posted 2 years ago ¬∑ 93rd in this Competition arrow_drop_up 1 more_vert Awesome work @craigmthomas , congratulations on your 4th place finish! Tuning hyper parameters in this competition was kinda strange. Untuned models did a better job of predicting the rare cases that you mentioned above. I found that when I tuned my models to optimize my local CV scores all of the predictions for those models would fall in the 5 -7 range. My assumption was that the quality in the test set would follow a similar distribution to the quality in the train set. This led me to using a blend of tuned and untuned models so my predictions would somewhat match the distribution of quality in the train set. Funnily enough, the best submission I could have a picked ended up being a solo LGBM model that I created in the first couple days of the competition with predictions that didn't match the train set distribution and a private score of 0.58648 . Live and learn. icfoer Posted 2 years ago ¬∑ 86th in this Competition arrow_drop_up 1 more_vert Great job. I liked your approach about finding new features. Dinesh Posted 2 years ago ¬∑ 60th in this Competition arrow_drop_up 1 more_vert Great description. Your point is valid on rare cases. I tried to remove only outliers which did not removed all rare cases and so my score was less. Andrew Schleiss Posted 2 years ago ¬∑ 39th in this Competition arrow_drop_up 2 more_vert Fantastic placement and writeup. Thank you @craigmthomas Can you give a bit more around the stacking process. Did you do anything out of the ordinary here? Craig Thomas Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 5 more_vert @slythe certainly. For an actual working model of the stacking process, I'll point to a notebook I wrote several years ago for the March 2021 Tabular Playground Series . The stacking process this time was relatively straightforward again. A stack is an ensemble technique that takes the out-of-fold (OOF) predictions from a large number of models, and uses machine learning on the predictions to make yet another set of (hopefully better) predictions. In this competition, I trained a large number of models using a 10-fold cross validation. Each model used the same 10 folds of data during training and cross-validation. OOF predictions were  made for both the training set and the testing set. The key here is to save the OOF predictions that were made on the training set, as well as those made on the testing set, as you will use the OOF training predictions for the next layer in the stack. Next, I looked at the performance of each of the models. For the second layer in the stack, I chose models based on a threshold quadratic weighted kappa of 0.57 - this meant that to be included in the second layer, the model must have had a minimum cross-validated score of 0.57 or better. This threshold was not arbitrary - I created a large number of first level stacks, moving the threshold value around, looking for the point where the second level performed best. Finally, I took the predictions from each of the models that met or exceeded the threshold score, and ran a ridge regressor on the newly combined training data. In this case, a simple ridge regressor worked well. This is likely due to the fact that there was high correlation between each of the models - ridge regressors work particularly well in those instances. I tried using other regression models such as stochastic gradient descent, LightGBM, CatBoost, etc, but what I usually find is that these methods result in an overfit. This can easily be observed by generating the model, and tracking how the local metric does when compared to the leaderboard - you usually get a big disagreement between the two. I then tuned the ridge regressor, looking for good values of alpha until I found one that had a good local score, and a score that also trended in the same direction on the public LB. Hope this helps! Jose C√°liz Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 1 more_vert Hi @craigmthomas , this is great post but following the question, did you implement the OptimzedRounder in oof stacked or each of the first level models had the rounder? I'm kind of confused in that part. 6 more replies arrow_drop_down Adam Wurdits Posted 2 years ago arrow_drop_up 0 more_vert Congratulations @craigmthomas and thank you for the excellent write-up! Jay Deep Posted 2 years ago arrow_drop_up 0 more_vert Very nice",
      "Ordinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules Anil Ozturk ¬∑ 13th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 15 more_vert 13th Place Solution Hi! Congratulations to everyone on their results, and huge kudos to my teammate @karakasatarik ! It was another playground competition that made us learn a new metric and concept again. I would like to briefly talk about our solution. Approaches: Symbolic Regression (Public Approach) Multi-Class Neural Networks (Modified Version of this Public Approach) XGB/LGB Cherrypick Ensemble (Public Approach) CatBoost Nested CV Blending (Private Approach) Features Used df[ 'mso2' ] = df[ 'free sulfur dioxide' ]/( 1 + 10 **(df[ 'pH' ] -1.81 ))\n\ndf[ 'acidity_ratio' ] = df[ 'fixed acidity' ] / df[ 'volatile acidity' ]\ndf[ 'total_acid' ] = df[ 'fixed acidity' ] + df[ 'volatile acidity' ] + df[ 'citric acid' ]\ndf[ 'mean_acid' ] = df[[ 'fixed acidity' , 'volatile acidity' , 'citric acid' ]].mean(axis= 1 )\ndf[ 'std_acid' ] =  df[[ 'fixed acidity' , 'volatile acidity' , 'citric acid' ]].std(axis= 1 )\n\ndf[ 'free_sulfur/total_sulfur' ] = df[ 'free sulfur dioxide' ] / df[ 'total sulfur dioxide' ]\ndf[ 'sugar/alcohol' ] = df[ 'residual sugar' ] / df[ 'alcohol' ]\n\ndf[ 'sugar/citric' ] = df[ 'residual sugar' ] / df[ 'citric acid' ]\n\ndf[ 'BSO2' ] = df[ 'total sulfur dioxide' ] - df[ 'free sulfur dioxide' ]\ndf[ 'FSO2/alcohol' ] = df[ 'free sulfur dioxide' ] / df[ 'alcohol' ]\ndf[ 'TSO2/alcohol' ] = df[ 'total sulfur dioxide' ] / df[ 'alcohol' ]\ndf[ 'BSO2/alcohol' ] = df[ 'BSO2' ] / df[ 'alcohol' ]\n\ndf[ 'chlorides/TSO2' ] = df[ 'chlorides' ] / df[ 'total sulfur dioxide' ]\ndf[ 'sulphates/pH' ] = df[ 'sulphates' ] / df[ 'pH' ]\n\ndf[ 'alcohol/density' ] = df[ 'alcohol' ] / df[ 'density' ]\ndf[ 'alcohol_density' ] = df[ 'alcohol' ]  * df[ 'density' ]\ndf[ 'sulphates/chlorides' ] = df[ 'sulphates' ] / df[ 'chlorides' ]\ndf[ 'alcohol/pH' ] = df[ 'alcohol' ] / df[ 'pH' ]\ndf[ 'alcohol/acidity' ] = df[ 'alcohol' ] / df[ 'total_acid' ]\ndf[ 'alkalinity' ] = df[ 'pH' ] + df[ 'alcohol' ]\ndf[ 'mineral' ] = df[ 'chlorides' ] + df[ 'sulphates' ] + df[ 'residual sugar' ]\ndf[ 'density/pH' ] = df[ 'density' ] / df[ 'pH' ]\ndf[ 'total_alcohol' ] = df[ 'alcohol' ] + df[ 'residual sugar' ]\n\ndf[ 'acid/density' ] = df[ 'total_acid' ]  / df[ 'density' ]\ndf[ 'sulphate/density' ] = df[ 'sulphates' ]  / df[ 'density' ]\ndf[ 'sulphates/acid' ] = df[ 'sulphates' ] / df[ 'volatile acidity' ]\ndf[ 'sulphates*alcohol' ] = df[ 'sulphates' ] * df[ 'alcohol' ] content_copy Submissions Method Public Score Private Score Selected Symbolic Regression 0.62977 0.57276 Multi-Class Neural Networks (Modified) 0.60113 0.58702 XGB/LGB Cherrypick Ensemble 0.62485 0.58220 CatBoost Nested CV 0.59901 0.57403 8 CatBoost Nested CV + 1 NN 0.59030 0.57623 X SR + Modified NN + XGB/LGB Cherrypick 0.62949 0.59121 X (Best) SR + CatBoost Nested CV + XGB/LGB Cherrypick 0.61127 0.59656 It was a fun competition, it reminded us to trust CV and diversity in blending again! Please sign in to reply to this topic. comment 1 Comment Hotness Nguy·ªÖn Ng·ªçc B√¨nh Posted a year ago arrow_drop_up 0 more_vert hi @nlztrk plz update link XGB/LGB Cherrypick Ensemble thanks.",
      "Ordinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules Sergey Saharovskiy ¬∑ 14th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 20 more_vert 14th place solution NN (surviving the big shakeup) Quick intro: Congratulations to the winners of this competition! it was challenging to maintain the position, especially when the metric would not forgive a bad misclassification. I wish it was a topic of the top-3 solution but finishing in 14th place was no small feat here. This was the one there I survived among the few. CFG: config = { 'model_config' : { 'act_fn' : F.mish, 'learning_rate' : 1e-3 , # 1e-3, 'num_classes' : 6 , 'hidden_sizes' : [ 1024 , 512 , 128 , 256 ], 'drop_out' : .50 , # 0.3, 'norm_last_layer' : True , 'weight_decay' : 1e-5 ,\n            }, 'seed' : s, 'num_folds' : 6 , 'batch_size' : 128 , 'num_epochs' : 13 , 'plateau_factor' : .5 , 'plateau_patience' : 3 , 'exp_num' : 1 , 'combined_data' : True } content_copy Feature Engineering: df[ 'alcohol_density' ] = df[ 'alcohol' ] * df[ 'density' ]\ndf[ 'sulphate/density' ] = df[ 'sulphates' ] / df[ 'density' ]\ndf[ 'sulphate/alcohol' ] = df[ 'sulphates' ] / df[ 'alcohol' ]\ndf[ 'pH_round1' ] = df[ 'pH' ]. round ( 1 )\ndf[ 'log1p_residual_sugar' ] = np.log1p(df[ 'residual_sugar' ])\ndf[ 'citric_acid_per_alcohol' ] = df[ 'citric_acid' ] / df[ 'alcohol' ]\nconditions = (df[ 'citric_acid' ].eq( 0 ), df[ 'citric_acid' ].eq( .49 ))\ndf[ 'alcohol_mean_group_by_pH' ] = df.groupby( 'pH_round1' )[ 'alcohol' ].transform( 'mean' ) content_copy Model: nn.CrossEntropyLoss(weight=torch.tensor([1.10,  1.5,  1.,  1.,  1.5, 1.5]), reduction='sum') ----------------------------------------------------------------\n        Layer ( type )               Output Shape         Param # ================================================================\n            Linear- 1 [- 1 , 512 ] 8 , 192 BatchNorm1d- 2 [- 1 , 512 ] 1 ,024\n           Dropout- 3 [- 1 , 512 ] 0 Linear- 4 [- 1 , 256 ] 131 ,072\n       BatchNorm1d- 5 [- 1 , 256 ] 512 Dropout- 6 [- 1 , 256 ] 0 Linear- 7 [- 1 , 128 ] 32 , 768 BatchNorm1d- 8 [- 1 , 128 ] 256 Dropout- 9 [- 1 , 128 ] 0 Linear- 10 [- 1 , 5 ] 645 ================================================================ content_copy Please sign in to reply to this topic. comment 8 Comments Hotness Minh T. Nguyen Posted 2 years ago arrow_drop_up 1 more_vert @sergiosaharovskiy Thanks for sharing the architecture! kmkmks Posted 2 years ago ¬∑ 738th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for sharing! Did you use Mish instead of Relu for the activation function? Is there any reason for that? Sergey Saharovskiy Topic Author Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert @kmkmks Yes, It works well with big hidden sizes and small number of epochs. Overall, Mish was simply giving a better performance during the runs. Shiva Kant Mishra Posted 2 years ago ¬∑ 692nd in this Competition arrow_drop_up 1 more_vert congratulations @sergiosaharovskiy ,and thankyou for sharing the solution. Jose C√°liz Posted 2 years ago ¬∑ 36th in this Competition arrow_drop_up 1 more_vert Congrats on surviving @sergiosaharovskiy , and thanks for sharing you HP. Matt OP Posted 2 years ago ¬∑ 93rd in this Competition arrow_drop_up 2 more_vert Very nice work @sergiosaharovskiy , I appreciate you sharing your approach. Kosh Posted 2 years ago ¬∑ 452nd in this Competition arrow_drop_up 2 more_vert Could you share some of your insights with the feature engineering? What tipped you off to combine some of the attributes the way you did? Sergey Saharovskiy Topic Author Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 2 more_vert @koshrai nothing too fancy this time. I used this discussion opened by @jcaliz as a barebone for the feature elimination. Then, I manually tried features one by one, by pairs and etc. In the end I ended up with what you see above. Kosh Posted 2 years ago ¬∑ 452nd in this Competition arrow_drop_up 1 more_vert Ah, I see. Thanks for the reply, and congratulations on P14!",
      "Ordinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules John Mitchell ¬∑ 16th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert 16th place Solution Thanks to the organisers for another challenging and interesting competition. I tried a  number of different things, of which probably the most interesting was to rank the test instances from highest to lowest predicted class, using real numbers rather than simply integers, and then to fit this to the expected distribution based on the {training plus original} sets. In the {train plus original} data, once duplicates are removed, we have quality values as follows: 8: 1.8% 7: 14.8% 6: 38.6% 5: 41.4% 4: 2.9% 3: 0.6% We import a file containing integers in these proportions, the same number of integers as there are in the test set. Then we match the top 1.8% of our wines to category 8, the next 14.8% to category 7, and so forth. I implemented this here . Based on the success other people were having with simple ensembles, I also tried using a simple median of top public solutions here . Ultimately, the generally poorer public LB scores from the idealised matching persuaded me to select two of the median-based submissions, which turned out not to be optimal. In fact, I had even a simple ensemble of two models that would have placed sixth had I known to select it. Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Ordinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules DavidHGuerrero ¬∑ 24th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 14 more_vert 24th Solution. Hello, Congratulations to the winners of this competition! üôå Code Submit: https://www.kaggle.com/code/davidhguerrero/drop-features-reduction-with-t-sne-model?scriptVersionId=119026084 Overview First and foremost Is this competition a lottery? by ambrosm Yes, of course to me üòä Simple solution: Stacking Algorithms and Explore best K-Fold with Optuna without no more params of each algorithm. Agregatted original + syntetic dataset Model A: LGBMClassifier Model B: CatBoostClassifier Model C: LGBMRegressorWithRounder Train many quick models with Optuna (explore fewer params). First only  random_state &  n_splits of k-Fold. scores =[]\n\ndef find_out_params_model(trial):\n    random_state = trial.suggest_int( 'random_state' , 1000, 2000)\n    n_splits = trial.suggest_int( 'n_splits' , 8, 20)\n    cv = StratifiedKFold( n_splits =n_splits, shuffle = True , random_state =conf.random)\n    my_model = LGBMClassifier( \n        random_state = random_state\n    ) for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y)):\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train , y_valid = y.iloc[train_idx] , y.iloc[valid_idx]\n        my_model.fit(\n            X_train, y_train,\n            eval_set= [(X_valid,y_valid)],\n            early_stopping_rounds = 50, verbose =0\n        )\n\n        preds_valid = my_model.predict(X_valid)\n        score = cohen_kappa_score(y_valid,  preds_valid, weights = \"quadratic\" )\n        scores.append(score)\n    return np.mean(scores) content_copy And this class let me clean & organizate the notebook code class conf: index = 'Id' target = 'quality' random = 2023 load_original = True only_positive = False include_optuna = False include_lgbm = False include_catboost = False include_lgbm_regression = True n_trials = 10 np. random .seed(conf. random ) content_copy Measures the dependency between the variables with Mutual information add and remove some Feature Engineering Ideas thanks to Jose C√°liz Remove Model A & B. Better score with only one: LGBMRegressorWithRounder. Set a side other ones models Esemble best models LGBMRegressorWithRounder if conf .include_lgbm_regression:\n    scores = [] for train_index, val_index in LGB_skf. split (X, y):\n        x_train, x_val = X.iloc[train_index], X.iloc[val_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[val_index] m = LGBMRegressorWithRounder(**LGReg_best_param) m . fit (x_train, y_train, verbose = False)\n\n        models. append ( m )\n        scores. append (cohen_kappa_score(y_val, m . predict (x_val), weights = \"quadratic\" )) print (f' mean score : {np. mean (scores):.4f}')` content_copy and check confidence of model submission['quality'].value_counts() vs train['quality'].value_counts() What  Worked [High Impact]: Transform Your Regressor with Rounder Integration and www.kaggle.com/competitions/playground-series-s3e5/discussion/383429g (opens in a new tab)\">Feature Engineerhttps:// www.kaggle.com/competitions/playground-series-s3e5/discussion/383429g Ideas thanks to Jose C√°liz Drop the features that provide no useful information mutual_info_classif + Add Features. Explore with Optuna ONLY with random_state and num of Kfolds. Set a side models with bad with scores Model A: LGBMClassifier & Model B: CatBoostClassifier. Check a few other hyper params gets better score and add to model. What Didn't Work Dimensionality Reduction to visualize features (high-dimensional9 data with TSNE to Aggregate this features into promising new features to train data set. Feature scaling: standardize or normalize features get worse score. Thanks and Acknowledgements Thanks again to all the competitors and to the organizers at Kaggle for another fun round in the Playground Series. üòÑ Beginner Tabular Please sign in to reply to this topic. comment 0 Comments Hotness"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 5 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Wine Quality dataset . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 225.6 kB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 225.6 kB sample_submission.csv test.csv train.csv 3 files 27 columns ",
    "data_description": "Ordinal Regression with a Tabular Wine Quality Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Ordinal Regression with a Tabular Wine Quality Dataset Playground Series - Season 3, Episode 5 Ordinal Regression with a Tabular Wine Quality Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2023 Kaggle Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! Your Goal: For this Episode of the Series, your task is to use regression to predict the quality of wine based on various properties. Good luck! Start Jan 31, 2023 Close Feb 14, 2023 Evaluation link keyboard_arrow_up Submissions are scored based on the¬†quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the¬†metric may go below 0. The quadratic weighted kappa is calculated as follows. First, an N¬†x¬†N histogram matrix O is constructed, such that O i,j corresponds to the number of Id s i (actual) that received a predicted value j .¬†An N-by-N matrix of weights, w , is calculated based on the difference between actual and predicted values: w i , j = ( i ‚àí j ) 2 ( N ‚àí 1 ) 2 An N-by-N histogram matrix of expected outcomes, E , is calculated assuming that there is no correlation between values.¬† This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that E and O have the same sum. From these three matrices, the quadratic weighted kappa is calculated as: Œ∫ = 1 ‚àí ‚àë i , j w i , j O i , j ‚àë i , j w i , j E i , j . Submission File For each Id in the test set, you must predict the value for the target quality . The file should contain a header and have the following format: Id ,quality 2056 , 5 2057 , 7 2058 , 3 etc . content_copy Timeline link keyboard_arrow_up Start Date - January 31, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  February 13, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Ordinal Regression with a Tabular Wine Quality Dataset. https://kaggle.com/competitions/playground-series-s3e5, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 2,131 Entrants 933 Participants 901 Teams 8,644 Submissions Tags Beginner Tabular Cohen Kappa Score Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e6",
    "discussion_links": [
      "/competitions/playground-series-s3e6/discussion/389140",
      "/competitions/playground-series-s3e6/discussion/389145",
      "/competitions/playground-series-s3e6/discussion/389139",
      "/competitions/playground-series-s3e6/discussion/389151",
      "/competitions/playground-series-s3e6/discussion/389155"
    ],
    "discussion_texts": [
      "Regression with a Tabular Paris Housing Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Paris Housing Price Dataset Playground Series - Season 3, Episode 6 Regression with a Tabular Paris Housing Price Dataset Overview Data Code Models Discussion Leaderboard Rules Viktor Taran ¬∑ 3rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 21 more_vert 3rd place. Hello, everybody! I'd like to believe it wasn't just luck. My work here https://www.kaggle.com/code/viktortaran/ps-s-3-e-6 . tips: Using the original data ( https://www.kaggle.com/competitions/playground-series-s3e6/discussion/388361) . CV with n_splits = 5, n_repeats =10. Applying Permutation Importance for each model separately. Using three models (CAT, XGB, RF) for the first level and LGBM as a metamodel. Friends, good luck with the books! Please sign in to reply to this topic. comment 11 Comments 5 appreciation  comments Hotness George Papachristou Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert @viktortaran well deserved!! Thanks for sharing your solution ideas with us!! Keep going!! Pardeep Singh Posted 2 years ago ¬∑ 268th in this Competition arrow_drop_up 1 more_vert Thank you for sharing, @viktortaran , and congratulations on achieving 3rd place in this competition! üëè Shiva goud Posted 2 years ago ¬∑ 525th in this Competition arrow_drop_up 1 more_vert Excellent work! Felipe Akio Matsuoka Posted 2 years ago ¬∑ 340th in this Competition arrow_drop_up 1 more_vert Great work! Congratulations! Fadli Muhammad Posted a year ago arrow_drop_up 0 more_vert Thanks for sharing, really great notebook! Isaac90 Posted 2 years ago arrow_drop_up 0 more_vert This is great and has really helped me get a better grasp on what i need to do to better optimize my models Appreciation (5) Ahsan Raza Posted 2 years ago ¬∑ 485th in this Competition arrow_drop_up 1 more_vert Congrats, and thanks for sharing üëç yqz Posted 2 years ago ¬∑ 638th in this Competition arrow_drop_up 1 more_vert Great stuff, thanks for sharing. Alexandre Descomps Posted 2 years ago ¬∑ 560th in this Competition arrow_drop_up 2 more_vert Thanks for sharing ! Hakan ƒ∞rek Posted 2 years ago arrow_drop_up 0 more_vert Thank you for sharing .. summerwineY Posted 2 years ago arrow_drop_up 0 more_vert Thanks for sharing!",
      "Regression with a Tabular Paris Housing Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Paris Housing Price Dataset Playground Series - Season 3, Episode 6 Regression with a Tabular Paris Housing Price Dataset Overview Data Code Models Discussion Leaderboard Rules George Papachristou ¬∑ 5th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 10 more_vert Playground Series - Season 3, Ep.6 üéâ 5th place üéâ Hey kagglers üëã!! My first attempt in open Kaggle competition is end with great results! I  trust 98% my cross validation (CV) and I took the 5th place, and If I trust it 100% I would take the 3rd place, but this should be a lesson for me! DON'T TRUST THE PUBLIC LEADERBOARD !! My solution notebook is here: https://www.kaggle.com/code/mscgeorges/pss36-multi-stratifiedkfold-votingensemble Things that worked üî• Remove outliers - Run IQR for each column and remove the rows with the outliers (just 20 records). The outlier removal was not so important for the final model performance but it was important for the CV, because the RMSE metric is very sensitive to outliers and can lead to missleading results. Piece-Wise model (I like the terminology from @PRASAD )- Split the models based on different periods (based on made column). The best score was by spliting the data in 4 periods. Multi-StratifiedKFold - Run StratifiedKFold many times with different SEEDS. This give me the confidence to have good statistics about model performance (MeanScore +/- SD). The second key point is the selection of the \"Statified\" CV. Due to \"Piece-Wise model\" approach I had to stratify the data based on the \"made\" column in order to have consistent results. Ensemble Model - From CV I observed that different models (e.g Random Forest, XGBoost) had case that were good and others are not, so an ensemble model created a more stable solution. Low number of estimators - I used few estimators because with more estimators the models were overfitting in data noise Use all the features (except CityCode) - The most dominant feature was the size of the house (squareMeters column). It was the only feature with significan correlation (~ =53%) with the target and feature importance (~ =99%), all the others were look to be noise. But the CV show me that the other features were play some role and maybe in edge cases were helping the model to take the right decision, so I keep them! No extra features - After searching for artificial features that would probably help the model performance, I couldn;t find features which improve the CV score, so I didn't use any extra features. Include original data - The original data were looked different (based on Adversarial validation) but If we keep only the squareMeters columns the dataset were approximately the same, so I tried to include them in the training dataset and the results were better. Beginner Intermediate Please sign in to reply to this topic. comment 2 Comments Hotness Pardeep Singh Posted 2 years ago ¬∑ 268th in this Competition arrow_drop_up 1 more_vert Congrats on getting 5th position in this competition @mscgeorges , Thanks for sharing your approach as well! George Papachristou Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Thanks @pardeep19singh üôè for the support!",
      "Regression with a Tabular Paris Housing Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Paris Housing Price Dataset Playground Series - Season 3, Episode 6 Regression with a Tabular Paris Housing Price Dataset Overview Data Code Models Discussion Leaderboard Rules Kirderf ¬∑ 8th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert 8th place - Trust the CV - 409 jump in places on the private ds :) One need to trust the local CV when picking the submissions when they are ranked in the public rank 400-500 :) glad it worked here. The best single model was a work with some feature engineering and than tuning it with a XGB model via HPO. For the FE I used the orginal data to the comp. data and first manually removed the outliers below: train[train['made']!=10000] train[train['floors']!=6000] train[train['squareMeters']!=6071330] train[train['garage']!=9017] train[train['garage']!=2048] Then I applied some common FE such as scaling, remove perfect collinearity, cardinality reduction, power transform etc. I used a custom created HPO FE for finding the best FE for the data. Binned CV FE HPO on the train data is relevant and treat the val data as the coming test data per fold. After this I used the best CV FE HPO for tuning the XGB in the same way. This was the best single model, I also used a ensemble with different models, kernels, versions etc for the second submissions. Thats it! Please sign in to reply to this topic. comment 5 Comments Hotness Mir Abir Hossain Posted 2 years ago ¬∑ 416th in this Competition arrow_drop_up 1 more_vert Do you have a notebook that I can try to replicate and try? George Papachristou Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Congrats @kirderf !!!üéâüéâ I trust 98% my CV and I took the 5th place, if I trust it 100% I would took the 3rd place, thats life and a great lesson!! George Papachristou Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert @kirderf quick question: how you link this topic with your Private leaderboard rank? Kirderf Topic Author Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Thanks and congrats to you as well! 98% is a good trust, I took the rest procent in an ensemble ;) Here is the link to how you do the link to the topic https://www.kaggle.com/discussions/product-feedback/373153 George Papachristou Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Thanks for the help!! See you in the next challenge! Pardeep Singh Posted 2 years ago ¬∑ 268th in this Competition arrow_drop_up 0 more_vert Thanks for sharing your approach @kirderf üôè Michael Mellinger Posted 2 years ago ¬∑ 457th in this Competition arrow_drop_up 0 more_vert Yes, I track my CV scores. https://www.kaggle.com/code/mmellinger66/ps3e6-paris-housing-models?scriptVersionId=119789369 My best XGB CV scores were in earlier versions but I thought the newer scores were close enough. https://www.kaggle.com/code/mmellinger66/ps3e6-paris-housing-models?scriptVersionId=119506547 These older versions had some pretty bad Public scores so I was happy to take the LB improvement, over a slight worse CV. I dropped 300 spots instead of finishing around 100-125. Now I need to figure out an easy way to track CV across versions.",
      "Regression with a Tabular Paris Housing Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Paris Housing Price Dataset Playground Series - Season 3, Episode 6 Regression with a Tabular Paris Housing Price Dataset Overview Data Code Models Discussion Leaderboard Rules Brendan Moore ¬∑ 9th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 9th Place Solution My brief write-up of the 9th place solution can be found in the attached link. https://www.kaggle.com/code/brendanmoore14/9th-place-solution A simple automl approach using autogluon, with the original data included within the training loops, but not the cross-validation, ended up being my highest local cross-validation. I used a 5-fold StratifiedKFold split, with the classes being the three groups of price/sqm (~10/sqm, ~100/sqm, and ~1000/sqm). I tried multiclass classification on these three groups, but it ultimately did not improve the modelling. Feature engineering, oversampling of the minority classes (~10/1000/sqm), and undersampling of the majority classes (~100/sqm) were also not helpful in my experimentation to improve the local cross-validation. In the notebook, I have outlined what worked for me and what did not. Unlike some of the other top scores, I was not able to achieve high local cross-validation using a single model. Please sign in to reply to this topic. comment 4 Comments 1 appreciation  comment Hotness Nick Erickson Posted 2 years ago arrow_drop_up 1 more_vert Awesome example of using AutoGluon to get a strong placement, great work! Brian Okechukwu Posted 2 years ago arrow_drop_up 1 more_vert @brendanmoore14 Great work Brendan! Pardeep Singh Posted 2 years ago ¬∑ 268th in this Competition arrow_drop_up 0 more_vert Great work @brendanmoore14 thanks for sharing your approach! Appreciation (1) Junyi Wang Posted 2 years ago arrow_drop_up 1 more_vert Thank you so much",
      "Regression with a Tabular Paris Housing Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Paris Housing Price Dataset Playground Series - Season 3, Episode 6 Regression with a Tabular Paris Housing Price Dataset Overview Data Code Models Discussion Leaderboard Rules AmbrosM ¬∑ 43rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 10 more_vert #43 Explainable model almost without machine learning Sometimes, proper methodology is useful in these competitions. When the public leaderboard was dominated by notebooks which skipped cross-validation and trained on subsets of the available data , I decided to compare different cross-validation strategies. These experiments showed that samples with duplicated squareMeters should be treated differently from samples with unseen squareMeters : A. When I cross-validated with GroupKFold(groups=train.squareMeters) , linear regression gave the best results, better than all tree-based models: B. When I cross-validated with KFold , I got the best results by predicting the mean price of all houses with identical squareMeters , made and sometimes cityCode (this is the method I announced in Quasi-duplicates in the data ). My final model includes the original dataset and uses only three features: squareMeters , made and cityCode . It predicts test prices in three phases: If the training data contains houses with identical squareMeters , made and cityCode , predict the mean price of these houses. If the training data contains houses with identical squareMeters and made (regardless of cityCode ), predict the mean price of these houses. Otherwise do a linear regression. Please sign in to reply to this topic. comment 1 Comment Hotness jo≈Çmenik Posted 2 years ago ¬∑ 39th in this Competition arrow_drop_up 2 more_vert Nice, I was thinking about doing the same thing but in the end I used KNeighborsRegressor(n_neighbors=5) which is actually kinda similar to your solution. I've used only \"squareMeters\" and \"made\" from both original and training data (after dealing with the outliers) and got 39th place."
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 6 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Paris Housing Price Prediction . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 2.7 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 2.7 MB sample_submission.csv test.csv train.csv 3 files 37 columns ",
    "data_description": "Regression with a Tabular Paris Housing Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Paris Housing Price Dataset Playground Series - Season 3, Episode 6 Regression with a Tabular Paris Housing Price Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Feb 7, 2023 Close Feb 21, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same‚Äîto give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! With the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = ‚àö 1 N N ‚àë i = 1 ( y i ‚àí ÀÜ y i ) 2 where ÀÜ y i is the predicted value and y i is the original value for each instance i . Submission File For each id in the test set, you must predict the value for the target price . The file should contain a header and have the following format: id ,price 22709 , 200689 . 01 22710 , 398870 . 92 22711 , 1111145 . 11 etc . content_copy Timeline link keyboard_arrow_up Start Date - February 7, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  February 20, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Regression with a Tabular Paris Housing Price Dataset. https://kaggle.com/competitions/playground-series-s3e6, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 1,587 Entrants 715 Participants 703 Teams 5,797 Submissions Tags Beginner Tabular Housing Root Mean Squared Error Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e7",
    "discussion_links": [
      "/competitions/playground-series-s3e7/discussion/390976",
      "/competitions/playground-series-s3e7/discussion/390956",
      "/competitions/playground-series-s3e7/discussion/390979",
      "/competitions/playground-series-s3e7/discussion/390962",
      "/competitions/playground-series-s3e7/discussion/390961",
      "/competitions/playground-series-s3e7/discussion/390977"
    ],
    "discussion_texts": [
      "Binary Classification with a Tabular Reservation Cancellation Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Reservation Cancellation Dataset Playground Series - Season 3, Episode 7 Binary Classification with a Tabular Reservation Cancellation Dataset Overview Data Code Models Discussion Leaderboard Rules Hardy Xu ¬∑ 1st in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 57 more_vert 1st place solution Feature Engineering Although all of the data was numeric, a look at the data description revealed that the features type_of_meal_plan, room_type_reserved, and market_segment_type were actually categorical in nature. I handled these variables in 3 different ways: Leaving them as is One-hot encoding them Marking them as categorical features (used for LGBM) I played around with creating additional features, but I didn't find anything that improved my CV significantly. Data Leakage Within the train and test set, there were 1531 pairs of records that were duplicates if you dropped booking status. 562 of those pairs were in the train set, and upon closer inspection, each of those pairs had opposite booking statuses. 253 of those pairs were in the test set. The remaining 716 pairs included 1 record in the train set and 1 record in the test set. Out of curiosity, I tried modifying a submission by setting the predictions for those 716 records in the test set to the opposite of their corresponding record in the train set, and got a boost of +.014 to my leaderboard score. I naively hoped to keep this secret to myself, but it was quickly discovered by the rest of the community üòÖ However, I do have to thank @siukeitin for their suggestion to predict .5 for the 253 pairs of duplicates in the test set. This boosted my score by +.003, and I don't believe this trick was as well-known. Additionally, I removed from the train set the 562 duplicate pairs, plus the 716 records that had a duplicate in the test set, as I found that doing so improved my score on a holdout set I constructed to test this. I think this only resulted in a minor increase in my private score though. However, I do think that perhaps removing the train duplicate pairs helped make my CV more reliable. Data Cleaning When running adversarial validation between the train and original datasets, I plotted for the original dataset the predicted probability of belonging to the train set and got a bimodal distribution like this: This led me to experiment with removing the chunk of the original dataset that looked the least like the train set. I ended up using models that both included the entire original dataset and models that removed ~17% of the original dataset that looked the least like the train set. I also played around with things like fixing dates, but didn't see significant improvement in CV score from doing so. Modeling My CV setup was a stratified 3-fold with 3 repeats. For creating submissions, predictions from each fold were averaged. When tuning hyperparameters, one thing I noticed was that increased tree expressiveness seemed to improve performance to a much greater degree than what I had been used to. This meant using XGBoost with the 'exact' algorithm instead of 'hist' and max_depths around 12-13, and using LGBM with larger max_leaves and max_bin values. My winning submission was an average of the following 4 XGB submissions and 2 LGBM submissions: XGB XGB with categorical variables one-hot-encoded XGB with reduced original dataset XGB with categorical variables one-hot-encoded and reduced original dataset LGBM with reduced original dataset LGBM with categorical features and reduced original dataset Please sign in to reply to this topic. comment 15 Comments 1 appreciation  comment Hotness icfoer Posted 2 years ago ¬∑ 85th in this Competition arrow_drop_up 4 more_vert Congratulations! I knew that you would be the first. About the leak : in fact, I only found it because you came in first place. I couldn't believe that someone had found the \"golden bullet\". I heard that on kaggle, in competition with AUC, there is a trick for this - people submit (1-predict) so as not to be at the top of the leaderboard. Hardy Xu Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Thank you! I heard that on kaggle, in competition with AUC, there is a trick for this - people submit (1-predict) so as not to be at the top of the leaderboard. Haha, that's actually pretty clever. Joshmi Joshua Mable Posted 2 years ago arrow_drop_up 1 more_vert Congartulations‚Ä¶ Jose C√°liz Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Congartulations on achieving first place I do have to thank @siukeitin for their suggestion to predict .5 for the 253 pairs of duplicates in the test set . This boosted my score by +.003, and I don't believe this trick was as well-known. content_copy Yes I did the same on my submissions, a value of .5 is standard but the best value can be obtained by calculating the median because AUC is about ranking. Hardy Xu Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks, congratulations on your second place! Actually I tried predicting other values, including mean and median, and found they decreased my score compared to predicting .5. Also, I'm not convinced that predicting the median is theoretically correct. If we assume the booking status percentage is around 38% like it was in the train set, then I would think you would want to predict higher than the median, since the booking rate for the pairs of duplicates is 50%. Jose C√°liz Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert I think you are correct, the right value is the percentile (1 - 0.38) because is where the optimal threshold should be to perfectly divide both sets, I got confused while writing it üòÖ Yue Sun Posted 2 years ago ¬∑ 88th in this Competition arrow_drop_up 1 more_vert I did not notice that there exists a data leakage to use. Good findings! Manas Tripathi Posted 2 years ago arrow_drop_up 0 more_vert Is there a notebook to look at ? Pardeep Singh Posted 2 years ago ¬∑ 67th in this Competition arrow_drop_up 0 more_vert Congratulations on winning this competition @hardyxu52 and thanks for sharing your solution writeup üôåüôè Pasquale Posted 2 years ago ¬∑ 39th in this Competition arrow_drop_up 0 more_vert Congratulations, I've also used the strategy suggested by @siukeitin (great tip btw) for the duplicates in the test set and that improved my score üòè Matt OP Posted 2 years ago ¬∑ 37th in this Competition arrow_drop_up 0 more_vert Well deserved win @hardyxu52 , nice solution writeup as well. Predicting 0.5 for the 253 pairs of duplicates in the test data was a clever strategy! Shiva Kant Mishra Posted 2 years ago arrow_drop_up 0 more_vert @hardyxu52 ,Thanks for the sharing your winning solution gemammi Posted 2 years ago ¬∑ 167th in this Competition arrow_drop_up 0 more_vert Nice explanation there and congratulations!, especially to point out the categorical features and how to handle them, I did notice about it earlier but all the discussion didnt mention it, so i got carried away to leave them as is This comment has been deleted. This comment has been deleted. Appreciation (1) Mohammad Tanvirul Posted 2 years ago arrow_drop_up 0 more_vert Thank you for your winning solution",
      "Binary Classification with a Tabular Reservation Cancellation Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Reservation Cancellation Dataset Playground Series - Season 3, Episode 7 Binary Classification with a Tabular Reservation Cancellation Dataset Overview Data Code Models Discussion Leaderboard Rules Jose C√°liz ¬∑ 2nd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 34 more_vert 2nd Place Solution Hello all, This is the description on how I achieved the second place on this competition: CV My CV was done using booking date as the feature to stratify, I decided to use this column instead of target because there were month with few observations and all of them were assigned to the same fold. Some models I ensembled used 30 Folds and other 20 Folds just because computation was expensive. Original dataset was concated to each fold, i.e for a 20 Fold CV each fold had the entire original dataset + 95% of the train dataset. The Imputation Strange dates were imputed using last day of month. The Duplicates All duplicates came in pairs and always there was 1 positive label and one negative label. There were two leaks, most people figured out the first but the second was not disclosed. For duplicates on test that were in train, the trick was to assign the unknown label to the opposite of the known label. For duplicates on test that were also in test, the trick is assign a score that maximizes oof. Explanation: if you have a pair of duplicates, any model will predict the same value for them, so any value close to 0 or 1 will hurt your model. Since AUC is a rank metric, and the positive is around 38%, then ideally, the best threshold should be in the  62nd  percentile of your predictions, this value can be used to overwrite the duplicates. Features I found that most of the out of box features worked well. Some additional features I used was is_generated and is_inconsistent , both features are indicators. Dayofyear features also was selected in some models when doing Sequential Feature Selection. The ensemble Final Ensemble had 16 models that were selected using Hill Climb Technique and the weights were optimized using Nelder-mead algorithm. I did not rank predictions. Most of the models were LGBM, a couple of XGBoost. Everything was trained using EarlyStopping and the test predictions were calculating using the mean of each fold-model. What didn't work Holidays features, day of month feature, an indicator column that is positive whether the day was inconsistent, more than 30 Folds. My CV was perfectly correlated with the leaderboard, I felt like if I had more time maybe I would had used some public notebooks to do a final ensemble. Please sign in to reply to this topic. comment 8 Comments Hotness Yue Sun Posted 2 years ago ¬∑ 88th in this Competition arrow_drop_up 1 more_vert Good observation of the data. Pardeep Singh Posted 2 years ago ¬∑ 67th in this Competition arrow_drop_up 0 more_vert Great job getting to 2nd position in this competition @jcaliz . Thanks for sharing your solution writeup! üôåüî• Can you please share a bit on Hill Climb Technique for model selection and the Nelder-mead algorithm for weights optimisation? Pasquale Posted 2 years ago ¬∑ 39th in this Competition arrow_drop_up 0 more_vert Congratulations, clever solution. Have you tried both assigning 0.5 and the 0.62 percentile to the duplicates in test set? Has the latter performed significantly better? Jose C√°liz Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Choosing 0.5 is a good idea when your classifiers are well calibrated. Tree-based models are known for not being well calibrated on imbalaced datasets. Using a late submission these would have been the results, green uses 62nd and red uses 0.5: I'm pretty sure I would have used the green submission because it scored better on the public and public - private had a strong correlations because of the same relationship between CV and Public. Pasquale Posted 2 years ago ¬∑ 39th in this Competition arrow_drop_up 0 more_vert There is not a huge difference, I think that your score is mainly the result of how well you prepared the data and built the model üòâ Matin Posted 2 years ago ¬∑ 373rd in this Competition arrow_drop_up 0 more_vert Congrats! Did you end up testing any of the other found datasets? Jose C√°liz Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert I didn't, the dataset was big enough and after seeing the direct correlation of CV and LB I suspected that faster iteration would be a key to find a good ensemble. Jim Gruman Posted 2 years ago ¬∑ 290th in this Competition arrow_drop_up 0 more_vert Congrats. Well done This comment has been deleted.",
      "Binary Classification with a Tabular Reservation Cancellation Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Reservation Cancellation Dataset Playground Series - Season 3, Episode 7 Binary Classification with a Tabular Reservation Cancellation Dataset Overview Data Code Models Discussion Leaderboard Rules Craig Thomas ¬∑ 3rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 27 more_vert 3rd Place Solution Overview The goal of this competition was to maximize AUC ROC score for hotel booking cancellations. My solution implemented a 2-level stacked model. The first level of the stack consisted of the best 123 performing CatBoost, XGBoost, and LightGBM models that I generated. Each model was trained using a different selection of hyper-parameters. No engineered features were used, and the original dataset was mixed in for training purposes. All models were generated with training data split into 10-folds, predictions were made out-of-fold (OOF), and metrics were gathered OOF. The second level of the stack was a simple TensorFlow neural network. Objective was SigmoidFocalCrossEntropy from TensorFlow Additions. The neural network was trained using the training predictions from the first level, and predictions were made using the testing predictions from the first level. Post processing on the final set of predictions was performing utilizing the train / test duplication inversion technique discussed below. Model Stack Diagram Neural Network Configuration What Worked [High Impact] Train / Test Duplication Prediction Inversion Result: public LB increase of about 0.0141 Reasoning and context: inverting scores for duplicates in the testing data to lift performance - see discussions from @icfoer here , and from @sergiosaharovskiy here [Medium Impact] Stacking with Neural Network Result: CV increase of about 0.0026 compared to single models Reasoning and context: best neural network configuration combined 123 models, and had an increase over the best single model of 0.0026. [Medium Impact] Using Original Dataset Result: CV increase of about 0.002 Reasoning and context: I added the original dataset into the training set, making sure to perform CV metrics on competition data only. What Didn't Work [Medium Impact] Any Feature Engineering Result: CV drop of about 0.004 Reasoning and context: exhaustive investigation into features along with model testing revealed no significant gains with any engineered features (e.g. time based features). See more below in additional context. Additional Context My first big insight - no feature engineering - was key. As I mentioned above, I failed to find any way to boost class separation through feature engineering. My public facing EDA ended up being incomplete, because I stopped updating it when I began a more in-depth hunt for engineered features that would result in better separation between the positive and negative classes. After spending a lot of time digging through various ways of combining features, my conclusion was that I couldn't find anything significant that I could qualitatively or quantitatively measure as giving any type of lift to my models. I wasn't happy with that result, so I spent time doing the opposite - proving to myself that most of the feature combinations were hurting performance. Any time-based feature engineering such as adding day of week, quarter, day of year, etc. worked to bring down local CV scores. My best model using time-based features scored 0.900011 compared to my best model without them of 0.904148. My second biggest insight in this competition came from the agreement between local metrics and the public LB. Adversarial validation between train and test suggested that the two datasets were nearly identical. Given that we had a fairly generous ratio of positive cases vs negative cases, and that we had a decent number of training and testing samples to use (plus additional data we could mix in), my initial hypothesis was that local CV and public LB should match up fairly well. To test this, I submitted a number of models - each one meant to see how an increase or decrease in local CV was reflected on the public LB. About 10 models in, it became apparent that my metrics and the LB were in good agreement. With regards to the testing / training duplication and the inversion post-processing, after testing out agreement between my local CV and the leaderboard, I looked at the discussion of duplicates between the training and testing data, the original from @icfoer here and a followup from @sergiosaharovskiy here . I implemented an inversion on the testing duplicates and saw a good increase on the public LB by doing so. For every model I generated, I made sure to generate a set of testing predictions that contained the inversion as well, and tested to make sure that all notebooks that inverted the data saw lift on the public LB, which they did. Still not trusting the public LB entirely, I hedged bets, and for final model selection used the best model that inverted the predictions for the duplicates in test and train, and the model that just kept them as was. The inverted prediction model won third place. To ascertain whether I should use the original data, I performed an adversarial validation that resulted in an AUC ROC score of 0.7062. This suggested that there were differences between the datasets, but local CV testing suggested that there was a performance benefit to using it, and the public LB scores remained in-line with the CV results. The trick was ensuring that the CV metrics were done using only the competition data, so that I could compare my models to each other correctly. In total, I generated 623 models - 553 of them first level, 67 second level, and 3 of them third level. First level models were a mixture of CatBoost, XGBoost, and LightGBM. The 553 first level models were exploratory in nature, usually testing out features and random combinations of hyper-parameters. Second level models were combinations of the first. I attempted blending and stacking of first level models. For blending, I used between 6 and 10 of my top performing models and searched for the best co-efficients to blend them together that maximized AUC. I would then drop out any models that had 0 weight, and swap in another. Best blend had CV increase over best single model of 0.0007. I then swapped to using a stacking technique using a Ridge regression. The best Ridge model had a CV increase over best single model of 0.0019. Finally, because my first level model predictions were already ranged nicely between 0 and 1, I explored regression using neural networks. Best neural network configuration combined 123 models, and had an increase over the best single model of 0.0026. Thanks and Acknowledgements Thank you to the Kaggle organizers for continuing to deliver the Playground series. I very much appreciate the time and effort that goes into finding the datasets and generating novel ones from them so that we can compete and learn! Thanks to @sergiosaharovskiy and @icfoer for sharing their insights about the duplicates in the training and test datasets. Finally, thanks to everyone competing for great notebooks and discussions - always great to see such great engagement and enthusiasm! Please sign in to reply to this topic. comment 9 Comments Hotness Pardeep Singh Posted 2 years ago ¬∑ 67th in this Competition arrow_drop_up 1 more_vert Great stuff @craigmthomas and congrats on getting 3rd position in this competition. I am a bit confused on NN model inputs. Did you just use 123 models' output as inputs to NN or both (train dataset + output from 123 models)? Craig Thomas Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert @pardeep19singh the input to the NN was just the output from the 123 level 1 models. Pardeep Singh Posted 2 years ago ¬∑ 67th in this Competition arrow_drop_up 1 more_vert Got it @craigmthomas , thank you! Arthur Posted 2 years ago ¬∑ 174th in this Competition arrow_drop_up 1 more_vert It's amazing wow ! is that possaible to share your code, please ? Craig Thomas Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert @dann12 sorry, the model code is based on a custom framework that I am not releasing publicly at the moment. In brief though, the framework abstracts the mechanics of building different types of models away from the feature engineering and exploratory analysis work. In other words, I don't really worry about the mechanics of hyper-parameter tuning, building cross-validation folds, etc - that's all automated and performed by the framework. I essentially point it at the training data and test data, tell it to do pre-processing, tell it what features to use, and then set it to build x number of models of various types, with either fixed hyper-parameters or ones it generates at random. It logs all experiments back to an online experiment tracker. I focus the bulk of my time on exploring the data through Jupyter lab, and looking for features that could potentially be exploited. Yue Sun Posted 2 years ago ¬∑ 88th in this Competition arrow_drop_up 1 more_vert 623 models. That's amazing.üò≤ icfoer Posted 2 years ago ¬∑ 85th in this Competition arrow_drop_up 1 more_vert Congratulations! Sharov Anton Posted 2 years ago ¬∑ 295th in this Competition arrow_drop_up 0 more_vert Hi. Am I right that it was observed that inverting duplicates in test and train raise score on LB. And one could get good result just selecting model which inverts the most of duplicates? Craig Thomas Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert @sharovanton the duplicate inversion was performed as a post-processing step. If you take your best model, and then invert the duplicates after you make your predictions, you will likely increase your score. Sharov Anton Posted 2 years ago ¬∑ 295th in this Competition arrow_drop_up 0 more_vert Got it, thank you. But could it be also another metric between (good) models just observing that one model inverted more values in dup set then another? Put it simple, it is weired, but suppose we have some good models with good CV, and we should choose one among them. Can in this very specific case we select model that inverts more in dup set? (We can call it weired cv). I've read your post  as you did specifically this analysis on your final models and not simple postprocessing. It is basically theoretical question for this competition and dup set only. Craig Thomas Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert @sharovanton I think I understand what you're saying. You're asking if we could not just simply see what good models already perform this type of inversion when making predictions on the test set, and use that model for the submission instead of having to do a post-processing step instead. I don't think that you would be able to find a good model that does this naturally, nor do I know of any CV metrics that you could employ to ensure this was being done. If you think about the nature of the problem, we're trying to build a classifier that properly identifies booking cancellations (1) from non-cancellations (0). In the training set, the specific cases we're talking about have the exact same feature values in the testing set, but their class labels have been swapped for some reason. For example, there was a subset of the duplicates that are labelled as cancelled (1) in the training set, but are labelled as not-cancelled (0) in the testing set. When we build the classifier, it's going to try and learn from the training data that those examples should be labelled 1. A good classifier will have learned that, and will predict that on the testing data when it is presented with the same feature values. The same good classifier will need to do this as well with all of the other data in order for our local CV metrics to be high. Therefore, if the model has learned from our examples and has a high local CV score, it is always going to try and predict the wrong class label for these special cases, because for some reason, examples in the testing data with the same values have somehow had their class labels inverted. If it predicted the opposite class labels - in this case the wrong ones - while it would have done well with these inverted examples, it probably would have performed badly on all the other training data, and would not have been considered a good model base on local CV metrics to begin with (i.e. the local CV metrics would probably be bad, because if it learned the wrong class labels for the examples we needed to swap, it probably learned the wrong class labels for all the other examples too).",
      "Binary Classification with a Tabular Reservation Cancellation Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Reservation Cancellation Dataset Playground Series - Season 3, Episode 7 Binary Classification with a Tabular Reservation Cancellation Dataset Overview Data Code Models Discussion Leaderboard Rules Ryan Barretto ¬∑ 4th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 15 more_vert 4th Place Solution (Simple) Feature Engineering: None CV: Stratified KFold with 10 splits. I was happy with this competition because unlike the past few, this one had a relatively stable correlation between cv and lb. My final solution's cv score was 0.90667. Models: Lightgbm, XGBoost, CatBoost (of course). Each model was tuned by optuna to maximize the auc score for a validation set. Ensemble: For each fold, the optimal weighted average was calculated between the predictions from each of the three models. To calculate the weights, I used scipy to minimize the negative auc score. Post-Processing: I used the code suggested in this discussion . What didn't work: Feature engineering -- new features I added had little impact on the score Pseudo Labeling -- I attempted to pseudo label the original test set, as well as the competitions test set but I didn't find success (improvement in cv, but sharp decline in lb. Probably overfitting). Soft labels also didn't work. Code: https://www.kaggle.com/code/ryanbarretto/4th-place-solution?scriptVersionId=120547715 Please sign in to reply to this topic. comment 2 Comments Hotness Pardeep Singh Posted 2 years ago ¬∑ 67th in this Competition arrow_drop_up 1 more_vert Great stuff @ryanbarretto Thanks for sharing your solution and approach! Yue Sun Posted 2 years ago ¬∑ 88th in this Competition arrow_drop_up 2 more_vert Congrats. I did not notice the discussion about the post processing. It seems to have a great effect on improving the lb score.",
      "Binary Classification with a Tabular Reservation Cancellation Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Reservation Cancellation Dataset Playground Series - Season 3, Episode 7 Binary Classification with a Tabular Reservation Cancellation Dataset Overview Data Code Models Discussion Leaderboard Rules Sergey Saharovskiy ¬∑ 9th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 12 more_vert 9th Place Solution XGB stack Congratulations to the winners of this competition! It was a head-to-head run for top 10 contenders except the first place. My solution: CV : 30 Fold stratified. + Inner 5-Fold. RFE : I have removed 2 features no_of_previous_cancellations , no_of_previous_bookings_not_canceled . ( I burned quite a lots of cores to find a good combination out of 800+ ). Date Anomalies were fixed by imputing the max date of the month by following the discussion I initiated in the very beginning . Target Encoding : All categorical features were target encoded by using smooth mean algorithm with 2 level CV to prevent overfitting. Target transformation : I have transformed some of the contradicting duplicated targets with mode. (I have assigned the most common value as the target to the duplicated rows. 50%-50% examples I have left untouched). Ensemble : Hill Climbing. 7 Ensembled XGB models. Postprocessing (Date Leak Boost) : was implemented in accordance with Data Leak Code Snippet . What did not work : Honestly, everything worked. Every idea I tried improved my CV and LB until the very last moment. At some point I got stuck with a bunch of well trained XGB models. I have discarded LGBM and CatBoost since the yielded worst CV - LB score. Also their score correlation with LB was awful. What could be improved : I took an immense task to write double cross-validation frameworks for XGB, LGBM, CatBoost which took too much time and efforts. In the end of the day I made it work and the models with the target encoding stopped to overfit. I spent too much time creating smart features, most of which proved not working. Although one of my model with smart features yielded straight Top 10 ( mb I don't know something and it is better to throw everything into the XGB-LGBM-CAT auto-solver ).  I felt as I over-engineered the things themselves. Top 10 is a reasonable achievement for still not polished frameworks. Update, after applying 0.5 to 253 pairs of duplicates in the test was giving straight top1 public and top2 private with a margin, at least my ensemble was really good! On top of the above mentioned, here is what should have been done: test = pd.read_csv( 'data/test.csv' )\nidx = test[test.drop(columns= 'id' ).duplicated(keep= False )]. id .values\nsub = pd.read_csv( 'submissions/hill_20_19_17_21_5_6_11__wmean_public_leak.csv' )\nsub.loc[sub. id .isin(idx), 'booking_status' ] = 0.5 sub.to_csv( 'submissions/0.5.csv' , index= False ) content_copy Please sign in to reply to this topic. comment 4 Comments Hotness Sitraka_Forler Posted 2 years ago arrow_drop_up 1 more_vert Dam nice! Bravo Sharov Anton Posted 2 years ago ¬∑ 295th in this Competition arrow_drop_up 1 more_vert Hi, thank you and congrats with gold! I took an immense task to write double cross-validation frameworks Can you share code (or nb) or discuss this idea? Thanks in advance. Yue Sun Posted 2 years ago ¬∑ 88th in this Competition arrow_drop_up 1 more_vert Hill Climbing Technique. That's cool. I've seen it mentioned many times in the discussion board. This comment has been deleted.",
      "Binary Classification with a Tabular Reservation Cancellation Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Reservation Cancellation Dataset Playground Series - Season 3, Episode 7 Binary Classification with a Tabular Reservation Cancellation Dataset Overview Data Code Models Discussion Leaderboard Rules Martynov Andrey ¬∑ 15th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert 15th place solution Congratulations to all the winners ! üëç My solution: The code: PS-3-7 EDA CV weights optimizer CV: 30 folds stratified. The CV function was written with full classification report and 5 plots to compare solutions. Ensembling I think that was the main feature of the solution , ensembling was made in each fold with weight optimized, using what i called it \" Averager \" - class with fit and predict methods to find optimal weights for N solution given. I'm to use it in the future. See the code in the notebook. Models 3 regressors + 3 classifiers (XGB, CAT, LGB) + Random Forest FE None In the notebook I collected CV and LB progress data with comparision table and plot, it helped a lot to choose final submission. Many thanks to everyone who read my humble work! Please sign in to reply to this topic. comment 0 Comments Hotness"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 7 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Reservation Cancellation Prediction dataset . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 3.86 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 3.86 MB sample_submission.csv test.csv train.csv 3 files 39 columns ",
    "data_description": "Binary Classification with a Tabular Reservation Cancellation Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Reservation Cancellation Dataset Playground Series - Season 3, Episode 7 Binary Classification with a Tabular Reservation Cancellation Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Feb 14, 2023 Close Feb 28, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same‚Äîto give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! With the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each id in the test set, you must predict the value for the target booking_status . The file should contain a header and have the following format: id ,booking_status 42100 , 0 42101 , 1 42102 , 0 etc . content_copy Timeline link keyboard_arrow_up Start Date - February 14, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  February 27, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Binary Classification with a Tabular Reservation Cancellation Dataset. https://kaggle.com/competitions/playground-series-s3e7, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 1,600 Entrants 703 Participants 678 Teams 5,288 Submissions Tags Beginner Tabular Binary Classification Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e8",
    "discussion_links": [
      "/competitions/playground-series-s3e8/discussion/392828",
      "/competitions/playground-series-s3e8/discussion/392824",
      "/competitions/playground-series-s3e8/discussion/392820",
      "/competitions/playground-series-s3e8/discussion/392937",
      "/competitions/playground-series-s3e8/discussion/392860"
    ],
    "discussion_texts": [
      "Regression with a Tabular Gemstone Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Gemstone Price Dataset Playground Series - Season 3, Episode 8 Regression with a Tabular Gemstone Price Dataset Overview Data Code Models Discussion Leaderboard Rules Craig Thomas ¬∑ 2nd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 27 more_vert 2nd Place Solution Overview The goal of this competition was to minimize Root Mean Squared Error (RMSE) for gemstone price predictions. My solution was a 2-level stacked model. The first level of the stack consisted of 60 different models utilizing XGBoost, CatBoost, and LightGBM. First level model selection was based on a search through model space using RMSE as a guide - I started with collections of first level models that had local CV RMSE scores of 574 or lower, and stepped this threshold down by increments of 0.1, building second level Ridge models and searching for the model combination that gave the smallest local CV score for the second level. The optimal cutoff for selection of first level models was 572.6. All models were generated with training data split into 10-folds, predictions were made out-of-fold (OOF), and metrics were gathered OOF. Feature engineering is discussed in more detail below. What Worked [High Impact] Feature Engineering Result: CV lift of about -1.5 Reasoning and context: cut, clarity, and color features, with aspect ratio features was better able to segment the data into various price ranges. [High Impact] Use of Original Dataset Result: CV lift of about -1.6 Reasoning and context: additional data provided more samples to learn from. What Didn't Work Exploiting Duplicates in the Original Dataset Result: public LB lift of about -0.05, but drop in private LB of about +0.1 Reasoning and context: duplicates between the original data and test data existed. Using the price from the original data as predictions for the test duplicates appeared to provide good lift on the public LB, but didn't hold in private LB. See more below in additional context. Additional Context For this competition, I generated a total of 1,816 models. There were 1,709 models that were first level models - a mixture of XGB, CatBoost, Ridge, and LightGBM models.  The first level models were exploratory - they were focused mainly on testing engineered features along with hyper-parameter searches. I generated a total of 105 second level models using Ridge regression, blending, and neural networks. Most second level models were Ridge models that were searching for optimal selection of first level models. Second level blended models performed poorly compared to Ridge regressions. While second level neural networks provided a slightly better CV metric compared to the best Ridge model, all my neural net models resulted in a hard rebound on the public LB, suggesting bad overfit (which was subsequently confirmed via the private LB). Aside from the model stacks lending stability to the predicted results, the two key insights were feature engineering and duplicate handling. In terms of duplicates, the previous competition revealed duplicates between the training set and the test set, the handling of which turned out to be critical for top scoring solutions. In this competition, there were 398 rows from the original data that were duplicated in the testing data, and 1,440 rows from the training data that were duplicated in the testing data. I attempted several ways of overlaying the prices from the original dataset and the training dataset onto the testing dataset as a post-processing step. Using public LB scores as a guide, it appeared that overlaying the 398 rows from the original dataset onto their testing counterparts generated lift. However, I remained very skeptical of the results, and hedged bets on final submissions, using my best Ridge model with and without the exploit applied. In the end, the better model was the one without the exploit. The second insight - feature engineering - was particularly important. In my EDA I looked at several different engineered features. The first insight was that in the real world, the cut, clarity, and color categoricals correspond to a numbered scale, and that this scale is used to make quality comparisons between diamonds. Simple tests showed that considerable lift was possible if we converted the categoricals to a numeric scale, and made sure that LightGBM, CatBoost, and XGBoost didn't treat those columns as categorical in nature. This was confirmed by using different encoding methodologies on the categorical values, and comparing their relative scores (see figure below). While the color scale feature didn't appear to provide lift in my EDA, it did have a small impact when it came to tuned versions of models that used it. Another feature engineering insight came with respect to aspect ratio and carat value. By dividing up the standardized aspect ratio of the gemstone by an exaggerated version of the carat value, we could tease apart low value gemstones from higher value ones. Here is a summary that explained some of the different approaches and their results when used to generate untuned LightGBM models. Again, the specific details of each model type and what they were exploring is explained in more detail in my EDA. Thanks and Acknowledgements Once again, thank you to the Kaggle organizers for continuing to deliver the Playground series. Again, having access to such a large number of competitions in such a short timeframe is so valuable. And again, thank you to everyone who competed and took part in the discussions - it's always great to learn about the different approaches that everyone is using to approach the problem, and hear about what is working and what they are learning. Feature Engineering Ensembling Regression Please sign in to reply to this topic. comment 9 Comments 1 appreciation  comment Hotness Samuel Cortinhas Posted 2 years ago ¬∑ 30th in this Competition arrow_drop_up 3 more_vert Congrats on the great result again. Btw how did you train almost 2000 models - did you use some kind of automl? Craig Thomas Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 5 more_vert @samuelcortinhas it was a custom framework I built - similar to AutoML but different in that it leaves the feature exploration and stacking up to me. Basically it handles all the details about the mechanics of how to read the datasets and build the various models, and lets me focus on transforming features (scaling, quantizing, etc), injecting engineered features, or performing customized post processing. In addition to that, it does hyper-parameter tuning through random parameter space searching (kind of like Optuna, but again, custom). The typical workflow is to have it generate 40 or 50 models of one type (LGB, XGB, CatBoost, Random Forest, etc) using just basic features. That's usually enough to give you a sense of what the upper and lower performance bounds are going to be. By then I've usually done a detailed EDA with clues of what engineered features are useful, so then I start generating more models to test those features, again performing hyper-parameter searching at the same time. If I find good performing models, I can take the best hyper-parameters with those engineered features and re-create the models with a reduced learning rate that usually dials in better performance. Overall, it takes only a few minutes to set up a configuration for a competition, and then a few seconds to tweak it and test out feature ideas, post-processing tricks, objective functions, etc. I can queue all those experiments up and just let it run in the background. Overall it took ~72 hours of running time to generate all the models. Samuel Cortinhas Posted 2 years ago ¬∑ 30th in this Competition arrow_drop_up 1 more_vert Very interesting, thanks for sharing! Gopi M Posted 2 years ago ¬∑ 644th in this Competition arrow_drop_up 1 more_vert I am a beginner . Can I get a copy of your complete code for introductory learning. I look forward to your reply Craig Thomas Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert @gopimali I don't have the code available in a notebook to share. For the basics on stacking, see my stacking notebook from a previous competition. While the competition has different data, the approach is the same. John Ngugi Mwangi Posted 2 years ago ¬∑ 477th in this Competition arrow_drop_up 1 more_vert Congratulations on creating such an impressive ML model! I'm truly grateful that you were willing to share it. yann barthelemy Posted 2 years ago ¬∑ 120th in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for sharing your work! ShionMatsuoka Posted 2 years ago ¬∑ 32nd in this Competition arrow_drop_up 1 more_vert Thank CRAIG THOMAS for sharing the code. If you don't mind, I would like to know why my code shakes down. https://www.kaggle.com/code/shionmatsuoka/new-features-and-outlier-improvement kmkmks Posted 2 years ago ¬∑ 31st in this Competition arrow_drop_up 1 more_vert Congratulations! And thanks for sharing! I have done almost the same experiment. However, I found a big difference between 31st and 2nd place. I think this is because I used AutoML and skipped modeling‚Ä¶ That was a very fruitful solution sharing for me! Thanks again! Appreciation (1) T√πngg T√πngg Posted 2 years ago ¬∑ 168th in this Competition arrow_drop_up 0 more_vert oh my god this help me a lot. Thanks",
      "Regression with a Tabular Gemstone Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Gemstone Price Dataset Playground Series - Season 3, Episode 8 Regression with a Tabular Gemstone Price Dataset Overview Data Code Models Discussion Leaderboard Rules Tevfik Erkut ¬∑ 3rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 26 more_vert 3rd Place Solution Hey everyone! Thanks to all the competitors and organizers at Kaggle for this exciting competition! Before sharing my solution, I want to mention that I have experienced several challenging shake-ups in the past üôÉ, and as a result, my primary goal in this competition was to survive any shake-ups that might occur. Although we didn't experience any major shake-ups, my solution performed well thanks to my precious random seeds. I must give credit to these amazing notebooks, so shoutout to their authors: @sergiosaharovskiy 's EDA and submission notebook: https://www.kaggle.com/code/sergiosaharovskiy/ps-s3e8-2023-eda-and-submission @tetsutani 's XGB+LGBM+CAT ensemble baseline notebook: https://www.kaggle.com/code/tetsutani/ps3e8-xgb-lgbm-cat-ensemble-baseline @eamonntweedy 's XGB+LGB+CB ensemble notebook: https://www.kaggle.com/code/eamonntweedy/playground-s3-e8-gemstones-xgb-lgb-cb-ensemble My Strategy: Basically, I tried to use as many features / feature combinations as possible. Then I prepared a custom script, and inside of this script a LGBM model runs n-1 columns iteratively, given n equals to length of X. The goal was to determine which features are useful and vice versa. If dropping a feature decreases my model's success, I dropped this feature for good.  While doing this, usage of many seeds was crucial therefore I've used different seeds, and took the average for scores. But a small improvement in RMSE can be tricky and it can cause damage to your model if std of folds is higher than default one. ( fully trained one with n columns ) So I made few different datasets based on my above approach. Like at first, I tried to decrease my RMSE, without even looking at std numbers. Then once I checked the std's, I prepared another 2 more datasets.  In my second attempt, I tried to decrease both the standard deviation and RMSEs simultaneously and at third attempt I tried to decrease only STD of scores. ( 0.015 was my tolerance threshold for RMSE's, so if dropping a feature cause +0.015 RMSE and -0.01 STD, I accepted it. ) Once I saw my std_avg decreased from 4.5'ish to 3.8'ish, I stopped the process. ( At modeling part, I used optuna to tune parameters with a default value of n_iterations = 1000 / learning_rate = 0.1. After tuning the parameters, I increased n_iterations to 10000 and decreased learning rate to 0.01. ( ESR = 300) My final solution includes XGBoost + LightGBM + CatBoost predictions which are coming from these different datasets. ( like xgb1, xgb2, xgb3, lgb1, lgb2, lgb3 .. ) (btw i also used different seed combinations in parameter tuning&modelling too! At my latest sub, my predictions were coming from a train-predict loop which has 10 different seeds. That's why I named it as 'thats_the_code', I guess :P) Please sign in to reply to this topic. comment 8 Comments Hotness Samuel Cortinhas Posted 2 years ago ¬∑ 30th in this Competition arrow_drop_up 1 more_vert Congrats! Nice idea about using the different datasets to train separate models and then ensemble Tevfik Erkut Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Thanks @samuelcortinhas yann barthelemy Posted 2 years ago ¬∑ 120th in this Competition arrow_drop_up 1 more_vert Thank you for sharing your work! Congratulations! Adrian Muresan Posted 2 years ago ¬∑ 328th in this Competition arrow_drop_up 1 more_vert Excellent approach and good write-up! Thank you for sharing. ShionMatsuoka Posted 2 years ago ¬∑ 32nd in this Competition arrow_drop_up 1 more_vert Thank TEVFIK ERKUT for sharing the code. If you don't mind, I would like to know why my code shakes down. https://www.kaggle.com/code/shionmatsuoka/new-features-and-outlier-improvement Tevfik Erkut Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert I will check it mate, thanks for your kind words! @shionmatsuoka ShionMatsuoka Posted 2 years ago ¬∑ 32nd in this Competition arrow_drop_up 0 more_vert Thank you !Please check when you have time. @peridon !!!! Iqbal Syah Akbar Posted 2 years ago arrow_drop_up 1 more_vert Congratulations for reaching 3rd place! I have a question about the part where you used a lot of different seeds. I thought that seed only matters to make sure we have the same initialization state? Does random seed actually affect the accuracy of your model? Tevfik Erkut Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thanks a lot @iqbalsyahakbar . You are right about the initialization state, and that usage helps us to make our processes more deterministic. And for your question, yes it indirectly affects our model's accuracy. Let me give you an example: kf = KFold(n_splits=n_splits, shuffle=True, random_state=42) . this random_state = 42 affects the ordering of the rows. Therefore it affects our train and test rows too! If you use same specific random state in all of your CV process, then your train rows will always see the same ordering.  Therefore, it is important to use many different random seeds when training a model in order to obtain a more accurate estimate of the model's performance. Iqbal Syah Akbar Posted 2 years ago arrow_drop_up 0 more_vert Ah, I did not know that row order actually affect our models too. Thank you for the answer! Tevfik Erkut Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 3 more_vert By saying orders, I've meant the rows distribution. If you use this kf variable for your cv process, you'll get same fold distribution again and again. Your train rows distribution over 5 folds won't be changed unless you change the random_state parameter.",
      "Regression with a Tabular Gemstone Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Gemstone Price Dataset Playground Series - Season 3, Episode 8 Regression with a Tabular Gemstone Price Dataset Overview Data Code Models Discussion Leaderboard Rules Kirderf ¬∑ 6th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 17 more_vert 6th place solution in 3 days - AutoGluon + AutoXGB 6th place with only 8 submissions and 3 days to deadline, a record for me üéâ also started the first submission with 7th place, lucky entry as well :) I used AutoGluon framework and XGBoost with Autoxgb framework in a weighted ensemble based on the CV scores! For the AG I used two solutions, one trained 8 fold-1lv stacked and finally inference with LGBM,CatB,XGB,RF,NNTorch,NNFastAI and the other solution with psuedo labels from the first models. This took approx. 13h together. Then the best ~24h HPO tuned XGB 8 fold model was ensemble with above, Both the datasets where used, orginal and generated. That's it! Please sign in to reply to this topic. comment 3 Comments Hotness Nick Erickson Posted 2 years ago arrow_drop_up 1 more_vert Awesome solution! Kirderf Topic Author Posted 2 years ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert I can only say the same :) thanks for the 50% of the above solution üëç ShionMatsuoka Posted 2 years ago ¬∑ 32nd in this Competition arrow_drop_up 0 more_vert Thank KIRDERF for sharing the code. If you don't mind, I would like to know why my code shakes down.",
      "Regression with a Tabular Gemstone Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Gemstone Price Dataset Playground Series - Season 3, Episode 8 Regression with a Tabular Gemstone Price Dataset Overview Data Code Models Discussion Leaderboard Rules John Mitchell ¬∑ 7th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 11 more_vert [7th Place] Reversing the Polarity of the Information Flow Thanks to Kaggle for another interesting and enjoyable competition in this great series, and to everyone who participated and shared their insights (and models!). This competition gave me a chance to try something wacky which, while it‚Äôs the main focus of this write-up, actually yielded less than a quarter of the final ensemble. Executive Summary: I used the following three-step prediction workflow to engineer some extra features and generate more models: (Original) Training -> Test (Reverse) Training <- Test (Double-back) Training -> Test 0.    The Set-up Using the predictions of models on both training and test sets as features is something I‚Äôve thought about doing before. What I did was largely for interest, and it would perhaps with hindsight have been easier to obtain model predictions for the training set through cross-validation. However, the relatively even size ratio of the training and test sets suggested that this might be a favourable competition for the reversal and double-back approach. More pertinently, I hadn‚Äôt even considered using cv to get training predictions until the reversal was largely a sunk cost of several hours‚Äô work. I chose the following seven publicly available models to serve as the chassis for my reverse models, These were chosen because they were all both high scoring and apparently capable of being run without dependencies on private datasets: Model 1: PS3E8 Xgb/Lgbm/Cat Ensemble Baseline by @tetsutani , v11. Model 2: CB_TPGS_S3E8_v1 by @omarvivas , v17. Model 3: Samurai_s3e8ens+featureEngineering_ CB,XB,LB by @samuraikaggle , v 9. Model 4: Gemstone Price Prediction - S3E8 by @satoshiss , v11. Model 5: PS-S3-E8 EDA and modeling by @francescoliveras , v11. Model 6: XGboost regression baseline by @rkoirala129 , v3. Model 7: playground_season_3_episode_8 by @pawebiegun , v10. 1.    Running the Original Models Since I was going to be working on adaptations of these models, it was essential to get them all working for me in their original states; anything that I couldn‚Äôt run ‚Äúas is‚Äù would be abandoned as a lost cause for reversing. In practice, all seven models ran without too much difficulty. This yielded predictions which should all have been the same as those in the original notebooks, except that I later discovered that I had mistakenly copy-and-edited the ‚Äòwrong‚Äô version of Model 3, v9 rather than the better scoring v4. 2.    Engaging Reverse Gear I now had each model‚Äôs predictions for the test set, but still needed corresponding numbers for the training set before I could use each model‚Äôs output as a newly engineered feature. In order to obtain them, I required price values for the test set, which were obviously the target of the competition and hence could only be estimated. I used the predictions of the then best-scoring public model as pseudo-ground truth here, since this model was independent of my seven (an apparent dependency on a private dataset meant that I hadn‚Äôt tried to adapt it): Model 0: PS S3E8, 2023 EDA and Submission by @sergiosaharovskiy , v18. I thus added the predictions of @sergiosaharovskiy ‚Äôs model as the price column to the test set, see here , using this to train the reversed models, which in turn would predict price for the original training set (with its ground truth deliberately obscured). Next, I adapted each of the seven models to predict backwards, which was largely a matter of respecifying their training and test sets by editing a few paths. All seven of them produced sets of predictions without too much trouble. Thus, I now had predictions from all seven models for both test and training sets, and these seven columns were henceforth to be treated as engineered features. See the expanded sets of features: training and test 3.    Double-back With the expanded feature sets in hand, the next objective was to use the models to generate new predictions for the test set, with the hope that the predictions would be at least meaningfully different from the existing ones, adding diversity, and more optimistically better scoring. Changing the feature sets and removing dependencies on the original data were required here. Given more time, patience and coding skills, this could doubtless have been accomplished for all seven models. However, avoiding throwing good debugging time after bad, I ultimately settled for generating four double-back (DB) models. Their scores were: Model 1: (Original Priv 571.75670, Pub 575.16974; DB Priv 573.17408, Pub 577.21897); Model 4: (Original Priv 573.08148, Pub 577.55176; DB Priv 572.40829, Pub 576.53189); Model 6: (Original Priv 573.89711, Pub 578.15487; DB Priv 571.99507, Pub 575.98026); Model 7: (Original Priv 577.24019, Pub 578.28833; DB Priv 572.94775, Pub 576.50537) Thus, I improved the scores for three out of four models by adding the engineered features through this reverse and double-back workflow. 4.    Ensemble Given the number of diverse, but similarly scoring, models, median ensembling seemed a good strategy for the final submission. I looked at adding in any public models that were better than the weakest of mine, and also sought to take the median of an odd number of predictions. Hence, I added; Model 8: Simple Model by @satyaprakashshukl , v6. Model 9: Playground-S3_E8-Gemstones-XGB,LGB,CB-ensemble by @eamonntweedy , v1 (two output models). Model 10: Plotly EDA | Feature Engineering | Optuna by @ch124uec , v6. In my eventual submission , I took the median of seventeen models, of which four were the ones I engineered through the reversal. The 17 models were models: 0, 1, 2, 3 (v4 & v9), 4, 5, 6, 7, 8, 9 (two submission files), 10, 1DB, 4DB, 6DB and 7DB. This generated the scores of (Priv 570.52851, Pub 574.77083), and allowed me the somewhat atypical experience of benefitting for once from the shake-up, moving from 15th to 7th. Please sign in to reply to this topic. comment 2 Comments Hotness Pawe≈Ç Biegun Posted 2 years ago ¬∑ 282nd in this Competition arrow_drop_up 1 more_vert @jbomitchell Thanks for the post. I'm glad that my model was useful for you. That's a really interesting concept for competitions. Do you have some resources that you could share to learn it more in depth and with more examples? John Mitchell Topic Author Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 0 more_vert Thanks @pawebiegun . To be honest, I was pretty much making it up as I went along. I'm reluctant to make all the codes public, as they're basically other peoples' work with very minor changes. I annotate your code with the required changes below. The first reversal step required: (a) To change the paths & names of the training and test data, making sure that at this stage the model can't see the true ground truth values of the competition's training set (now in the role of test set); (b) Best estimate target values for the competition's test data (now in the role of training set). For reversal with your own code, there were only two changes, the first around lines 25-28 where some data paths now point to files in my own (public) dataset : data_paths = [ \"/kaggle/input/playground-series-s3e8/train.csv\" , \"/kaggle/input/gemstone-price-prediction/cubic_zirconia.csv\" ], content_copy becomes data_paths = [ \"/kaggle/input/gemstone-price-modelling/test_as_train.csv\" , \"/kaggle/input/gemstone-price-prediction/cubic_zirconia.csv\" ], content_copy and the other around line 134 where: test_ds = DiamondDataset(eval_dataset = True , data_paths = [ \"/kaggle/input/playground-series-s3e8/test.csv\" ]) content_copy becomes test_ds = DiamondDataset(eval_dataset = True , data_paths = [ \"/kaggle/input/gemstone-price-modelling/train_as_test.csv\" ]) content_copy The double-back step was more demanding of coding time and skill. The training and test data are now back in their original roles, but we now have extra columns of features (the reversed models' predictions) in each. How to handle this is going to be very dependent on the specific code being hacked. For this competition, I needed to exclude the 'original' data from all the training processes since I hadn't got model predictions for the original set (but I might alternatively have obtained such predictions). Changing the size of the feature matrix and adding new feature names certainly crashed some models; but each unhappy model is unhappy in its own way, as Tolstoy might have said. Your model was the easiest of the seven to edit, I think. These two locations in the double-back version are, as I removed cubic_zirconia.csv: data_paths = [ \"/kaggle/input/gemstone-price-modelling/train_m.csv\" ,\n    ], content_copy and test_ds = DiamondDataset(eval_dataset = True , data_paths = [ \"/kaggle/input/gemstone-price-modelling/test_m.csv\" ]) content_copy Modifications to other peoples' work were analogous, but harder to code.",
      "Regression with a Tabular Gemstone Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Gemstone Price Dataset Playground Series - Season 3, Episode 8 Regression with a Tabular Gemstone Price Dataset Overview Data Code Models Discussion Leaderboard Rules Sergey Saharovskiy ¬∑ 8th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 26 more_vert 8th place solution (Flying Over the 1st place again) Intro : Another week and another salute to the winners of the competition! This was painful to write, but @jcaliz cheered me up since the solution was not trivial. Preface : I was interested in gemology during my university days and was considering getting a GIA (Gemological Institute of America) certification to appraise precious stones. I even had plans to open my own pawn shop to sort diamonds not pandas dataframes. It's amazing how our interests and career paths can change over time. While my current focus is not in gemology at all, the past experiences and interests can still be valuable assets. Part 1. Fighting outliers : Take a look at these 3 illustrations where I tried to clip the outliers based on their reasonable price range (red boxes shows the areas where the train and original datasets are awfully different). Once I fixed the values of outliers I got extreme boost on CV, but poor results on LB‚Ä¶ In order to understand why that happened, I decided to look at the best predictions and voila! It happened to be that all models already squashed the outliers so there was no reason to fix it. Part 2. The models were good already: I looked at public works written by @tetsutani and @eamonntweedy since their modeling part was somewhat similar to mine. One thing I found in common is that the models were good and generalized well from the very beginning. Though I did not really like the squashed predictions, so I tried to help the model myself. I ventured to imagine myself a certified Appraiser and start labeling the data manually. Part 3. Visualizations and perfect stone: No self-promotion intended, but I started looking into When 4Cs Concept becomes an Art topic more and more. I chose 'Ideal\" cut stones of \"D\" color (best colorless), 'Flawless'  within a specified carat range from the train. The idea was to find the max possible value for a specified group of stones and compare it to the test predictions, if for some reason test predictions were higher - assign Q3 + (1.5 * IQR) from train to the test predictions. It gave +0.01 LB for 20 examples. When I decided to automate it a bit: def postprocess_clip_sub ( best_sub: pd.DataFrame, alpha: float ) -> pd.DataFrame: \"\"\"Clips test preds max values based on the train statistics. \n\n        Args:\n            best_sub: best ensembled submission at the moment.\n            alpha: tunable parameter for calibrating the upper bound for predictions.\n\n        Returns:\n            best_sub: modified df\n    \"\"\" # LB +0.011 (574.92723 - 574.9163) # LB +0.2 tuned alpha sub = best_sub.copy()\n    train = pd.read_csv( 'data/train.csv' ).drop(columns= 'id' )\n    test = pd.read_csv( 'data/test.csv' ).drop(columns= 'id' )\n    test_sub = pd.concat([test, sub], axis= 1 )\n\n    bins = np.arange( 0.19 , 4.5 , 0.035 )\n    train[ 'carat_bin' ] = pd.cut(train.carat, bins=bins, labels= list ( range ( len (bins)- 1 )))\n    test_sub[ 'carat_bin' ] = pd.cut(test_sub.carat, bins=bins, labels= list ( range ( len (bins)- 1 ))) for cl in tqdm(train.clarity.unique()): for cb in sorted (train.carat_bin.unique()):\n            price_frame = train.query( 'cut == [\"Ideal\", \"Premium\"] & clarity == [@cl] & color == [\"D\", \"E\"]' ).query( 'carat_bin == @cb' ) if price_frame.shape[ 0 ] > 5 :\n                Q1 = np.percentile(price_frame.price, 25 , interpolation= 'midpoint' )\n                Q3 = np.percentile(price_frame.price, 75 , interpolation= 'midpoint' )\n                IQR = Q3 - Q1\n                lower_bound = Q3 - ( 1.5 * IQR)\n                upper_bound = Q3 + ( 1.5 * IQR)\n\n                idx = test_sub.loc[test_sub.carat_bin.eq(cb) & test_sub.clarity.eq(cl)]. id sub.loc[sub. id .isin(idx), 'price' ]  = sub.loc[sub. id .isin(idx), 'price' ].clip( 0 , upper_bound*alpha) return sub content_copy This algorithm gave me straight +0.2. Part 4. Jumping to the first position on the public LB: Once I clipped my max predicted values, I came down to the min ones. And this is how I found myself +1.1 right away. The algorithm: iterate thru carat in train with sliding window with range (0.15) inside the range iterate thru cut inside the 'cut' iterate thru clarity inside the 'clarity' iterate thru color filter the train frame by this group of parameters and if there were more than 5 records found look for the same in the test predictions if the predictions were for some reason lower than the min possible train, assign min train to those records. This basically did impossible with any average model pushing it straight to the Top 10 . Part 5. Why it is painful. Winning a Kaggle competition can bring a sense of accomplishment and validation, similar to the rush of euphoria that comes with using drugs. However, Kaggle competitions can also be incredibly challenging and frustrating. Participants may spend countless hours tweaking their models, only to find that they still fall short of the competition leaders. This can lead to feelings of disappointment and sadness, similar to the lows experienced by drug addicts during withdrawal. In both cases, the highs and lows can create a cycle of addiction. Participants may become obsessed with winning Kaggle competitions, constantly seeking the next challenge to achieve that feeling of euphoria again. Similarly, drug addicts often chase the initial rush of euphoria, leading to a cycle of addiction and seeking out the next high. Be careful! Be save! Outro : Night changes many thoughts. it is a bit messy but I hope you found this write-up useful. Please sign in to reply to this topic. comment 7 Comments Hotness Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert My compliments for your 8th position and thank you for your write-up. I'm laughing with yours \"Flying Over the 1st place again\". Tevfik Erkut Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert I totally agree with Part 5.. It's definitely a painful process. ShionMatsuoka Posted 2 years ago ¬∑ 32nd in this Competition arrow_drop_up 1 more_vert I may have become one of the addicts in my first participation in kaggle. I found out what I had always wondered. Thank you very much. I too made LB worse and worse and worse by changing the anomaly‚Ä¶. sahilsg Posted 2 years ago ¬∑ 113th in this Competition arrow_drop_up 1 more_vert Can you please elaborate Part 4 algorithm or may be show some code? BTW Congratulation on 8th place. Sergey Saharovskiy Topic Author Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 2 more_vert @sahilsg it might be a messy one, but here you go: from joblib import Parallel, delayed def min_hard_assignment_sub ( i: float ): \"\"\"Assigns min train price to the test datapoints based on the train statistics. \n\n        Args:\n            i: the first carat sliding window point.\n\n        Returns:\n            l: list of pd.Series (changed predictions based on the min_train value)\n    \"\"\" sub = best_sub.copy()\n    test = pd.read_csv( 'data/test.csv' ).drop(columns= 'id' )\n    test_ = pd.concat([test, sub], axis= 1 )\n    l = []\n    i_10 = i+ 0.2 for ct in [ 'Ideal' , 'Premium' , 'Very Good' , 'Good' , 'Fair' ]: for cl in [ 'IF' , 'VVS1' , 'VVS2' , 'VS1' , 'VS2' , 'SI1' , 'SI2' , 'I1' ]: for clr in [[ 'D' , 'E' ], [ 'F' , 'G' ], [ 'H' , 'I' ], [ 'J' ]]:\n                query_str = \"carat >= @i & carat < @i_10 & cut == [@ct] & clarity == [@cl] & color == @clr\" min_train = train.query(query_str).price.describe().loc[ \"min\" ]\n                min_test =  test_.query(query_str).price.describe().loc[ 'min' ] if (min_test - min_train) < 0 : if (train.query(query_str).shape[ 0 ] > 4 ) and (test_.query(query_str).shape[ 0 ] > 3 ):\n                        idx = test_.query(query_str).query( 'price < @min_train' ).index.tolist()\n                        sub.loc[sub.index.isin(idx), 'price' ] = min_train\n                        l.append(sub[sub.index.isin(idx)][ 'price' ]) return l # Calculates the assignments. multiprocess_output = Parallel(n_jobs=- 1 )(delayed(min_hard_assignment_sub)(i) for i in tqdm(np.arange( 0.20 , 5 , 0.15 ))) # Unpacks the results. series = [j for j in multiprocess_output if len (j) != 0 ]\nseries_unpacked = [] for s in series: \n    series_unpacked.append(pd.concat(s))\nresult = pd.concat(series_unpacked)\nresult = result[~result.index.duplicated(keep= 'last' )] # Assigns the results to the best submission. best_sub.loc[best_sub.index.isin(result.index), 'price' ] = result.sort_index().values content_copy p.s. I have wrapped it into the Parallel (which makes it more complicated), otherwise it would take forever for high frequency carat window. Ravi Ramakrishnan Posted 2 years ago ¬∑ 21st in this Competition arrow_drop_up 2 more_vert Hearty congratulations @sergiosaharovskiy , I think getting an 8th rank in any assignment is an achievement in itself. I think we should celebrate our success rather than expressing pain at losing to 7 better results. Daragh Thomas Posted 2 years ago arrow_drop_up 0 more_vert Did you enjoy uncut gems?"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 8 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Gemstone Price Prediction dataset . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 18.96 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 18.96 MB sample_submission.csv test.csv train.csv 3 files 23 columns ",
    "data_description": "Regression with a Tabular Gemstone Price Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Gemstone Price Dataset Playground Series - Season 3, Episode 8 Regression with a Tabular Gemstone Price Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Feb 21, 2023 Close Mar 7, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same‚Äîto give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! With the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = ‚àö 1 N N ‚àë i = 1 ( y i ‚àí ÀÜ y i ) 2 where ÀÜ y i is the predicted value and y i is the original value for each instance i . Submission File For each id in the test set, you must predict the value for the target price . The file should contain a header and have the following format: id ,price 193573 , 3969 . 155 193574 , 8512 . 67 193575 , 1122 . 34 etc . content_copy Timeline link keyboard_arrow_up Start Date - February 21, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  March 6, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Regression with a Tabular Gemstone Price Dataset. https://kaggle.com/competitions/playground-series-s3e8, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 1,678 Entrants 757 Participants 734 Teams 5,789 Submissions Tags Beginner Tabular Regression Root Mean Squared Error Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e9",
    "discussion_links": [
      "/competitions/playground-series-s3e9/discussion/394592",
      "/competitions/playground-series-s3e9/discussion/394600"
    ],
    "discussion_texts": [
      "Regression with a Tabular Concrete Strength Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Concrete Strength Dataset Playground Series - Season 3, Episode 9 Regression with a Tabular Concrete Strength Dataset Overview Data Code Models Discussion Leaderboard Rules AmbrosM ¬∑ 1st in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 89 more_vert #1 Solution: Cross-validation and diversity win My final solution differs only slightly from what I published in my EDA which makes sense . It is based on a few principles: Optimize the cv score and don't look at the public leaderboard! The cv score is based on 5407 samples while the public leaderboard is based on only 721 samples. In a Kaggle competition, a 5407-sample average is a measurement - a 721-sample average is random variable. If you want to test the quality of a dice, you better throw it eight times rather than only once. This rule has a few consequences: You don't need more than two submissions because you can't gain any information from your public leaderboard score. Ok, I used seven submissions because I was curious. Don't copy code from high-scoring public notebooks unless the quality of that code shows up in a good cross-validation score. The best-scoring public notebooks are at the top of the list only because they overfit the public leaderboard. Cross-validate correctly: An ordinary KFold is enough for this competition. A train_test_split is not. Implement a diverse ensemble! Everybody uses gradient boosting, but you need to find good hyperparameters. Optuna doesn't. Just run Optuna and then change the seed of the KFold. You'll see that the Optuna-found hyperparameters don't survive the change of seed. I optimized the LGBM hyperparameters manually. Using more than one gradient boosting implementation adds diversity to the ensemble. I used LightGBM and GradientBoostingRegressor . Random forests are simple to optimize: The most important hyperparameter is min_samples_leaf . Whereas the tree-based algorithms don't need much feature engineering, linear regression does. After the partial dependence plots of the EDA showed the nonlinearity of the game, I spent quite some time creating features. It is important to not just add features and hope for the best - in the same process you have to drop the features which don't improve the cross-validation score. Remember not to optimize your ensemble against the public leaderboard. Optimize the cv score. AgeInDays has only a few different values and its relationship to the target is highly nonlinear and not monotone. I treated it as a categorical value and target-encoded it, replacing the feature values by the corresponding average target values. This target encoding even helped for some of the tree models. Source code is here. Please sign in to reply to this topic. comment 40 Comments 2 appreciation  comments Hotness Tariq Mahmood Posted 2 years ago arrow_drop_up 5 more_vert Hi Dear @ambrosm , Congratulations on your winning solution! Your approach to focusing on cross-validation and diversity is a great reminder of the importance of optimizing for a reliable measure of performance. Your insights on using multiple gradient boosting implementations and random forests for diversity, as well as the importance of feature engineering and target encoding, are very helpful. Thank you for sharing your thought process and experience with the community.üëç Siddharth Sah Posted 2 years ago arrow_drop_up 6 more_vert Great work on developing a robust solution by focusing on optimising the CV score and maintaining diversity in the ensemble. Your emphasis on cross-validation, manual hyperparameter optimisation, and careful feature engineering shows a solid understanding of the competition's requirements. Congratulations on your success, and thank you for sharing valuable insights with the community! AKR Posted 2 years ago ¬∑ 515th in this Competition arrow_drop_up 4 more_vert Congrats!! @ambrosm I optimized the LGBM hyperparameters manually. Manually in the sense you used grid search or tried values one by one to see change. Also for GradientBoostingRegressor how you tuned hyperparameters? AmbrosM Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert Hi @raj401 Manually in the sense that I looked at every single result - either one by one or with a for loop. Grid search in two dimensions can be visualized; it's difficult to visualize in more than two dimensions. AKR Posted 2 years ago ¬∑ 515th in this Competition arrow_drop_up 1 more_vert Ok thanks, I generally use Optuna. My understanding is, let say we are using xgboost model and we are tuning 7 hyperparameters. I know with experience people understand which features are more important. But usually hyperparameters are not linearly depended with each other. So if we have hyperparameters[a,b,c] , increasing a, decreasing b , c some constant--> increases score. Now this behavior will change on changing value of c to some other constant. So understanding relationship between all the hyperparameters is very difficult. So I generally use Optuna and let it decide which is better based on it's understanding of how hyperparameters are related to each other. But I think it is still better than Grid Search in terms of reducing no of trials. Do you have any explanation on why Optuna-found hyperparameters don't survive the change of seed. AmbrosM Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert @raj401 Optuna's understanding of hyperparameters isn't perfect, either: It just selects a hundred points in a seven-dimensional space and evaluates them with a randomized algorithm. You can hope that it finds a local optimum, but this local optimum can be far from the global one. The point which has the best score for a given seed will have a lower score when evaluated with another seed. Sharov Anton Posted 2 years ago ¬∑ 92nd in this Competition arrow_drop_up 4 more_vert Thank you very much as usual! Few questions: 1)Can you explain this statement: Cross-validate correctly: An ordinary KFold is enough for this competition. A train_test_split is not. Isn't KFold always better then train_test_split? I suppose it uses train_test_split in some determined order. I mean train_test_split can be used for one cv score, KFold is used for average cv scores. I have no clue in what scenarios train_test_split can be more useful‚Ä¶ 2) Remember not to optimize your ensemble against the public leaderboard. Optimize the cv score. But couldn't to much optimizing cv score lead to overfitting also? Rather don't push to hard with n_estimators parameter in order to get better results. 3)The most important hyperparameter is min_samples_leaf In my case: param = { \"criterion\":trial.suggest_categorical('criterion', ['squared_error', 'absolute_error','friedman_mse',\"poisson\"]), #'bootstrap':trial.suggest_categorical('bootstrap',[True,False]), 'max_depth': trial.suggest_int('max_depth', 3, 15), 'max_features': trial.suggest_categorical('max_features', [None, 'sqrt','log2']), 'max_leaf_nodes' : trial.suggest_int('max_leaf_nodes', 2, 30), 'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 150, 400), 'n_estimators':  trial.suggest_categorical('n_estimators', [  100, 250,500,1000]), 'max_samples' : trial.suggest_float('max_samples', .2, 1), 'random_state':trial.suggest_categorical('random_state', [37]), } optuna said that 'max_samples' is 0.94 of importance, while 'min_samples_leaf' is 0.03. I've used 50 trials. 4)Please, can you share code of AgeInDays target encoding? Thanks in advance and congrats once again. AmbrosM Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 4 more_vert Hi @sharovanton Yes, KFold is always better than train_test_split (except that it takes more time). I mentioned this point only because the authors of some public notebooks seem not to know. Maybe - I don't know. Optuna considers max_samples as important because setting it to 0.2 destroys everything. I consider it unimportant because the default value of 1 usually is ok. Tuning min_samples_leaf is important because the default value of 1 usually is too low. You knew that and let Optuna search only in the range between 150 and 400. If you let Optuna search between 1 and 400, min_samples_leaf will be more important. Yes. it's here . Kosh Posted 2 years ago ¬∑ 39th in this Competition arrow_drop_up 1 more_vert That point you made about optuna could not be more correct! My top submission was one where I manually tuned hyperparameters for an xgboost model, and my second highest one was the one where I just let optuna optimize my average cv score. Incredibly glad that someone took some time to put the \"illusion\" of the public LB into perspective. Manav Posted 2 years ago ¬∑ 37th in this Competition arrow_drop_up 1 more_vert Thank you @ambrosm Would you suggest not to bet on public leaderboard in general or just in this case due to sample size ? AmbrosM Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert @manavd22 In general, I'd not look at the public leaderboard unless there is a good reason to do so. Time series competitions are an example: If the training dataset consists of the years 2000 - 2019, public lb is 2020 and private lb is 2021, the public lb score can give valuable information about trends. Mahmood Bayeshi Posted 2 years ago arrow_drop_up 1 more_vert Thanks, Ensembling different models is a pretty good idea mLiammm Posted 2 years ago ¬∑ 750th in this Competition arrow_drop_up 1 more_vert Congrats! @ambrosm . Thanks for sharing :) RostislavBer Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert Hello @ambrosm , congrats on 1st place! When I've trying to solve this problem, I've guiding your solution to divers ensemble and taked several boostings models, so thank you). Best regards! Ashvanth s Posted 2 years ago ¬∑ 226th in this Competition arrow_drop_up 1 more_vert Would really love to know how to trust your CV scores , i had almost 220 place jump in the private lb yet i do find it difficult to understand where i went right and were i went wrong . It would be great of you if you are able to explain or point out to resources which helps in explaining this process in detail! Timur Talikbayev Posted 2 years ago ¬∑ 489th in this Competition arrow_drop_up 1 more_vert Thanks for the solution and your comments to it. Very helpful. Best regards Tevfik Erkut Posted 2 years ago ¬∑ 111th in this Competition arrow_drop_up 1 more_vert Hey @ambrosm , congrats on your 1st place! Got a quick question, you said you manually tuned your parameters. I think you did that by looking at different seeds, right?. My question is : why didn't you do that with optuna? Timeline was enough to use optuna with 5 different seeds, I think. And the return in optimize function will be the average of those 5 seeds. What do you think? AmbrosM Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Hi @peridon It's not about the seeds. I changed the hyperparameters manually and tried to understand the effect of every change. It takes more time than Optuna, but Optuna just gives you a set of supposedly optimal hyperparameters without any explanation. You don't know if you can trust the result of Optuna, and you don't learn anything in the process. Camilla Posted 2 years ago arrow_drop_up 0 more_vert Do you start off with the default values? How do you decide which hyperparameters to tune for a model with a large set of hyperparameters? Thanks! 3 more replies arrow_drop_down Ravi Shah Posted 2 years ago arrow_drop_up 2 more_vert Congrats @ambrosm ! Very interesting solution, thanks for sharing! Lucy Posted 2 years ago ¬∑ 341st in this Competition arrow_drop_up 2 more_vert Thank you for your solid advice! I made huge mistakes in this competition exactly like what you said in #1 and #4. #5 is an interesting one. Why and when will you decide a feature not a numerical but a categorical one when the thing is not that clear? You did say \"AgeInDays has only a few different values and its relationship to the target is highly nonlinear and not monotone. I treated it as a categorical value and target-encoded it,..\"  Why not use nonlinear regression model for a Numerical \"AgeInDays\"? So much to learn‚Ä¶    Thanks again! AmbrosM Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Hi @qiaoningchen , when I thought about the problem, target encoding came to my mind first. Maybe a nonlinear regression model would have been better. Only a cross-validation experiment can tell‚Ä¶ Raihan Zaki Posted 2 years ago ¬∑ 352nd in this Competition arrow_drop_up 2 more_vert Thanks @ambrosm . I have learned many valuable lessons here emoji_people Pasquale Posted 2 years ago ¬∑ 48th in this Competition arrow_drop_up 2 more_vert First of all congratulations and thanks because your post are always very informative and useful. I've also noticed how the optuna results are not always reliable, specially when there are a lot of trials it can happen that one trial gives a very low score compared to the others but then it's not necessary the best hyperparameter combination. I use cross validation even in optuna, that should help right? and when you say that you tuned the LGB manually do you mean that you tried different values and checked the results on cv score? PS: I also tried to use linear model with some success using logAge in place of AgeInDays and transforming some feautures to dummy feautures after reading your post about model portfolio (ended up with 12.2 cv score pre hyp optimization) but then I didn't use it in ensembling RomainBdt Posted 2 years ago ¬∑ 90th in this Competition arrow_drop_up 2 more_vert Congratulations for this new win!! Just 2 questions, do you think RepeatedKFold can do the job for hyper parameter tuning with Optuna? Why you did not used the original dataset? AmbrosM Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert @romainbdt RepeatedKFold can help Optuna. You can check the Optuna result for plausibility by slightly perturbing the hyperparameters found by Optuna. If a small perturbation of the hyperparameters leads to a big change in the cv score, you better don't trust them. And I didn't use the original dataset because it didn't improve the cv score. The code is still in the notebook: #         X_tr = pd.concat( [X_tr, original [features_used] ] , axis=0)\n#         y_tr = pd.concat( [y_tr, original [target] ] , axis=0) content_copy @kaggleqrdl Posted 2 years ago arrow_drop_up 3 more_vert You can also check the visualization plots that come with Optuna which graphically show the point above. Davis Nyagami Posted 2 years ago ¬∑ 45th in this Competition arrow_drop_up 2 more_vert I would never have thought about encoding the age variable. That's smart. Dolapo Adebo Posted 2 years ago ¬∑ 264th in this Competition arrow_drop_up 2 more_vert I had a discussion with a friend of mine about the age column, I said it feels more like a categorical column than a numerical column. Thanks for the info, will definitely implement them in my next competition. Quick one, when you add extra features to the train data, do you also do the same for the test data? if yes, then how do you make sure the same is done to the data used on the Kaggle leaderboard? AmbrosM Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Sure, @dolapoadebo , I add the same features to the train and the test data. See this line in the code: for df in [train, test ]:\n   ... content_copy Andrew Schleiss Posted 2 years ago ¬∑ 23rd in this Competition arrow_drop_up 2 more_vert Fantastic write up thank you @ambrosm I wonder if the AgeInDays encoding pushed you into first place. Very interesting take and congratulations Sergey Gusev Posted 2 years ago arrow_drop_up 0 more_vert Thank you! I am a beginner and it is very helpful! Lucy Posted 2 years ago ¬∑ 341st in this Competition arrow_drop_up 0 more_vert Hi @ambrosm , This is the question for the newest episode: Media Cost. I am asking here because I want some helps with target encoding.  The target encoding object will be 'store_sales'.  After applying MEstimateEncoder, this is what I got . Since this encoding is only done on the train_data, how about the test data? After you train the model with the target encoded variable, the trained model will have one more dimension than the test data. Then you can't use the same model to fit the test data because of the dimension conflict, can you? I am stuck here. Thank you very much! AmbrosM Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Hi @qiaoningchen , MEstimateEncoder has a fit() and a transform() method. You can do it like this: encoder = MEstimateEncoder(cols=[ 'store_sqft' ]) X_tr = encoder.fit_transform(train[features], train.cost) X_te = encoder.transform(test[features]) content_copy or (better) you put it into a pipeline: model = make_pipeline ( MEstimateEncoder (cols= [ 'store_sqft' ] , handle_unknown= 'error' ), RandomForestRegressor ()) content_copy Lucy Posted 2 years ago ¬∑ 341st in this Competition arrow_drop_up 1 more_vert Thank you for the quick response! I see, that is how test data is being encoded. Thanks again! Iqbal Syah Akbar Posted 2 years ago arrow_drop_up 0 more_vert Thanks for sharing! I noticed that you said parameter optimizer like Optuna won't survive the change of seed. Is there any way to find the optimal seed? Because it seems to be the most random out of all parameters in a model. AmbrosM Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert @iqbalsyahakbar There is no optimal seed. If the seed is good for cross-validation, it won't be good for the public lb. If it is good for the public lb, it won't be good for the private one. We have to accept the randomness caused by the seed and find architectures / hyperparameters which perform well for all seeds. See Can you find the best seed? for an experimental result. Iqbal Syah Akbar Posted 2 years ago arrow_drop_up 0 more_vert I see, so finding the \"optimal seed\" is just a fruitless effort. Thanks for the answer! The topic you posted there is also very great! Also one more thing. It seems that you do parameter tuning manually instead of relying on tools like Grid Search, Bayes Optimization, or Optuna. I actually tried doing Bayesian Optimization for another competition and while it's nice to find a parameter that gives me the best result for CV, it actually gives me more headache since it doesn't solve my overfitting problem. I end up going back on doing parameter tuning manually. issuebombom Posted 2 years ago arrow_drop_up 0 more_vert I would never have thought about encoding the age variable Daragh Thomas Posted 2 years ago ¬∑ 184th in this Competition arrow_drop_up 0 more_vert As a beginner this is very useful thank you, I'm going to bookmark this",
      "Regression with a Tabular Concrete Strength Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Concrete Strength Dataset Playground Series - Season 3, Episode 9 Regression with a Tabular Concrete Strength Dataset Overview Data Code Models Discussion Leaderboard Rules Andrew Schleiss ¬∑ 23rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 20 more_vert #12th place solutions:  My 6 step process for any competition I wanted to detail my process that I created to be reapplied for each competition or problem. It is a 6 step/ notebook process which includes all code linked: EDA Feature Creation Feature Selection (RFECV and manual selection) Single Model Tuning Multi Model CV / Multi Model : best fold Ensembling The first and foremost is an investigation of the data, I prefer doing this myself even well into the competition with hundreds of EDAs, as this gives me insights into the data without being  influence by others Step 2 is a separate notebook just for feature creation , I use a number of automated and intuitive processes here. My favorite is Symbolic Regression Transformation which tries to apply mathematical operations to the features that explain the target. Very interesting but it actually didn't work that well this episode (it did well in episode 5) Step 3 is a feature selection notebook where I take all the features created in step 2 and run recursive feature elimination on them (manually and automatically using sklearn's REFCV ) Step 4 is self explanatory, I added the features created previously and try optimize the model. I dont like Optuna as it tends to overfit and try do this manually. Step 5 I take every Tuned model and run a nested CV as well as best-in-fold CV. Similar to step 4 just with many models. Note: I try to save all the validation predictions and test predictions for later use. Also optimizing the validation predictions give a good indication of the models performance (NB dont use the training predictions for this --although I have to for Multi Model : best fold Step 6 is to apply ensembling techniques to the outputs from all the above test and validation predictions. Scipy Optimize seems to do very well here as well as calibration using linear models #### What I did differently this competition #### I tested additional features found in other top notebooks and they didn't seem to improve my CV --but I think I was wrong here What I eventually used was to group the duplicates in the trainset by each column and get a range of target values Features are then created from these target values by using them as bins The idea here is that the duplicates have differing targets so why not use the target mean for each grouping def Additional_Features ( df_in ):\n    df = df_in.copy(deep = True ) for col in df_test.drop( \"is_generated\" ,axis = 1 ).columns: \n        grp_target = df_trn[df_trn.drop(target,axis = 1 ).duplicated()].groupby(col).mean()[target] if 0 in grp_target.index:\n            bins = list (grp_target.index)+ [ max (df_trn[col])+ 1 ] else : \n            bins = [ 0 ] + list (grp_target.index) #add cols df[ f\" {col} _grp_mean\" ] = pd.cut(df[col], bins=bins, labels =grp_target.values )\n        df[ f\" {col} _grp_mean\" ] = df[ f\" {col} _grp_mean\" ].astype( 'float64' ).fillna( 0 ) return df\n\ndf_trn = Additional_Features(df_trn)\ndf_tst = Additional_Features(df_tst)\ndf_trn content_copy This improved my linear models tremendously from approx. 14 RMSE to 12 Well that's it, I hope these notebooks help someone and can be used in future competitions. Please let me know if you see any references to your code as I only recently made them public Please sign in to reply to this topic. comment 4 Comments Hotness Random Draw Posted 2 years ago ¬∑ 50th in this Competition arrow_drop_up 1 more_vert This is a useful list. Also, I  didn't know about SciPy Optimize and will check it out. Kevin Morgado Posted 2 years ago ¬∑ 513th in this Competition arrow_drop_up 1 more_vert Thanks for this valuable information @slythe . I liked how each of the steps helped to improve the overall performance! Lucy Posted 2 years ago ¬∑ 341st in this Competition arrow_drop_up 1 more_vert Thank you for taking time to share your thinking process! The approach you took is solid and grounded.üëç Tariq Mahmood Posted 2 years ago arrow_drop_up 0 more_vert Hi Dear @slythe It's great to see you are sharing your process for tackling competitions, especially one that has led to a 12th place finish! Your six-step notebook process is thorough and detailed, allowing for a methodical and comprehensive approach to feature selection, model tuning, and assembling. Your willingness to experiment and adapt your approach based on results is also commendable. Keep up sharing your valuable good work!"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 9 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Concrete Strength Prediction dataset . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 485.23 kB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 485.23 kB sample_submission.csv test.csv train.csv 3 files 21 columns ",
    "data_description": "Regression with a Tabular Concrete Strength Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Concrete Strength Dataset Playground Series - Season 3, Episode 9 Regression with a Tabular Concrete Strength Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Feb 28, 2023 Close Mar 14, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to last year's Tabular Playground Series. And many thanks to all those who took the time to provide constructive feedback! We're thrilled that there continues to be interest in these types of challenges, and we're continuing the series this year but with a few changes. First, the series is getting upgraded branding. We've dropped \"Tabular\" from the name because, while we anticipate this series will still have plenty of tabular competitions, we'll also be having some other formats as well. You'll also notice freshly-upgraded (better looking and more fun!) banner and thumbnail images. Second, rather than naming the challenges by month and year, we're moving to a Season-Edition format. This year is Season 3, and each challenge will be a new Edition. We're doing this to have more flexibility. Competitions going forward won't necessarily align with each month like they did in previous years (although some might!), we'll have competitions with different time durations, and we may have multiple competitions running at the same time on occasion. Regardless of these changes, the goals of the Playground Series remain the same‚Äîto give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. We hope we continue to meet this objective! With the great start and participation in January, we will continue launching the Tabular Tuesday in February every Tuesday 00:00 UTC, with each competition running for 2 weeks instead. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = 1 N ‚àë i = 1 N ( y i ‚àí y ^ i ) 2 where y ^ i is the predicted value and y i is the original value for each instance i . Submission File For each id in the test set, you must predict the value for the target Strength . The file should contain a header and have the following format: id ,Strength 5439 , 55 . 2 5440 , 12 . 3 5441 , 83 . 4 etc . content_copy Timeline link keyboard_arrow_up Start Date - February 28, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  March 13, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Regression with a Tabular Concrete Strength Dataset. https://kaggle.com/competitions/playground-series-s3e9, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 1,669 Entrants 782 Participants 765 Teams 6,998 Submissions Tags Beginner Tabular Regression Root Mean Squared Error Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e10",
    "discussion_links": [
      "/competitions/playground-series-s3e10/discussion/396345",
      "/competitions/playground-series-s3e10/discussion/396346",
      "/competitions/playground-series-s3e10/discussion/396261",
      "/competitions/playground-series-s3e10/discussion/396425",
      "/competitions/playground-series-s3e10/discussion/396374",
      "/competitions/playground-series-s3e10/discussion/396263",
      "/competitions/playground-series-s3e10/discussion/396319",
      "/competitions/playground-series-s3e10/discussion/396471"
    ],
    "discussion_texts": [
      "Binary Classification with a Tabular Pulsar Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Pulsar Dataset Playground Series - Season 3, Episode 10 Binary Classification with a Tabular Pulsar Dataset Overview Data Code Models Discussion Leaderboard Rules Piotr Szulc ¬∑ 1st in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 22 more_vert 1st place solution My solution is here . I used GAM, however, with two additional features created with XGB and LASSO. That is, the final model is ensemble, although I think a little less \"classic\". Importantly, these additional features were not so much important here. The GAM model was my first approach, and without these features I was getting only slightly worse results. I chose this model for a few reasons: We have very few variables that are already high-level features. Feature engineering is basically impossible, it's just a matter of finding the right relationship. Boosting is based on trees, which, in my opinion, are not able to find \"real\" relationships. Because real world relationships are rather continuous in nature --- and decision trees only approximate it. Of course, they are great and you can approximate almost anything (e.g., very high-level interactions), but the world does not work in that way. GAM has built-in regularization; it selects parameters by performing internal crossvalidation. This makes it very convenient, and I could freely add interactions and features created from the XGB and LASSO model. Please sign in to reply to this topic. comment 12 Comments 1 appreciation  comment Hotness paddykb Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 5 more_vert Beautiful Piotr. Thank you for sharing your approach. GBMs are such a heavy Kaggle default that it is instructive to see other methods do well. Smoothness & continuity FTW :) mateuszgrzyb.pl Posted 2 years ago ¬∑ 53rd in this Competition arrow_drop_up 1 more_vert Jestem pod wra≈ºeniem. Dziƒôki za udostƒôpnienie kodu. Gratulacje! Piotr Szulc Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Dziƒôki i pozdrawiam! Tariq Mahmood Posted 2 years ago arrow_drop_up -3 more_vert Hi Dear @seascape The approach  you have taken is very interesting and provides insight into thethought process in selecting the appropriate model for this problem. The use of GAM with additional features created by XGB and LASSO, and the explanation of why these features were not crucial, shows your very careful consideration of the problem and their approach to solving it. And your reason that why you chose GAM over boosting is also insightful, as it highlights the limitations of tree-based models and the importance of continuous relationships in real-world data. The use of built-in regularization in GAM also makes it a convenient choice for this problem. No doubt , your sharing provides  valuable insights into the process of selecting and combining models for solving a real-world problem, and your  success in achieving 1st place is a testament to the effectiveness of your approach.üëç Morozov Oleg Posted 2 years ago arrow_drop_up 0 more_vert Congratulations! mino Posted 2 years ago arrow_drop_up 0 more_vert Congratulations and thank you for sharing this! @seascape I am still learning about the GAM model, and I have a question about this part where you mention that you \"add interactions and features created from the XGB and LASSO model.\" I believe this line of code is the exact part: mutate(Class = predict(gam_fit, pulsar_test, type = \"response\") . Could you please explain how the \"interactions and features\" are being added to GAM? Thank you! Piotr Szulc Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert @isaaccha That's the definition of my model: formula_gam <- as.formula(Class ~ s(Mean) + s(SD) + s(EK) + s(Skewness) +\n  s(Mean_curve) + s(SD_curve) + s(EK_curve) + s(Skewness_curve) +\n  ti(SD, EK) + ti(Skewness, EK) + ti(Skewness, Mean_curve) +\n  ti(Mean_curve, SD_curve) + ti(SD_curve, Skewness_curve) +\n  s(prob_xgb) + s(prob_glm)) (from https://www.kaggle.com/code/seascape/how-to-detect-pulsars-with-gam-1st-place) . The terms ti() represent interactions and can be interpreted in a similar way to a linear regression model. The features prob_xgb and prob_glm are predictions from XGB and Lasso and are not related to GAMs, that is these features can be used in any model, not just GAM. mino Posted 2 years ago arrow_drop_up 0 more_vert I see -- thank you for the kind explanation! Helps a lot. üôè Siddharth Sah Posted 2 years ago ¬∑ 156th in this Competition arrow_drop_up 0 more_vert Thank you for sharing your solution and the reasoning behind your choice of model. Congratulations on achieving 1st place in the competition! Iqbal Syah Akbar Posted 2 years ago ¬∑ 477th in this Competition arrow_drop_up 0 more_vert This is the first time I hear GAM, I am very interested to see how it will be used on other datasets in Kaggle. I hope I can try using GAM in Python, tho I am not sure whether GAM in Python has the same quality as the one in R (and maybe I need to learn R if I want to use it in the future). I also agree with you on feature engineering. I think the features provided have already described Integrated Profile and DM-SNR curve as best as they can here. Lucy Posted 2 years ago ¬∑ 81st in this Competition arrow_drop_up 0 more_vert Congradulations! Thanks for the insights and your notebook! This comment has been deleted. Appreciation (1) Ericka42 Posted 2 years ago arrow_drop_up 0 more_vert Thank you for sharing your solution",
      "Binary Classification with a Tabular Pulsar Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Pulsar Dataset Playground Series - Season 3, Episode 10 Binary Classification with a Tabular Pulsar Dataset Overview Data Code Models Discussion Leaderboard Rules aldparis ¬∑ 3rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 18 more_vert #3 : GAM & GBM Hi, 3rd solution is a blend of XGBoost, LightGBM and Generalized Additive Models here : with from 4 to 10 folds Stratified CV with log transformation of some features with some interactions features with forward selection of interactions features and values permutations of predictions to do backward features selection and to drop useless features and to avoid overfitting. with weak learners for XGBoost and LightGBM : only 5 ou 6 leaves with overfitting control by computing difference between val_logloss and trn_logloss for each fold, to fit regularization hyperparameters in XGBoost and LightGBM, with the hands (See @ambrosm in https://www.kaggle.com/competitions/playground-series-s3e9/discussion/394592 ) hence without optuna or other optimization tools without calibration of predictions, because by CV I saw it was useless without original data. with a little mistake in final submission in diversity of models, which costs me the second place (see difference betwwen version v40 & v39 - v40 was what i wanted to do but I did v39). By computing difference between val_logloss and trn_logloss , we can see that there was less over-fitting with GAM than with GBMs. I tried RandomForest and ExtraTrees, but my CV was only 0.033. I tried to fit a LogisticRegression without sucess, I wasn't able to add interactions with polynomialfeatures, thank's @ambrosm for your solution , I'll read it carefully. Special thank's to @paddykb for GAM in this notebook and log transformation of some features. Thank's to @mateuszgrzybpl for pyGAM in python (pygam) in comments here And thank's too to @pourchot for GAM with pyGAM here Please sign in to reply to this topic. comment 4 Comments Hotness Mohammad O. Siddiq Posted 2 years ago ¬∑ 438th in this Competition arrow_drop_up 1 more_vert Why did you went through the process you went through for summing? I couldn't understand the purpose of subtracting the corresponding feature values from the maximum value and adding the minimum values aldparis Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert @mohammadsdg , Because I wanted that class 1 peaks to be always on the same side, for all features in cell n¬∞ 5, first column. Those sym_feats are just symetric of original feats, but blue peaks are now all the right side. And when sum2 is low, it means that we have more chance to have a class 1 target. Without those sym_feats, sums don't mean anything. But finally sum2 was not a very usefull feature. I've tried‚Ä¶ import pandas as pd\nimport matplotlib .pyplot as plt\nimport seaborn as sns\n\ntrain = pd .read_csv ( \"../input/playground-series-s3e10/train.csv\" , index_col = \"id\" )\ntest = pd .read_csv ( \"../input/playground-series-s3e10/test.csv\" , index_col = \"id\" )\n\ndf_stats_train = pd .DataFrame (index = test .columns , columns =[ \"min\" , \"median\" , \"max\" ]) for f in test .columns :\n    df_stats_train .loc [f] = train [f] .agg ( [ \"min\" , \"median\" , \"max\" ] ) for df in [train] : for f in [ \"EK\" , \"Skewness\" , \"Mean_DMSNR_Curve\" , \"SD_DMSNR_Curve\" ] :\n        df [f \"sym_{f}\" ] = - (df [f] - df_stats_train .loc [f, \"max\" ] + df_stats_train .loc [f, \"min\" ] )\n\n    df [ \"sum1\" ] = df [[ 'Mean_Integrated' , 'SD' , 'sym_EK' , 'sym_Skewness' ] ] .sum (axis= 1 )\n    df [ \"sum2\" ] = df [[ 'sym_Mean_DMSNR_Curve' , 'sym_SD_DMSNR_Curve' , 'EK_DMSNR_Curve' , 'Skewness_DMSNR_Curve' ] ] .sum (axis= 1 )\n    df [ \"sum3\" ] = df [[ 'sum1' , 'sum2' ] ] .sum (axis= 1 )\n\n\ncols = [f for f in list(test.columns) if f not in [ \"EK\" , \"Skewness\" , \"Mean_DMSNR_Curve\" , \"SD_DMSNR_Curve\" ] ] + [f \"sym_{f}\" for f in [ \"EK\" , \"Skewness\" , \"Mean_DMSNR_Curve\" , \"SD_DMSNR_Curve\" ] ] + [ \"sum1\" , \"sum2\" , \"sum3\" ] df_temp1 = pd .concat ( [train.loc[train[ \"Class\" ] == 1 ], train .loc [train[ \"Class\" ] == 0 ] .sample (frac=. 1 )], axis= 0 )\n\nfig, ax = plt .subplots (( len (cols)+ 1 ) //2, 2, figsize=(16,len(cols)), tight_layout = True) ax = ax .flatten () for i , f in enumerate (cols):\n    sns .kdeplot (data = df_temp1, hue = 'Class' , fill = True, x = f, legend = False, ax = ax[i])\n    ax [i] .spines [[ \"top\" , \"right\" ] ] .set_visible (False) content_copy Laurent Pourchot Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert @adaubas , Bravo ! üòâ Mar√≠lia Prata Posted 2 years ago arrow_drop_up 0 more_vert Toutes mes f√©licitations Paris.",
      "Binary Classification with a Tabular Pulsar Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Pulsar Dataset Playground Series - Season 3, Episode 10 Binary Classification with a Tabular Pulsar Dataset Overview Data Code Models Discussion Leaderboard Rules AmbrosM ¬∑ 7th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 23 more_vert #7: A diverse ensemble First of all: Thanks and congratulations to @paddykb , who came up with a really good generalized additive model (GAM) and published it so that everybody could learn from it. When I saw @paddykb 's notebook, it was clear that I wanted to understand it and then include a GAM in my ensemble. I tried to recreate the GAM in Python, but with modest success - perhaps pygam is a bad implementation, or it's simply my lack of experience. I thus decided to use the original GAM for the ensemble. The challenge was that to determine ensemble weights I needed the oof predictions. This forced my to learn enough R so that I could make @paddykb 's code save the oof predictions to a csv file. Then I applied the principles I was preaching in this post two weeks ago, optimizing eight other models for best cv score: A pipeline of PolynomialRegression(3) , logistic regression and CalibratedClassifierCV (this is the only model which needed calibration) Three HistGradientBoostingClassifier with early stopping and soft voting CatBoost with max_depth=3 GradientBoostingClassifier regularized by min_samples_leaf=1000 LightGBM with a very low learning rate XGBoost A pipeline of PolynomialRegression(2) and XGBoost with max_depth=2 and a high learning rate DART The tree models were trained with the original dataset, the GAM and logistic regression performed better without the original data. The correlation matrix shows that the seven tree-based models give similar predictions; only the GAM and the logistic regression deviate from the mainstream: Finally I chose ensemble weights which gave a good cv score. Source code is here . Please sign in to reply to this topic. comment 10 Comments Hotness mateuszgrzyb.pl Posted 2 years ago ¬∑ 53rd in this Competition arrow_drop_up 1 more_vert Congrats! Great work, and thanks for sharing. Tariq Mahmood Posted 2 years ago arrow_drop_up 1 more_vert Hi Dear @ambrosm , Really impressive work to see. Thanks for sharing some amazing visuals.üëç emoji_people JEANMPIA Posted 2 years ago ¬∑ 19th in this Competition arrow_drop_up 1 more_vert Very impressive, You mentionned recently that we shouldn't reduce our plots portfolio too much, thats what you meant ! Matt OP Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 0 more_vert Very nice work as always @ambrosm ! That's a really long execution time for LGBM. Lucy Posted 2 years ago ¬∑ 81st in this Competition arrow_drop_up 0 more_vert Very intensive study indeed! Great job! Thanks for sharing the notebook. Iqbal Syah Akbar Posted 2 years ago ¬∑ 477th in this Competition arrow_drop_up 0 more_vert Thanks for sharing! I want to ask a question: what makes you think that logistic regression with cubic pre-processing is the only model that needs calibration? AmbrosM Topic Author Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert @iqbalsyahakbar I ran all models with and without calibration. Logistic regression was the only one whose cv score was improved by calibration. The gradient boosters are well calibrated by themselves so that their score doesn't improve with calibration. Remove the calibration from the logistic regression pipeline and you'll see the difference in the calibration plot immediately. Laurent Pourchot Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Nice visualization, thank you. I noticed you did not use NN, I think it could bring more diversity as LG and GAM üòâ AmbrosM Topic Author Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 3 more_vert After studying the GAM and the R code, as well as tuning KNN, ExtraTrees, random forests, an SVM with various preprocessing steps, RBFSampler, PolynomialCountSketch, Nystroem, QuantileTransformer, PowerTransformer, I no longer had the energy for implementing an NN. Franklyn Dsouza Posted 2 years ago ¬∑ 73rd in this Competition arrow_drop_up 0 more_vert Congrats, and thanks for sharing the notebook.",
      "Binary Classification with a Tabular Pulsar Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Pulsar Dataset Playground Series - Season 3, Episode 10 Binary Classification with a Tabular Pulsar Dataset Overview Data Code Models Discussion Leaderboard Rules Matt OP ¬∑ 8th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 13 more_vert 8th Place Solution: Regression models for binary classification Thank you all for another fun and exciting Playground Series episode! I will try to quickly go over the main points of my approach. Data I used StandardScaler() for feature scaling. I did not use the original dataset in my training data as I saw a consistent performance drop across many different models. Feature Engineering At the beginning of the competition I spent a lot time experimenting with feature engineering. I thought there was definitely potential to create some interaction features between the mean, standard deviation, skewness, and kurtosis features. This included ideas like squaring the standard deviation to compute a variance feature and trying a bunch of different mathematical combinations. None of these experiments resulted in a significant increase in CV score. Then @siukeitin discovered that the Skewness and EK columns got swapped in the original data. I went back and tried the feature engineering ideas I had that included Skewness and EK again. Once again there was not a significant enough increase in CV score. The only effective idea I had was using 2nd degree PolynomialFeatures with LogisticRegression where I saw the CV go from around 0.038 --> around 0.032 . Models Here is a list of (most of) the models I used in my ensemble with their CV scores: 'XGBRegressor': 0.03049 'CatBoostClassifier': 0.03069 'XGBClassifier': 0.03070 'LGBMClassifier': 0.03073 'LGBMRegressor': 0.03076 'TFNN': 0.03177 'MLPClassifier': 0.03198 'HistGradientBoostingRegressor': 0.0323 'GradientBoostingClassifier': 0.0323 'LogisticRegression': 0.0324 'RandomForestClassifier': 0.0325 'ExtraTreesClassifier': 0.0325 With TFNN being a Tensorflow neural network. So how does XGBRegressor relatively outperform the classifiers in a binary classification problem? The answer is confident predictions. XGBRegressor would predict values outside of the 0-1 range whereas the classifiers would not be as confident (never predicting exactly 0 or 1). I believe using a regression model worked well because of the strong separation between the target classes. Something interesting about the LogLoss metric is that it does not punish predictions for being outside of the 0-1 range (even though you could just clip the predictions to fall in this range). I also used @paddykb 's excellent GAM model in my final ensemble as I theorized it would help to diversify the predictions. Additionally, I tried using CatBoostRegressor but it ended up drastically underperforming vs the other regressors. Calibration @sergiosaharovskiy suggested that probability calibration could help to prevent incorrect decisions based on the classifier's predictions, especially if those decisions are based on threshold values for the predicted probabilities. I experimented with calibration for all the models I was using. Here are the results: Better with calibration: GradientBoosting LogisticRegression ExtraTrees RandomForest SVC KNearestNeighbors Better without calibration: CatBoost XGBoost LGBM Neural networks Cross Validation At the beginning of the competition I was using StratifiedKFold due to the large class imbalance. However, at some point I experimented with using just KFold and actually saw an improvement in CV score. I am still a little puzzled by this but I think this was because of the strong separation between the target classes. That's pretty much all I have to say! Thank you to everyone once again and I hope to see you in the next episode! Binary Classification Regression Astronomy Tabular Tabular Classification Please sign in to reply to this topic. comment 1 Comment Hotness Sarmad_mueen Posted 2 years ago ¬∑ 89th in this Competition arrow_drop_up 0 more_vert Thanks for sharing your work, can you share the notebook ?",
      "Binary Classification with a Tabular Pulsar Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Pulsar Dataset Playground Series - Season 3, Episode 10 Binary Classification with a Tabular Pulsar Dataset Overview Data Code Models Discussion Leaderboard Rules John Mitchell ¬∑ 9th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 11 more_vert [9th Place] Model Predictions as Features Thanks to Kaggle for another great competition and especially to all those who participated and shared models. My approach here was broadly similar to that in the Gemstone Price competition, in that I used model predictions as additional features. However, I took a more conventional approach to generating predictions for the training set, which came simply from a cross-validation. Specifically, I took the following models: Model 1: PS S3E10, 2023 EDA and Submission by @sergiosaharovskiy v15. Model 2: PS S3E10 | Pulsar research | 0.0321 by @dmitryuarov v1. Model 3: PS3E10 EDA| XGB/LGBM/CAT Ensemble Score 0.03174 by @tetsutani v29. Model 4: PS s3e10 FLIM-FLAML thank you ma'am by @paddykb v1. Model 5: PS S3 E10 by @alexandershumilin v5. From each of these, I generated predictions for the training (via cross-validation), test and original datasets. The predictions of Model 1  are then treated as a new feature, as are each of those of Models 2-5. This provides an additional five features that I subsequently use for prediction. The five models were rerun using the expanded feature sets to generate what I call Models 1M, 2M, 3M & 4M (there was no 5M, as this turned out identical to the predictions of Model 5). I combined these models with an additional submission file (Model 1A) from Model 1, and these further five public models: Model 0: PS s3e10 GAM - Finger on the pulsaRRRRR by @paddykb v2. Model 6: PS3E10: R-GAM by @syerramilli v4. Model 7: https://www.kaggle.com/code/eamonntweedy/playground-s3e10-pulsars-gbdt-ensemble-optuna by @eamonntweedy v1. Model 8: PS3E10 : Ensemble Model Score - 0.03138 by @ashenranaweera v5. Model 9: PG3E10 | MLJAR by @andreychubin v4. Thus, if we keep count, there are now 15 models to ensemble: 0, 1, 1A, 1M, 2, 2M, 3, 3M, 4, 4M, 5, 6, 7, 8, 9. I tried this in two ways. In the Gemstone competition a simple median did well, but here I found that a very loosely fitted Boltzmann ensemble was better - this was parameterised to have model weights totalling the equivalent of about six models. Please sign in to reply to this topic. comment 5 Comments Hotness Tariq Mahmood Posted 2 years ago arrow_drop_up 1 more_vert Hi @jbomitchell Congratulations on your 9th place finish in the Model Predictions as Features competition! It's great to see how you used model predictions as additional features and combined them with other models to create an effective ensemble. Your approach and methodology are impressive, and it's clear that you put a lot of effort into this competition. Well done! John Mitchell Topic Author Posted 2 years ago ¬∑ 9th in this Competition arrow_drop_up 0 more_vert Thanks @tariqbashir ! Lucy Posted 2 years ago ¬∑ 81st in this Competition arrow_drop_up 1 more_vert Congrates! Very interesting idea!  How long did it take to get a run after you tuned everything? John Mitchell Topic Author Posted 2 years ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert I guess there's two major factors: (1) My time in debugging things; (2) Run time of the models. For (1), this competition was quicker than the gemstone one for me, probably because I had more experience of what I was doing in adapting these well-written codes. I had some format-related issues with incorporating the original data (which has no id column), and particularly in the case where the original is the prediction fold and I'm either eliminating \"add original to training data\" from the workflow or putting a suitably formatted dummy file in its place. However, this worked much more smoothly and quickly than (in my hands) in the previous competition. (2) For run times, we do need to run multiple folds to predict test, training data by folds, and original data. After that, the predictions must be incorporated as extra features before the final run predicting test again. This run is generally a little longer than the first test prediction as there are more features, how long depends completely on the model being used. In this case, some models took several hours. In practice, they only need watching for long enough to ensure they haven't crashed on launch (~5 minutes) before being left. There are also annoying cases where the model crashes on finishing, this is usually due to my failure to replace the sample_submission.csv with a suitable version for the fold I'm trying to predict. Lucy Posted 2 years ago ¬∑ 81st in this Competition arrow_drop_up 0 more_vert Thank you for your quick response!  Recently I used a GPU accelerator which made the code run faster, but still testing different features combinations with GridSearch/or Optuna for better parameters took a very long time. I wonder if you can give me some advice on how to do things more efficiently. Thanks advance! John Mitchell Topic Author Posted 2 years ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert Recently I used a GPU accelerator which made the code run faster, but still testing different features combinations with GridSearch/or Optuna for better parameters took a very long time. I wonder if you can give me some advice on how to do things more efficiently. It wasn't something I investigated as such, to be honest. Obviously some kinds of model do this implicitly by zeroing the relevant coefficient (like Lasso) or simply not selecting a given feature to split a tree (like RF). Matt OP Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert I did not have much luck with my feature engineering ideas. Very nice idea to use model predictions as additional features. Good work @jbomitchell !",
      "Binary Classification with a Tabular Pulsar Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Pulsar Dataset Playground Series - Season 3, Episode 10 Binary Classification with a Tabular Pulsar Dataset Overview Data Code Models Discussion Leaderboard Rules Sergey Saharovskiy ¬∑ 10th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 11 more_vert Top 10 Solution Congratulations to the winners of this competition! It was a really tight leaderboard. My solution: Dataset : I trained only on the synthetic data. CV: 15 Fold stratified. RFE: I have not removed any features. RFA: I have added the following features which worked - ['EK_diff_Mean_DMSNR_Curve', 'Mean_DMSNR_Curve_diff_EK', 'Mean_DMSNR_Curve_mul_EK_DMSNR_Curve', 'Mean_DMSNR_Curve_mul_Skewness_DMSNR_Curve', 'Mean_DMSNR_Curve_div_SD_DMSNR_Curve', 'SD_DMSNR_Curve_div_Mean_DMSNR_Curve'], I did that using 2 models separately and added only features that improved both models. Preprocessing: I removed the observations based on the @dmitryuarov proposal here Ensemble: 7 Ensembled XGB models across different seeds of Stratified KFold with different xgb params each time. What did not work: LGBM, NN, RF, RF with Global Refinement. What could be done differently: I did not trust the sub with @paddykb ensemble. It was giving 0.0307 private, 03101 public (straight top2). Please sign in to reply to this topic. comment 3 Comments Hotness Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert Congratulations for yours 10th place on Ep. 10. By the way, you're already doing great on Episode 11. TY for writing your solution Saharovskiy emoji_people JEANMPIA Posted 2 years ago ¬∑ 19th in this Competition arrow_drop_up 1 more_vert Congratulation ! Same happened to me, didn't submit my best solution.. Don't be mad about it, that's how it is, you did very well staying on top for all the comp Sergey Saharovskiy Topic Author Posted 2 years ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert @janmpia thank you, actually I am not, I got used to it. I am on my way. Matt OP Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Also happened to me. It's just part of the Kaggle experience! Live and learn.",
      "Binary Classification with a Tabular Pulsar Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Pulsar Dataset Playground Series - Season 3, Episode 10 Binary Classification with a Tabular Pulsar Dataset Overview Data Code Models Discussion Leaderboard Rules Ravi Ramakrishnan ¬∑ 13th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 17 more_vert #13 approach- simple ensemble with tuning Hello all, Firstly, many thanks to Kaggle for the assignment and many thanks to all the contributors for making the experience great! Special thanks to @sergiosaharovskiy , @pourchot and @paddykb for their public contributions. My approach:- Feature engineering:- a. Used multiplicative and additive features (example:- Mean_Integrated *  SD and Mean_Integrated + SD) as total features b. Selected features using mutual information, correlation and permutation importance, chose top 25, 30, 32, 35, 40, 45 features for the model (final submission contained 35 features). I also chose some features based on leaderboard probing and based on CV scores manually at times. c. Did not do any explicit outlier treatment and feature transforms like log, power transforms, logit, etc. d. Did not resort to any scaling too, based on CV e. Precluded the original data (CV score was slightly lower on model training) Models:- Used the below classifier algorithms:- a. LGBM b. XGB c. CatBoost d. Gradient Boosting e. Histogram Gradient Boosting f. Random Forest g. Logistic Regression with robust scaler h. Generalized additive model (thanks to @paddykb and @pourchot for the public work) Model tuning I tuned the parameters with optuna and perturbed the tuned parameters to check the impact on the CV score. I had to adjust some parameters based on this exercise (in LGBM and GBM classifiers). I did not tune the scale_pos_weight for this assignment. I resorted to tuning just the basic parameters as below- a. learning rate b. max_depth/ depth c. reg_alpha d. reg_lambda e. n_estimators Model training a. CV strategy:- Repeated stratified KFold 10x5 b. Early stopping rounds:- 120 Ensemble and calibration:- a. I calibrated only the GAM results using isotonic regression b. I built an ensemble using optuna (1000 trials) c. I adjusted the weights for GAM (I overweighted it manually and underweighted random forest and logistic regression for my final submission) What did not work:- a. TabNet classifier b. Neural Network- MLP c. Calibrating all probability predictions across all models d. Platt scaling for GAM calibration What I could have done better:- a. Better feature engineering - I used only multiplicative and additive features, I could have used ratio/ log/ other transforms b. I could have tuned my models in a better manner, and not rely on optuna for most classifiers Note on GAM:- From my work experience, I may posit that GAM is a great method for classification and regression problems alike. I used GAM for the first time on python in this challenge, but have used it frequently on SAS. I encourage SAS users to use PROC ADAPTIVEREG , PROC TRANSREG and PROC GAM for various assignments. Best regards and happy learning! Tabular Binary Classification Astronomy Beginner Intermediate Please sign in to reply to this topic. comment 3 Comments Hotness amj12321 Posted 2 years ago ¬∑ 47th in this Competition arrow_drop_up 1 more_vert Great work! I only tried GAM and three types of boosting methods. Calibration doesn't work for me and I didn't use any feature engineering. Maybe I should give it a try later. Thank you for sharing! Ravi Ramakrishnan Topic Author Posted 2 years ago ¬∑ 13th in this Competition arrow_drop_up 0 more_vert Calibration did not work for me too @amj12321 Ms. Nancy Al Aswad Posted 2 years ago arrow_drop_up 1 more_vert Well done You abbreviate ideas üí° and make your own summary in great way Keep progressing up @ravi20076",
      "Binary Classification with a Tabular Pulsar Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Pulsar Dataset Playground Series - Season 3, Episode 10 Binary Classification with a Tabular Pulsar Dataset Overview Data Code Models Discussion Leaderboard Rules emoji_people Mohamed Ifreen ¬∑ 19th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 11 more_vert 19th Place : Solution Approach and how we got a huge boost in score! Me and my teammate combined our solutions at the end only to receive a very huge boost in score! The following notebook details my approach to this problem statement. My Solution Approach I basically used an ensemble of 3 models (XGB,CAT,LGBM) along with hyper-parameter tuning. Adding the original dataset didn't improve my score. But the interesting thing I noticed was, Tuning the models based on the original dataset boosted my score a lot on the CV and LB. For hyper-parameter tuning, I used Optuna. I performed a 5 fold CV within the optuna to obtain the mean log loss for each selected hyper-parameters. This ensured that I was tuning based on my CV within the optuna function. Finally, I combined the models using weighted average, I tried various weights combining approach like considering the CV score, Optuna based tuning and many more. But what worked was manual selection. I had to do an exhaustive search to get the best combination of weights. What I tried but didn't work : 2 Level stacking approach :  It gave me the same CV score. So I didnt bother to complicate the model complexity and risk overfitting. Feature selection : My idea was to fetch the most important feature (EK) and combine it with all other features and to check for the resulting accuracy. It improved my CV a bit but gave a bit dip in LB, So I didnt risk the difference. KNN Approach : This was something unique I tried. Basically I got the predictions of the ensemble on the entire training data (Obviously using CV), and stored it separately in a data frame. During prediction, Once I got the predicted value using ensemble, I found the top N nearest predictions and tried averaging it with more weightage to the actual prediction. My intuition was that, If predictions are very similar, The data points must be very similar,. So I tried this approach but it didnt work out unfortunately. Probably it ended up adding more and more bias! How we got a huge Boost in Score : Basically, Me and my teammate developed our own solutions separately. His approach was more focused towards feature engineering which worked well for him. Mine was focused more on model building stage. The diversity of our models when combined gave us a huge boost in the LB. My partners solution thread : https://www.kaggle.com/competitions/playground-series-s3e10/discussion/396259 Hope I helped you guys gain some ideas for your next comp! :) Please sign in to reply to this topic. comment 7 Comments Hotness graham broughton Posted 2 years ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert I think your KNN approach didn't work because the classes were already easily seperable . To further justify that statement, just observe how the leaderboard progressed, all you needed was a decent forest model to get a fairly good score. Getting anything better than that though was a lot trickier. emoji_people Mohamed Ifreen Topic Author Posted 2 years ago ¬∑ 19th in this Competition arrow_drop_up 0 more_vert Even if the classes are easily separable, it was all about optimising the log loss this comp! So, maybe if my predictions was being too over or under confident, I thought adding bias by averaging would help. What you said makes sense too! Lucy Posted 2 years ago ¬∑ 81st in this Competition arrow_drop_up 1 more_vert Great teamwork! Thanks for the insights! emoji_people Mohamed Ifreen Topic Author Posted 2 years ago ¬∑ 19th in this Competition arrow_drop_up 0 more_vert Thank you Lucy Daragh Thomas Posted 2 years ago ¬∑ 364th in this Competition arrow_drop_up 1 more_vert That's fascinating, it would be great to hear his side of the story too. emoji_people JEANMPIA Posted 2 years ago ¬∑ 19th in this Competition arrow_drop_up 0 more_vert hey @daraghthomas , I already posted a topic about it here ! mino Posted 2 years ago arrow_drop_up 0 more_vert Hi @ifreenibrahim just curious how you guys combined into a single solution, after having two different approaches? Did you stack the models at the end? Thank you!"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 10 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Pulsar Classification . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 20.94 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 20.94 MB sample_submission.csv test.csv train.csv 3 files 21 columns ",
    "data_description": "Binary Classification with a Tabular Pulsar Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Pulsar Dataset Playground Series - Season 3, Episode 10 Binary Classification with a Tabular Pulsar Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Mar 7, 2023 Close Mar 21, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in March every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Submissions are scored on the log loss: LogLoss = ‚àí 1 n n ‚àë i = 1 [ y i log ( ÀÜ y i ) + ( 1 ‚àí y i ) log ( 1 ‚àí ÀÜ y i ) ] , where n is the number of rows in the test set ÀÜ y i is the predicted probability the Class is a pulsar y i is 1 if Class is pulsar, otherwise 0 l o g is the natural logarithm The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value. Submission File For each id in the test set, you must predict the value for the target Class . The file should contain a header and have the following format: id ,Strength 117564 , 0 . 11 117565 , 0 . 32 117566 , 0 . 95 etc . content_copy Timeline link keyboard_arrow_up Start Date - March 7, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  March 20, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Binary Classification with a Tabular Pulsar Dataset. https://kaggle.com/competitions/playground-series-s3e10, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 1,869 Entrants 820 Participants 807 Teams 6,319 Submissions Tags Beginner Tabular Log Loss Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e11",
    "discussion_links": [
      "/competitions/playground-series-s3e11/discussion/399401",
      "/competitions/playground-series-s3e11/discussion/399489",
      "/competitions/playground-series-s3e11/discussion/399463",
      "/competitions/playground-series-s3e11/discussion/399393"
    ],
    "discussion_texts": [
      "Regression with a Tabular Media Campaign Cost Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Media Campaign Cost Dataset Playground Series - Season 3, Episode 11 Regression with a Tabular Media Campaign Cost Dataset Overview Data Code Models Discussion Leaderboard Rules AmbrosM ¬∑ 1st in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 89 more_vert #1: A Zoo of Models My solution is based on the following observations: Only a subset of the features is useful, although it's not completely clear which features belong to the subset. Being in doubt which subset is the right one, we can make models for different feature subsets and blend them. After feature selection, there are lots of duplicates in the training data. We can reduce training time by grouping these duplicates. Training with 3000 groups rather than 360'000 samples speeds up the development cycle massively. If we add the original data to the training dataset, the score improves. store_sqft , which has only twenty unique values, looks like a numerical feature, but it is categorical. As most regression algorithms in the library have been developed to optimize rmse, we should make all models predict the transformed target log1p(cost) and submit expm1(pred) . A \"zoo of models\", i.e. a diverse ensemble, averages out the prediction errors of the models. Training and test data adhere to the same distribution so that we may trust the cv scores completely. Point 4 is illustrated by the following diagram. The partial dependency plot for store_sqft doesn't look like a typical regression curve - it looks like a categorical feature which has been mistaken for a continuous one. This observation suggests that we might one-hot encode or target-encode the feature. The diversity of the model zoo can be analyzed with a dendrogram. The dendrogram represents a hierachical clustering of the models where similar models end up in the same cluster. We can easily identify four clusters: four models which were trained with the additional unit_sales or store_sales feature all random forests and extra-trees models the neural network all gradient-boosting models The bar chart shows the cv scores and training times of all models and ensembles. The final submission is a weighted blend of all 18 models with the weights determined by ridge regression. More details can be found in the source code . Please sign in to reply to this topic. comment 24 Comments 4 appreciation  comments Hotness Sergey Saharovskiy Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 3 more_vert @ambrosm well done, at very end I was convinced you used ET, so I did, though I again screwed the final pick up. Warm_start ET was given the boost: JEANMPIA Posted 2 years ago ¬∑ 17th in this Competition arrow_drop_up 1 more_vert hey @sergiosaharovskiy , you beat us again ! you mind explaining briefly what ET is ? Sergey Saharovskiy Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert @janmpia I beat myself again, answering your question ExtraTreesRegressor AmbrosM Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert @sergiosaharovskiy Yes, I know that feeling after having selected the wrong submissions. Your record submission count of 71 in 14 days suggests that the public leaderboard might have distracted you from optimizing the cross-validation scores‚Ä¶ Sergey Saharovskiy Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 2 more_vert @ambrosm that‚Äôs true, I contaminated the overall thing. My cv was really good and approach was really solid, we are talking about .00002 in the end of the day ;) 4 more replies arrow_drop_down Matthijs Hogervorst Posted 2 years ago ¬∑ 431st in this Competition arrow_drop_up 1 more_vert Did you average over the predicted log1p(cost) of the different models, or over the exponentiated values? AmbrosM Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert @mhogervo I averaged over the logs and exponentiated afterwards. See these lines at the end of https://www.kaggle.com/code/ambrosm/pss3e11-zoo-of-models : def submit (test_preds, path):\n    ...\n    submission = pd. Series (np. expm1 (test_preds), index=test.index, name= 'cost' )\n    ...\n\ntest_preds = optimum_blend. predict (test_preds) submit (test_preds, 'submission_weighted.csv' ) content_copy ML_wishforall Posted 2 years ago arrow_drop_up 1 more_vert That's a surprising piece of data. Thank you for comparing various combinations of ensemble. .. and Congratulation on winning #1 Tucker Lovell Posted 2 years ago arrow_drop_up 1 more_vert Thanks for sharing, congrats on your win! Mar√≠lia Prata Posted 2 years ago ¬∑ 182nd in this Competition arrow_drop_up 1 more_vert That's a position (1st) that makes sense for you AmbrosM. Now, I'm updated with yours Zoo code. Congratulations for your first place! Bob Yelram Posted 2 years ago ¬∑ 270th in this Competition arrow_drop_up 1 more_vert Thank you for sharing this clean model comparison and clustering ! üôè shoab ahamed Posted 2 years ago ¬∑ 45th in this Competition arrow_drop_up 1 more_vert Thanks for sharing I learned a lot of new things. Congratulation on winning Jose C√°liz Posted 2 years ago arrow_drop_up 1 more_vert Excellent post @ambrosm , thanks for all your inputs and congratulations on excelling another competition. Jeshua C√©spedes Posted 2 years ago ¬∑ 242nd in this Competition arrow_drop_up 1 more_vert Thank you for sharing! This competition was enjoyable. I got to learn a lot!! Congrats! and keep up the good work! Lucy Posted 2 years ago ¬∑ 141st in this Competition arrow_drop_up 1 more_vert @ambrosm Bravo! Wow, it is a very thorough work! Well deserved as always!  I used the TargetEncode in the end which helped improve the model a bit. Thank you for your mentorship! Congratulations again! Chase Rendall Posted 2 years ago ¬∑ 18th in this Competition arrow_drop_up 1 more_vert Thanks for sharing! Congratulations on a well-deserved win. Abdenour Madani Posted 2 years ago ¬∑ 318th in this Competition arrow_drop_up 1 more_vert Thank you very much for sharing your knowledge and solution! JEANMPIA Posted 2 years ago ¬∑ 17th in this Competition arrow_drop_up 1 more_vert Always very interesting to know what you did ! I was sure you used many models again, and you were able to do it thx to the duplicates removal, thats impressive. Learned something new again here ! Ericka42 Posted 2 years ago arrow_drop_up 0 more_vert Thanks for sharing I learned a lot of new things. Congratulation on winning! Tanvir Ahmed Posted 2 years ago arrow_drop_up 0 more_vert nice work! Jagger Stephens Posted 2 years ago arrow_drop_up 0 more_vert Thank you for this awesome write-up, and congrats on the win!!! This comment has been deleted. Appreciation (4) haisei Posted 2 years ago ¬∑ 151st in this Competition arrow_drop_up 1 more_vert Thanks for sharing üòÑ Arif Hossain Posted 2 years ago arrow_drop_up 0 more_vert Thanks for the sharing ‚Ä¶. Pragyanand Sahoo Posted 2 years ago arrow_drop_up 0 more_vert Thanks for the sharing ‚Ä¶. tyro logist Posted 2 years ago ¬∑ 778th in this Competition arrow_drop_up 0 more_vert Thank you for sharing.",
      "Regression with a Tabular Media Campaign Cost Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Media Campaign Cost Dataset Playground Series - Season 3, Episode 11 Regression with a Tabular Media Campaign Cost Dataset Overview Data Code Models Discussion Leaderboard Rules Benedikt Droste ¬∑ 4th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 20 more_vert #4 solution: Feature engineering made the difference Thanks Kaggle for hosting this Playground series. These synthetic datasets allow many different experiments to be performed quickly. My solution is basically a blending of a total of 5 LightGBM (2x), XGBoost (2x) and Catboost (1x) models. Data I added the original data to my cv-process. I used the original data for the training process of each fold, but validated exclusively on the synthetic competition data: I therefore mark the original data with fold -1, so that it is not used as a validation dataset in any iteration I transformed the target variable with np.log(cost) and used rmse as objective I used a subset of the original variables: 'store_sqft', 'florist', 'salad_bar', 'prepared_food', 'coffee_bar', 'video_store', 'total_children', 'avg_cars_at home(approx).1', 'num_children_at_home' Feature Engineering I spent a lot of time on feature engineering. The most important feature was an overall score for the store-specific attributes: store_features= ['coffee_bar', 'video_store', 'salad_bar', 'prepared_food', 'florist'] df['store_score'] = df[stores_features].sum(axis=1) I also calculated the ratio in relation to the store size: df['store_score_ratio'] = df['store_sqft'] / df['store_score'] With these two features, I was able to train a single-lightgbm model, which alone had a public score of 0.2926 and a private score of 0.29326 (private leaderboard range from 9 to 37). The cv score was also much better: It was important to pass the new features to LGBM as categorical variables For XGBoost and Catboost, I was not able to get this significant improvement. I formed another feature for this and did not include the ratio: (df['florist']*3) + (df['food_proxy']*2) + df['coffee_bar'] + df['video_store'] df['food_proxy'] is the sum of prepared food and salad bar and then captured at 1 and I have passed the feature as a numeric feature. The feature engineering was the biggest boost for me compared to the public solutions Ensembling Simple weighted blend optimized with Optuna What didn't work Blending with multiple models and different feature subsets Stacking was worse for me with each variant I tried to clean up the inconsistent values of the store features based on the original data, however the results got worse that way Models other than LGBM, XGB, and Catboost were significantly worse and didn't add value when blending either Thank you all for the interesting competition. I am looking forward to the next parts of the playground series. Please sign in to reply to this topic. comment 4 Comments Hotness JEANMPIA Posted 2 years ago ¬∑ 17th in this Competition arrow_drop_up 1 more_vert Very inspiring, I always like to find new features than people didn't think of but I skipped simple ones too fast. lesson learned for future comps ! Ravi Ramakrishnan Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 2 more_vert You will get the merchandise in my opinion. This happened to me last year. I finished 4th and received my prize as one among the top 3 had won it earlier @benbla Congrats @benbla and happy learning!! Benedikt Droste Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Great to hear :) thanks for your feedback. Priyanshu54200 Posted a year ago arrow_drop_up 0 more_vert feature engineering is best",
      "Regression with a Tabular Media Campaign Cost Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Media Campaign Cost Dataset Playground Series - Season 3, Episode 11 Regression with a Tabular Media Campaign Cost Dataset Overview Data Code Models Discussion Leaderboard Rules Ravi Ramakrishnan ¬∑ 7th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 15 more_vert # 7 approach| Hand-picked features and simple ensemble Hello all, Firstly, I wish to thank Kaggle for this episode of the ongoing playground challenge season. Also, I wish to thank all the participants who contributed regularly and made this an interesting and rewarding experience. I wish to extend since congratulations to @paddykb and @janmpia for their recent progression to the kernels master tier as well and shall thank them specifically for their valuable contributions throughout the past 2 weeks. My approach could be summarized as a ridge ensemble of tree-based and GAM models using features selectively in each base model . This is illustrated as below- Feature engineering - The training set was incredibly noisy and had lots of quasi-duplicates too. I expected lots of churn while doing my initial EDA and hence, decided to approach the problem by using an ensemble of several models with different feature choices. I realized that this approach had a positive impact on my CV and my public leaderboard position too and decided to use this throughout the competition. Thanks to @janmpia , I tried and used the mean features too in some of my base models. Please peruse his work (kernel and discussions) to know more in this regard- https://www.kaggle.com/competitions/playground-series-s3e11/discussion/399393 I retained store square feet, cars at home and florist in all my models and hand-picked other selectively for all my base models. Each base model choice had 4-6 features, including the above 3 features always and 1-3 features from the remaining ones. I did not do any further feature transforms/ scaling Target- I used np.log1p(train.cost) and an RMSE metric in my regression analysis. I think this is easier than formulating a custom metric and objective in my models While training the model, I inverted the predictions to facilitate np.expm1(preds) to get back the predictions in the desired format Base Models- I used 20+ base models, comprising of hand-tuned XGB, LGBM, Catboost and GAM regressors in total with hand-tuned parameters, mainly adjusting the learning rate, max_depth, reg_alpha and reg_lambda. I also used early stopping while fitting the model too. I used R to prepare my GAM. My code is similar to @paddykb . Thanks to him for the work in the current assignment too. My CV strategy was a simple target encoding, with 10-20 target bins with a pd.qcut(train.cost). Some of my base models were built on 10 bins while others on 20 bins. I used a 10x3 repeated stratified k-fold thence to design my CV using the target bins to build the groups. I used the original data in all folds entirely to build the model training fold, while the evaluation (dev-set) was the corresponding competition fold. Using the original data in this manner improved my CV score and my public leaderboard position quite well Ensemble- My ensemble meta layer was a simple ridge regression (5000 iterations, random_state = 42) to merge the predictions. I used an optuna ensemble too at the start of the assignment, but ridge regression outperformed optuna and was used for the final submission Models that did not work- TabNet regressor Neural networks - I should have focused on my features more to make this work. Linear base models What I could have done better- I could have used quasi-duplicates to create a sample weight like the winning solution and this would have helped me reduce my training time. I am perplexed how I missed out on this obvious point Better feature engineering- I could have tried better secondary features I could have used ensemble models like extra-trees and random forest. I did not use them throughout and relied on the GBM based tree models only My key learnings and takeaways- From the winning solution, I take away the importance of using sample weights. I know this and have used it before, I just did not use it this time. I will be careful to adopt this going ahead whenever feasible I should be open to trying a wider model suite rather than relying on a limited choice of ensemble trees for most work R is a powerful language for such tabular assignments, I should improve my R skills and use it more going ahead for such assignments Finally, wishing all of you the best and see you in the next episode! Happy learning and warm regards! Tabular Marketing Regression Please sign in to reply to this topic. comment 4 Comments Hotness Shashwat Raman Posted 2 years ago ¬∑ 17th in this Competition arrow_drop_up 1 more_vert Congratulations for the amazing result!! @ravi20076 Trusting the cv score did help :) Ravi Ramakrishnan Topic Author Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 0 more_vert Yes @shashwatraman , I have had several episodes of a free fall on Tuesday morning this year‚Ä¶ It is very difficult to trust the CV and sometimes ignore the public leaderboard, especially when one slides down the ranking. I think this time, my idea of trusting the CV and nothing else prevailed.. Thanks to you and your team for the constant contribution throughout the past 2 weeks! Best regards and happy learning! JEANMPIA Posted 2 years ago ¬∑ 17th in this Competition arrow_drop_up 1 more_vert Thank you for your kind word ! Your participation is also appreciated. Didn't know what sample weight was, very nice information that I believe @ambrosm also used, sounds like it's important. Ravi Ramakrishnan Topic Author Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 0 more_vert Sample weight and scale pos weight are used extensively in tree models and even otherwise (that support this). I have used them a lot in imbalanced classifiers till date. We use the parameter to overweight certain elements in the data that could elicit class imbalances and quasi-duplicates. As indicated in the winning solution, this is an array that attributes relative importance to every row element in the training data (1 being the standard and higher numbers being elements of higher importance). Hope this helps @janmpia",
      "Regression with a Tabular Media Campaign Cost Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Media Campaign Cost Dataset Playground Series - Season 3, Episode 11 Regression with a Tabular Media Campaign Cost Dataset Overview Data Code Models Discussion Leaderboard Rules JEANMPIA ¬∑ 17th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 17 more_vert 17th place solution | Hand-Tuning and features First, our key method was to take 3 different approaches without telling each other what was our plans and then ensemble. Like this: . On this topic, I'll only tackle my solution, which you can find a good part here . The ensemble has been dealt with by @shashwatraman @shashwatraman solution : https://www.kaggle.com/competitions/playground-series-s3e11/discussion/399485 @ifreenibrahim solution : https://www.kaggle.com/competitions/playground-series-s3e11/discussion/399438 Global Idea: My main focus for this model was the hyperparameters of my models, and to make sure my CV was rebust enough to allow for such search, without overfitting either the train or even the CV valid set. For the tuning part, I did it all by hand so I can't give you a go-to guide on optuna, I beleive it is too expensive in terms of GPU time for poor improvement unpon a certain point. Hand tuning works best for me and you also learn a great deal about the model's architecture. Features: As many have pointed out, not all features were relevent, my selection was: FEATS = [\"total_children\", \"num_children_at_home\", \"avg_cars_at home(approx).1\", \"store_sqft\", \"coffee_bar\", \"video_store\", 'florist',\"prepared_food\"] I also created some features for my catboost model which are the following: concat_train_hold_test = pd.concat([train,hold,test],ignore_index=True) for feature in INIT_FEATS: if feature in [ 'units_per_case' , 'store_sales(in millions)' , 'total_children' ]:\n        avg_df[ f'avg_ {feature} ' ] = concat_train_hold_test.groupby( 'store_sqft' )[feature].mean()\n        avg_df_test[ f'avg_ {feature} ' ] = concat_train_hold_test.groupby( 'store_sqft' )[feature].mean()\n        avg_df_hold[ f'avg_ {feature} ' ] = concat_train_hold_test.groupby( 'store_sqft' )[feature].mean()\n        avg_df_original[ f'avg_ {feature} ' ] = concat_train_hold_test.groupby( 'store_sqft' )[feature].mean()\n\n        CAT_FEATS.append( f'avg_ {feature} ' ) content_copy You might be wondering what that code does and whats the purpose of those features, check out this topic . Models: XGB CAT LGBM Thats it for my solution ! Kindly check out my mates solution once they are posted ! Please sign in to reply to this topic. comment 5 Comments Hotness Priyanshu54200 Posted a year ago arrow_drop_up 0 more_vert greate , nice work SOUMENDRA PRASAD MOHANTY Posted 2 years ago arrow_drop_up 0 more_vert Very effective work @janmpia . Excited to read the Solution from our rest team members @shashwatraman , @ifreenibrahim JEANMPIA Topic Author Posted 2 years ago ¬∑ 17th in this Competition arrow_drop_up 0 more_vert @ifreenibrahim has posted his recently and @shashwatraman is making is as we speak Jeshua C√©spedes Posted 2 years ago ¬∑ 242nd in this Competition arrow_drop_up 0 more_vert Really appreciate that you have shared this information so we can keep learning from our most experienced peers. JEANMPIA Topic Author Posted 2 years ago ¬∑ 17th in this Competition arrow_drop_up 0 more_vert Glad its helpful"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 11 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Media Campaign Cost Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 49.83 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 49.83 MB sample_submission.csv test.csv train.csv 3 files 35 columns ",
    "data_description": "Regression with a Tabular Media Campaign Cost Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Tabular Media Campaign Cost Dataset Playground Series - Season 3, Episode 11 Regression with a Tabular Media Campaign Cost Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Mar 20, 2023 Close Apr 4, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in March every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Root Mean Squared Log Error (RMLSE) Submissions are scored on the root mean squared log error (RMSLE) (the sklearn mean_squared_log_error with squared=False ). Submission File For each id in the test set, you must predict the value for the target cost . The file should contain a header and have the following format: id ,cost 360336 , 99 . 615 360337 , 87 . 203 360338 , 101 . 111 etc . content_copy Timeline link keyboard_arrow_up Start Date - March 20, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  April 3, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Regression with a Tabular Media Campaign Cost Dataset. https://kaggle.com/competitions/playground-series-s3e11, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 2,387 Entrants 1,007 Participants 952 Teams 7,443 Submissions Tags Beginner Tabular Regression Mean Squared Log Error Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e12",
    "discussion_links": [
      "/competitions/playground-series-s3e12/discussion/402403",
      "/competitions/playground-series-s3e12/discussion/402416",
      "/competitions/playground-series-s3e12/discussion/402398",
      "/competitions/playground-series-s3e12/discussion/402347"
    ],
    "discussion_texts": [
      "Binary Classification with a Tabular Kidney Stone Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Binary Classification with a Tabular Kidney Stone Prediction Dataset Playground Series - Season 3, Episode 12 Binary Classification with a Tabular Kidney Stone Prediction Dataset Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules Tanoi ¬∑ 5th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 37 more_vert #5 | Beginner's luck Hello Kaggle, This is my first competition and happily finished fifth, even if I highly suspect there's some beginner's luck here. Also, as discussed elsewhere, the private LB may not reflect well CV scores. A fellow kaggler published here despite a high CV score of 0.841 , he ranked 182 on the private LB, and as we'll see later my CV score wasn't that high. Helpful posts to check out The following posts were really helpful and I learnt a lot of stuff through them, especially how to CV properly, when to use train_test_split vs KFold and the importance of trusting CV scores. I highly recommend them to all beginners who want to build a solid foundation for playground challenges. https://www.kaggle.com/competitions/playground-series-s3e12/discussion/401113 https://www.kaggle.com/competitions/playground-series-s3e12/discussion/399412 https://www.kaggle.com/competitions/playground-series-s3e12/discussion/401344 https://www.kaggle.com/competitions/playground-series-s3e12/discussion/400837 https://www.kaggle.com/competitions/playground-series-s3e12/discussion/400152 My strategy My complete notebook, as I submitted it, is available here . My strategy for this competition was to select a few basic models, finetune them with optuna using RSKfold, have a look at each feature's importance and drop non important important features. Following this, I checked CV scores to make sure they were higher without non important features. I did this for a random forest, xgboost and logistic regression and also added @ambrosm 's KNN model . I then built a soft voting ensemble classifier which showed a CV score of around 0.817 if I remember correctly, which highlights once again the limits of the private LB. To be noted, the standalone RF model had a slightly better CV. So I submitted predictions from both. All in all, as you can see, I don't feel the fifth place is deserved, but this is still rewarding and good for motivation. I'd be highly thankful if you have any comments or suggestions about the way to go forward from here. Cheers Please sign in to reply to this topic. comment 10 Comments 1 appreciation  comment Hotness Nicola Di Cicco Posted 2 years ago arrow_drop_up 1 more_vert Very interesting. Thanks especially for all the useful resources. Smit0505 Posted 2 years ago arrow_drop_up 1 more_vert Nice and it's a very great work Mario Bianchi Posted 2 years ago ¬∑ 472nd in this Competition arrow_drop_up 2 more_vert Congrats, thank you for sharing your solution! Sasidhar77 Posted 2 years ago arrow_drop_up 0 more_vert Is there any notebook to run above solution moth Posted 2 years ago arrow_drop_up 2 more_vert I should probably start using Optuna, congrats! serangu Posted 2 years ago arrow_drop_up 2 more_vert @antoinerogeau good result for the first competition! Congrats! Ol√∫bini Posted 2 years ago ¬∑ 603rd in this Competition arrow_drop_up 0 more_vert Congrats my friend! waticson Posted 2 years ago arrow_drop_up 0 more_vert Congrats @antoinerogeau Thank you for sharing helpful posts! sam alavi Posted 2 years ago arrow_drop_up 0 more_vert Nice Solution Appreciation (1) alan Posted 2 years ago arrow_drop_up 0 more_vert Thanks it's very helpful",
      "Binary Classification with a Tabular Kidney Stone Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Binary Classification with a Tabular Kidney Stone Prediction Dataset Playground Series - Season 3, Episode 12 Binary Classification with a Tabular Kidney Stone Prediction Dataset Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules Donato Riccio ¬∑ 8th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 11 more_vert #8 Solution Hey guys, it was probably luck too, but I placed in the top 10. Here is what I've done: Feature engineering creating two new datasets ( notebook for details) On these new datasets I've created two stacking models by taking the OOF predictions (notebook) Stack 1: LGBM, Gradient Boost, CatBoost, Random Forest. Stack 2 : KNN, Logistic Regression, XGB, AdaBoost, ExtraTrees Both used Logistic Regression as level 1 model. Average between the predictions of the two stacks In the end, it was a very complex model, but I think the main advantage was given by the feature engineering process. My best tip for this competition (and for every other one) is to ignore the public LB score, focus on your CV score . In my case it was a simple 10-fold CV. Please sign in to reply to this topic. comment 3 Comments Hotness serangu Posted 2 years ago arrow_drop_up 1 more_vert @donatoriccio thank you. I can say - very interesting notebook you have. Sarmad_mueen Posted 2 years ago ¬∑ 800th in this Competition arrow_drop_up 0 more_vert Thank you, can you clarify this (focus on your CV score), how does Thank you, can you clarify this ( focus on your CV score ), how this is works if my score is lower than others in the public leaderboard? John Draper Posted 2 years ago ¬∑ 479th in this Competition arrow_drop_up 0 more_vert Congrats on the high rank.  Sounds like you earned it with your double stack average strategy!",
      "Binary Classification with a Tabular Kidney Stone Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Binary Classification with a Tabular Kidney Stone Prediction Dataset Playground Series - Season 3, Episode 12 Binary Classification with a Tabular Kidney Stone Prediction Dataset Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules Kirderf ¬∑ 24th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert #24 solution ‚Äì Xdata + XGB + LGBM + CAT + Opt roc curve Here comes a short summary of the 24th place solution. Data Used the competition data + the extra org. data available with no reduction, with only scaling as Feature Engineering. Models As it was not an imbalanced target, I used 8 folds training and added LGBM, XGB and CATBoost as models with some before good known parameters for similar problem. Used balanced and weighted settings of the target for the models, to get the target perfect balanced. After every finished trained model per fold it also did a calibration of the roc curve based on the validation set before predict the probability. Postprocessing Run a optimizing of the best weighted value for the three models and also optimizing of the best power average value. Conclusions Noticed a very low validation score in one of the folds. With some more analyzing maybe cleaning and filtering of that data could help the final score. I also tried some other massive feature engineering and models but I picked the solution based in the CV score not the public, it ended up to be a good choice. That‚Äôs it! Happy Kaggling! üòä Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Binary Classification with a Tabular Kidney Stone Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Binary Classification with a Tabular Kidney Stone Prediction Dataset Playground Series - Season 3, Episode 12 Binary Classification with a Tabular Kidney Stone Prediction Dataset Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules Oleksii Zhukov ¬∑ 28th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 4 more_vert # 28 Solution | Problem with private LB score Hi Kaggle, First and foremost, I would like to express my gratitude to all the participants who took part in this competition. I am thrilled to have secured a top 3% position in this playground series, especially considering that this is only my second time participating in such an event. I must acknowledge that my success is attributed to the valuable insights and knowledge-sharing that took place during the competition. I have learned a great deal from these discussions and contributions, and I am truly appreciative of the efforts made by fellow Data Scientists in enhancing our collective education. I would like to give special thanks to these notebooks: https://www.kaggle.com/code/tetsutani/ps3e12-eda-ensemble-baseline https://www.kaggle.com/code/mmellinger66/ps3e12-eda-xgb-cat-lgbm https://www.kaggle.com/code/johnycooly/random-forest-with-optuna https://www.kaggle.com/code/klyushnik/episode-12 https://www.kaggle.com/code/sujithmandala/playground-s3e12-eda-logistic-regression And also this discussion topic: https://www.kaggle.com/competitions/playground-series-s3e12/discussion/401188 My Best Subbmited Solution I think, like many others, my best private score remained unsubmitted, but I will write about this later in this topic. Now I want to discuss the decision that led me to 28th place. Feature Engineering For the most part I ended up using feature engineering from this notebook: https://www.kaggle.com/code/tetsutani/ps3e12-eda-ensemble-baseline/notebook I also added some binary features like hydration status , osmo_status and calc_status The part with outlier removal was also very important and it played a huge role in my understanding of public score Models During competition I was trying differend combinations of models ensembles and in this solution I used weighted ensemble of optimized with optuna XGBoost, CatBoost, LGBM, GradientBoosting, Logistic Reggression, RandomForest, SVC and KNN. I also used 10 kfold splits to train models. While the dataset is very small 10 kfolds may seem like too much, but I have tested different values like 3, 5, 7, 8 and 12 and 10 seemed the most optimal for me. Problem I found with LB scoring In total, I made 26 submissions for this competition, most of which were aimed at optimizing the model ensemble by trying different combinations of models. Whenever I attempted to change something with feature engineering, the score always fell (not including the outlier part). As a result, most likely due to my inexperience in competitions, I reached a point where I could no longer achieve a higher score than the one I had already received. Although theoretically, it seemed to me that some solutions should perform better because I took more care to prevent overfitting and ensured that the model performed better on cross-validation. Despite my efforts, I could not surpass 140th place in the top public scores. However, I noticed something strange when I started experimenting with the IQR method for outliers and tried different k values. I assumed that most participants are likely to overfit their models for the public leaderboard, even though it only accounts for 20% of all data. Therefore, I decided to choose the most optimal solutions for my final submission. It turned out that my assumptions were still not quite correct. I had a solution that was similar to the one described above but used fewer models. I excluded KNN and GradientBoosting and used 0.3 and 0.7 quantiles instead of 0.25 and 0.75. This decision resulted in a modest score on the public leaderboard, but would have brought me to 16th place in the current top. Conclusion As a result, although I have some assumptions, but I am still not completely sure what caused such a result in this competition. However, in any case, I'm happy with the result, I'm glad that I participated and learned a lot for myself, and thanks to everyone again for the wonderful experience and see you in the next competitions. Please sign in to reply to this topic. comment 2 Comments Hotness Dinesh Posted 2 years ago ¬∑ 106th in this Competition arrow_drop_up 2 more_vert For most of playground competitions, hypertuning + ensemble is the way to go. In optuna, did you hypertuned all params or only few params? Did you considered multiple vesion of same algo i.e. one with default params, another with tuned params and then take mean? Could you please elaborate more on ensemble? Oleksii Zhukov Topic Author Posted 2 years ago ¬∑ 28th in this Competition arrow_drop_up 1 more_vert In optuna I hypertuned three main models, which are CatBoost, XGB and LGBM and also tuned models weights for the final ensemble. For the rest, like Random Forrest, or Logistic Regression, models which performed decently, I tuned them seperatly and only their main params, and it was without optuna, using crossval and others people notebooks. I also tried to pretune each model seperatly on simple train-val split and then use their params in ensemble but it didn't work well with the public score. But, as I learned after, it actually could work. One of my subbmissions, when I pretuned CatBoost, XGB and LGBM on simple split and then used their params for ensamble got 0.77413 private score which is actually higher then what I have in my final submission. Finaly, models like GradientBoosting and KNN i almost didn't tune at all, and used them with deafault params, sometimes trying to increase their performance by choosing higher n_estimators for Gradient and higher k in KNN, but it didn't make a big difference. As fot the technique you described in which as I understand you train one version of the algorithm with default parameters and another version with tuned parameters. Then, you would use these models to make predictions on a new dataset, and take the average of their predictions to obtain the final prediction. That is an interesting idea, and to tell you the truth I have never used it before, due to my inexperience, but now I will know about this option, so thank you. I hope I have answered your question well @dineshydv ."
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 12 NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook . The dataset for this competition (both train and test) was generated from a deep learning model trained on the Kidney Stone Prediction based on Urine Analysis dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 25.56 kB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 25.56 kB sample_submission.csv test.csv train.csv 3 files 17 columns ",
    "data_description": "Binary Classification with a Tabular Kidney Stone Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Tabular Kidney Stone Prediction Dataset Playground Series - Season 3, Episode 12 Binary Classification with a Tabular Kidney Stone Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Apr 4, 2023 Close Apr 18, 2023 Description link keyboard_arrow_up NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook . Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in April every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each id in the test set, you must predict the probability of target (likelihood of the presence of a kidney stone). The file should contain a header and have the following format: id ,target 414 , 0 . 5 415 , 0 . 1 416 , 0 . 9 etc . content_copy Timeline link keyboard_arrow_up Start Date - April 4, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  April 17, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Binary Classification with a Tabular Kidney Stone Prediction Dataset. https://kaggle.com/competitions/playground-series-s3e12, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 2,507 Entrants 1,108 Participants 1,088 Teams 8,778 Submissions Tags Beginner Tabular Binary Classification Custom Metric Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e13",
    "discussion_links": [
      "/competitions/playground-series-s3e13/discussion/406433",
      "/competitions/playground-series-s3e13/discussion/407829",
      "/competitions/playground-series-s3e13/discussion/406409",
      "/competitions/playground-series-s3e13/discussion/406812",
      "/competitions/playground-series-s3e13/discussion/406313",
      "/competitions/playground-series-s3e13/discussion/406395",
      "/competitions/playground-series-s3e13/discussion/406366",
      "/competitions/playground-series-s3e13/discussion/405616"
    ],
    "discussion_texts": [
      "Classification with a Tabular Vector Borne Disease Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Classification with a Tabular Vector Borne Disease Dataset Playground Series - Season 3, Episode 13 Classification with a Tabular Vector Borne Disease Dataset Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules Umar ¬∑ 1st in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 32 more_vert Surprisingly #1, Public LB: 0.37196 | Private LB: 0.53179 First of all, I would like to thank Kaggle for organising this Playground Series. It was such a nice experience for a competition beginner like me. To be honest, my main aim in joining this competition was to learn how to use autoencoders within an ensemble model. Practically, it was heavily inspired by this legendary thread . The second aim is to get the merchandise :D. Therefore, I understand that I did not put a lot of effort into pre-processing the data, and could say that I was very lucky in this competition. I joined this competition a bit late, so I was only able to produce 4 types of models. No feature engineering, all features being used were scaled using a standard scaler. Model Name Public LB Private LB Notes LightGBM 0.31677 0.41337 Nothing fancy, just a simple LightGBM model with default parameters Neural Network 0.33995 0.44078 Simple NN using 64-64relu-32relu-11softmax as the layers. The first 64 is for the input layer = num of features on the dataset Autoencoder 0.37196 0.46052 The autoencoder uses bottleneck architecture 64-64relu-32relu-16relu-32relu-64linear. Take the encoder part (up until the 16relu), freeze it, and add 16relu-11softmax on top of it Ensemble 0.35871 0.53179 This is a simple averaging ensemble model from the previous three models. Below this table is the explanation. Each of the models would be able to generate each class' probability. I thought that maybe by averaging each class' probability on each model (for example, averaging the probability of \"Malaria\" from LightGBM, Neural Network, and Autoencoder), I can somewhat make an educated guess. I assumed that if something is very convincing for most models (let's say, Neural Network and Autoencoder are convinced that the outcome is Malaria), but not that convincing for the other (maybe Dengue for LightGBM), I would still choose the majority vote here. Therefore, what I did was to average the probabilities and get the top 3 probabilities after averaging them. My notebook is a mess and clearly lacks any explanation, so I am sorry that I could not post the notebook publicly. I was thinking of doing stacking as well but I haven't managed to get my code working, so I stopped here. I am very open to discussion and would like to hear any thoughts from you. Have a great week ahead! Also, I would like to thank @belati and @mpwolke . You guys being very active in this competition has encouraged me to keep trying! Please sign in to reply to this topic. comment 17 Comments Hotness JRY321 Posted 2 years ago ¬∑ 70th in this Competition arrow_drop_up 5 more_vert Congrats! Haven't thought about using an autoencoder, gona try it out now Umar Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thank you! Yes I think it's interesting to use autoencoder. Initially, I was sceptical because of the small number of data that we have. Thus, I tried to mitigate it by using a small neural network architecture too. PietroAC Posted 2 years ago arrow_drop_up 3 more_vert Thanks for sharing! Umar Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert You are welcome! John Draper Posted 2 years ago ¬∑ 359th in this Competition arrow_drop_up 2 more_vert Brilliant, and \"why didn't I think of that!\".  Very kind of you to share your secret sauce. Umar Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thank you! If anything, it's not entirely my secret sauce. I just used the inspiration from Michael's post and came up with a new cuisine :) JMAscacibar Posted 2 years ago ¬∑ 99th in this Competition arrow_drop_up 2 more_vert Great work! It seems that the combination of the Neural Network and Autoencoder makes the Ensemble model quite powerful. Thank you for posting your solution. Umar Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert You are welcome! I agree with you, though I still have not explored which part of the models actually improved the score significantly. My current hypothesis is that I used a soft-voting approach for the ensemble part, which theoretically can give better performance than hard-voting. JMAscacibar Posted 2 years ago ¬∑ 99th in this Competition arrow_drop_up 1 more_vert I also used soft voting in my voting classifier and felt the same way. I'm really looking forward to seeing your notebook if you decide to publish it. As someone who's new to the field, I find your approach very creative and inspiring. Keep going mate! SOUMENDRA PRASAD MOHANTY Posted 2 years ago arrow_drop_up 2 more_vert @mufathurrohman , congrats on this achivement Umar Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thank you! Ivan Isaev Posted 2 years ago arrow_drop_up 2 more_vert @mufathurrohman congratulations!üéâ Interesting approach! Thanks for sharing!üôÇ Umar Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thank you! Yes indeed it is interesting. I first learned about representation learning from Michael Jahrer's post, and have been trying to implement it whenever I got the chance. Feels like it has a lot of potential to be explored. shoab ahamed Posted 2 years ago ¬∑ 203rd in this Competition arrow_drop_up 2 more_vert Congrats on being first. Did you use only the competition data or mix of competition and original data to train the models Umar Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Thank you. I only used the competition data here and did not include any original/external dataset VYSHAKH G NAIR Posted 2 years ago ¬∑ 354th in this Competition arrow_drop_up 0 more_vert Congrats on the achievement and Thanks for sharing your solution! Sandy Posted 2 years ago arrow_drop_up 0 more_vert can you share code ? This comment has been deleted.",
      "Classification with a Tabular Vector Borne Disease Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Classification with a Tabular Vector Borne Disease Dataset Playground Series - Season 3, Episode 13 Classification with a Tabular Vector Borne Disease Dataset Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules rjZhang97 ¬∑ 2nd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 13 more_vert #2 solution 0.52521 After returning from my holiday, I was pleasantly surprised to find I won second place on the private leader board! This was my first competition, and it was an awesome experience. I would like to thank Kaggle for organizing this series of competitions, which have been super helpful for us newcomers. Also, thanks to everyone who shared their code, solutions, and experiences - you guys inspired me! Getting back to the main point, I initially wanted to share my code, but I found it to be so messy that I didn't even know where to start organizing it. So, I've decided to share some tricks that I use. I didn't focus much on the model, just went with XGB, and used StratifiedKFold while training. Here are my parameters: XGBClassifier(objective = 'multi:softprob',\n                           tree_method = 'exact',\n                           colsample_bytree = 0.6,  \n                           gamma = 0.8,  \n                           learning_rate = 0.01, \n                           max_depth = 6,\n                           min_child_weight = 3, \n                           n_estimators = 300, \n                           subsample = 0.6) StratifiedKFold(n_splits = 4, random_state = 42, shuffle = True) In terms of feature engineering, I first borrowed @sergiosaharovskiy 's clustering method for symptoms within the same category. And, I combined every pair of features and performed AND, OR, and XOR operations, generating over 6000 new features in the process. The reason I did this is because I think different people might have different symptoms for the same illness. For instance, when I have a cold, I don't experience a sore throat but do have a headache, while others might have the complete opposite or both symptoms. Also, the features provided in the data are in 'one hot' format, and since I'm not well-versed in medicine, I went with the rather simple method mentioned above. Next, I carried out the feature selection process. Starting with the original features as the initial set, I added the newly created features and used the model's performance(MAP@3) as a criterion to filter out the following additional features: 'cluster_0', 'cluster_1', 'cluster_2', 'cluster_3', 'weakness_or_yellow_skin', 'jaundice_or_abdominal_pain', 'weakness_or_yellow_eyes', 'stomach_pain_or_abdominal_pain', 'back_pain_or_yellow_skin', 'toenail_loss_xor_bullseye_rash', 'weakness_or_light_sensitivity', 'yellow_skin_or_prostraction', 'coma_or_yellow_skin', 'inflammation_or_light_sensitivity', 'weakness_or_urination_loss', 'weakness_or_slow_heart_rate', 'abdominal_pain_or_irritability' Finally, train and submit. That's all for my work. During the competition, I also tried some feature selection methods, such as Sklearn's SelectKBest and the Boruta-SHAP package, but their performance on this dataset was not as good as the method I used above. Additionally, I've been focusing on researching various feature selection techniques recently, so I'm very eager to discuss them with all of you. If you have any great suggestions, please be sure to let me know! Please sign in to reply to this topic. comment 5 Comments Hotness Yusuph Mustapha Ladi Posted 2 years ago ¬∑ 498th in this Competition arrow_drop_up 0 more_vert Great work InDisQualified Posted 2 years ago ¬∑ 272nd in this Competition arrow_drop_up 0 more_vert Generating over 6000 new features by combining AND, OR, and XOR operations within clusters is really valuable approach in terms of feature engineering. rjZhang97 Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 0 more_vert Thank you! This comment has been deleted. This comment has been deleted.",
      "Classification with a Tabular Vector Borne Disease Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Classification with a Tabular Vector Borne Disease Dataset Playground Series - Season 3, Episode 13 Classification with a Tabular Vector Borne Disease Dataset Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules Unworried1686 ¬∑ 3rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 19 more_vert #3 solution 3rd place solution Playground Series - Season 3, Episode 13 I want to express my gratitude to the organizers at Kaggle for putting together the Playground Series. As a beginner, participating in these competitions can be very challenging but they can teach valuable skills. I would also like to give a special thanks to @Matt OP, @Tanoi , @Elric and @broccoli beef for sharing their expertise in their respective notebooks: https://www.kaggle.com/code/mattop/ps-s3-e13-important-features-for-each-disease/notebook https://www.kaggle.com/competitions/playground-series-s3e13/discussion/402933 https://www.kaggle.com/competitions/playground-series-s3e13/discussion/403493 , https://www.kaggle.com/competitions/playground-series-s3e13/discussion/403156 Their insights were invaluable and helped me greatly. Here's what worked for me: To start, I wanted to come up with a reliable validation method. Based on the previous Playground competition, I decided to use RepeatedStratifiedKFold. I selected k and n based on the average validation score, standard deviation, and standard error of the mean. I found that 10 repeats of 10-folds seemed to be a good choice to assess the validation scores. For my baseline method, I used SVC, which had a validation score of 0.367 +- 0.025 std. I used the features provided by Tanoi and found that including 'kidney-failure' resulted in a cross-validation score of 0.370 +- 0.025. I also grouped features based on their names and added up their values. For example, I added up the values of features that contained the words pain, loss, or inflammation. Including the summed-up features containing the word pain improved the cross-validation score to 0.375 +- 0.026. I also included polynomial features and selected a feature pair that resulted in the highest cross-validation score: 'back_pain' and 'yellow_skin', this gave a score of 0.3937. Next, I further tested including additional pairs of polynomial features ('itchiness', 'bullseye_rash', and 'diarrhea', 'hypoglycemia'): the corresponding validation score further improved to 0.3989. Feature selection: I preselected some algorithms that performed well on the initial feature set and checked what would be a good VarianceThreshold cutoff rate for them. I tested XGBClassifier, SVC, BernoulliNB, NuSVC, and LGBMClassifier. All of them resulted in the highest cross-validation score when using a threshold of 0.1. These were all the algorithms that I used for the final ensemble without weighing. The public score of my final submission was 0.37089, and the private score was 0.52302. Here's what did not work or might have worked: Due to the vast amount of original features in the dataset, I thought that clustering techniques would be beneficial to this task. I tried several of them, including MeanShift, Birch, BisectingKMeans, and KMeans. MeanShift had the best cross-validation score of 0.357 +- 0.039, so it did not provide any improvement. I also tried dimension reduction techniques, such as PCA, but did not invest too much time into them. It might be worth exploring more in the future. Please sign in to reply to this topic. comment 3 Comments Hotness shoab ahamed Posted 2 years ago ¬∑ 203rd in this Competition arrow_drop_up 3 more_vert Thanks for sharing your work and congrats. But what is variancethreshold cutoff for feature selection Unworried1686 Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 5 more_vert There is a nice article about this on TDS . I used a very similar approach as described in the article. This was the very last step in my pipeline after feature engineering. Hope this helps! Ivan Isaev Posted 2 years ago arrow_drop_up 2 more_vert @Unworried1686 great job!üí™ Thank you for sharing!üëäüôÇ",
      "Classification with a Tabular Vector Borne Disease Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Classification with a Tabular Vector Borne Disease Dataset Playground Series - Season 3, Episode 13 Classification with a Tabular Vector Borne Disease Dataset Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules mateuszgrzyb.pl ¬∑ 4th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 10 more_vert #4 solution - simple model WITHOUT CV ;-) Hey guys. Sorry for posting the code so late. Below is a summary of my approach. The dataset was quite small if we consider the number of levels of the target variable, so I assumed that: CV with low number of folds is not a good idea GBT/NN + wrong validation = overfitting What I did instead was: RandomForrest + OOB scores (to not split the data with so many levels) + Optuna. The results (OOB, public LB, private LB) turned out to be perfectly correlated. Here is my full code. Please sign in to reply to this topic. comment 3 Comments Hotness Serhii Kovalenko Posted 2 years ago ¬∑ 20th in this Competition arrow_drop_up 2 more_vert Thank you for sharing! Great job! Indeed, Random Forest is a fantastic tool to prevent severe overfitting (as it was in this competition), but it is hard to say that you didn't use CV in your approach, since RF OOB works similarly to the classic Monte Carlo cross validation (except the fact that RF selects samples with replacement, but MCCV - without). mateuszgrzyb.pl Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Hey Sergii! Thanks for your comment. it is hard to say that you didn't use CV in your approach You're absolutely right. \"simple model WITHOUT CV ;-)\" - I would say it's a mix of clickbait and small joke. That's why I added \";-)\". I consider OOB score as a fast alternative to CV. When you combine RF OOB with sklearnex implementation of RF, then it becomes really fast, even with big datasets. mateuszgrzyb.pl Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert @skovalenko I looked at the leaderboard and You were one of guys who survived this leaderboard shapekup (you are pretty high on both: public & private LB). How did you do this? Could you please explain your solution in couple sentences? Serhii Kovalenko Posted 2 years ago ¬∑ 20th in this Competition arrow_drop_up 1 more_vert @mateuszgrzybpl Well, there is no any magic here. From the very beginning I decided to move in two different directions: with and without original data. My CV (Repeated (n = 5) Stratified 10-fold CV) indicated that adding the original data set decreased CV score. At the same time, results on public leaderboard were better namely for the models trained on joined data. In fact, my best public scores appeared suddenly from these submissions. At least, I wasn't going to be on top of the public leaderboard. In any case, I prefer to trust my validation (but this time I could not achieve totally stable validation results, your approach with RF OOB was a bit better, I think), so for my final submission I took one model trained without the original data and an another one trained on joined data set with the best correspondent CV scores. As I expected, the first variant achieved better result on private leaderboard. It is funny, but I used to exploit RF several times in my real practice, but during the last several years I haven't faced with this algorithm, and I completely forgot about its OOB functionality. You reminded me about that. Thank you for that! mateuszgrzyb.pl Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert @skovalenko Thanks for sharing your approach. Based on what you've written, I think validation strategy was your key to stable results. your approach with RF OOB was a bit better, I think I'm not sure. Kaggle is a mix of luck and this time I was a bit luckier than you were. Thanks and all the best!",
      "Classification with a Tabular Vector Borne Disease Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Classification with a Tabular Vector Borne Disease Dataset Playground Series - Season 3, Episode 13 Classification with a Tabular Vector Borne Disease Dataset Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules Oleksii Zhukov ¬∑ 5th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 36 more_vert #5 Solution Hello everyone, First of all, I am very happy and even a little surprised that I took fifth place in this competition. It appears that I am one of the few participants who achieved excellent results on both public and private leaderboards. At one point, I even held the top 1 spot on the public leaderboard. I think one of the main things that brought me here is that I was dismayed by how the synthesized data differed from the original data in this competition. Therefore, I decided to work on two solutions simultaneously. I submitted two different solutions: one using mixed data and the other using only synthesized data. The first solution, which utilized mixed data, scored 43.598 on the public leaderboard and 0.5 on the private leaderboard, a commendable result that would have placed me in the top 30. The second solution, which relied solely on synthesized data, scored lower on the public leaderboard and cross-validation (with a public score of 41.501). However, I was right to be indignant about the similarity between the synthesized and original data, as this solution scored an impressive 51.535 on the private leaderboard, ultimately earning me fifth place. In the last few days of the competition, I've been working on refining my solutions, realizing that I could potentially secure a good spot and might publish my work after the competition. As a result, I created a notebook containing both of my submissions, which helped me to achieve my fifth-place finish. I have provided a link to the notebook in which I pinned the version with my final solution. You can read the solution there and leave your questions in the comments: https://www.kaggle.com/code/zhukovoleksiy/5-solution-ps3e13-ensemble While I have around 40-45 versions of private notebooks, these contain mostly chaotic notes on data exploration and model hyper-tuning. Therefore, I do not believe that they will be useful to others and I have chosen not to publish them. The notebook at the link above contains everything you need. Thank you all for your contributions in public works and discussions. Thanks to Kaggle for hosting this competition and I look forward to participating in future competitions. Please sign in to reply to this topic. comment 11 Comments 1 appreciation  comment Hotness szebiniso Posted 2 years ago arrow_drop_up 1 more_vert Congrats! Thanks a lot for sharing @zhukovoleksiy ! Ivan Isaev Posted 2 years ago arrow_drop_up 1 more_vert @zhukovoleksiy congratulations and thank you for sharing your code!üëç The notebook is very clearly written and definitely deserve up-voting. No too often highest place notebooks published and so accurately organized. Well done!üëçüí™üíØ lmadsen Posted 2 years ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert Congratulations! I was in the middle of the pack in the public leaderboard at around 330 but jumped to 11th position in the private leaderboard. I've uploaded a copy of the notebook version that scored the 11th place here: https://www.kaggle.com/code/larsmadsen/11th-place-in-competition Srishti Negi Posted 2 years ago ¬∑ 339th in this Competition arrow_drop_up 1 more_vert Congratulations! Thanks for sharing your solution! yann barthelemy Posted 2 years ago ¬∑ 522nd in this Competition arrow_drop_up 1 more_vert Congratulations and thank you for sharing your work! Alican Noyan Posted 2 years ago ¬∑ 147th in this Competition arrow_drop_up 1 more_vert Congrats and thanks for sharing the solution! Demyan Pavlyshenko Posted 2 years ago ¬∑ 64th in this Competition arrow_drop_up 1 more_vert Congratulations, @zhukovoleksiy for #5 place! Mar√≠lia Prata Posted 2 years ago ¬∑ 17th in this Competition arrow_drop_up 1 more_vert Congratulation Zhukov (private and public LB) and many thanks for sharing your solution. Ravi Ramakrishnan Posted 2 years ago arrow_drop_up 1 more_vert Good work @zhukovoleksiy , staying high in both leaderboards in such a challenge is a feat in itself. Your diverse approach strategy is commendable! Best regards and hearty congratulations for the feat! Epikt Posted 2 years ago ¬∑ 37th in this Competition arrow_drop_up 2 more_vert Congratulations! Yes, it was tempting to use the previous data even though it looked different since it boosted the public scores a lot, but you're right, it might not have been that good an idea to use it in the end. I guess the lesson is to trust what we see in the data and not just the public scores when logic says otherwise. I still have trouble understanding why it helped so much on the public leaderboard though. Good work! Appreciation (1) Rayhan Adi Posted 2 years ago ¬∑ 637th in this Competition arrow_drop_up 1 more_vert Congratulation, Thanks for sharing!",
      "Classification with a Tabular Vector Borne Disease Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Classification with a Tabular Vector Borne Disease Dataset Playground Series - Season 3, Episode 13 Classification with a Tabular Vector Borne Disease Dataset Overview Data Code Models Discussion Leaderboard Rules Tussalo ¬∑ 15th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 6 more_vert #15 with original data Hey everyone, although I am not that close to the top of the private leaderboard, it seems like I am one of the few people who managed to get good results in private (31) and public leaderboard (15) WITH THE SAME SUBMISSION. So I thought a brief description of the tricks I used would be interesting to some people. The top submissions on the leaderboard probably did not use the original data shown by their big jumps. @zhukovoleksiy obtained great results on the public leaderboard (30.) using original data and also great results on the private leaderboard (5.) with a different model not using the original data. My models all used the original data but here are some tricks I used to not get biased results: Only use the original data for the fitting of the model and early stopping but not for the model selection. I think this is pretty common. Assign weights to the original data based on their prognosis to counteract the different distribution and artificially recreate the prognosis distribution of the artificial data. To do so the weight for an observation of prognosis p in the original dataset should be set as follows: w(p, orig) = (n_obs(p, artificial)/n_obs(p, original) ) * (959/707) Assign higher weights to artificial observations and lower weights to original data by multiplying them with a weight multiplier. I used a hzperparameter for this and tuned it in the grid search. Without this step the original data makes up for 707/959 of the total weights and original data for 303/959. During testing, setting this hyperparameter for original data to 95% and artificial data 5% worked best. Use the weights as sample weights and Eval set sample weights when fitting an xgboost model. You can also use other models but xgboost worked best for me. I'll try to publish a detailed notebook later. Thanks for reading, hope this helped some. See you all next week! Please sign in to reply to this topic. comment 2 Comments Hotness Sharov Anton Posted 2 years ago ¬∑ 523rd in this Competition arrow_drop_up 0 more_vert Hi, congrats. Can you elaborate more on your 1 point why shouldn't we use original data for model training? Tussalo Topic Author Posted 2 years ago ¬∑ 15th in this Competition arrow_drop_up 2 more_vert Hey thanks for your comments. What I meant was that if you choose to use the original data, which is substantially different to the artificial data, you need to make sure to not use it for the evaluation during model selection. Only for training. When you train multiple models and want to compare them to find the best model, the performance measure (e.g. logloss) should be calculated only on the artificial validation data. The reason is that any performance measured on original data is biased and does not give an accurate estimate of the performance on the test data. Therefore you are not able to identify the model that will perform best on the test data if you use original data for selection. I hope this helps. Sharov Anton Posted 2 years ago ¬∑ 523rd in this Competition arrow_drop_up 0 more_vert Thank you for reply. So, what you are saying is that it was bad idea to union train and original data and validate local cv., right? Because from one point of view, I took more data, better local cv score and lb, but private is worse comparing to only  train data. Also I didn't explore (my fault) original data vs train&test data, but usually  adding original data helps, but not in this time. My best private scores was with simple model like plain log_reg(C=100) and xgb with only train data. 3 more replies arrow_drop_down",
      "Classification with a Tabular Vector Borne Disease Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Classification with a Tabular Vector Borne Disease Dataset Playground Series - Season 3, Episode 13 Classification with a Tabular Vector Borne Disease Dataset Overview Data Code Models Discussion Leaderboard Rules eryawww ¬∑ 23rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 5 more_vert Top 29 Using Baseline Hello folks! I think the shaking of the leaderboard is predictable and can be observed by using adversarial validation. Although I knew there would be a shake, I didn't have a strategy to avoid it. It was quite shocking that my CatBoost baseline model achieved a private score of 0.49890 with a public score of 0.35651. It literally just fit the data directly, without any FE and preprocessing. Here's the link to my notebook about this baseline. My best submission is the submission I pick luckily, it was produced by Ensembling my top submission of diverge approach. Fortunately, I didn't include that base model on my ensemble üôÉ.  It turns out pretty effective, best csv I put inside my ensemble is scored private 0.481, turn into private 0.502. This diverge approach include Medical approach by Tanoi Cluster Engineering by belati Dimensionality Reduction by tetsu213 What a great experience! I'm looking forward to the next competition. Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Classification with a Tabular Vector Borne Disease Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Classification with a Tabular Vector Borne Disease Dataset Playground Series - Season 3, Episode 13 Classification with a Tabular Vector Borne Disease Dataset Overview Data Code Models Discussion Leaderboard Rules E. Tolga Ayan ¬∑ 31st in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 8 more_vert 31th Place Solution | Using NLP for tabular data! Hello everyone, Please take a look at my notebook on Kaggle: https://www.kaggle.com/code/tolgayan/language-modeling-for-tabular-data I used a unique approach to process tabular data by converting rows into text format. For instance, a row with 1s for sudden_fever, headache, nose_bleed columns would become \"a person with the symptoms sudden fever, headache, nose bleed.\" I trained a BERT model using the emilyalsentzer/Bio_ClinicalBERT backbone model, which is well-suited for medical data. A few observations: No preprocessing was done, just raw data, demonstrating the potential of the BERT approach. My best public LB score is 0.40728, with 0.39072 being the best in the Kaggle notebook due to random states. Due to the small size of the data, NLP models are prone to overfitting. Synthetic data created using external data could be beneficial. The base scores are promising, making it suitable for an ensemble architecture. I am also curious about training a binary sentence-pair classification model where the first sentence is the row data in text format and the second sentence is the disease name. Please share your opinions about it. I am curious to see what could be done with this approach. Thank you! Please sign in to reply to this topic. comment 3 Comments 1 appreciation  comment Hotness Jim Gruman Posted 2 years ago ¬∑ 623rd in this Competition arrow_drop_up 2 more_vert had a look at a similar embeddings approach, but couldn't beat TF/IDF on just the original tokens. Serhii Kovalenko Posted 2 years ago ¬∑ 20th in this Competition arrow_drop_up 0 more_vert Great idea! I was going to do something similar, but didn't find enough free time to do that. It's interesting to see what you get on private LB as a final result. Good luck! Appreciation (1) Ericka42 Posted 2 years ago arrow_drop_up 0 more_vert Cool!Thanks for sharing.üëç"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 13 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Vector Borne Disease Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note that in the original dataset some prognoses contain spaces, but in the competition dataset spaces have been replaced with underscores to work with the MPA@K metric. 3 files 284.43 kB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 284.43 kB sample_submission.csv test.csv train.csv 3 files 133 columns ",
    "data_description": "Classification with a Tabular Vector Borne Disease Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Classification with a Tabular Vector Borne Disease Dataset Playground Series - Season 3, Episode 13 Classification with a Tabular Vector Borne Disease Dataset Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules Overview Start Apr 18, 2023 Close May 2, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in April every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Submissions will be evaluated based on MPA@3 . Each submission can contain up to 3 predictions (all separated by spaces), and the earlier a correct prediction occurs, the higher score it will receive. Submission File For each id in the test set, you must predict the target prognosis . The file should contain a header and have the following format: id ,prognosis 707 ,Dengue West_Nile_fever Malaria 708 ,Lyme_disease West_Nile_fever Dengue 709 ,Dengue West_Nile_fever Lyme_disease\netc. content_copy Timeline link keyboard_arrow_up Start Date - April 18, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  May 1, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade, Will Lifferth, and Ashley Chow. Classification with a Tabular Vector Borne Disease Dataset. https://kaggle.com/competitions/playground-series-s3e13, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 2,553 Entrants 963 Participants 934 Teams 7,765 Submissions Tags Beginner Tabular Multiclass Classification Health Conditions MAP@{K} Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e14",
    "discussion_links": [
      "/competitions/playground-series-s3e14/discussion/410627",
      "/competitions/playground-series-s3e14/discussion/410639",
      "/competitions/playground-series-s3e14/discussion/410666"
    ],
    "discussion_texts": [
      "Regression with a Wild Blueberry Yield Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Wild Blueberry Yield Dataset Playground Series - Season 3, Episode 14 Regression with a Wild Blueberry Yield Dataset Overview Data Code Models Discussion Leaderboard Rules Sergey Saharovskiy ¬∑ 1st in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 134 more_vert 1# Winning solution Hi, I would like to express my deepest gratitude to the organizers, winners and the community who made this competition. Introduction: The competition attracted a record number of participants. The community was extremely active. The discussions were fruitful creating a collaborative environment that fostered learning and rapid score improvement. The number of gold/silver notebooks and discussions speaks for itself. It was not an easy journey for me. It took me almost 5 months of polishing tabular competition pipelines. Few times I was first on the public leaderboard and did not survive shakeups. Few more times I chose the wrong submissions. It burned my fingers but it made me understand: \"if the approach is solid you don't need to submit 5 times a day. The solution: Context: From the very beginning of the competition, I found out that the models will be limited in performance by looking at one of the most important features. I posted the discussion here, showing how much variance was synthesized from the original dataset. No matter how good the ensemble was going to be, it would not give a winning edge I got. Early discovery: At a very early stage of model validation, I found that the model struggled with some obvious predictions. Most of the common error was yield == 1945.53061 . You will see it in the attached work . I noticed that the values for fruitset and fruitmass 0.335339    0.233554 which were the same for the train and origin were always 1945.53061 no matter what. So I assigned 1945.53061 to those predictions and saw how oof behaved. As you can assume they did fantastic. Then I found a couple more examples, you will see them in the first phase of correction in the notebook. Automating the process: I figured out that the manual search was not scalable, so I came up with the whole script which ran through common fruitset and fruitmass values for both origin, train and test. 2073 total samples in test. But what values should have I assigned? Well, easy enough, I ran through all unique yields 776 iteratively and saw which one were better. Though it improved the oof, it did not perform well on the leaderboard, so I assigned the value from the original dataset and it was a nuke bomb (further I continue on this analogy). Strategy: I decided to play slow. The phase 2 postprocessing included waterfall of different submissions in terms of risk. I named them according to this classification where anomaly was just correction of 1945.53061 . Then you can go deeper with these lines of code: if len (dsp_train) > 2 and len (dsp_test) > 1 : if not dsp_origin.empty:\n        orig_value = dsp_origin[ 'yield' ].values[ 0 ]\n        tr_idx = train.loc[train.fruitset.eq(txt[ 0 ]) & train.fruitmass.eq(txt[ 1 ]), 'pred' ].index.tolist()\n        te_idx = test.loc[test.fruitset.eq(txt[ 0 ]) & test.fruitmass.eq(txt[ 1 ]), 'pred' ].index.tolist() content_copy len(dsp_train) > 2 and len(dsp_test) > 1 : means there should be at least more than two common records in train , one in test and one in origin . Serious Incident : > 4 > 3 Zone With consequences :  > 3 > 2 Dangerous Zone :  > 2 > 1 Ground Zero :  > 1 > 0 (not submitted) The risk was great to fail terribly on the private leaderboard that's why you will see _suffix _private_600 in the submissions screenshot later on. @adaubas helped the strategy: One submission and the second place, what a shot, right! It was @adaubas who generously shared the config and the work later on. How did it help? Well, it could have seemed natural that I jumped to the second place right after the discussion once the config got into the trends. So I even posted the teaser with my close result config with almost the same parameters (sneaky me‚Ä¶). Stacking Masterclass: 1st level models included 8 different oofs. They could be found in this dataset . Here is the model pipeline and works of those who helped the winning solution: high resolution or ctrl + scroll up to zoom in. LAD vs scipy.minimize : It was nice to learn about LAD from @adaubas . What I found out is that I used the same backend with very similar results. But for the sake of code simpicity and slightly better performance I chose LAD. Here is the comparison: Validation: Outro: Tribute to @naganohikaru : The effect of the above postprocessing was immense. Various model blendings gave floating point improvement when at the same time the postprocessing above gave an integer sometimes. You saw me jumping on the leaderboard. I waited the next move of @naganohikaru and send the bomb understanding the risk. I stopped at the Dangerous Zone since I did not see any improvement from my opponent anymore, mitigating the already high risks on the private leaderboard. Taking the risks with submissions : I chose two best public scoring submissions (they happened to be my best private). The motivation was simple, I realized that the portion of Alex models in the final stack was not big, and the only way I could win was to take the full risk. Otherwise, I would compete against hundreds of the same variety of ensembles. Acknowledgements : p1 @zhukovoleksiy notebook link p2, p5 @paddykb notebook link p3, p6 @yzokulu notebook link p4 @tetsutani notebook link p7/p8/p9 @adaubas notebook link @mattop post_processing Icing on the cake : What would be my last move if my opponent above found some miracle in the end? You are right - the ground zero submission, here you go: The winning solution code . Dataset . Tabular Regression SciPy Please sign in to reply to this topic. comment 55 Comments 1 appreciation  comment Hotness Aparna Das Pias Posted 2 years ago arrow_drop_up 3 more_vert Congratulations @sergiosaharovskiy Anan Posted 2 years ago ¬∑ 724th in this Competition arrow_drop_up 3 more_vert Congratulations! Learned a lot from this solutionÔºÅThx for sharing! pineapple Posted 2 years ago ¬∑ 39th in this Competition arrow_drop_up 3 more_vert awesome solution, the postprocessing was very smart Alex D'Ippolito Posted 2 years ago ¬∑ 89th in this Competition arrow_drop_up 3 more_vert Absolutely incredible, thanks for sharing.  The approach you took is brilliant and will have a major impact on how I approach future competitions Lucy Posted 2 years ago ¬∑ 862nd in this Competition arrow_drop_up 3 more_vert Hello @sergiosaharovskiy . Thanks for the summary. It is very helpful! In my notebook I used Huber/Ridge Regression for an ensemble. Huber Regression showed better cv scores/private/public scores than Ridge Regression. I would like to see the advantage of LAD over than the other two. If you have time, can you take a look at my notebook Here ? The question I want to ask you is that why my local cv score around 306ish didn't correlate to my LB score ~342? Did I miss something crucial here? Thanks a lot. Sergey Saharovskiy Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Hello @qiaoningchen , I don't see the obvious reason for your notebook to show such a gap between individual models oofs and stacked ones. I assume the bug might be in the place where you append the obtained scores and average them after. The best practice would be taking your final oofs (after Ridge let's say) and check mae(y_train, ridge.predict(oofs)). The appended scores don't show you the precise score. Hope it makes sense. Lucy Posted 2 years ago ¬∑ 862nd in this Competition arrow_drop_up 2 more_vert Just checked it. No difference. I have to look for other reasons. Thanks anyway! Demyan Pavlyshenko Posted 2 years ago ¬∑ 189th in this Competition arrow_drop_up 3 more_vert Thanks for the solution @sergiosaharovskiy ! I've also seen a 1945 prediction a lot, but i didn't even realized that it can be a good shot CatterPingu Posted 2 years ago ¬∑ 178th in this Competition arrow_drop_up 3 more_vert congrats! We'll be waiting!! Demyan Pavlyshenko Posted 2 years ago ¬∑ 189th in this Competition arrow_drop_up 4 more_vert Congratulations for 1th place! Waiting impatiently for the solution notebook! vimukthi1997 Posted 2 years ago arrow_drop_up 1 more_vert Congratulations, thank you for your amazing work! Jeong Taewoo Posted 2 years ago arrow_drop_up 1 more_vert Congrats! Very helpful notebook and writing! Yuning Wang Posted 2 years ago arrow_drop_up 1 more_vert amazing, i love your solution Tamanna Akter Swarna Posted 2 years ago arrow_drop_up 1 more_vert Congrats üéâ and Thanks for sharing @sergiosaharovskiy Laurent Pourchot Posted 2 years ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert Congrats  ! Always difficult to keep the first place Oscar Aguilar Posted 2 years ago ¬∑ 16th in this Competition arrow_drop_up 1 more_vert Congrats @sergiosaharovskiy . This achievement really shows your hard work and dedication to the field. Thanks for your contribution on this compüëç Samuel Cortinhas Posted 2 years ago arrow_drop_up 1 more_vert Congrats on 1st place! Hozaifa Ahmed Posted 2 years ago arrow_drop_up 1 more_vert Kudos @sergiosaharovskiy for acing this competition. Keep it up üéâ Rakha Abid Bangsawan Posted 2 years ago ¬∑ 743rd in this Competition arrow_drop_up 1 more_vert Congrats! Waiting for the solution Mohammad Husain Almouwannes Posted 2 years ago ¬∑ 1415th in this Competition arrow_drop_up 1 more_vert Congrats! Waiting eagerly for your work Ravi Ramakrishnan Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Congratulations @sergiosaharovskiy , I admire your amazing consistency of top performances in the series. Gaurav Malik Posted 2 years ago ¬∑ 126th in this Competition arrow_drop_up 1 more_vert congrats, waiting for the solution Matt OP Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Bro! Awesome job getting 1st! Shubham Kumar Singh Posted 2 years ago ¬∑ 1260th in this Competition arrow_drop_up 1 more_vert Congratulation for the achievement @sergiosaharovskiy Epikt Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Congratulations on first place! Aman2909 Posted 2 years ago arrow_drop_up 2 more_vert Congratulations, well explained and thank you for sharing. The first paragraph about struggling with polishing tabular competition was motivating. I'm too struggling with the same and the journey has been quite on and off. Any tips for being more consistent that you would share? Sergey Saharovskiy Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Hi @amanpathak2909 well, consistency is driven by the goals, motivation, routine and many other variables. It will be difficult to define the recipe not knowing the case. I don't know how consistent I was but time was spent definitely. Aman2909 Posted 2 years ago arrow_drop_up 0 more_vert Sure. Well said. Thanks a lot. Nooruddin Hyderabadwalla, CFA Posted 2 years ago arrow_drop_up 2 more_vert @sergiosaharovskiy congratulations and thank you for sharing!",
      "Regression with a Wild Blueberry Yield Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Wild Blueberry Yield Dataset Playground Series - Season 3, Episode 14 Regression with a Wild Blueberry Yield Dataset Overview Data Code Models Discussion Leaderboard Rules Matt OP ¬∑ 4th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 49 more_vert 4th Place Solution: Introducing hillclimbers Hey guys, thank you so much for another fun Playground Series competition! Ever since Season 3 of the PS started in January, I have learned so much and have had such good time participating in these competitions. I have a lot of people to thank for placing in 4th (please upvote all of these notebooks!): @paddykb set the public LB on fire early on in the competition with his excellent notebook PS s3e14 -FLAML BFI Be-bop-a-blueberry-do-dah and then once again when he added the post-processing trick to his model (more on that later). @adaubas also set the public LB on fire with another great notebook PS s3e14 -Stacking - LeastAbsoluteDeviation Reg and discussion post Some tricks for a 337 score on Public LB . This post details how fruitset , seeds , and fruitmass are highly correlated and linearly related to the target and how PCA and PLSRegression can be used for feature engineering along with the code to create the features. @francescoliveras shared the highest scoring notebook on the public LB 336.695 üåüPS-S3-E14üåü | üìäEDA | Model [EN/ES] which utilizes @paddykb 's FLAML AutoML setup and @adaubas ' LADRegression blend strategy. @tetsutani shared an awesome notebook PS3E14 EDA| Various models & Ensemble baseline which has good visualizations and where they used a bunch of different models (some models more than once with different hyper parameters) blended together with OptunaWeights to create a high scoring ensemble. @chayaphatnicrothanon shared his nice notebook (LB Score: 338.63) EDA+CatBoost+LightGBM+KFolds which has some very cool evaluation based visualizations. @zhukovoleksiy shared another great notebook [PS S3E14] Simple EDA + Ensemble which uses a similar strategy to @tetsutani . The post-processing trick What did all of the above notebooks have in common? They (and many others) all used the post-processing trick. However, they did not use it all in the same way. Some people used it in their cross validation splits and some people used it after the splits' predictions were blended together. I personally found it beneficial to use it in the cross validation splits and after the splits' predictions were blended together. What is hillclimbers? The reason I just showed my gratitude to the all of the above notebooks is because these were all the models I used in my final ensemble! One of the other reasons I did this was to try out my project hillclimbers which is a python module that uses hill climbing to iteratively blend machine learning model predictions. Big shoutout to @cdeotte for his original post explaining and showcasing hill climbing 3rd Place Solution and @samuelcortinhas for his amazing notebook üìà PS S3E3 - Hill Climbing like a GM . I could not and would not have created hillclimbers without you guys! Why did you create hillclimbers? I created hillclimbers because after competing in a couple of these competitions I quickly realized how important it is to have a diverse model. With hill climbing, the models with the best cross validation scores are not always chosen first. Instead hill climbing chooses diverse models. I started playing around with the code in @samuelcortinhas ' notebook and have tried to use it in the past couple Playground competitions. I found myself editing the code a lot from competition to competition and wanted to create something that could be more adaptable. Here are the things I implemented to make it a piece of cake to use hill climbing for almost any tabular problem: !pip install hillclimbers from hillclimbers import climb_hill, partial def climb_hill( train =None, oof_pred_df =None, test_pred_df =None, target =None, objective =None, eval_metric =None, negative_weights = False , precision =0.01, plot_hill = True , plot_hist = False ) -> np.ndarray: # Returns test predictions resulting from hill climbing content_copy target : Let's you specify the target column you are trying to predict. objective : Set to \"maximize\" or \"minimize\" depending on the evaluation metric you are using. eval_metric : Define the evaluation metric! negative_weights : Do you want to use negative weights? precision : Specifies the step to be taken in the array of weights. Please visit the github repo for more detailed explanations. Results Now let me show you how hillclimbers works: And here is the hill climbing plot: You can find all the code for this solution in this notebook Data Visualization Ensembling Beginner Intermediate Advanced Please sign in to reply to this topic. comment 16 Comments Hotness Mohamed Khaled Posted 2 years ago arrow_drop_up 1 more_vert Nice work ‚Ä¶.Congratulation Matt OP Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Thank you @moro146 , I appreciate the comment. Oscar Aguilar Posted 2 years ago ¬∑ 16th in this Competition arrow_drop_up 1 more_vert Nice approach @mattop ! Thanks for your contribution on this compüëç The post-processing really help to improve model performance. Matt OP Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert @oscarm524 Thank you, I really appreciate your feedback. The post-processing got much more attention than I initially thought it would. I am glad it was able to improve your model performance. Samuel Cortinhas Posted 2 years ago arrow_drop_up 1 more_vert Really nice work, congrats! Matt OP Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 3 more_vert @samuelcortinhas Thank you so much for the comment and sharing your hill climbing implementation & discussion topic Hill Climbing for ensembling , I can't tell you how much it helped me creating this project. Also, this picture gets me every time haha: aldparis Posted 2 years ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert Congratulations ! Thank's for having shared your solution. Matt OP Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert @adaubas Thank you again for being so generous to share your approach near the end of the competition. It helped me in this competition and will continue to do so in the future. WinEisEis Posted 2 years ago ¬∑ 209th in this Competition arrow_drop_up 1 more_vert Thank you for this interesting concept of hill climbing! Matt OP Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert @chayaphatnicrothanon Thank you again for sharing your notebook, I wouldn't have gotten this result without it. Really interesting evaluation visualizations as well! Iqbal Syah Akbar Posted 2 years ago ¬∑ 193rd in this Competition arrow_drop_up 1 more_vert Congrats @mattop ! And thanks for introducing hillclimbing to us! Matt OP Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Thank you very much @iqbalsyahakbar . Please feel free to use hillclimbers in future tabular competitions! SOUMENDRA PRASAD MOHANTY Posted 2 years ago arrow_drop_up 1 more_vert @mattop , Congratulations . Your efforts are highly appreciated . abdullahsamiir Posted 2 years ago ¬∑ 355th in this Competition arrow_drop_up 1 more_vert Congratulations ‚ù§Ô∏è‚ù§Ô∏è Priyanshu54200 Posted a year ago arrow_drop_up 0 more_vert well explained graphs ivan9394 Posted 2 years ago arrow_drop_up 0 more_vert thanks for creating the method. but in the method, I see the best cv score will be chosen first, it seems different from your introduction  \"With hill climbing, the models with the best cross validation scores are not always chosen first.\" Is there any misunderstanding I've made?",
      "Regression with a Wild Blueberry Yield Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Wild Blueberry Yield Dataset Playground Series - Season 3, Episode 14 Regression with a Wild Blueberry Yield Dataset Overview Data Code Models Discussion Leaderboard Rules Ravi Ramakrishnan ¬∑ 7th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 53 more_vert #7 Private #3 Public approach| Simple ensemble with post-processing Hello all, I wish to extend sincere thanks to the Kaggle team for the episode and the playground series in totality. I think these playground series competitions are incredibly rewarding, offering lots of opportunities to beginners and practitioners alike. Firstly, I wish to congratulate the winners of the challenge and extend sincere thanks to all the participants for making this a memorable experience. I shall also extend sincere thanks to the below users in particular for their contributions over the past fortnight- a. @paddykb b. @adaubas c. @mattop d. @tetsutani Feature engineering- I used only the features provided in the dataset without creating additional features. I used PCA and PLS to create association based features as mentioned in the below link- https://www.kaggle.com/code/adaubas/ps-s3e14-stacking-leastabsolutedeviation-reg I did not use any scaling/ centering options I adjusted some data values in certain features (RainingDays and MaxOfUpperTRange) as mentioned in the below link, thanks to @paddykb and @adubas for the idea- https://www.kaggle.com/code/adaubas/ps-s3e14-stacking-leastabsolutedeviation-reg I did not ensue any transformation on the target and used it as-is I included the original data in my training, it helped me improve my CV score drastically compared to its non-usage. I created the OOF score purely from the competition data (as per my public notebook). Base Models- I initiated the challenge with lots of common methods, most of which did not work. I used Repeated K-Fold - 10x3/ 10x2 for all my models. I used optuna and FLAML for the model training and tuning, taking cues from the link below, thanks to @paddykb - https://www.kaggle.com/code/paddykb/ps-s3e14-flaml-bfi-be-bop-a-blueberry-do-dah I tuned very basic parameters on optuna with emphasis on the below parameters- a. max depth b. learning rate c. reg-alpha and reg-lambda d. number of leaves I also resorted to manual perturbation of these parameters to ensure I do not end up overfitting to the noise in the data. I used the original data to build the model training folds, while the evaluation (dev-set) was the corresponding competition fold. Using the original data in this manner improved my CV score and my public leaderboard position quite well I resorted to feature subsets across multiple models, using fruitmass, fruitset and seeds in all my models. I then used 1-3 features from the remaining categorical features in my base models. In particular, I used 1 temperature range column and 0-2 columns from the remaining category features to complete the feature subset for base models I used the below models for my ensemble- a. LightGBM - biggest contributor to the ensemble b. Catboost c. Random forest d. Gradient Boosting Regression Ensemble- LAD regression worked for me in comparison to other options like ridge/ optuna based tuning. I used the method suggested in the below link- https://www.kaggle.com/code/adaubas/ps-s3e14-stacking-leastabsolutedeviation-reg I blended the top public notebooks with my base models using LAD and prepared my submission I manually adjusted the ensemble weights to a small extent based on my public LB score. Post-processing- Thanks to @mattop for the rounding idea, it helped improve my CV score and my leaderboard position as well. I used the idea from his discussion post as below- https://www.kaggle.com/competitions/playground-series-s3e14/discussion/407327 I post-processed my predictions on the test set after the ensemble (single round of post-processing). Models that did not work- TabNet regressor Neural networks - I should have focused on my features more to make this work. Linear models XGBoost, especially XGBoost with objective = absolute error (this performed extremely poorly) GAM Extra trees regression What I could have done better- Better quasi-duplicate handling Better feature engineering- I could have tried better secondary features Selected a better final submission. My best submission overall performed slightly worse on the public leaderboard but could have given me a better private leaderboard position. Better ensemble strategy- I fine-tuned the ensemble weights after the model results based on the public leaderboard score. Perhaps had I not done this, I could have performed slightly better on the private leaderboard My key learnings and takeaways- Rounding off predictions to the nearest training data values - this could be powerful method in many practical assignments in and outside Kaggle LADRegression- I used it for the first time and shall use it well going ahead FLAML - this is quite useful for tabular data. I am predisposed to using PyCaret and LAMA for auto ML based model creation. FLAML is incredibly good for tabular assignments as well. Even if the CV correlates to the public leaderboard, rely on the CV . Basically, one should rely on his/ her CV always. Finally, wishing all of you the best and see you in the next episode! Happy learning and warm regards! Tabular Regression Agriculture Please sign in to reply to this topic. comment 13 Comments 1 appreciation  comment Hotness Priyanshu54200 Posted a year ago arrow_drop_up 1 more_vert nice work congrats Gaurav Malik Posted 2 years ago ¬∑ 126th in this Competition arrow_drop_up 6 more_vert To be honest, these are the kind of discussions which new kagglers should read again and again, thanks paddykb Posted 2 years ago ¬∑ 107th in this Competition arrow_drop_up 5 more_vert Old ones too ü§ì (Even if you are familiar with the overall approach there is always an interesting nuance to learn from). Thiago Lima Santos Posted 2 years ago ¬∑ 449th in this Competition arrow_drop_up 2 more_vert I agree with you, I'm a beginner in Kaggle and I've been learning a lot by reading these notebooks from both this competition and the old one. Tamanna Akter Swarna Posted 2 years ago arrow_drop_up 1 more_vert Congratulations and thanks for sharing. @ravi20076 S Sakthi Saravanan Posted 2 years ago arrow_drop_up 2 more_vert As someone new to Kaggle, reading solutions of contest for first time feels like I found gold. Oybek Eraliev Posted 2 years ago arrow_drop_up 2 more_vert @ravi20076 thanks for sharing this post. Very useful post as usual üëç Trish Cornelissen Posted 2 years ago ¬∑ 543rd in this Competition arrow_drop_up 2 more_vert Thank you, Ravi, for your excellent explanations.  I am new to this and always curious about how you top-scorers do so well.  I focused on your suggestion of using FLAML so I spent several hours looking into FLAML and now I have something new I can try. Muhammad Bilal Hussain Posted 2 years ago arrow_drop_up 2 more_vert @ravi20076 you are  a hardworking lad , and motivation me as well keep shining‚ú® Demyan Pavlyshenko Posted 2 years ago ¬∑ 189th in this Competition arrow_drop_up 2 more_vert Congratulations for the 7th place @ravi2007 ! OMID BAGHCHEH SARAEI Posted 2 years ago ¬∑ 276th in this Competition arrow_drop_up 2 more_vert Congratulations, RAVI. aldparis Posted 2 years ago ¬∑ 13th in this Competition arrow_drop_up 2 more_vert Congratulations and thank you for having shared your solution ! I had the same bad experience than you in this competition with  ExtraTrees, linear models, NN and XGBoost with this L1 objective. UPDATE about ExtraTrees : I had a 342 OOF MAE score, not so bad, but it didn't improve my final LADRegression. Appreciation (1) abdullahsamiir Posted 2 years ago ¬∑ 355th in this Competition arrow_drop_up 2 more_vert Congratulations! thanks for sharing!"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 14 NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook . The dataset for this competition (both train and test) was generated from a deep learning model trained on the Wild blueberry Yield Prediction Dataset . (Since this is Playground 3.14, it seems like we need a Blueberry Pie joke here?) Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 2.99 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 2.99 MB sample_submission.csv test.csv train.csv 3 files 37 columns ",
    "data_description": "Regression with a Wild Blueberry Yield Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Wild Blueberry Yield Dataset Playground Series - Season 3, Episode 14 Regression with a Wild Blueberry Yield Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2023 Kaggle Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! Your Goal: For this Episode of the Series, your task is to use regression to predict the yield of wild blueberries. Good luck! NOTE: You can now create your own synthetic versions of this dataset by forking and running this notebook . Start May 2, 2023 Close May 16, 2023 Evaluation link keyboard_arrow_up Evaluation Submissions will be evaluated using Mean Absolute Error (MAE) , where each x_i represents the predicted target, y_i represents the ground truth, and n is the number of rows in the test set. Submission File For each id in the test set, you must predict the target yield . The file should contain a header and have the following format: id ,yield 15289 , 6025 . 194 15290 , 1256 . 223 15291 , 357 . 44 etc . content_copy Timeline link keyboard_arrow_up Start Date - May 2, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  May 15, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Regression with a Wild Blueberry Yield Dataset. https://kaggle.com/competitions/playground-series-s3e14, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 4,197 Entrants 1,904 Participants 1,875 Teams 12,227 Submissions Tags Beginner Tabular Regression Mean Absolute Error Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e15",
    "discussion_links": [
      "/competitions/playground-series-s3e15/discussion/414048",
      "/competitions/playground-series-s3e15/discussion/413826",
      "/competitions/playground-series-s3e15/discussion/414027",
      "/competitions/playground-series-s3e15/discussion/413742",
      "/competitions/playground-series-s3e15/discussion/413839",
      "/competitions/playground-series-s3e15/discussion/413808",
      "/competitions/playground-series-s3e15/discussion/413977",
      "/competitions/playground-series-s3e15/discussion/413749"
    ],
    "discussion_texts": [
      "Feature Imputation with a Heat Flux Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Feature Imputation with a Heat Flux Dataset Playground Series - Season 3, Episode 15 Feature Imputation with a Heat Flux Dataset Overview Data Code Models Discussion Leaderboard Rules Probably Overfitting ¬∑ 1st in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 26 more_vert 1st Place Solution | A Diverse Ensemble This has been a great place to learn from members of the community and here is my small contribution in return. What a pleasant surprise! I have only started competing on Kaggle recently. I was hoping to jump up some places but not all the way to the top. As @iqbalsyahakbar said, the shake-up was massive. Key takeaways Ensemble with models that have diverse internal workings like tree-based methods, NNs, linear models, and nearest-neighbors, even if they have worse individual performance. Use domain knowledge to manipulate the data. Use the original data for training but not validation. Trust in your CV. I tried to look away from the public leaderboard scores and focus on improving the local CV score. I was hitting a limit and couldn't improve further. I knew either I was missing a trick (which I would be curious to learn about after the competition ended) or that ~0.072 was actually the limit of the performance for this dataset. A note on that: I had a submission with a better private LB score but didn't pick it as it had a worse CV score. Things that worked Using domain knowledge for imputation and range clipping: @shalfey made an interesting remark about this towards the end of the competition . I realized that if the \"author\" column can be appropriately imputed, we can clip values of other features and reduce the noise in the dataset based on the \"author\" value. Also, originally posted by @arunklenin in a comment thread. Categorical encoding for \"author\" and \"geometry\" column. Iterative imputation : perform missing value filling iteratively using trees. Thanks to @arunklenin for sharing this here . I am curious how you tuned those parameters because they worked really well. Tune model parameters with optuna. Diverse models for ensembling. Tree-based and NNs among others. Even though some of these had worse performance, the way they work added diversity to the ensemble as pointed many times by @ambrosm in previous competitions. CV for the single best model was 0.0730 but ensembling brought it down to 0.726. Proper cross-validation 10-fold CV that lead me to a CV score of Ensemble RMSE score 0.07265 ¬± 0.00202 . Things that didn't work Clipping target variable values based on domain knowledge. One hot encoding for \"author\" and \"geometry\" columns didn't really help. It also increased the fit time as there were more features available. Adding PCA features didn't help. I experimented with an auto-encoder but that didn't seem to add much. Probably didn't cross-validate this one properly. I also experimented with clipping all features values to +-3 standard deviations to reduce outlier values. This gave a better score in the CV but seemed too optimistic at around ~0.069. Indeed the leaderboard for that submission was worse. Out-of-the-box imputers were worse. I believe the tree based worked best as there was almost an \"if-else\" pattern depending on the values of other features like \"author\" and \"geometry\" among others. Thanks to everyone who participated and shared interesting ideas. Until next time! Please sign in to reply to this topic. comment 8 Comments Hotness Minato Namikaze Posted 2 years ago ¬∑ 142nd in this Competition arrow_drop_up 3 more_vert Congratulations! @ankitdhall , thanks for sharing your great work. Regarding the iterative imputation, I used the same baseline Catboost parameters used in model fitting for numerical features imputation, and the categorical features were imputed using an untuned model. It worked better when I considered the starting point as the mean rather than the median. And thanks for the mention :) Probably Overfitting Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks for sharing your insights as always @arunklenin :) Mar√≠lia Prata Posted 2 years ago arrow_drop_up 2 more_vert A huge Congratulation Probably Overfitting (ankitdhall). I hope other Kagglers read and show their appreciation for your 1st Place Solution. Probably Overfitting Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thank you for the wishes :) @mpwolke Mykhailo Savchenko Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert @ankitdhall congratulations on getting the 1st place and thanks for mentioning my post! This playground was indeed more complex than usually, because it needed both optimal imputation strategy and good modeling and the CV was uncorrelated to LB, but we learned a lot from this one, which is the most important üî• Probably Overfitting Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thank you! @shalfey yes, the past (playground) competitions had almost no missing data so that was definitely something refreshing. Thanks again for sharing your insights with all of us. :) Alex D'Ippolito Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert Congratulations on the win!  Excellent work! Probably Overfitting Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Thank you! Congratulations on the podium too @alexdippolito",
      "Feature Imputation with a Heat Flux Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Feature Imputation with a Heat Flux Dataset Playground Series - Season 3, Episode 15 Feature Imputation with a Heat Flux Dataset Overview Data Code Models Discussion Leaderboard Rules Mykhailo Savchenko ¬∑ 2nd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 29 more_vert 2nd place solution explained: the power of original dataset First of all, I would like to express my gratitude to Kaggle team for hosting this competition. It's a great opportunity for beginners to get our hands dirty with some close-to-real-world data and get some practise. As someone who embarked on this data science journey just 5 months ago, coming from an entirely unrelated field, it holds significant meaning for me to see my progress and achievements in this competition. Earlier I have made a post regarding feature range comparison between original and competition datasets. My solution is based on those observations and my plan was as follows : 1) Do a brief EDA of competition dataset to gain initial understanding of data. 1.1. 'D_h' and 'D_e' are highly correlated and are mostly the same (influenced my imputation technique). 1.2. There was class imbalance, 'tube' geometry is the majority class (influenced choice of model). 1.3. Data is missing at random, which is probably the effect of artificially adding noise (influenced both choice of model and imputation strategy). 2) I decided to go with gradient boosting only, as it can handle both missing values and class imbalance quite well. In the previous imputation competition winning solutions were based on deep learning, but I believed that because of smaller size of the dataset and large amount of missing values it wasn't worth trying. I created a few baseline models and filled the missing features statistically (filled NaNs with mean of each feature grouped by 'author' ) to check its performance. 2.1. FLAML single LGBM model gave CV of 0.0733 (used @paddykb FLAML notebook from 3.14 competition with few modifications, thanks for the great framework). 2.2. Optuna-based baseline also gave CV around 0.073 (thanks @tetsutani for this model , I used some code from that notebook). 3) CV scores of baseline models looked very good already, so instead of spending time on finding a better model, I decided to work on better imputation technique and finding some \"tricks\", knowing this data is synthetically generated. 3.1. As I already saw the dataset had a lot of added noise, I decided to compare it with original dataset. I have shared the insights earlier in this post . 3.2. After I discovered that some features always go together and are unique to a single author, I imputed the competition data by finding the unique pairs and triplets in original dataset. 3.3. Some features were still missing, so I have decided to try different MICE-based packages like miceforest , missingpy , etc. Also, there was a great notebook by @arunklenin where he used custom implementation of CatBoost-based MICE algorithm, which also performed very good. 3.4. I have also tried to round the imputed features to the closest unique from original data, but this made the CV worse. Also thanks @alexdippolito for this topic . He also explored features from original dataset, which made me feel I am on a right path. Also congratulations on getting the third place! 4) My final model uses the ensemble of LGBM and XGBoost, and scores were as follows: I also tried some experimental models, the best one was the blend of two ensembles, which were trained on competition data, imputed using the method above, and oversampled original data. However, its CV was so close to the one for model above and there was a chance that this model would score too low on private leaderboard, so I decided the improvement wasn't better than random and I didn't select it (but actually, it would give me the first place). What did I learn from this competition? Spend more time on EDA and knowing your data. Many notebooks were having a lot of visualisations without any explanations. Experiment more and always trust your CV. Always rely on your CV, use public leaderboard as one of the tools, but don't rely on it. Also don't avoid checking notebooks with lower score, as they may have more useful information than top notebooks. Use the fact that Playground datasets are synthetic. By exploring the original data you will most likely come up with some tricks which make your final score a lot better. Don't be shy as a beginner. I was hesitant at first, as it's been only 5 months since I started learning, but it turned out my chances to get the top score were just the same as everyone else's. Also Kaggle community is super friendly and always helps. Thanks to all Kagglers who are sharing notebooks and ideas! Good luck with further competitions! Please sign in to reply to this topic. comment 10 Comments Hotness Andrey Posted 2 years ago ¬∑ 208th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for sharing! Did you take note of your CV score standard error by any chance? Vladimir Sim√µes da Luz Junior Posted 2 years ago ¬∑ 38th in this Competition arrow_drop_up 1 more_vert Congratulations @shalfey ! Thanks for sharing your learnings with the community üî•üí™ Mykhailo Savchenko Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Thank you for reviewing it and congratulations on reaching 38th place! Vladimir Sim√µes da Luz Junior Posted 2 years ago ¬∑ 38th in this Competition arrow_drop_up 1 more_vert Thank you my friend! Next time you will be 1stüôè KISSEL Posted 2 years ago ¬∑ 75th in this Competition arrow_drop_up 2 more_vert Great job @shalfey , congrats on 2nd place, next time you'll be 1st :) About me, I overfitted my model and fell 46 positions, but your approach was really great, thanks for sharing your solution üëç Mykhailo Savchenko Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Thanks for reviewing this post! 75th place is also a very good result, because this competition (at least to me) seemed harder than the previous episode of playgrounds üòÑ Good luck with the next one! KISSEL Posted 2 years ago ¬∑ 75th in this Competition arrow_drop_up 2 more_vert That's true, not for the first time I am convinced that the metrics on a private datasets can differ significantly and therefore models are needed that are good at finding common patterns, but not specific. Minato Namikaze Posted 2 years ago ¬∑ 142nd in this Competition arrow_drop_up 2 more_vert Congrats! Great work @shalfey , thanks for being active and engaging with the community :) Mykhailo Savchenko Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Thank you @arunklenin , you did great on this competition and your notebook on iterative imputation was very insightful! Demyan Pavlyshenko Posted 2 years ago ¬∑ 6th in this Competition arrow_drop_up 2 more_vert Congratualtions for 2th place @shalfey ! As for 5 months of learning you did a very great job! Mykhailo Savchenko Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert @demko1 thanks and congratulations on 6th place! Your post regarding feature engineering ideas was great and your score proves it ;) This comment has been deleted.",
      "Feature Imputation with a Heat Flux Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Feature Imputation with a Heat Flux Dataset Playground Series - Season 3, Episode 15 Feature Imputation with a Heat Flux Dataset Overview Data Code Models Discussion Leaderboard Rules Alex D'Ippolito ¬∑ 3rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 3rd Place Solution | KNN Imputation + LightGBM/XGBoost Ensemble I'd like to start with a big thank you to Kaggle for running these playground competitions and everyone in this great community who has shared their valuable knowledge.  I have learned so much from participating in these competitions over the last couple months! Below is a summary of my approach, notebook can be found here . Pre-Processing The initial steps involved a lot of manual work going through the data and looking for patterns to solve the easier missing values.  This filled out most of the values in columns D_e, D_h, and length.  A small sample of that code for author ‚ÄòInasaka‚Äô rows is below data.loc[(data['author'] == 'Inasaka') & ((data['D_h'] == 3.0) | (data['length'] == 100.0)) & (data['D_e'].isnull()), 'D_e'] = 3 data.loc[(data['author'] == 'Inasaka') & ((data['D_e'] == 3.0) | (data['length'] == 100.0)) & (data['D_h'].isnull()), 'D_h'] = 3 data.loc[(data['author'] == 'Inasaka') & ((data['D_e'] == 3.0) | (data['D_h'] == 3.0)) & (data['length'].isnull()), 'length'] = 100 My next step of pre-processing was using the K-nearest neighbors imputer to fill in the remaining holes in pressure, mass flux, and whatever was remaining in D_e, D_h, and length.  Thanks to @validmodel for this post which laid out many different imputation techniques. This ultimately led me to choosing sklearn's KNN imputer to finish the remaining numerical values.  I tried several different values for the n_neighbors parameter and found I was getting the best CV scores for my models in the 87 to 89 range. The missing categorical columns were imputed with some more manual code, such as below data.loc[(data['author'] == 'Inasaka') & ((data['D_h'] == 3.0) | (data['length'] == 100.0)) & (data['D_e'].isnull()), 'D_e'] = 3 data.loc[(data['author'] == 'Inasaka') & ((data['D_e'] == 3.0) | (data['length'] == 100.0)) & (data['D_h'].isnull()), 'D_h'] = 3 data.loc[(data['author'] == 'Inasaka') & ((data['D_e'] == 3.0) | (data['D_h'] == 3.0)) & (data['length'].isnull()), 'length'] = 100 This did reasonably well and I was getting solid CV scores but not the strongest public LB scores (more on this later). However, near the end of the competition @shalfey made this excellent post .  I placed all of this code to the front of my pre-processing.  It led to a noticeable improvement in my scores and I most certainly would not have finished quite as high as I did without it. Model Building and Cross Validation I first built a basic untuned XGBoost model to get a baseline CV score.  I was very surprised when it came in at around 0.07305.  That score was much better than the 1st place score of the public LB (around 0.0743 at the time).  This caused me a sense of dread üò¨. It was extremely unlikely that this basic model would not only put me in 1st but put me in 1st by a huge margin.  I continued forward anyways and tuned the model up, made some test predictions, and submitted.  The public score came to 0.07555. There were two possible reasons for the gap between my CV score and the public LB. My CV was flawed somewhere (I strongly suspected that my imputation pre-processing strategy had caused target leakage) The 20% of data used for the public LB contained an abnormal section of data that my model didn‚Äôt do great on but would perform closer to my CV over the entire test set. I was pretty sure the problem was number 1.  I spent a lot of time changing up my workflow to eliminate any possibility of target leakage.  However, my CV scores were still coming out to around 0.0732.  I was also scoring much worse on my public LB scores when submitting those models.  These results were starting to convince me that reason number 2 was actually true. Based on this, I went back to my original workflow of doing all pre-processing and imputation ahead of my cross validation instead of doing it fold by fold.  In this case, there was a tradeoff between slight target leakage (since the KNN Imputer utilized the target feature) and increasing the data quality/accuracy of input features for model training. If I was setting up a real world cross validation experiment the correct choice would be to do all imputing on a fold by fold basis and gather more data if necessary to improve results.  However, in this case, gathering more data is obviously not an option.  I was limited to the competition/original datasets and needed to squeeze every bit of information out of them to maximize the accuracy of the KNN Imputer step.  Even if doing so caused a tiny bit of target leakage. Utilizing the Original Dataset One unique feature of doing these playground series competitions is that all of them use synthetic data generated from an original dataset.  This always leads to the question ‚ÄúWhat do we do with the original data?‚Äù  Based on the playground competitions I have participated in, the answer is that you always need to find a way to utilize it. I have to credit @adaubas for this post during the blueberry yield competition for really getting me to think about how the original dataset should be used.  I used to just treat it as simply additional data and immediately join it to the train dataset. But, I have found that is not the best way to handle it. The problem with doing this is that it will change the distribution of your OOF validation data when doing CV.  The synthetic data seems to always be much noisier than the original data. This post from @sergiosaharovskiy is a great visualization of the observed phenomenon.  Including the original data in the OOF validation data will give you a better CV score than what you can expect when using your model to predict purely synthetic test data. I have found the best way to use the original data is to keep it separated from your train data and then concat the entire original data back to each fold in your cross validation.  Example code below where X_original and y_original are the entire original dataset. ` kf = KFold (n_splits= 10 , random_state= 8 , shuffle=True) for train_idx, val_idx in kf .split (X_tr, y_tr):\n    X_t, X_val = X_tr .iloc [train_idx] , X_tr .iloc [val_idx] y_t, y_val = y_tr .iloc [train_idx] , y_tr .iloc [val_idx] X_train = pd .concat ( [X_t, X_original] , ignore_index = True)\n    y_train = pd .concat ( [y_t, y_original] , ignore_index = True)\n\n    model = LGBMRegressor ()\n\n    model .fit (X_train, y_train, eval_set= [(X_val, y_val)] )\n\n    y_pred = model .predict (X_val)\n\n    score = mean_squared_error (y_val, y_pred, squared=False) content_copy ` Models and Ensemble I ended up tuning 4 models that I was happy with and wanted to see how they would perform as an ensemble. Model 1: LightGBM using all features of the original dataset except for geometry Model 2: LightGBM same as model 1 but also added two new features ‚Äòadiabatic_surface_area‚Äô and ‚Äòsurface_diameter_ratio‚Äô.  Credit to @tetsutani and his notebook for the feature ideas Model 3: LightGBM same as model 2 but also added a single feature/component using PLSRegression on features mass_flux, pressure, and chf_exp.  Once again thanks to @adaubas for this post which led me to learning about PLS. Model 4: XGBoost same features as model 1 I had originally planned to explore @tetsutani ‚Äôs notebook more and try out LAD regression and hill climbing from @samuelcortinhas notebook here .  Unfortunately, I simply ran out of time and my ensemble ended up just being 4 basic weights for each model. Conclusion One thing that this competition further reinforced in me is that the most important thing to strive for is getting a CV workflow that you trust in.  Let your CV scores guide your decision making and don‚Äôt place too much weight on the public LB scores. Once again, thanks to everyone in the community for being so welcoming and eager to share their knowledge!  I have learned so much! Best of luck in your future competitions! Please sign in to reply to this topic. comment 4 Comments Hotness Samuel Cortinhas Posted 2 years ago arrow_drop_up 1 more_vert Well done @alexdippolito , very nice write up too! Mykhailo Savchenko Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Congratulations @alexdippolito on getting 3rd place, glad to see that my post helped! Including the original data in the OOF validation data will give you a better CV score that what you can expect when using your model to predict purely synthetic test data. You can append a column 'type' with 0 and 1 for competition and training dataframes respectively, then define a custom RMSE function to accept only rows with 'type' == 1 , or just use .iloc[:first_row_of_original] instead of a dummy column, that should also work. Alex D'Ippolito Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Congrats on 2nd place!  Your post was definitely a big help! aldparis Posted 2 years ago arrow_drop_up 2 more_vert @alexdippolito , Congratulations for your work and your stratregy ! Glad to see my previous post helped  you. Your CV strategy was the good one for the big jump in private LB !",
      "Feature Imputation with a Heat Flux Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Feature Imputation with a Heat Flux Dataset Playground Series - Season 3, Episode 15 Feature Imputation with a Heat Flux Dataset Overview Data Code Models Discussion Leaderboard Rules Iqbal Syah Akbar ¬∑ 5th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 26 more_vert 5th Place Solution | Imputation Without Any Imputers Well‚Ä¶ the shakeup is bigger than I thought, and I certainly didn't expect that my gamble has paid off‚Ä¶ or sort. Also I've finally reached top 5 after all this time joining Playground Series! So if you read the title at first, you might be thinking that it's impossible. However, that's exactly what I did: I wrote my own code to impute all features except x_e_out [-] . Imputation A few days ago @shalfey posted an interesting stuff about the characteristics of original dataset, such as how each author strictly only has one geometry . The thing is, I found the exact same thing one week earlier , with only Pandas Dataframe's groupby method. Here are most of the characteristics that I've found: Each author has their own unique geometry For some authors, there is only one unique pressure [MPa] For some authors, there is only one unique pair of D_e [mm] and D_h [mm] For some authors, there is only one unique length [mm] For each pair of chf_exp [MW/m2] and length [mm] , there is only one unique geometry Each D_h [mm] has exactly one unique geometry Each D_h [mm] has exactly one unique D_e [mm] My imputation is basically finding the missing value based on the value of other features in original dataset. For example, in order to impute D_e [mm] , I have to use available D_h [mm] value on the competition dataset and use it to search D_e [mm] on the original dataset. Now, some of you have probably known that since we're using synthesized dataset, making some relationship between features get messed up and creating noises in the dataset. Obviously this will make competition dataset different from original dataset. However, this isn't a problem. If you can't find the value in the original dataset, I just need to find the closest value to it . This is also how I impute features that need more than one other features, such as relationship 5. For example, if I want to impute geometry based on chf_exp [MW/m2] and length [mm] , I have to find all unique values of length [mm] in original dataset that is related to the value of chf_exp [MW/m2] on the competition dataset, then find the closest unique values to length [mm] on the competition dataset. Only after that, I try to find geometry based on the chf_exp [MW/m2] and length [mm] unique values that we've found. These are a lot. However, it's only enough to almost fully impute both geometry and D_e [mm] . In order to impute the rest, I have to do some improvisation , such as imputing D_h [mm] based on D_e [mm] , imputing author based on length [mm] , imputing geometry based on chf_exp [MW/m2] and length [mm] , etc.  I also have to repeat some imputations to get rid of the missing values. These aren't fully accurate to the original dataset of course, but our competition dataset has messed up relationship with noises anyway, so it doesn't matter. The full code and almost fully imputed dataset is available here if you want to read it. Feature Engineering Some of my feature engineering ideas are based on cylinder. I was inspired to do this based on the existence of length and diameter features. My ideas are as follows: Difference between heated and hydraulic diameter Cylinder surface area and its difference between heated and hydraulic version Cylinder volume and its difference between heated and hydraulic version There is also one other idea. It's based on how x_e_out [-] doesn't have any metric, as you can see from its feature name. The idea is to do some simple math operations on pressure [MPa] , mass_flux [kg/m2-s] , and chf_exp [MW/m2] in order to get rid of their metrics. The effect of those ideas are‚Ä¶ mixed to be honest. Some models got improvement, some got worse. However, it didn't matter as I use different subset of feature engineering ideas to build my ensemble anyway. Models and Ensemble This is nothing special to be honest, and it's probably the cause of me failing to get higher position. I use different subset of feature engineering ideas to creat different models, use different estimators on some of them. In order to find the optimal weight, I use Ridge regression, allowing both interceptor and negative coefficient to be fitted. I also use original dataset to train my models (I don't include it on my validation of course). Finally, in order to tune my models, I use Optuna for Gradient Boosting such as XGBoost and LightGBM, and use manual tuning for everything else. That's it, there is not much else to explain here. The full code of my model building and EDA is available here . You can also see comparison between my imputed dataset and the competition dataset here. Thank you for reading, and I hope this will be useful to you! Please sign in to reply to this topic. comment 11 Comments Hotness Samuel Cortinhas Posted 2 years ago arrow_drop_up 1 more_vert Congratulations @iqbalsyahakbar ! Good result KISSEL Posted 2 years ago ¬∑ 75th in this Competition arrow_drop_up 1 more_vert Congrats with 5th place, @iqbalsyahakbar , interesting approach to feature engineering, keep going :) Iqbal Syah Akbar Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Thanks @alekseyfomin ! I actually wanted to share my ideas but I don't have much free time lately üôÉ My time on Kaggle last week could only be spent on fixing my notebook. bogoconic1 Posted 2 years ago ¬∑ 215th in this Competition arrow_drop_up 1 more_vert Congrats on 5th place! My earlier submissions with manual and KNN imputation ended up with a better private LB score (which I did not choose) than using IterativeImputer (I chose this submission which got hit by the shakeups pretty badly) - quite unexpected lol Iqbal Syah Akbar Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Thanks @yeoyunsianggeremie ! I actually tried IterativeImputer early on but the result was mixed, so I abandoned it. I think SimpleImputer was much more efficient for getting almost the same score. bogoconic1 Posted 2 years ago ¬∑ 215th in this Competition arrow_drop_up 0 more_vert No probs! In my case, IterativeImputer got a better public LB score and OOF RMSE than my KNN and SimpleImputer submissions Public -> Private KNN 0.755 -> 0.72855 SimpleImputer 0.762 -> 0.7295 IterativeImputer 0.745 -> 0.733 I guess overfitting for imputation is a thing here Iqbal Syah Akbar Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Yes, you have to trust your own CV instead of public LB. There is also a matter of how sensitive RMSE is to outlier and well‚Ä¶ you can see the shake-up. SeanInAction Posted 2 years ago ¬∑ 22nd in this Competition arrow_drop_up 1 more_vert Congrats on getting top 5! I think the key to this competition is stubborn manual imputation lol. I only manually imputed the D_h, D_e and geometry on the rows missing geometry and used CatBoost to impute the rest, with CatBoost+XGBoost+LGBM ensemble for training. Your feature engineering and manual imputation techniques are very useful to me, thanks for sharing your epic solution! Iqbal Syah Akbar Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Thanks @seaninaction ! I'm sure you can use imputers on mass_flux [kg/m2-s] and maybe pressure [MPa] . Other than that however, it's best to try manual imputation as there are very few unique values. SeanInAction Posted 2 years ago ¬∑ 22nd in this Competition arrow_drop_up 0 more_vert Yeah I was just too lazy to manually impute the other values lol SOUMENDRA PRASAD MOHANTY Posted 2 years ago arrow_drop_up 1 more_vert @iqbalsyahakbar , It is a great achievement of a guy who started leasing ML before 3 months & now topping the LB . Impressive work go on . Demyan Pavlyshenko Posted 2 years ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Congratulations for 5th place @iqbalsyahakbar ! It was a very good solution‚úä Mykhailo Savchenko Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert @iqbalsyahakbar , congratulations on getting the 5th place and thanks for mentioning my post. You did a great job and found a great way to impute missing features, which was the key to winning this competition ü§ó Iqbal Syah Akbar Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Thanks! Also congrats on reaching 2nd @shalfey ! It makes me wonder if there's better way to impute the dataset since everything I did past the unique characteristics I've mentioned are improvisation Also now I'm curious what will happen if I did the same process on x_e_out [-] , since I didn't do it back then because I couldn't know how reliable the CV will be üòÇ Mykhailo Savchenko Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert It makes me wonder if there's better way to impute the dataset since everything I did past the unique characteristics I've mentioned are improvisation I will write a post describing the imputation techniques I used, but in general, that's very similar to what I have written before in the topic you mentioned: I combined imputation based on the original dataset with tree-based imputers. Also now I'm curious what will happen if I did the same process on x_e_out [-], since I didn't do it back then because I couldn't know how reliable the CV will be üòÇ @alexdippolito earlier pointed out that this time the rows with same features can have different targets, so I think imputing target values manually wouldn't help here. Makes sense though, because equilibrium quality also depends on some other variables which aren't present in the dataset, which made data more \"real\" than in the last competition, where there were a lot of duplicated targets (which I believe is impossible IRL) and we predicted yield using already known fruit mass üôâ Iqbal Syah Akbar Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert That's a good point, I was afraid to do the same process because there were too many unique values from the original (not to mention the noise and synthesized values) and too many missing values.",
      "Feature Imputation with a Heat Flux Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Feature Imputation with a Heat Flux Dataset Playground Series - Season 3, Episode 15 Feature Imputation with a Heat Flux Dataset Overview Data Code Models Discussion Leaderboard Rules Demyan Pavlyshenko ¬∑ 6th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 12 more_vert 6th place solution Hello everyone! In this solution I used model blending with 5 LGBM models based on different imputation techniques and parameters and concatenating their train set with original set (when the data is splitted on valid and train set, in other words inside CV loop) , and with a little feature engineering/imputation techniques also. Also note, there will be a lot of techniques which are already discussed in my discussion . I'll point all moments which are already discussed in my discussion to not waste time for explanation Feature Engineering In feature engineering I've only applied logarithm to D_e , D_g and chf_exp in order to skew their distribution to right a bit, since they had left-skewed distribution, as was discussed in my discussion. And that's whole feature engineering Feature Imputation I was blending 5 LGBM models using different feature imputation techniques, so, about them I will tell about it in Blending section. But there are also some general feature imputation techniques which are applied to all models in the blending. As was discussed in my disscusion, I've imputed geometry values using author values As @aadinesarkar pointed in my discussion, I've imputed geometry values D_e and D_h values. If they are equal, then geometry is tube If D_e is equal to 15.0 or D_h is equal to 120.0, then geometry is plate As for annulus there are a few values of D_e , the imputation can be inaccurate, so I've omitted it Similarly as we imputed geometry values, we impute D_e and D_h in reverse Blending Notes We will use general imputation for all models. We are tuning only these parameters - learning rate , num_leaves , max_depth , colsample_bytree , subsample , min_child_samples . We are concatenating train set with original set (valid set must be from synthetic dataset) When we say \"without imputing \" it doesn't mean that imputer don't fit on these features, it means that we just leave these features as they are lgbm_without_imputation this model is just a LGBM without any feature imputation (except the general imputation) and optimized by optuna lgbm_lgbmimputer1 this model is trained on imputed data by LGBMImputer(n_iters=200) , but without imputing D_h , length , D_e (we are still using general imputation, as for all models). Also we are using lgbm_without_imputation tuned parameters changing max_depth and min_child_samples parameters lgbm_lgbmimputer2 this model is trained on imputed data by LGBMImputer(n_iters=200) , but without imputing mass_flux , D_e , length , D_h . Also we are using lgbm_without_imputation tuned parameters changing max_depth and min_child_samples parameters lgbm_lgbmimputer3 this model is trained on imputed data by LGBMImputer(n_iters=200) , but without imputing mass_flux , D_e , D_h . Also we are using lgbm_without_imputation tuned parameters without changing lgbm_lgbmimputer4 this model is trained on imputed data by IterativeImputer(LinearRegression(), max_iter=20, initial_strategy='constant') , but without imputing mass_flux , pressure . Also we are using lgbm_without_imputation tuned parameters changing only max_depth parameter And concatenating it using LinearRegression without interecept That's it! Also, I wanted to tune parameters by minimizing loss of the other model, which is trying to predict chf_exp feature, since x_e_out is the most important feature to predict chf_exp , but the score didn't change, so I didn't do things complicated. The implementation of all these things is in my solution notebook Good luck in the next competitions! Please sign in to reply to this topic. comment 1 Comment Hotness KISSEL Posted 2 years ago ¬∑ 75th in this Competition arrow_drop_up 1 more_vert Congrats with 6th place, @demko1 , nice solution, keep going! :)",
      "Feature Imputation with a Heat Flux Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Feature Imputation with a Heat Flux Dataset Playground Series - Season 3, Episode 15 Feature Imputation with a Heat Flux Dataset Overview Data Code Models Discussion Leaderboard Rules Kirderf ¬∑ 9th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 9 more_vert 9th place solution + Pseudo label + IterativeImputer + Multi AutoGluon Only had a few hours with this competition as I started the same day but nevertheless everything was possible and glad to see the 9th place üòä The final solution is a mix of some ideas that I thought was possible before deadline: Extend the training data with pseudo labeling: Pseudo label the missing target with an ensemble of the top 5 public solutions . Feature Engineering with Sklearn IterativeImputer with a DecisionTreeRegressor: As the training data had many missing values both in target and in the features Sklearn IterativeImputer with a DecisionTreeRegressor could be a good alternative and later ensembled to the other standard handling of missing values. Use AutoML frameworks to speed things up and is a great option specially in regression problems as they comes with lots of different models, which is good to have in the final ensemble. With the above ideas I trained a mix of them in total of 5 solutions with Auto Gluon framework, e.g. one plain untouched training pipeline and one with Feature Engineering with many iterations of Sklearn IterativeImputer + DecisionTreeRegressor, 2 stage pseudo labeling (first on public then from the trained AG models) and also with the AG FTTransformer to the models. The 5 trained solutions then where weighted ensembled based on the result. That‚Äôs it! üòä Please sign in to reply to this topic. comment 2 Comments Hotness Andrey Posted 2 years ago ¬∑ 208th in this Competition arrow_drop_up 0 more_vert Thanks for the summary. Could you share your solution if that's not too much to ask? I want to see what's possible within a couple of hours and how pseudo-labelling is done. This comment has been deleted.",
      "Feature Imputation with a Heat Flux Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Feature Imputation with a Heat Flux Dataset Playground Series - Season 3, Episode 15 Feature Imputation with a Heat Flux Dataset Overview Data Code Models Discussion Leaderboard Rules ÍπÄÍ¥ëÎ•ú ¬∑ 11th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 2 more_vert 11th AutoML is all you need ÏïàÎÖïÌïòÏÑ∏Ïöî Ï∫êÍ∏ÄÎü¨ Ïó¨Îü¨Î∂Ñ ! Hello Kagglers ! It was a fresh competition that filled many None values, so I had fun preprocessing it. Thank you Kaggle. In my experience, baseline codes are often overfitted, so I didn't refer to other codes at all. I did the EDA and found some patterns. EDA 1) Relationship between \"author\" and \"geometry\" 2) Relationship between D_e [mm] and D_h [mm] Preprocess Apply discovered patterns fillna('Unknown') fillna Mode() fillna Mean() Label Encoding I was lucky Modeling 1) mljar 2) AutoGluon Check My Notebook(Maybe not easy run on Kaggle) ! 11th AutoML is all you need Thanks ! Please sign in to reply to this topic. comment 2 Comments Hotness Ankit Hanwate Posted 2 years ago arrow_drop_up 0 more_vert I had a question regarding missing values. How do you know when to fill the missing values with mean and when to use median? ÍπÄÍ¥ëÎ•ú Topic Author Posted 2 years ago ¬∑ 11th in this Competition arrow_drop_up 0 more_vert Just try and chek",
      "Feature Imputation with a Heat Flux Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Feature Imputation with a Heat Flux Dataset Playground Series - Season 3, Episode 15 Feature Imputation with a Heat Flux Dataset Overview Data Code Models Discussion Leaderboard Rules Matt OP ¬∑ 14th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 22 more_vert 14th Place Solution: hillclimbers w/ negative weights Hi everyone, I didn't have as much time to work on this competition but I still wanted to share my solution & thoughts. These competitions are always fun to be a part of and I really appreciate everyone who contributed and participated. Here are the people that created strong models that I would like to give thanks (please upvote their work): @arunklenin | notebook: üåã PS3E15 | Iterative CatBoost Imputer|Ensemble @tetsutani | notebook: PS3E15 EDA| Ensemble and Stacking baseline @onurkoc83 | notebook: KNN ƒ∞mputer (LB score :0.075094) @iqbalsyahakbar | notebook: PS3E15 | EDA, Imputing, Ensemble for Beginners I ended using my project hillclimbers (again) to create the final ensemble. I won't go through all the details of hillclimbers, but here is a link to my 4th Place Solution in the previous Playground Series episode where I also used hillclimbers and you can find more in-depth explanations. Here is a visual presentation of the final submission: A couple things to note: My model (the first hillclimbers model) used @arunklenin 's preprocessing & feature engineering. I manually tuned the hyper-parameters for every model. I noticed there was heavy reliance on the following 3 models in a lot of the public notebooks: XGBoost, CatBoost, LGBM. This is not specific to this particular competition, but more so to the Playground Series as a whole. The goal of building my model was an attempt to diversify the predictions even though I was still using several gradient boosting models. The model I personally created ended up with a better Private LB score: 0.072698 than the final submission 0.072741 despite having a lower CV score which took me by surprise. I didn't end up choosing it as one of my two submissions. Here is the weights used for the hillclimbing models: Thanks again, I am open to any and all feedback! Please sign in to reply to this topic. comment 4 Comments Hotness Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert Congratulations dear friend and thanks for yours 14th solution! Matt OP Topic Author Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 1 more_vert Thank you @mpwolke , I always appreciate your feedback! Alpay Abbaszade Posted 2 years ago ¬∑ 179th in this Competition arrow_drop_up 2 more_vert Hello @mattop First of all, congratulations to you.üëç Being included in the rankings for the second time using the same optimization algorithm shows that the results of this method are not coincidental. Honestly, I still have some biases because hill climber is a local optimum search algorithm, so it can cause the issue of overfitting. Am I mistaken? Or does the package you use have any parameters to prevent this? Do you take any measures to trust the results obtained with Hill Climbers? Matt OP Topic Author Posted 2 years ago ¬∑ 14th in this Competition arrow_drop_up 2 more_vert Hi @alpayabbaszade , thanks for the comment. Hill climbing at it's core is just looking at each model you would like to add to the ensemble and iterating through an array of weights to see which weight will improve the local CV score on the training set. So there is an element of how similar the training and test sets are to each other. If they are very similar (which is the case with these synthetically generated datasets) you should expect to have good & accurate performance. There are not really any specific parameters to prevent overfitting since it is just trying to improve the local CV score on the training set so it's best to cross validate every model you are looking to potentially add to the ensemble. Here I will show an example of how you can kind of \"trick\" hill climbing to add models that won't really help to generalize the predictions: mean_constant and median_constant are the mean & median value of the target feature on the training set respectively. mean_mean and the others are the code as shown below using different variations of mean and median in line 4 and 7: def mean_mean_preds(df): for col in df. columns : if col not in [ \"id\" , target]:\n            mean_values = df.groupby( col )[target]. mean ()\n            df[f \"{col}_mean\" ] = df[ col ]. map (mean_values)\n\n    df[ \"pred\" ] = df[[ col for col in df. columns if col .endswith( \"_mean\" )]]. mean (axis= 1 ) return df[ \"pred\" ]. values content_copy This CV score is the best by far but the Private LB score goes way down: 0.072953 (the Public LB score as well) Alpay Abbaszade Posted 2 years ago ¬∑ 179th in this Competition arrow_drop_up 1 more_vert Thank you for this excellent explanation @mattop"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 15 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Predicting Critical Heat Flux dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 2 files 1.73 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 1.73 MB data.csv sample_submission.csv 2 files 12 columns ",
    "data_description": "Feature Imputation with a Heat Flux Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Feature Imputation with a Heat Flux Dataset Playground Series - Season 3, Episode 15 Feature Imputation with a Heat Flux Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start May 16, 2023 Close May 30, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in May every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc.. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = ‚àö 1 N N ‚àë i = 1 ( y i ‚àí ÀÜ y i ) 2 where ÀÜ y i is the predicted value and y i is the original value for each instance i . Submission File This is an imputation problem. You are to predict the missing values of the feature x_e_out [-] (with the corresponding row id ). The file should contain a header and have the following format: id ,x_e_out [-] 4 , 0 . 00 7 . 0 . 12 10 ,- 0 . 02 etc . content_copy Timeline link keyboard_arrow_up Start Date - May 16, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  May 29, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Feature Imputation with a Heat Flux Dataset. https://kaggle.com/competitions/playground-series-s3e15, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 2,077 Entrants 713 Participants 693 Teams 5,853 Submissions Tags Beginner Tabular Physics Regression Root Mean Squared Error Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e16",
    "discussion_links": [
      "/competitions/playground-series-s3e16/discussion/416783",
      "/competitions/playground-series-s3e16/discussion/416769",
      "/competitions/playground-series-s3e16/discussion/416819"
    ],
    "discussion_texts": [
      "Regression with a Crab Age Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Crab Age Dataset Playground Series - Season 3, Episode 16 Regression with a Crab Age Dataset Overview Data Code Models Discussion Leaderboard Rules Ravi Ramakrishnan ¬∑ 3rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 67 more_vert 3rd private 6th public | Brute-force ensemble| Post-processing Hello all, Thanks to Kaggle for the challenge/ episode! It was a great experience to participate in a good regression challenge in this episode. Also, thanks to the fellow competitors for making this a memorable experience. My approach is as below- Data engineering I considered the data generation notebook provided here as a base and generated a number of samples with varying parameters. I appended this additional synthetic data with the original data and used it collectively to train the model I used the below additional features in the model otherwise. These features are mentioned in the below discussion post- https://www.kaggle.com/competitions/playground-series-s3e16/discussion/415721 , many thanks to @pandeyg0811 for the efforts. a. Meat yield b. Surface Area c. Weight/ Shuck Weight d. Pseudo BMI e. Weight/ Length Squared f. Viscera Ratio I also used log(1+x) transform on the weight column, but it did not help me greatly Model strategy I used a regression model approach rather than a classification approach using a stratified 5-fold CV. Validation was performed on the competition data only. CV correlated well with the public LB I trained several candidate models with feature subsets, using the provided features and additional feature-subsets, from 6-10 features per model run. I trained several candidates using the same OOF structure, but different features per run. I used tree models as alongside- XgBoost, LightGBM, Catboost, Gradient Boosting Machine, HistGradientBoostingRegressor as base learners I used optuna and LAD regression to tune the predictions in a subsequent ensemble per run with(out) post-processing and discovered the favorable impact of post-processing (rounding to the nearest integer) on the CV score and LB score. Post-processing my predictions helped me improve my LB score considerably throughout the challenge I did not post-process any predictions based on the training/ original data. Most of my models posited predictions in the range of 4-20 years only. I did not impute/ overlay any train/ original predictions on this data. Whenever I tried to do this, my score would decrease. Perhaps my method was plain and sub-optimal, while some creativity was needed herewith. What did not work TabNet regressor Random Forest Extra Tree Linear approaches like Ridge/ LASSO What I could have done better I could have perhaps selected a better submission as my final submission. My best private set submission could be yielded a 2nd place in the challenge. It fared slightly worse on the public LB but had a better CV score. I choose the submission faring the best on the public LB as a candidate I could have tried to perhaps use the original/ train data to impute edge cases better. I did not find sufficient time to do this creatively in the final days of the challenge, but I think this could have improved my score. I may try and do this later as a personal experiment. My learnings Trust the CV Never deviate from rule 1 Devise a good pipeline from Day1 to assist one throughout the challenge and beyond. Messy code structure seldom helps one across several challenges Fail quickly- conduct several quick experiments and devise a strategy in the initial days Include features that add value and stay away from value-destroying features. I got a first hand experience of this point from this assignment to great effect Submissions on the final day may not be the best overall. I experienced this truly well in the episode. My final submissions were created much before the final day. Wishing everyone the best, happy learning and regards! See you in the next episode! Tabular Regression Ensembling Please sign in to reply to this topic. comment 24 Comments 3 appreciation  comments Hotness Khoa Tran Posted a year ago ¬∑ 729th in this Competition arrow_drop_up 2 more_vert This was very helpful for beginners like me! I appreciate it! Ravi Ramakrishnan Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 3 more_vert I wish to extend thanks to of you for wishing me! I would have perhaps thanked each one of you individually, but it may be misconstrued as spam. Happy learning and best regards! @atefbouzid @alihaider12 @abhishek14398 @danilalapokin @pandeyg0811 @alekseyfomin @k201725muhammadosama @poojach7611 @datascientistsohail @samuelcortinhas @san2deep @roberttrypuz @yerokhinartem ! Robert Trypuz Posted 2 years ago ¬∑ 150th in this Competition arrow_drop_up 3 more_vert Congrats, @ravi20076 ! üéâ Thank you for sharing your thoughts! I'm not sure it's fair to ask, but I'd like to kindly as you if you could elaborate on point 1 in Data engineering paragraph, i.e., how did you choose the params to generate a number of samples. Tarek Elkhateb Posted 2 years ago arrow_drop_up 2 more_vert Well done Ravi. Shrijayan Posted 2 years ago arrow_drop_up 2 more_vert Excellent! Adamelloos Posted 2 years ago ¬∑ 272nd in this Competition arrow_drop_up 2 more_vert Well done! Ifeanyichukwu Nwobodo Posted 2 years ago ¬∑ 646th in this Competition arrow_drop_up 2 more_vert Congratulations üéâüéâüéâ What I like the most about your strategy is that they are easy to implement and have little computing power. I also noticed you've been consistent with your technique for the past few competitions and it never seems to fail. Thanks for sharing Sanket Kumar Mali Posted 2 years ago ¬∑ 613th in this Competition arrow_drop_up 2 more_vert How do you come up with derived features. How to build the insights from a dataset like this. Pooja Chauhan Posted 2 years ago arrow_drop_up 2 more_vert Impressive approach! Your use of data engineering, ensemble models, and post-processing techniques shows a deep understanding of regression modeling. Well done on your performance in the challenge! @F Posted 2 years ago ¬∑ 222nd in this Competition arrow_drop_up 2 more_vert Great job @ravi20076 ! I'm curious to know if you specifically addressed the issue of outliers in your analysis. Did you implement any strategies or techniques to handle outliers in the dataset? Ravi Ramakrishnan Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert I thought of doing that but refrained from doing it. Myopic loss aversion took over perhaps @atefbouzid I may treat outliers and then assess my CV score as an experiment later on! @F Posted 2 years ago ¬∑ 222nd in this Competition arrow_drop_up 2 more_vert Best of luck with your future experiments! @ravi20076 Muhammad Osama Posted 2 years ago arrow_drop_up 2 more_vert @ravi20076 nice work, would highly appreciate if you visit my work too. https://www.kaggle.com/code/k201725muhammadosama/sample-selection Sohail Ahmed Posted 2 years ago ¬∑ 913th in this Competition arrow_drop_up 2 more_vert Nice Work, Congratulations Allena Venkata Sai Abhishek Posted 2 years ago ¬∑ 196th in this Competition arrow_drop_up 2 more_vert Thanks for the post! @ravi20076 Congratulations Samuel Cortinhas Posted 2 years ago arrow_drop_up 2 more_vert Well done Ravi, nice result! Danila Lapokin Posted 2 years ago ¬∑ 19th in this Competition arrow_drop_up 2 more_vert Thanks for the post! in reality, in the competition, all high months have similar solutions, but as a result, micromoments influenced emoji_people SANDEEP KUMAR Posted 2 years ago ¬∑ 935th in this Competition arrow_drop_up 2 more_vert Thanks a lot for sharing your knowledge. This was my first competition and really it helps me a lot. KISSEL Posted 2 years ago ¬∑ 32nd in this Competition arrow_drop_up 2 more_vert Congratulations, @ravi20076 , great result, thanks for sharing the strategy of solving the task. I'm very happy for you üéâüòä –ê—Ä—Ç–µ–º –ï—Ä–æ—Ö–∏–Ω Posted 2 years ago ¬∑ 1178th in this Competition arrow_drop_up 2 more_vert Thank you for sharing your insights from the episode! emoji_people Gaurav Pandey Posted 2 years ago ¬∑ 95th in this Competition arrow_drop_up 2 more_vert Hey @ravi20076 , Happy to know, that these features helped you. This comment has been deleted. Appreciation (3) Priyanshu54200 Posted a year ago arrow_drop_up 2 more_vert Thanks helped a lot Jayesh Jain Posted 2 years ago arrow_drop_up 2 more_vert Thank you !! Ali Haider12 Posted 2 years ago ¬∑ 25th in this Competition arrow_drop_up 2 more_vert Thanks for sharing.",
      "Regression with a Crab Age Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Crab Age Dataset Playground Series - Season 3, Episode 16 Regression with a Crab Age Dataset Overview Data Code Models Discussion Leaderboard Rules Oscar Aguilar ¬∑ 5th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 23 more_vert #5 Solution | five models + LADRegression Hi all, First, I would like to start with a big thank you to Kaggle for running this episode of the playground series. In this post, I will briefly explain my approach, which most of it can be found in my notebook . Preprocessing I did not preprocess the data at all. Feature Engineering There were some awesome features engineering post in the competition. However, in my model only a few features improve the performance of my models. The features that help are: X [ 'Meat Yield' ] = X [ 'Shucked Weight' ] / ( X [ 'Weight' ] + X [ 'Shell Weight' ]) X [ 'Shell Ratio' ] = X [ 'Shell Weight' ] / X [ 'Weight' ] X [ 'Weight_to_Shucked_Weight' ] = X [ 'Weight' ] / X [ 'Shucked Weight' ] X [ 'Viscera Ratio' ] = X [ 'Viscera Weight' ] / X [ 'Weight' ] content_copy Notice that the above features were suggested in this post by @pandeyg0811 . Modeling & Ensemble I considered a 10-fold CV framework with the following five models: GradientBoosting HistGradientBoosting LightGBM XGBoost CatBoost And I ensemble those five model predictions using LADRegression . Notice that I rounded the five model predictions to the nearest integer before I ensemble them with LADRegression (this boost the CV a lite bit). Then, I re-run the above framework with different seeds and ensemble the predictions by computing the mode of the predictions for each of the id in the test dataset. What didn't work I tried several features (some of them I engineered on my own, and other suggested in a few discussion posts) but only the ones listed on the feature engineering improve model performance. I also tried FLAML but due to my lack of experience with the framework, I could not build a decent enough model. Conclusion In this competition, my local CV aligned pretty well with the LB and private LB. So, the moral of the story is to trust on your CV. Regression Ensembling Tabular Please sign in to reply to this topic. comment 8 Comments Hotness @F Posted 2 years ago ¬∑ 222nd in this Competition arrow_drop_up 1 more_vert Excellent work, @oscarm524 ! Did you employ any specific strategies or techniques to effectively handle outliers in your dataset? Oscar Aguilar Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Hi @atefbouzid . I tried a couple of the suggestions posted on the discussion forum for dealing with outliers, but none of those ideas help to boost model performance. Thus, I didn‚Äôt do anything to the outliers; that is, I modeled the data the way it is. @F Posted 2 years ago ¬∑ 222nd in this Competition arrow_drop_up 1 more_vert Good luck with your future experiments @oscarm524 ! aldparis Posted 2 years ago arrow_drop_up 1 more_vert Happy to see LADRegression did work again in a MAE competition. Congratulations @oscarm524 Oscar Aguilar Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Thanks @adaubas for introduce me to LADRegression . I didn't use it before you mentioned in Episode 14. aldparis Posted 2 years ago arrow_drop_up 1 more_vert Neither did I. Mar√≠lia Prata Posted 2 years ago ¬∑ 338th in this Competition arrow_drop_up 1 more_vert A huge congratulation Aguilar for your 5th place position. Thank you for sharing what worked and what didn't work too. Robert Trypuz Posted 2 years ago ¬∑ 150th in this Competition arrow_drop_up 1 more_vert Congrats, @oscarm524 ! Thank you for sharing! Best üëç Mykhailo Savchenko Posted 2 years ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert @oscarm524 you did a great job on this competition, thank you for your contribution and insights! emoji_people Gaurav Pandey Posted 2 years ago ¬∑ 95th in this Competition arrow_drop_up 1 more_vert Hey @oscarm524 , Glad to know that the features helped you.",
      "Regression with a Crab Age Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Crab Age Dataset Playground Series - Season 3, Episode 16 Regression with a Crab Age Dataset Overview Data Code Models Discussion Leaderboard Rules Mykhailo Savchenko ¬∑ 11th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 16 more_vert #11 solution: what worked and what didn't Congratulations to the winners and all the participants! Also, I would like to say thank you to Kaggle who hosts playground competitions, making it easier for the beginners to start working on real competitions in simplified form. This playground was very interesting because of how many things I tried and how many didn't work out. My plan was as follows: build a good baseline model using AutoML tools try different feature engineering techniques to capture more relationship between weight-related features try to generate more data and use it for training try to find some \"tricks\" like in previous competitions, like manually correcting the rows with noise (something we have discussed with @epiktistes a day earlier) I have tried the following and most of those didn't work out: feature engineering with creating various ratio rows like \"shell-to-total\", \"body condition index\" didn't improve the score and took longer to train I generated 400000 additional samples and used it for training, which reduced score a little then I noticed that the predictions are always in the range from 3 to 19 month, so I tried to find the outliers manually and fix it. Unfortunately, that turned out to be impossible due to how similar crabs are after they reach certain age after that I tried to generate more samples for old crabs, but that one didn't work too So I just thought that well, maybe this competition doesn't have \"tricks\" like these and it will be all about tuning the model. I was quite right and, to my surprise, the highest scoring submission was the first baseline I built with three LGBM and two XGBoost models, which I have tuned using FLAML. Unfortunately, I didn't select that one. Secondly, I messed up submission names and didn't write the CV score in description for some files, so I ended up selecting not the best model as a final submission. And finally, I had some more hypothesis I could test, like generating more samples for younger crabs, I didn't have enough time for those. However, the 11th place is good anyway. What have I learned from this one? Sometimes ideas seem to be great, but they don't work in practice. In this case, just stick to a model with the best score and tune it further. As always trust your own CV, though this time the shake-up wasn't too bad. Keep the work clean and organized using proper file descriptions with detailed explanations what have been done and what score has been reached. Spend some time on discussions with the community, a lot of ideas are discussed in public and implementing those often helps. Thanks everyone who participated and see you in the next one. Big thanks to @epiktistes , @phongnguyen1 , @oscarm524 , @arunklenin , @ravi20076 and @pandeyg0811 for staying active and sharing a lot of valuable insights during this competition. Keep Kaggling! Please sign in to reply to this topic. comment 2 Comments Hotness Robert Trypuz Posted 2 years ago ¬∑ 150th in this Competition arrow_drop_up 2 more_vert Contrats, @shalfey ! üéâ Thank you for sharing your thoughts and your overall engagement in community discussion! Many of your hints were really useful! Good luck! Best! Mykhailo Savchenko Topic Author Posted 2 years ago ¬∑ 11th in this Competition arrow_drop_up 2 more_vert Thank you @roberttrypuz , I am glad my posts were helpful! Good luck with the next competition! üî•"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 16 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Crab Age Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note: You can use this notebook to generate additional synthetic data for this competition if you would like. 3 files 9.05 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 9.05 MB sample_submission.csv test.csv train.csv 3 files 21 columns ",
    "data_description": "Regression with a Crab Age Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Crab Age Dataset Playground Series - Season 3, Episode 16 Regression with a Crab Age Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2023 Kaggle Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! Your Goal: For this Episode of the Series, your task is to use regression to predict the age of crabs given physical attributes. Good luck! Start May 30, 2023 Close Jun 13, 2023 Evaluation link keyboard_arrow_up Evaluation Submissions will be evaluated using Mean Absolute Error (MAE) , where each x_i represents the predicted target, y_i represents the ground truth, and n is the number of rows in the test set. Submission File For each id in the test set, you must predict the target Age . The file should contain a header and have the following format: id ,yield 74051 , 10 . 2 74051 , 3 . 6 74051 , 11 . 9 etc . content_copy Timeline link keyboard_arrow_up Start Date - May 30, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  June 12, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Regression with a Crab Age Dataset. https://kaggle.com/competitions/playground-series-s3e16, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 3,807 Entrants 1,467 Participants 1,429 Teams 10,531 Submissions Tags Beginner Tabular Regression Animals Mean Absolute Error Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e17",
    "discussion_links": [
      "/competitions/playground-series-s3e17/discussion/419730",
      "/competitions/playground-series-s3e17/discussion/419643",
      "/competitions/playground-series-s3e17/discussion/419648",
      "/competitions/playground-series-s3e17/discussion/419708"
    ],
    "discussion_texts": [
      "Binary Classification of Machine Failures | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification of Machine Failures Playground Series - Season 3, Episode 17 Binary Classification of Machine Failures Overview Data Code Models Discussion Leaderboard Rules Mathurin Ach√© ¬∑ 3rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 31 more_vert 3rd solution | based on 90% multi AutoML solutions Many thanks to Kaggle for this Season 3, Episode 17 Playground Series competition. A big thank you to the people who contributed on forum and notebooks and share with us good insights. Overview I used several AutoML approaches and used a meta model based on them Light AutoML Statmining AutoML (ISoft private solution) H2O AutoML Lazy Classifier FLAML => the first 3 solutions are those that contribute the most to the final model. => the 10% of the model comes from the 2 best public submissions Execution Time The execution time of the solution on a machine with 4 cores and 8Go RAM is about 6h. I limit each automl solution to 1 hour max execution time. LB Scores Public LB Score: 0.97921 Private LB Score: 0.98541 The chosen solutions are almost the best obtained the public leaderboard and the private leaderboard Note that a solution including pseudo labeling performed slightly better. Please sign in to reply to this topic. comment 9 Comments Hotness bogoconic1 Posted 2 years ago ¬∑ 16th in this Competition arrow_drop_up 1 more_vert Congrats for 3rd place @mathurinache ! Ensembling the top public notebooks into the model predictions really worked for this one. Also experienced the same outcome! in both public and private LB (it jumped ~100 spots for our team's submission) Thiago Mantuani Posted 2 years ago ¬∑ 148th in this Competition arrow_drop_up 1 more_vert Did you consider using or did you use autoGluon in this competition? Mathurin Ach√© Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert not this time, next time probably. GyoukChu Posted 2 years ago arrow_drop_up 0 more_vert I tried Autogloun Tablularpredictor and I got Private Score 0.97590! I hope this helps :) Alban NYANTUDRE Posted 2 years ago ¬∑ 80th in this Competition arrow_drop_up 1 more_vert Congratulations for the 3rd place and thanks for sharing your approach @mathurinache ! SHADMAN Posted 2 years ago arrow_drop_up 0 more_vert I appreciate your work @mathurinache , but I prefer making a custom ML pipeline so that you get to learn new things along the way , also it's like a hack using an AutoML because if its the case even the best programmers at codeforces would seem like average , just in my opinion üëç Prajesh Sanghvi Posted 2 years ago arrow_drop_up 0 more_vert Congratulations on the 3rd place!, and thank you so much for sharing! Dove. J. Kim Posted 2 years ago ¬∑ 1083rd in this Competition arrow_drop_up 0 more_vert Congratulations for 3rd place! Through this competition, I felt that I should study harder. Hiroshi Sakiyama Posted 2 years ago arrow_drop_up 0 more_vert Congratulations!",
      "Binary Classification of Machine Failures | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification of Machine Failures Playground Series - Season 3, Episode 17 Binary Classification of Machine Failures Overview Data Code Models Discussion Leaderboard Rules Iqbal Syah Akbar ¬∑ 11th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 44 more_vert 11th Place Solution | Unexpected Top 1% I already expected that the top places will get few shake-up but getting myself up to top 1% despite telling all my approaches? That's different story. Anyway, here's what I've done to get to 11th position Cross-Validation Process I used MultilabelStratifiedKFold from iterative-stratification library because I want to make sure that the percentage for all types of failure is kept throughout the fold. I also explained this here . The amount of split I used was 10 folds, because 5 folds isn't correlated enough with the public LB. I always try to do everything inside cross-validation pipeline, such as adding the original data, encoding, scaling, etc. This way, I don't have to worry a lot about leakage. Feature Engineering I use category-encoders' CatBoost Encoder in all my models, except CatBoost itself, since I noticed that CatBoost performs exceptionally well for some reason. I've already posted about this here actually, the main difference is that I put it inside the model pipeline instead of using it before doing a CV, thus preventing major leakage. Example code is as follows: Encoder = CatBoostEncoder(cols = [ 'Product ID' , 'Type' ])\nmodel = make_pipeline(Encoder, model) content_copy As for creating new features, I've only created one: IsFailure . This feature describes whether there is any type of failure (TWF, HDF, etc.) that is happening. I only use this feature in one model: Gaussian Naive Bayes. The rest is just encoding. Tuning and Ensembling I used 6 models: Gaussian Naive Bayes, Random Forest, XGBoost, LightGBM, LightGBM's Dart, and CatBoost. For gradient boosting models, I used Optuna to tune them. For Naive Bayes and Random Forest, I didn't tune them at all, only calibrated them (I also posted about it here ), and did further preprocessing for Naive Bayes (such as scale normalization and Yeo-Johnson transformation). Once I've done building all of them, I used LogisticRegression to find optimal weight for my voting ensemble. The CV score is 0.98006. Finally, I retrained my ensemble on the whole dataset for better final result instead of just relying on the CV process. Edit: I just realized that my best submission didn't even use Random Forest and LightGBM's DART Edit 2: Full code can be found here: https://www.kaggle.com/code/iqbalsyahakbar/11th-place-solution-code Please sign in to reply to this topic. comment 32 Comments 3 appreciation  comments Hotness Ravi Ramakrishnan Posted 2 years ago ¬∑ 108th in this Competition arrow_drop_up 4 more_vert Congratulations @iqbalsyahakbar , this is a well deserved result! Thanks for sharing your knowledge throughout the past 2 weeks and the approach note too! All the best and regards! Lu√≠s Fernando Torres Posted 2 years ago ¬∑ 667th in this Competition arrow_drop_up 1 more_vert Using Logistic Regression to optimize the weights in the VotingClassifier was a great idea! I'll try this approach too in my next submissions. Good job, @iqbalsyahakbar ! Pooja Chauhan Posted 2 years ago arrow_drop_up 1 more_vert It's impressive that you implemented a thorough cross-validation process to ensure a balanced evaluation and minimize leakage. Well done! K392_alpaca Posted 2 years ago ¬∑ 906th in this Competition arrow_drop_up 1 more_vert Thanks a lot for sharing your knowledge. Niveditha Vudayagiri Posted 2 years ago ¬∑ 119th in this Competition arrow_drop_up 1 more_vert I had always wondered why Catboost was performing so well. Thanks for this solution! Akshay Vyas Posted 2 years ago arrow_drop_up 1 more_vert @iqbalsyahakbar Congratulations on your achievement! . Your detailed explanation of the process (cross-validation, feature engineering, tuning, and ensembling techniques) and the code you provided will benefit me to improve. Harry Tan Posted 2 years ago arrow_drop_up 1 more_vert wow this machine learning model is very insightful TAK Posted 2 years ago ¬∑ 149th in this Competition arrow_drop_up 1 more_vert @iqbalsyahakbar , thank you so much for sharing your solution which is great! This is very useful for me, which will help me to improve. Next, I want to try what I got here. qooq Posted 2 years ago ¬∑ 360th in this Competition arrow_drop_up 1 more_vert Thank you for sharing your approach! Your approach shows, that simplicity can be the best option! Utkarsh Prajapati Posted 2 years ago ¬∑ 143rd in this Competition arrow_drop_up 1 more_vert Congratulations @iqbalsyahakbar , Thanks for sharing your approach as it will help us for the future competitions. Flavia Felicioni Posted 2 years ago ¬∑ 345th in this Competition arrow_drop_up 1 more_vert Well done! Thanks for sharing your solution bogoconic1 Posted 2 years ago ¬∑ 16th in this Competition arrow_drop_up 1 more_vert Congrats @iqbalsyahakbar well deserved finish! Lots of effort put in to dive into various approaches and share your knowledge in public notebooks and discussion posts Alban NYANTUDRE Posted 2 years ago ¬∑ 80th in this Competition arrow_drop_up 1 more_vert Congratulations @iqbalsyahakbar and thanks for sharing your approach with us. It's a well-deserved place üëç Pranav Jadhav Posted 2 years ago arrow_drop_up 2 more_vert Its the easiest method i got as per my search till now. Thanks a lot for sharing. SOUMENDRA PRASAD MOHANTY Posted 2 years ago arrow_drop_up 2 more_vert @iqbalsyahakbar , well done champ . Keep going on. ARAVINDaarav Posted 2 years ago arrow_drop_up 2 more_vert Excellent Eugeniy Osetrov Posted 2 years ago arrow_drop_up 2 more_vert Excellent. Thank you for  sharing. A lot of work has been done! Thiago Mantuani Posted 2 years ago ¬∑ 148th in this Competition arrow_drop_up 2 more_vert thanks for sharing the solution, good approach that provides a lot of knowledge for next competitions. One question, you used logistic regression to define the weights of your weighted average of the ensemble of models, but some of the models as output from the logistic regression gave negative weights, is that useful? Note: I was based on a notebook in which you posted. thanks. Iqbal Syah Akbar Topic Author Posted 2 years ago ¬∑ 11th in this Competition arrow_drop_up 2 more_vert @thiagomantuani That's a great question. Based on my experience, allowing negative weights can work well, though it depends on the estimators. For example, you need RidgeClassifier to have strictly positive correlation to get good result. On the other hand, it's impossible to disallow negative weight for LogisticRegression. In the end, everything is experiment. There is also this post from Grandmaster Chris Deotte, where he allowed negative weight to boost his team's score and got 3rd place. Karthi Vijayakumaar Posted 2 years ago arrow_drop_up 2 more_vert Congratulations @iqbalsyahakbar ! Thank you for sharing your approach. Ivan Smirnov Posted 2 years ago ¬∑ 34th in this Competition arrow_drop_up 2 more_vert Nice and neat solution. By the way, why did you not decide to go with more feature engineering? Iqbal Syah Akbar Topic Author Posted 2 years ago ¬∑ 11th in this Competition arrow_drop_up 2 more_vert @tornadoski Partly because I already tried some and they did not improve my model, and partly because I was too lazy and rather do something outside of this competition üòÇ. Maganti IT Posted 2 years ago ¬∑ 140th in this Competition arrow_drop_up 2 more_vert Nice approach, and thanks for sharing your experience üëç Abhijeetsingh Meena Posted 2 years ago ¬∑ 205th in this Competition arrow_drop_up 2 more_vert @iqbalsyahakbar Congratulations, Thanks for the knowledge you shared with us all üòÉ. Your approach is extraordinary. Pavee Udomkarnpaisarn Posted 2 years ago ¬∑ 945th in this Competition arrow_drop_up 2 more_vert Congratulations! Thank you for posting this! Sarmad_mueen Posted 2 years ago ¬∑ 437th in this Competition arrow_drop_up 2 more_vert Thanks for sharing your solution. Could you give more detail on how you used Logistic Regression to find the optimal weight for the ensemble Iqbal Syah Akbar Topic Author Posted 2 years ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert @etrdfdfd What I did is basically a stacking ensemble, where I use a meta-classifier to predict the outcome based on the prediction of all estimators. Scikit-learn library actually has this implementation called StackingClassifier . However, in my opinion, getting just the coefficient used in meta-classifier is way more faster than using that StackingClassifier . Sarmad_mueen Posted 2 years ago ¬∑ 437th in this Competition arrow_drop_up 0 more_vert Thanks @iqbalsyahakbar , do you mean that you used LogisticRegressor as a  meta-classifier in the  StackingClassifier? 3 more replies arrow_drop_down Rodrigo Stall Sikora Posted 2 years ago arrow_drop_up 0 more_vert Nice thread!",
      "Binary Classification of Machine Failures | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification of Machine Failures Playground Series - Season 3, Episode 17 Binary Classification of Machine Failures Overview Data Code Models Discussion Leaderboard Rules datadote ¬∑ 17th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 14 more_vert 17th Place Solution | Only 1 Catboost Local CV Private Public 0.98125 0.98426 0.97764 Here are my notes. The starred ones are probably the differentiators. Dataset: *Use train data & original train: +0.002 Only remove id and UID columns Remove duplicates from combined train dataset 3 new features. Seems to help public leaderboard (+0.004), but no difference on private (~ 0.000) 1) tmp['tratio'] = tmp['proc_temp'] / tmp['air_temp'] 2) tmp['torque_mult_rspeed'] = tmp['torque'] * tmp['rspeed'] 3) tmp['torque_mult_toolwear'] = tmp['torque'] * tmp['tool_wear'] Used LabelEncoder to encode the categories Nothing done for dataset imbalance. Boosting algorithms should be able to handle this No feature scaling done. Boosting algorithm should be able to handle this Split into Train on 80%, validate 20%. Also, had a 5 fold stratified kfold option, but the 80/20 split seemed good enough, and is quicker to iterate on For final model, train on all data, and stop at the best iteration for Catboost Modeling *Only 1 Catboost model. Picking Catboost for this dataset is important. Catboost generally seems to be slightly more accurate than Xgboost or LightGBM (although can be slower). It is not always true as there are competitions where LightGBM did better. Ensembling multiple Catboosts is probably better, but I did not try *Optuna + Wandb to sweep hyperparameters, and pick best one continuous columns: ['air_temp', 'proc_temp', 'rspeed', 'torque', 'tool_wear', 'tratio', 'torque_mult_rspeed', 'torque_mult_toolwear'] categorical columns: ['PWF', 'pid', 'TWF', 'Type', 'OSF', 'RNF', 'HDF'] (pid = product id) *Use product id, and make sure to use the \"cat_features\" argument. (+0.013) *CPU accuracy > GPU accuracy. Catboost CPU and GPU have a few different settings. CPU is also deterministic, whereas GPU is not Used a smaller learning rate (0.025 vs 0.1 default) For Catboost, increasing border_count parameter may help (default CPU is 254) Finally, LightGBMs should be able to get close to Catboost performance, but you need to add a \"categorical_feature\" to the LightGBM. I only discovered this in the last couple days. Please sign in to reply to this topic. comment 1 Comment 1 appreciation  comment Hotness Appreciation (1) MeatDad Posted 2 years ago ¬∑ 441st in this Competition arrow_drop_up 1 more_vert Thank you for your job!",
      "Binary Classification of Machine Failures | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification of Machine Failures Playground Series - Season 3, Episode 17 Binary Classification of Machine Failures Overview Data Code Models Discussion Leaderboard Rules Anna ¬∑ 47th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 3 more_vert 47th Place Solution | RepeatedKFold CatBoost This was my first competition, so I'm glad it went well. The solution uses the CatBoostClassifier algorithm with repeated K-Fold cross validation to split the data into training and validation sets. My notebook link to the notebook Strategies adopted Utilizing both the training data and the original dataset Feature selection: I removed just the \"id\" and \"UID\" columns CatBoostClassifier: The decision to use the CatBoostClassifier algorithm yielded highly satisfactory results. Its built-in handling of categorical features proved to be a winning choice. Repeated K-Fold cross validation: I adopted repeated k-fold cross-validation to create robust training and validation sets I would like to acknowledge the following valuable resource that contributed to my understanding and implementation of CatBoostClassifier in this competition: CatBoost notebook by Jimmy Yeung: link to the notebook Please sign in to reply to this topic. comment 1 Comment Hotness Swapnil Chowdhury Posted 2 years ago arrow_drop_up 0 more_vert congrats @annafabris on your new achievement"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 17 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Machine Failure Predictions . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 12.86 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 12.86 MB sample_submission.csv test.csv train.csv 3 files 29 columns ",
    "data_description": "Binary Classification of Machine Failures | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification of Machine Failures Playground Series - Season 3, Episode 17 Binary Classification of Machine Failures Overview Data Code Models Discussion Leaderboard Rules Overview Start Jun 13, 2023 Close Jun 27, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in June every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each id in the test set, you must predict the probability of a Machine failure . The file should contain a header and have the following format: id ,Machine failure 136429 , 0 . 5 136430 , 0 . 1 136431 , 0 . 9 etc . content_copy Timeline link keyboard_arrow_up Start Date - June 13, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  June 26, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Binary Classification of Machine Failures. https://kaggle.com/competitions/playground-series-s3e17, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 4,165 Entrants 1,544 Participants 1,502 Teams 11,708 Submissions Tags Beginner Tabular Roc Auc Score Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e18",
    "discussion_links": [
      "/competitions/playground-series-s3e18/discussion/432011"
    ],
    "discussion_texts": [
      "Explore Multi-Label Classification with an Enzyme Substrate Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Explore Multi-Label Classification with an Enzyme Substrate Dataset Playground Series - Season 3, Episode 18 Explore Multi-Label Classification with an Enzyme Substrate Dataset Overview Data Code Models Discussion Leaderboard Rules Âç©ÔæëÂ∞∫ÔΩ≤„Çì ¬∑ 1st in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 11 more_vert 1st Place Winning Solution Sorry for publishing the solution a bit late. Notebook can be found here: https://www.kaggle.com/code/nihilisticneuralnet/1st-place-winning-solution Importing Libraries : Importing essential libraries for data handling, preprocessing, model training, and evaluation. Loading Data : Loading the data Data Preprocessing and Feature Engineering : Preprocesses the data by removing unnecessary columns (\"id\") and transforming the \"mixed_desc\" column by splitting its values. Additionally, new features are generated using groupby operations on categorical and numerical columns. These engineered features enhance the dataset for better model performance. Defining Features and Targets : The features and target variables are established, separating the input data for training purposes. The additional features are generated by combining categorical and numerical columns, creating a more informative dataset. Model Initialization and Parameters : Configuration parameters for two classifiers, XGBoost and LightGBM, are defined. These parameters determine how the models learn from the data. The MultiOutputClassifier wrapper is employed to handle multi-output prediction tasks. Creating Pipelines : Two pipelines are set up for training using the defined classifiers. These pipelines streamline the process of training and prediction, encapsulating the necessary steps within each model. Training and Validation Loop : Cross-validation is performed using RepeatedMultilabelStratifiedKFold. Then iterating through the specified number of folds, training the classifiers on training data and evaluating their performance on validation data. Predictions are generated, and ROC AUC scores are calculated to gauge how well the models perform. Performance Evaluation and Averaging : The average ROC AUC scores for each classifier are calculated over all folds, both for training and validation data. An \"overall\" prediction set is created by averaging predictions from both classifiers, aiding in robust performance assessment. Test Set Predictions : Predictions are generated for the test dataset using the trained classifiers. The predictions from XGBoost and LightGBM are averaged to form the final predictions for the test set. Submission : And at last, submitting to the competition. Please sign in to reply to this topic. comment 2 Comments Hotness Yuji Itadori Posted 2 years ago ¬∑ 400th in this Competition arrow_drop_up 1 more_vert df['BertzCT_MaxAbsEStateIndex_Ratio']= df['BertzCT'] / (df['MaxAbsEStateIndex'] + 1e-12) can u specify what's the purpose of this as i am new to feature creation. Did you combined both EC1 and EC2 feature ( that gives 2d array ?). What's the idea behind using Gaussian Mixture ? Âç©ÔæëÂ∞∫ÔΩ≤„Çì Topic Author Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 6 more_vert Sorry for the late reply. I used the concept of feature engineering to manipulate and transforming the existing data ( 'BertzCT', 'MaxAbsEStateIndex', etc ) to generate new features by studying their co-relation with each other. One can use a heatmap to visualize the co-relation: import seaborn as sns\ndataplot = sns.heatmap(train.corr(), cmap= \"YlGnBu\" , annot= False ) content_copy Also if you check the description of the feature 'MaxAbsEStateIndex' (by using train.MaxAbsEStateIndex.describe() ), you could see that the minimum value of the 'MaxAbsEStateIndex' is 0. Since it is in the denominator and we don't want to divide by 0, I added a very small number along with it in the denominator. No, I did not combine the EC1 and EC2 features. Gaussian Mixture is a type of probabilistic model used for clustering and density estimation. The purpose of applying it is to create new categorical features that represent clusters or groups within certain numerical features. Each cluster is represented by a different class label. Hope this helps‚Ä¶ Yuji Itadori Posted 2 years ago ¬∑ 400th in this Competition arrow_drop_up 0 more_vert thanks a lot, you wonderfully explained it."
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 18 The dataset for this competition (both train and test) was generated from a deep learning model trained on a portion of the Multi-label Classification of enzyme substrates . This dataset only uses a subset of features from the original (the features that had the most signal). Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note: For this challenge, you are given 6 features in the training data, but only asked to predict the first two features ( EC1 and EC2 ). 3 files 7.63 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 7.63 MB sample_submission.csv test.csv train.csv 3 files 73 columns ",
    "data_description": "Explore Multi-Label Classification with an Enzyme Substrate Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Explore Multi-Label Classification with an Enzyme Substrate Dataset Playground Series - Season 3, Episode 18 Explore Multi-Label Classification with an Enzyme Substrate Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Start Jun 27, 2023 Close Jul 11, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in June every Tuesday 00:00 UTC, with each competition running for 2 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. üí° Getting Started Notebook To get started quickly, feel free to take advantage of this starter notebook . Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the ground truth for each target, and the final score is the average of the individual AUCs of each predicted column. Submission File For each id in the test set, you must predict the value for the targets EC1 and EC2 . The file should contain a header and have the following format: id ,EC1,EC2 14838 , 0 . 22 , 0 . 71 14839 , 0 . 78 , 0 . 43 14840 , 0 . 53 , 0 . 11 etc . content_copy Timeline link keyboard_arrow_up Start Date - June 27, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  July 10, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Explore Multi-Label Classification with an Enzyme Substrate Dataset. https://kaggle.com/competitions/playground-series-s3e18, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 3,190 Entrants 1,065 Participants 1,047 Teams 8,801 Submissions Tags Beginner Tabular Binary Classification Multilabel Classification Roc Auc Score Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e19",
    "discussion_links": [
      "/competitions/playground-series-s3e19/discussion/428385",
      "/competitions/playground-series-s3e19/discussion/428682",
      "/competitions/playground-series-s3e19/discussion/428335",
      "/competitions/playground-series-s3e19/discussion/428347",
      "/competitions/playground-series-s3e19/discussion/428514",
      "/competitions/playground-series-s3e19/discussion/428368",
      "/competitions/playground-series-s3e19/discussion/428300",
      "/competitions/playground-series-s3e19/discussion/428438",
      "/competitions/playground-series-s3e19/discussion/428433"
    ],
    "discussion_texts": [
      "Forecasting Mini-Course Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Forecasting Mini-Course Sales Playground Series - Season 3, Episode 19 Forecasting Mini-Course Sales Overview Data Code Models Discussion Leaderboard Rules paddykb ¬∑ 2nd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 32 more_vert 2nd Place Solution 2nd place solution is just a few changes to the public version of the GAMMY notebook. I removed CCI and added more holiday processing. (My final two submissions were with and without CCI). The discussions were a lot of fun. Thanks @siukeitin , @yeoyunsianggeremie , @seaninaction , @albertzhang314159 , @oscarm524 , @iqbalsyahakbar , @kdmitrie , @ravi20076 Mostly I blame @manishkumar7432698 for complaining about the validation-lb difference. I joined the competition to investigate that. Please sign in to reply to this topic. comment 13 Comments Hotness christph Posted 2 years ago ¬∑ 34th in this Competition arrow_drop_up 1 more_vert love to see more R notebooks, great work Flavia Felicioni Posted 2 years ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert Congratulations @paddykb and many thanks for sharing your postprocessing strategy. Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert A huge congrats Patrick. I hope you reach GM position soon. You're already one of the most experienced/skilled/generous Kagglers in activity. Thanks for sharing. Remember the new: \"$100K+ to Support High-Quality Solution Write-Ups Authors\" I don't know if that applies to Playground: ) https://www.kaggle.com/discussions/product-feedback/373153 https://www.kaggle.com/discussions/general/427114 paddykb Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Thank you @mpwolke . TBH I'm not prolific enough to reach GM for a few years. I joined Kaggle 9 years ago, and dip in and out depending on interest. Maybe around 2040 :D Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert Professionals like you are busy. And Kaggle is for fun. You're low-profile (not commenting in every topic just to be there). That's another characteristic of the real GMs, and generosity to help the others. You've both of this. And that doesn't require other users validation or recognition for your precious participation. For those that know you, you're already a GM. The \"full yellow circle\" is just a question of time. And long before 2040 : ) Prasenjit Sharma Posted 2 years ago arrow_drop_up 1 more_vert Congratulations for the 2nd place.üëèüëè Mohammed Suleiman Ahmed Posted 2 years ago arrow_drop_up 1 more_vert @paddykb great idea to consider. Thank you Dr. Alvinleenh Posted 2 years ago ¬∑ 41st in this Competition arrow_drop_up 1 more_vert Congrats @paddykb and your notebook sharing on how to preprocess data, especially resolving data construction error by recommended ratios bogoconic1 Posted 2 years ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert Many congrats @paddykb for 2nd place, thanks for the best scoring public notebook (even though we had to make a small modification) ! seymoo Posted 2 years ago ¬∑ 130th in this Competition arrow_drop_up 1 more_vert congratulations! your ideas really helped me, thank you! MZF Posted 2 years ago ¬∑ 56th in this Competition arrow_drop_up 1 more_vert @paddykb Congrats on winning second place and thank you so much for your contribution, learned a lot from your work! Konstantin Dmitriev Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Thank you very much for your notebook and discussion! I think, I've learned a lot from it. And my congratulations for the 2nd place! Ravi Ramakrishnan Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Hearty congratulations @paddykb and thanks for the GAM notebook! Best regards! Oscar Aguilar Posted 2 years ago ¬∑ 57th in this Competition arrow_drop_up 2 more_vert Congrats @paddykb . You made great contributions in this comp. You are a GAM expertüëç",
      "Forecasting Mini-Course Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Forecasting Mini-Course Sales Playground Series - Season 3, Episode 19 Forecasting Mini-Course Sales Overview Data Code Models Discussion Leaderboard Rules Albert Zhang ¬∑ 3rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 13 more_vert 3rd Place Solution: EDA is the key I'm very happy to get 3rd place on my first kaggle competition. Maybe I will write a notebook later, but it depends. I want to read other solutions first. Thanks @paddykb , @kdmitrie and all other people who contributed ideas. EDA and data processing I think EDA is the most important part for this competition, and probably for most such competitions. Correcting the trend Based on the plot of daily total sales, it's obvious that trend in 2020 is abnormal. My method to correct the trend: Calculate daily fraction of each of the 75 categories. Use STL decomposition to extract the trend from daily total sales series. Replace the trend from 2020-02-15 to 2020-11-15 with the average trend of the other years. Restore the daily total sales series. Restore the series of each category based on its fraction. Modelling the baselines Baseline for total daily sales seems to be discrete on each year. The baseline of each category can be modeled as: B y , p , c , s = B y F y , p F y , c F y , s where B is baseline level, F is fraction, y is year, p is product, c is country, s is store. The subscript denotes the corresponding combined categories. So to find baselines of 2022 one needs to find all these values with y=2022 . Fraction of country by year Country fractions fluctuate with time. It turned out that these fractions are correlated with GDP per capita.  However for unknown reason all countries in 2022 have the same fraction, i.e., 0.2. Probably due to data construction error. F y , c = f y , c ; F 2022 , c = 0.2 where f denotes empirical fraction and y = 2017, 2018 ‚Ä¶ 2021 . Fraction of product by year Product fractions are roughly constant through years. There seems to be some seasonality of two-year period. But I'd rather capture that by temporal embedding. Here I set them constant as weighted average: F 2022 , p = F y , p = ( 2 ( f 2017 , p + f 2019 , p + f 2021 , p ) + 3 ( f 2018 , p + f 2020 , p ) ) / 12 Fraction of store by year Store fractions seems to be constant through years. The fluctuation is negligible. F 2022 , s = F y , s = f s Baseline of total daily sales. I used median as baseline. For 2022 I left it unknown and determined it by LB probing at the end. B y = m e d i a n y ; B 2022 ‚âà 18900 EDA by category Standardization With the baseline model, standardization can be done directly: S y , p , c , s , d = N y , p , c , s , d / F y , p / F y , c / F y , s / B y ‚àí 1 where N is num of sold, S is standardized num of sold, d is date. Subplots of daily means (after standardization) by category By country Holidays behave differently among countries, especially for Japan. By product Seasonalities are different among products. Note that, though weak, some products have seasonalities of two years instead of one year. By store All stores seem to behave the same. Sum the store sales Reason to sum the store sales: Based on the analysis above, store fractions seem to be constant. And there is no difference of seasonality and holiday effect among stores. For some categories, e.g. Using LLMs to Win Friends and Influence People - Argentina - Kaggle Learn,  the num of sold is too small and it oscillates between several integers. Such series may affect model training. So I used the sum of daily sales by product and country for training and validation. Similar standardization applied: S y , p , c , d = N y , p , c , d / F y , p / F y , c / B y ‚àí 1 For prediction, convert this intermediate value back to num of sold: N y , p , c , s , d = ( S y , p , c , d + 1 ) B y F y , p F y , c F y , s I put all model parameters to an excel table. See attachment. Training and prediction Pipeline Since seasonalities are different among products, I trained each product separately. So there are 5 models. Training data is split by year to 5 folds, followed by cross validation. Feature engineering Since private score is based on the last 75% of 2022,  lag features are not needed. Temporal embedding For week seasonality, I used Fourier terms up to order 3 . For two-year seasonality, I used Fourier terms up to order 5 . Holiday and date related features I generated two categorical features: c_holiday: country + corresponding holidays c_year_date: country + 365 dates of a year These categorical features are then converted to numeric levels by target encoding. I used linear mixed model with S ~ temporal variables grouped by the categorical variable. Of course, encoding is done separately for each product. To avoid overfitting, encoding is done with 4~5 fold OOS predictions. See: https://www.statsmodels.org/stable/mixed_linear.html https://arxiv.org/pdf/2104.00629.pdf I didn't do feature selection Modeling I used lightbm with linear tree and dart. I didn't do much hyper-parameter tuning except I set num_leaves to a small value (~5). The optimal num_iterations is determined by the cross-validation RMSE. The final model is trained on the whole training set with optimal num_iterations . I didn't do any ensmeble. Result The validation SMAPE is 4.87. Feature importance: As mentioned above, baseline of total daily sales of 2022 is determined by LB probing. This is simply done by: Randomly guess a few values and get the scores Use a parabola to fit. Try the minimum point. Add the result and fit again. Repeat 3-4 one or two times. That's it. Hope it helps. baseline_parameters.xlsx Please sign in to reply to this topic. comment 2 Comments Hotness Robert Trypuz Posted 2 years ago ¬∑ 172nd in this Competition arrow_drop_up 1 more_vert Congrats! Thank you for sharing your EDA. I learned a lot. Best! üòÄ This comment has been deleted.",
      "Forecasting Mini-Course Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Forecasting Mini-Course Sales Playground Series - Season 3, Episode 19 Forecasting Mini-Course Sales Overview Data Code Models Discussion Leaderboard Rules Michael Bryant ¬∑ 4th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 17 more_vert 4th Place Solution Solution Notebook I used the work of @paddykb , @siukeitin , and @ravi20076 so thank you for your insightful notebooks and discussion posts. My solution is a Poisson PyGAM with 20 features. The model was cross-validated using 5-fold sliding window where the test set for each fold was separated into two parts (3 months and 9 months) to get a leaderboard estimate for public and private, respectively. The final 9 months of each test set was used for feature selection and selecting the number of splines. The final predictions were releveled and rounded. The final features were: store and product (each label encoded) week day an indicator for an even year five sine and cosine features based on day of year and product indicators for December 26 through January 1 indicators for May 7 through May 9 in Japan log of GDP per capita Please sign in to reply to this topic. comment 8 Comments Hotness YASH PRAKASH VIRA Posted 2 years ago ¬∑ 1055th in this Competition arrow_drop_up 1 more_vert How to create and use sine & cosine features? Please Explain. Also I would like to learn Feature engineering. Please Assist. Michael Bryant Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Hi @yashprakashvira , I looked at the plots and thought about which function would best fit each of them. For example, below is a plot for Agentina / Kaggle Learn / Using LLMs to Improve Your Coding. If you change the country and store then the same sinusoidal pattern appears. If you change the product then you get different sinusoidal patterns ( see here ). So for the plot below, I saw that sine function should have the form ‚àí s i n ( 2 œÄ f t ) where f = 1 / 364 and t = d a y o f y e a r since the cycle repeats every year. I multiplied the sine function by an indicator function for the product where it is one if the product is Using LLMs to Improve Your Coding and zero otherwise. I repeated this for every product. YASH PRAKASH VIRA Posted 2 years ago ¬∑ 1055th in this Competition arrow_drop_up 0 more_vert Thankyou @michaelbryantds . This helped a lot! Are there any useful resources where I can learn feature engineering? Michael Bryant Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert @yashprakashvira I've found looking through notebooks for previous competitions that are similar to problem you're trying to solve to be the most helpful. Ravi Ramakrishnan Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Thanks for the mention @michaelbryantds . I am happy my work was useful for a lot of participants! I aim to do this with my work and am elated if people find it helpful. Congratulations for the work and the results too! Best wishes! Michael Bryant Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Thanks @ravi20076 , congrats on your results as well! Iqbal Syah Akbar Posted 2 years ago ¬∑ 79th in this Competition arrow_drop_up 2 more_vert I've been wondering how to make GAM works in Python for this competition or time series in general, and it looks like your solution is the answer! Thank you @michaelbryantds ! paddykb Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert @michaelbryantds Nice work Michael. I prefer the mgcv syntax - but its only syntax and your notebook has nearly convinced me :D @iqbalsyahakbar The python version of ISL has a section on GAMS and uses pygam for its examples: https://www.statlearning.com https://hastie.su.domains/ISLP/ISLP_website.pdf Iqbal Syah Akbar Posted 2 years ago ¬∑ 79th in this Competition arrow_drop_up 1 more_vert Thanks a lot @paddykb ! I will put that in my bookmark! Arnav Modi Posted 2 years ago ¬∑ 582nd in this Competition arrow_drop_up 2 more_vert Thanks for the comprehensive notebook, certainly a lot of learn from it @michaelbryantds . Just a question in general, why did you decide to use the Poisson PyGAM model for this competition? In what situations would you consider trying out that model amongst others? (I just started out on Kaggle and used a random forest for this competition) df[ 'coding_sin' ] = -np.sin((df[ 'dayofyear' ]/ 364 )* 2 *math.pi)*df[ 'Using LLMs to Improve Your Coding' ] # frequency = 1 cycle/year\n    df[ 'train_sin' ] = np.sin((np.where(df[ 'even_year' ]== 0 , df[ 'dayofyear' ], df[ 'dayofyear' ]+ 364 )/( 364 * 2 ))* 2 *math.pi)*df[ 'Using LLMs to Train More LLMs' ] # frequency = 1 / 2 cycle/year\n    df[ 'friends_sin' ] = np.sin((np.where(df[ 'even_year' ]== 0 , df[ 'dayofyear' ], df[ 'dayofyear' ]+ 364 )/( 364 * 2 ))* 2 *math.pi)*df[ 'Using LLMs to Win Friends and Influence People' ] # frequency = 1 / 2 cycle/year\n    df[ 'kaggle_cos' ] = np.cos((df[ 'dayofyear' ]/ 364 )* 2 *math.pi)*df[ 'Using LLMs to Win More Kaggle Competitions' ] # frequency = 1 cycle/year\n    df[ 'write_sin' ] = np.sin((df[ 'dayofyear' ]/ 364 )* 2 *math.pi)*df[ 'Using LLMs to Write Better' ] # frequency = 1 cycle/year\n    df[ 'product_sin' ] = df[ 'coding_sin' ] + df[ 'train_sin' ] + df[ 'friends_sin' ] + df[ 'kaggle_cos' ] + df[ 'write_sin' ] content_copy Also, could you explain what you are trying to achieve in this part of pre-processing? Michael Bryant Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Hi @arnavmodi why did you decide to use the Poisson PyGAM model for this competition? The idea to use a Poisson GAM came from this notebook and it ended up being what I used because no other model came close when comparing CV scores. You just have to try out several different models. In what situations would you consider trying out that model amongst others? Using Poisson regression makes sense since we're modeling count data. The other models I tried also performed best when their objective was set to Poisson (e.g., XGBoost). In general, if you want something more flexible than linear regression and less flexible than tree ensembles then GAMs are worth trying. could you explain what you are trying to achieve in this part of pre-processing? I added those sines and cosine features, because I noticed in my residual plots that the GAM was not capturing the seasonality. So I looked at the plots and made a guess at the functions that would generate them.",
      "Forecasting Mini-Course Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Forecasting Mini-Course Sales Playground Series - Season 3, Episode 19 Forecasting Mini-Course Sales Overview Data Code Models Discussion Leaderboard Rules Konstantin Dmitriev ¬∑ 5th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 11 more_vert 5th place solution First, I would thank organisers for a very interesting competition. My special thanks also to @siukeitin , @paddykb , and @ravi20076 . Discussions with you helped me a lot! I have previously published almost all my solution, which is based on the multiplication of several different factors ( https://www.kaggle.com/competitions/playground-series-s3e19/discussion/424520) . y t , c , s , p = GDP ( c , Œ∑ ) ‚ãÖ F ( p , t ) ‚ãÖ S ( s ) ‚ãÖ w ( w ) ‚ãÖ Covid ( m , Œ∑ ) ‚ãÖ D ( d ) ‚ãÖ H ( c , d ) 1) GDP ( c , Œ∑ ) is relative GDP per capita in each country c ant year Œ∑ , which was sugested in this post . 2) F ( p , t ) is the averaged sales of each product p with applied fourier filtering (only few lower frequency component are left); t t is the consequentive number of the day, starting from 2017-01-01. 3) S ( s ) is the constant factor depending on store s . 4) W ( w ) is the constant factor depending on weekday w ; 5) Covid ( m , Œ∑ ) is the COVID factor depending on year Œ∑ and on month m . It is always equal to 1, except for year 2020. And in 2020 it is equal to the total sales made per month in 2020 divided by the averaged total sales made per month in all other years; 6) D ( d ) = D 1 cos ( œÄ d 365 ) + D 2 sin ( œÄ d 365 ) + D 3 cos ( 2 œÄ d 365 ) + D 4 sin ( 2 œÄ d 365 ) is the factor depending on day-of-year d ; It is responsible to lower sales in summer and higher sales in winter season. D j values are selected with linear regression. To do this we also have to get rid of holidays. 7) H ( c , d ) is the holiday factor.  To deal with it, I noticed, that the form of each holiday response is the same. They all have gauss form with standard deviation equal to 2, and the maximum shifted forward at 4.5. I used ridge regression to estimate the amplitude of each such function. All holidays are grouped by their name, as python's holiday library recognise them. https://www.kaggle.com/code/kdmitrie/5th-place-solution-timeseries-decomposition Please sign in to reply to this topic. comment 2 Comments Hotness Ravi Ramakrishnan Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Thanks for the mention @kdmitrie I see that you were on top of the public and private LB almost throughout! This is a commendable achievement in my opinion. All the best and congratulations! Konstantin Dmitriev Topic Author Posted 2 years ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Thank you very much for the inspiring words, @ravi20076 !",
      "Forecasting Mini-Course Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Forecasting Mini-Course Sales Playground Series - Season 3, Episode 19 Forecasting Mini-Course Sales Overview Data Code Models Discussion Leaderboard Rules SeanInAction ¬∑ 7th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 6 more_vert 7th Place Solution Overall, this was a pretty enjoyable competition! The competition can be roughly divided into 3 parts/periods: implementing a solution, probing phase (credit to @paddykb ), and ensemble. Implementing a solution The whole first half of the competition was implementing this notebook https://www.kaggle.com/code/kitadakiyoto/tpssep22-predict-by-linear-regression-1st-place (opens in a new tab)\"> https://www.kaggle.com/code/kitadakiyoto/tpssep22-predict-by-linear-regression-1st-place onto this competition, using the holidays library to extract holidays and using the log of the relative GDP as an additional feature. Linear regression shows ~70 public MAPE, Random Forest shows ~30 public MAPE after some attempts at postprocessing. Probing Inspired by paddykb's findings in his notebook, I set the relative GDP to be 0.4 for all countries in 2022, effectively uplifting all countries to be around the same level. ~7.5 public MAPE after this one simple change. After some testing of standard regression models, Extra Trees was the best pick, with ~6.13 public MAPE after converting num_sold to int and deducting by 1. Ensemble I got lazy here and just used my best public scoring submission and paddykb's public notebook (almost forgot about the surprise here). Inspired by the discussion https://www.kaggle.com/competitions/playground-series-s3e19/discussion/428123 , I added 1 to paddykb's public csv. The best private score of ~5.93 MAPE (~5.00 public MAPE) was obtained with 0.825 (paddykb's csv) + 0.175 (my csv), followed by rounding the numbers. Some thoughts Blindly following public LB instead of using CV scores seemed to work out for me in this competition; I think the time series is too trend following & periodic for public LB to not follow in 2022 as well. Conclusively, I think some work and inspiration from public notebooks and discussions allowed me to get a decent placement, although luck is definitely involved, as in all things statistical. Please sign in to reply to this topic. comment 1 Comment Hotness Ravi Ramakrishnan Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 0 more_vert Great going @seaninaction This is a great result! Good luck for your other competitions too!",
      "Forecasting Mini-Course Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Forecasting Mini-Course Sales Playground Series - Season 3, Episode 19 Forecasting Mini-Course Sales Overview Data Code Models Discussion Leaderboard Rules Ravi Ramakrishnan ¬∑ 8th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 18 more_vert #8 private 6 public approach- Simple ensemble and probing Hello all, I wish to extend sincere thanks to the Kaggle team for this episode. It was indeed a great experience with lots of takeaways and learning that we can use outside of Kaggle too. I also wish to thank others participants for their generous contributions with special mention of the below users- Acknowledgement - @paddykb - his R-GAM notebook formed the base of my work for this challenge, his post-processing and levelling trick was key to my score in this challenge @ymatioun - his insinuation about post-processing helped me reduce my public LB score from 25.8 - 6 in few minutes @onurkoc83 and team - their contributions to the assignment also formed a base for my work in the challenge @oscarm524 - I was inspired to diversify my models in the assignment with the relevelled kernel by @paddykb @siukeitin - I took the macro-economic variable component from his discussion post Features used - Date features - I reused most of the features from my date feature class . This included trigonometric features for relevant date-parts (sine-cosine features) an weekday indicator. I had mentioned in my EDA and baseline work that this is an important feature I used a binary column for Dec26-Dec31 across countries I used GDP as mentioned in the above section I also used a holiday indicator as mentioned in my class I used a binary indicator for Japan between May6-9. I believe another high scoring solution has used it too (I could not mark the post here though). I found this to help me slightly in my CV results. Model structure - I used a 2 stage model approach to keep the process diversified and ensure I try and blend different and complementary models to capture the signal in the data efficiently. Base models included- a. LightGBM b. XGBoost c. CatBoost d. GAM I tuned the standard ML models (tree models) akin to my baseline work, with Optuna. Additionally, I calculated CV scores across 2-periods, from Jan-Mar and Apr-Dec to try and replicate the public private LB split. I think several public notebooks also used this strategy. I used a custom eval metric to stay within the requirements of the organizer's metric directives. Rather than using the MAPE metric, I built my own function/ class for the evaluation and embedded it with the tree models. This helped me tune and infer in line with the competition. One may refer to the below links to understand how to do so. In my opinion, developing a custom Loss Function would have also helped me more. a. https://xgboost.readthedocs.io/en/stable/tutorials/custom_metric_obj.html b. https://m-e-gorelli.medium.com/pass-a-custom-evaluation-metric-to-lightgbm-65ef062415ad c. https://stackoverflow.com/questions/69137780/provide-additional-custom-metric-to-lightgbm-for-early-stopping d. https://mljar.com/blog/catboost-custom-eval-metric/ I relied on the R-GAM notebook almost entirely (barring the surprise). I built a python equivalent of this notebook privately, but did not feel the need to use it as the results were almost the same. I adjusted a few weights across countries to probe and check the impact on the LB. The impact was small and sometimes worked for me too. Reference cell to edit from the original work is as below- pred_probe = round(pred * case_when( country == 'Argentina' ~ 3.372 , country == 'Spain' ~ 1.600 , country == 'Japan' ~ 1.394 , country == 'Estonia' ~ 1.651 , country == 'Canada' ~ 0.850 ))) content_copy I tuned my tree models just once (the day before) to submit my final models. I relied on features rather than tuning this time Model blending and post-processing - I blended my tree models using Optuna as mentioned above. I also fiddled with the weights manually at submission to probe and tune with the public LB score. I used post-processing here in line with the GAM work I blended the Optuna results with the GAM work to engender my final structure I post-processed my predictions with rounding I found that adding 1 to some of my submissions improved my public score a bit. I took this risk and it paid off. This was elicited yesterday as well. My final model submission was heavily weighted towards GAM and moderately weighted towards the blend of tree models. My model weights and final blending relied on the public LB score. Improvements - I should have looked into Ridge Regression. I myself posted about its efficacy in the past challenge and did not use it herewith. I think this could have boosted my score a lot I should have paid more attention to holidays. I treated them commonly across countries while other top approaches treated them individually and attributed more effort in this regard As mentioned above, a custom objective for the tree models perhaps would have helped my model process. My takeaways - Simple approaches have a lot of merit in time series at least. One needs to build good features and rely on them. I need to be more alert to the CV and LB relation. In this challenge, the public LB elicited insights about the final results too, especially after relevelling Using the metric directed by the organizer in our models in full to develop and infer have more benefits than using the provided metrics. I shall strive and use this in other areas of work as well. Using past experience from TPS-Sep22 as a base for this challenge was helpful. Happy learning and best regards! See you in Episode 20 and good luck for other competitions too! Tabular Regression Time Series Analysis E-Commerce Services Education Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Forecasting Mini-Course Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Forecasting Mini-Course Sales Playground Series - Season 3, Episode 19 Forecasting Mini-Course Sales Overview Data Code Models Discussion Leaderboard Rules bogoconic1 ¬∑ 9th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 11 more_vert 9th place solution This was the first time I finished with single digit rank in a Playground/ Featured competition. Many thanks to Kaggle for organizing the playground series and listening to @ravi20076 's feedback regarding the variety of challenges, and @onurkoc83 @nivedithavudayagiri for being great teammates! In this post, I will be outlining my approach to the S3E19 competition. Handling 2020 sales data I smoothed the data using this dataset https://www.kaggle.com/datasets/shivanimalhotra91/playground-s3e19-covid-data-smoothed , not just from Mar to May 2020, but from Mar to Dec 2020 [I also tried Jan to Dec 2020 but that got me a lower validation score] to remove the anomalous trend for the entire year. The data now looks like this (with no signs of deviation) Preprocessing Added additional features such as (a) Standard date features like year, month, dayofweek ‚Ä¶ (b) Holidays (ref: @nivedithavudayagiri ) (c) Seasonality features e.g. month_sin, month_cos Approach I did not do any cross validation. I used 2017-2020 smoothed data for training and 2021 for validation. The validation metric was not the entire 2021 data, but only the SMAPE from Apr to Dec 2021 ( https://www.kaggle.com/competitions/playground-series-s3e19/discussion/423657 , which hinted that we will be evaluated on Apr to Dec data). Postprocessing by multiplying final predictions was used, ref: @paddykb Link . During the competition, I published two public notebooks with different forecasting models, namely (1) https://www.kaggle.com/code/yeoyunsianggeremie/s3e19-prophet-baseline-with-postprocessing Validation score: 9.39 (5) https://www.kaggle.com/code/yeoyunsianggeremie/s3e19-catboost-smoothing-post-processing [Version 35] Validation score: 8.50 Both of the notebook submissions were used as candidates for blending into the final submission, together with these (2) https://www.kaggle.com/code/tetsutani/ps3e19-eda-ensemble-ml-pipeline-rnn-by-skorch by @tetsutani (3) https://www.kaggle.com/code/paddykb/ps-s3e19-gammy-sales [be careful!] by @paddykb (4) https://www.kaggle.com/code/oscarm524/ps-s3-ep19-eda-modeling-submission by @oscarm524 After some experiments, the best subset of submissions for blending are (2), (3) and (5). This gave a public LB score of 5.42826 and a private LB score of 6.01720, good for 9th place . The final submission notebook is attached here: https://www.kaggle.com/code/yeoyunsianggeremie/s3e19-9th-place-submission EDIT: Could have been 6th if I chose the submission with the best Public LB instead of the best CV. Lol Thanks for reading! Please sign in to reply to this topic. comment 4 Comments Hotness Niveditha Vudayagiri Posted 2 years ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert So happy to be teamed up with you guys @bogoconic1 and @onurkoc83 for bringing our diverse expertise into this. This was a very challenging problem and I had a lot to learn about time series from this competition than ever before. Special mention to the Time series notebook series by @konradb Ravi Ramakrishnan Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 2 more_vert Hearty congratulations for the result and thanks for the notebook too @yeoyunsianggeremie @onurkoc83 @nivedithavudayagiri ! This was a great challenge and a good change from the yester episodes. Episodes like these offer take-aways for the future to the greatest extent. I think this was one of the best episodes insofar but the next one seems to be a bit more interesting. All the best for episode 20 too! Niveditha Vudayagiri Posted 2 years ago ¬∑ 9th in this Competition arrow_drop_up 2 more_vert Thank you and congratulations on doing consistently well on the playground series! Robert Trypuz Posted 2 years ago ¬∑ 172nd in this Competition arrow_drop_up 0 more_vert Congrats! It's an outstanding \"single-digit rank\" achievement! Thank you for sharing. Best",
      "Forecasting Mini-Course Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Forecasting Mini-Course Sales Playground Series - Season 3, Episode 19 Forecasting Mini-Course Sales Overview Data Code Models Discussion Leaderboard Rules Vipin Kumar ¬∑ 16th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 10 more_vert 16th Place Solution \"Magic Trick\" Hello to everybody! First of all, I want to thank Kaggle for organising this fantastic contest. However, the Playground series is an excellent location to learn and advance in machine learning and data science. Many new participants find it difficult to deal with the enormous volumes of complex data in many highlighted competitions. Since this was my first best rank in time series forecasting, this competition really benefited me. I want to share what I've learned through this competition with others by sharing my solution. Mostly I referd top score public notbooks to get the idea about feature engineering and molding, I Traind three model but non of the get  SMAPE < 7.44. Model I used Below Listed- LGBM Regressor Cat Boost Regressor XGB Regressor In the end of the week i try ensombling the Different  models: Thanks to @paddykb and @christph For there works that help me lot to boast my public LB. https://www.kaggle.com/code/christph/gam-with-holidays https://www.kaggle.com/code/paddykb/ps-s3e19-tableau-eda-gam-fit So I decide to mean ensemble total 5 models and got 6.58 on LB and 7.44 on private LB Magic Trick In the last day of deadline i try some magic trick. Idea comes from @ravi20076 discussion about submission . As he suggested rounding the prediction increasing the LB so i decide to round my submission and got improvement on LB 6.58  to 5.800. After looking the improvement only rounding the prediction i got idea to add 1 in every prediction may increase LB and it does my LB improved 5.800 to 5.596 on LB and On private 6.47 and score 16th place. Thanks Happy Learning! Time Series Analysis Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Forecasting Mini-Course Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Forecasting Mini-Course Sales Playground Series - Season 3, Episode 19 Forecasting Mini-Course Sales Overview Data Code Models Discussion Leaderboard Rules mateuszk ¬∑ 17th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 7 more_vert Blending best public submissions - 17th | Simple approach - ~70th Hello all, I want to show you my simple solution to this competition which results in about 70th place and blending the best public submissions, which allows us to get 17th place. 17th Solution: The 17th solution is simple. I've noticed that @yeoyunsianggeremie published a dataset with the best public submissions, available here: https://www.kaggle.com/datasets/yeoyunsianggeremie/s3e19-top-public-notebook-submissions . Since @paddykb mentioned the probable data construction error, I've concluded that probing the LB may work in this competition. Therefore, I was just curious about what we can accomplish by blending these best public predictions with mean strategy. As it turned out, the result allowed us to gain the 17th result of private LB (top 2%). Okay, so there is no analysis, just blend these best public submissions and assuming there is a data construction error, it should work fine. The complete code snippet is available below: import glob from functools import partial import pandas as pd import numpy as np path = \"/kaggle/input/s3e19-top-public-notebook-submissions\" best_public_lb_paths = glob.glob(f \"{path}/*.csv\" ) best_lbs = pd.concat(map(partial(pd.read_csv, index_col= \"id\" ), best_public_lb_paths), axis= 1 ) best_lbs .columns = [name.split( \"/\" )[ -1 ] for name in best_public_lb_paths] best_of_best_lbs = best_lbs.drop(  # Several submissions are not postprocessed.\n    [ \"bogoconic1_48.86.csv\" , \"nivedithavudayagiri_38.87.csv\" , \"oscar_no_postprocessing.csv\" , \"paddykb_no_postprocessing.csv\" ,\n    ],\n    axis= 1 ,\n) submission = pd. DataFrame (\n    { \"id\" : best_of_best_lbs.index, \"num_sold\" : best_of_best_lbs.mean(axis= 1 ). as type (np.int32),\n    }\n).set_index( \"id\" ) submission .to_csv( \"submission.csv\" ) content_copy ~70th Solution: So, let's get to my original work, which resulted in about 70th on private LB (top 6%). We also don't have any rocket science here, and the approach is as simple as possible. Preprocessing supposes several steps as follows: Fixing the Covid period - There was a small breakdown during the Covid-19 lockdown, which is fine to resolve. Generally, I used a simple strategy of multiplying each observation by a certain factor from the beginning of March 2020 up to the end of July 2020. I've just taken a mean number of sold products for each observation from this period in the years 2017, 2018 and 2019. This way, we obtain factors by dividing these mean values by values observed during the Covid lockdown. Numeric Dates - I added a numeric date version to the dataset. These are obviously Day , Month , Year , and flags if there is a weekend - IsWeekend , and Sunday - IsSunday . Trigonometric Features - I added sine functions, i.e. MonthSin and DaySin . Holidays - I included a holiday flag, i.e. IsHoliday , for each country in the dataset using the holiday library. Original Features - Just encode these with OrdinalEncoder . Finally, the model is just an ordinary LGBMRegressor with a little bit of regularization found by Optuna . Additionally, I fit it to the logarithmic version of the target. The parameters are as follows: params = { \"random_state\" : 51 , \"learning_rate\" : 0.17 , \"min_child_samples\" : 336 , \"colsample_bytree\" : 0.5 , \"reg_lambda\" : 8.35 , \"reg_alpha\" : 0.36 ,\n} content_copy Since the public LB score obtained with that model was horrible, I started to probe, which has led to the following multipliers: probed_map = { \"Argentina\" : 4.5, \"Spain\" : 1.6, \"Japan\" : 1.2, \"Estonia\" : 1.7, \"Canada\" : 0.9,\n} content_copy resulting in 6.08955 on public LB and 7.21200 on private LB. Since I joined the competition a couple of days before it finished, I couldn't boost this result any more, but I think it's decent regarding the simplicity of the approach. In fact, probably to get this result, we don't need to bother with the model and only find the coefficients. Many thanks to: @yeoyunsianggeremie - for publishing dataset with best public submissions. @paddykb - for this post: Data construction error? @ravi20076 - for this post: Request for some variety in ongoing challenges , and discussions. My notebook is available here if somebody is interested: Playground Series S3E19 - Forecasting Sales Have a nice day! Please sign in to reply to this topic. comment 1 Comment Hotness bogoconic1 Posted 2 years ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert Congrats on placing 17th ! Thank you for the mention, happy that the dataset has helped. I am doing the same initiative for the new playground competition https://www.kaggle.com/datasets/yeoyunsianggeremie/s3e20-top-public-notebook-submissions (it has nothing now since the competition just started lol üòÖ)"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 19 For this challenge, you will be predicting a full year worth of sales for various fictitious learning modules from different fictitious Kaggle-branded stores in different (real!) countries. This dataset is completely synthetic, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. You are given the task of predicting sales during for year 2022. Good luck! 3 files 12.79 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 12.79 MB sample_submission.csv test.csv train.csv 3 files 13 columns ",
    "data_description": "Forecasting Mini-Course Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Forecasting Mini-Course Sales Playground Series - Season 3, Episode 19 Forecasting Mini-Course Sales Overview Data Code Models Discussion Leaderboard Rules Overview Start Jul 11, 2023 Close Aug 1, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in July every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Submissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0. Submission File For each id in the test set, you must predict the corresponding num_sold . The file should contain a header and have the following format: id ,num_sold 136950 , 100 136950 , 100 136950 , 100 etc . content_copy Timeline link keyboard_arrow_up Start Date - July 11, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  July 31, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Forecasting Mini-Course Sales. https://kaggle.com/competitions/playground-series-s3e19, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 3,599 Entrants 1,212 Participants 1,172 Teams 10,058 Submissions Tags Beginner Tabular Time Series Analysis SMAPE Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e20",
    "discussion_links": [
      "/competitions/playground-series-s3e20/discussion/433822",
      "/competitions/playground-series-s3e20/discussion/433567"
    ],
    "discussion_texts": [
      "Predict CO2 Emissions in Rwanda | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Predict CO2 Emissions in Rwanda Playground Series - Season 3, Episode 20 Predict CO2 Emissions in Rwanda Overview Data Code Models Discussion Leaderboard Rules Xuebin Jiang ¬∑ 3rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 31 more_vert 3rd Place Solution (convert the COVID affected 2020 week8-week32's emission) Thanks to the organizers and participants of this competition! 2020 week 8-32 emissions are outliers Inspired by ambrosm's post , the Year-Over-Year Growth by Quarter show's a COVID affected period from 2020 Q2,Q3,Q4 and 2021 Q1. I made a more detailed Year-Over-Year Growth by Month, as we can see that the most affected months are 2020 Mar, Apr, May, Jun, Jul, Aug with a double digits decrease in emission. That means the corresponding weeks 8-32 in 2020 are outliers! And from 2020 Sep, the decrease trend goes into digits, so it can be assumed that the affect of COVID isn't high compared to previous months, this can be regard as the recovery. So, based on this, split the training dataset into two: Convert outliers to the range of Non-COVID affected period Inspired by KACPER RABCZEWSKI's post , this post's idea is transfer the 2020's emission to the average level of 2019 and 2021. I did the conversion by following steps: Calculate the average emission by week in the Non-COVID dataset. Calculate the average emission by week in the COVID dataset. Calculate the ratio. Convert the COVID dataset‚Äôs emission by multiplying the ratio. Replace the training dataset‚Äôs emission value by the new value of the COVID period. Feature selection, training samples selection Select week_no <= 48 samples for training, which is the same week range as the test 2022.\nSelect 3 features ['latitude', 'longitude', 'week_no'] which has no missing values. Ensemble Model Submission fix AMBROSM's discussion topic , I multiply the result by the constant 1.07. CHUN FU's notebook , fix the bug at ['longitude'] == 29.321. Please sign in to reply to this topic. comment 7 Comments Hotness Riya Raizada Posted a year ago arrow_drop_up 0 more_vert Good work!! Mahmoud Ahmed Al-Shamakh Posted 2 years ago arrow_drop_up 0 more_vert Very very good work Zhuoqun Li Posted 2 years ago arrow_drop_up 0 more_vert You won't believe the incredible outcome of this! √Ålvaro Chapresto Posted 2 years ago arrow_drop_up 0 more_vert Awesome XUEBIN JIANG. Thanks for sharing. Best regards! DedSec Posted 2 years ago arrow_drop_up 0 more_vert Insightful Avesh Mishra Posted 2 years ago arrow_drop_up 0 more_vert nicely explained @xuebinjiang This comment has been deleted.",
      "Predict CO2 Emissions in Rwanda | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Predict CO2 Emissions in Rwanda Playground Series - Season 3, Episode 20 Predict CO2 Emissions in Rwanda Overview Data Code Models Discussion Leaderboard Rules Konstantin Dmitriev ¬∑ 4th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 43 more_vert 4th place solution: using PCA (least shakeuped solution) The solution notebook is available here Thanks to the organizers of this interesting competition, and thanks to all participants for the discussions! I think there was much valuable information to learn and improve! The pipeline I started this competition by building a pipeline to make CV and test different approaches. This pipeline consequently used each available year (2019, 2020, and 2021) for validation while the others were used for training. Finally, all the data was used for training and prediction of the 2022 emissions. Please, refer to the source code for details. Using PCA The core of my solution is using PCA. I decided to use 6 components, as I reported in this discussion by @ambrosm . Each of the 6 components is processed independently by some (abstract) algorithm, and the results are taken to inverse PCA. Finally, we have the predictions. Algorithms Each algorithm I used has a base estimator as a parameter, which is used to make a prediction. With such an architecture, it's possible to switch between them and optimize CV score. ‚úì PCA1 Algorithm This algorithm is very simple: it takes emission data only and processes it with an estimator. It was reported in many discussions that all other columns are not needed. So this is an implementation of this idea. ‚úì PCA2 Algorithm This algorithm is different: it uses all columns . PCA is applied separately to each of them, i.e., we have n_columns * n_pca_components input features. ‚úì PCA_SARIMA Algorithm Instead of simple estimators, I used 6 independent SARIMA models in the PCA1 Algorithm architecture. The best algorithm (both local CV and private LB) was the PCA2 Algorithm. Estimators I used Ridge/Lasso regression, RF, XGB, CatBoost, etc. There was not much difference between tree-based estimators, and they gave better results than linear regression. So I choose XGBRegressor(n_estimators = 300, max_depth = 4, learning_rate = 0.01, subsample = 0.5). Postprocessing I was inspired by @ambrosm 's post and other discussions, which propose to multiply the result by the constant. So, I made predictions with different multipliers: 1.00, 0.95, 1.05, 1.07, and 1.08. However, somehow I chose the worst submission of these five as my final prediction! It would be the 1st place, if I didn't use the multiplier at all! 2 Please sign in to reply to this topic. comment 11 Comments Hotness Random Draw Posted 2 years ago ¬∑ 144th in this Competition arrow_drop_up 3 more_vert Well done! How did your highest scoring submission on the public LB fare on the private LB? This comment has been deleted. Konstantin Dmitriev Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert My highest LB submission has 13.18 private and 17.99 public score. It corresponds to ~230th place. Muhandro Posted 2 years ago ¬∑ 127th in this Competition arrow_drop_up 1 more_vert Well done! How did you choose the XGBRegressor's parameters? The results of changing the settings were insignificant? Konstantin Dmitriev Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Yes, although I tried different settings, there was not much difference. Even different estimators gave approximately the same results. Chris Deotte Posted 2 years ago arrow_drop_up 2 more_vert Congratulations for staying high on both public and private LB! Well done! Mohamed El-Feghi Posted 2 years ago ¬∑ 87th in this Competition arrow_drop_up 2 more_vert Thanks for sharing! Is there a reason you use classes in your solution? Does this method have a name? I'm aiming to clean up my methodology so I'm trying to understand more methodologies. Konstantin Dmitriev Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert I think, PCA+XGB is a reasonable name since it is a combination of these two methods. The reasons of using classes are: to structure the code; to switch between different approaches quickly (just by selecting one class or another). If you look at my notebook, you may notice, that there are different Pipelines, Algorithms and Estimators. I developed this approach further in latter versions. Jean Posted 2 years ago ¬∑ 25th in this Competition arrow_drop_up 2 more_vert The choice of the submission is a bummer indeed. Well done in any case! Abdou Aziz Thiam Posted a year ago arrow_drop_up 0 more_vert well done lad This comment has been deleted."
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 20 The objective of this challenge is to create machine learning models that use open-source emissions data (from Sentinel-5P satellite observations) to predict carbon emissions. Approximately 497 unique locations were selected from multiple areas in Rwanda, with a distribution around farm lands, cities and power plants. The data for this competition is split by time; the years 2019 - 2021 are included in the training data, and your task is to predict the CO2 emissions data for 2022 through November. Seven main features were extracted weekly from Sentinel-5P from January 2019 to November 2022. Each feature (Sulphur Dioxide, Carbon Monoxide, etc) contain sub features such as column_number_density which is the vertical column density at ground level, calculated using the DOAS technique. You can read more about each feature in the below links, including how they are measured and variable definitions. You are given the values of these features in the test set and your goal to predict CO2 emissions using time information as well as these features. Important: Please only use the data provided for this challenge as part of your modeling effort. Do not use any external data, including any data from Sentinel-5P not provided on this page. 3 files 119.66 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 119.66 MB sample_submission.csv test.csv train.csv 3 files 153 columns ",
    "data_description": "Predict CO2 Emissions in Rwanda | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Predict CO2 Emissions in Rwanda Playground Series - Season 3, Episode 20 Predict CO2 Emissions in Rwanda Overview Data Code Models Discussion Leaderboard Rules Overview Start Aug 1, 2023 Close Aug 22, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in July every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. This Episode is a similar to the Kaggle/Zindi Hackathon that was held at the Kaggle@ICLR 2023: ML Solutions in Africa workshop in Rwanda, and builds on an ongoing partnership between Kaggle and Zindi to build community-driven impact across Africa. Zindi is a professional network for data scientists to learn, grow their careers, and get jobs. If you haven't done so recently, stop by Zindi and see what they're up to! Predicting CO2 Emissions The ability to accurately monitor carbon emissions is a critical step in the fight against climate change. Precise carbon readings allow researchers and governments to understand the sources and patterns of carbon mass output. While Europe and North America have extensive systems in place to monitor carbon emissions on the ground, there are few available in Africa. The objective of this challenge is to create a machine learning models using open-source CO2 emissions data from Sentinel-5P satellite observations to predict future carbon emissions. These solutions may help enable governments, and other actors to estimate carbon emission levels across Africa, even in places where on-the-ground monitoring is not possible. Acknowledgements We acknowledge Carbon Monitor for the use of the GRACED dataset, and special thanks Darius Moruri from Zindi for his work in preparing the dataset and starter notebooks. Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = ‚àö 1 N N ‚àë i = 1 ( y i ‚àí ÀÜ y i ) 2 where ÀÜ y i is the predicted value and y i is the original value for each instance i . Submission File For each ID_LAT_LON_YEAR_WEEK row in the test set, you must predict the value for the target emission . The file should contain a header and have the following format: ID_LAT_LON_YEAR_WEEK ,emission ID_ - 0 . 510 _29. 290 _2022_00, 81 . 94 ID_ - 0 . 510 _29. 290 _2022_01, 81 . 94 ID_ - 0 . 510 _29. 290 _2022_02, 81 . 94 etc . content_copy Timeline link keyboard_arrow_up Start Date - August 1, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  August 21, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Darius Moruri, Amy Bray, Walter Reade, and Ashley Chow. Predict CO2 Emissions in Rwanda. https://kaggle.com/competitions/playground-series-s3e20, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 5,072 Entrants 1,496 Participants 1,440 Teams 11,417 Submissions Tags Beginner Tabular Time Series Analysis Root Mean Squared Error Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e21",
    "discussion_links": [
      "/competitions/playground-series-s3e21/discussion/439142",
      "/competitions/playground-series-s3e21/discussion/438745"
    ],
    "discussion_texts": [
      "Improve a Fixed Model the Data-Centric Way! | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Improve a Fixed Model the Data-Centric Way! Playground Series - Season 3, Episode 21 Improve a Fixed Model the Data-Centric Way! Overview Data Code Models Discussion Leaderboard Rules Maxime Perez ¬∑ 4th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 11 more_vert 4th place solution Here is my notebook: https://www.kaggle.com/code/maximeperez/objective-remove-top-n-errors I register in the competition in the last days and spent all my time watching the MIT series. I was expecting to use their package cleanlab for the competition but end up to be impossible because the problem was a regression problem. So i end up testing to remove outliers, i was not expecting that result. Please sign in to reply to this topic. comment 5 Comments Hotness Christy Posted 2 years ago arrow_drop_up 1 more_vert Great work Ericka42 Posted 2 years ago arrow_drop_up 2 more_vert Congratulations üåü Muhamed Tuo Posted 2 years ago arrow_drop_up 2 more_vert Congrats mate üëè Ashish Agarwal Posted 2 years ago ¬∑ 27th in this Competition arrow_drop_up 2 more_vert Great Work! Rishabh Jain Posted 2 years ago ¬∑ 840th in this Competition arrow_drop_up 2 more_vert Great work @maximeperez . Congratulations on the achievement",
      "Improve a Fixed Model the Data-Centric Way! | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Improve a Fixed Model the Data-Centric Way! Playground Series - Season 3, Episode 21 Improve a Fixed Model the Data-Centric Way! Overview Data Code Models Discussion Leaderboard Rules Demyan Pavlyshenko ¬∑ 33rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 4 more_vert #33 Solution: Model distillation My solution was pretty simple - is to select a few features, remove a few outliers and distill XGB model into RandomForest (given in the competition), which is discussed here In feature selection , I choose only O2_1, O2_2, BOD5_5 columns to train both xgb and rf models, and added o2_mean feature only for xgb model, which is the overall mean of all o2_cols (including dropped cols like O2_3, O2_4 and etc.) In removing outliers I removed all datapoints containing BSOD5_5 bigger than 40 and NH4_5 bigger than 60 (even though NH4_5 is dropped) for all xgb and rf training data, but only for xgb, i marked O2_1, O2_2 cols which are less than 4 as nan . In model distillation , I first trained xgb model on its training data, and then used its training predictions as the training target for rf, and so that, rf must reatreat as much of xgb model efficiency as possible (In cross-validation, rf's been reatreating 88-90% efficiency). But also, we can think of distillation as a target normalizing, or, in other words, handling target outliers And that's all, I also tried to use IsolationForest to remove outliers, but it gave me poor results Good luck in the next competitions! Please sign in to reply to this topic. comment 1 Comment Hotness bogoconic1 Posted 2 years ago ¬∑ 545th in this Competition arrow_drop_up 1 more_vert Congrats! I did almost the same as you for one of my submissions‚Ä¶except that I didn‚Äôt drop outliers (and used a different feature subset) before training with LightGBM, guess that made a difference lol. My submission scored 1.05xxx. Glad to see that the approach has worked out!"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 21 This is a very different type of challenge! For this challenge, your task is to improve a dataset that is being used to train a random forest model; in other words, your submission will be training data, not predictions. A random forest model will be trained on your submission, used to make predictions, and then those predictions will be used to generate your score. The dataset for this competition is a synthetic dataset based off of the Dissolved oxygen prediction in river water dataset. You are free to use the original in any way that you find useful. Please see important information on the Evaluation tab about the model that will be trained on your submitted data. Good luck! 1 files 677 kB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 677 kB sample_submission.csv 1 file 37 columns ",
    "data_description": "Improve a Fixed Model the Data-Centric Way! | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Improve a Fixed Model the Data-Centric Way! Playground Series - Season 3, Episode 21 Improve a Fixed Model the Data-Centric Way! Overview Data Code Models Discussion Leaderboard Rules Overview Start Aug 22, 2023 Close Sep 12, 2023 Description link keyboard_arrow_up Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in August every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up This is a different type of competition. Instead of submitting predictions, your task is to submit a dataset that will be used to train a random forest regressor model. This model will then be used to make predictions against a hidden test dataset. Your score will be the Root Mean Square Error (RMSE) between the model predictions and ground truth of the test set. Model Your submission will be used as a training dataset to train the following model and make predictions against a hidden test dataset. from sklearn.ensemble import RandomForestRegressor\n\ny_train = train.pop( 'target' ) # train is your submission! rf = RandomForestRegressor(\n       n_estimators= 1000 ,\n       max_depth= 7 ,\n       n_jobs=- 1 ,\n       random_state= 42 )\nrf.fit(train, y_train)\ny_hat = rf.predict(test) # test set is hidden from you content_copy Submission File You are submitting a dataset that will be used train a random forest model. Your submission must have all of the columns contained in the sample_submission.csv on the Data tab . Your submission must not contain any NaN values (it will error if it does). In addition, your submission may have fewer rows than the provided sample_submission.csv , but may not have a greater number of rows. id ,target,O2_1,O2_2,O2_3,... 0 , 8 . 59 , 7 . 5 , 9 , 9 . 545 ,... 1 , 9 . 1 , 13 . 533 , 40 . 9 , 8 . 77 ,... 2 , 8 . 21 , 3 . 71 , 5 . 42 , 8 . 77 ,... etc . content_copy Timeline link keyboard_arrow_up Start Date - August 22, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  September 11, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade, Sohier Dane, and Ashley Chow. Improve a Fixed Model the Data-Centric Way!. https://kaggle.com/competitions/playground-series-s3e21, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 2,768 Entrants 986 Participants 955 Teams 10,344 Submissions Tags Beginner Tabular Data Cleaning Custom Metric Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e22",
    "discussion_links": [
      "/competitions/playground-series-s3e22/discussion/444642"
    ],
    "discussion_texts": [
      "Predict Health Outcomes of Horses | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Predict Health Outcomes of Horses Playground Series - Season 3, Episode 22 Predict Health Outcomes of Horses Overview Data Code Models Discussion Leaderboard Rules Oleksii Zhukov ¬∑ 14th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 25 more_vert # 14 Solution Hey everyone, not the best score out there, but got 14th place with my publick notebook: Version 34 got private score 0.76818 (14th place in LB): https://www.kaggle.com/code/zhukovoleksiy/ps-s3e22-eda-preprocessing-ensemble?scriptVersionId=143911248 Version 29 got private score 0.77121 (I didn't select it for final submission): https://www.kaggle.com/code/zhukovoleksiy/ps-s3e22-eda-preprocessing-ensemble?scriptVersionId=143671010 Of course, a 20/80 distribution for such a small dataset makes this competition quite random. Throughout the competition I tried to pay as much attention to overfitting as possible, and overall it had a good effect. I assume that the higher places have single model solutions, it will be interesting to look what they got. I won‚Äôt write more here about my solution; it will be easier for you to see for yourself. If you have any questions, ask here or in the comments for the notebook itself. Thanks to Kaggle for hosting the competition, and thanks to everyone who took part! Good luck ü§ó. Please sign in to reply to this topic. comment 14 Comments Hotness Moerlzzz Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! Well done! üëè Rafael Penido Posted 2 years ago ¬∑ 25th in this Competition arrow_drop_up 1 more_vert Great job, my friend. You did a fantastic job handling the overfitting. I also thought that simpler solutions would yield better results on the private leaderboard. One of my submissions using just a tuned XGBoost model with simple feature engineering achieved a Private Score of 0.78333 (unfortunately, I didn't select it). Thank you for sharing your notebook with us. Chan Lee Posted 2 years ago ¬∑ 171st in this Competition arrow_drop_up 1 more_vert @penidorafael @zhukovoleksiy I've generally understood that ensemble models tend to yield higher scores. However, I'm curious about why, in the case of small datasets, a straightforward single model often delivers better performance. Could you provide some insights into this phenomenon? ABOUABDALLAH Mohamed Anwar Posted 2 years ago arrow_drop_up 0 more_vert Th√© main Idea IS that a simple model lik√© d√©cision trees Can easly lead to overfitting problems bagging m√©thodes such random forest or boosting m√©thodes lik√© xgboos or Ada boost male mode les more robusts as it's using frequency Ankur Limbashia Posted 2 years ago ¬∑ 520th in this Competition arrow_drop_up 1 more_vert Congratulations! Al Sani Posted 2 years ago arrow_drop_up 1 more_vert Congratulations brother for this Zakari Salifu Posted 2 years ago ¬∑ 760th in this Competition arrow_drop_up 1 more_vert Congratulations üéâ. Great üëç Beket Myrzanov Posted 2 years ago ¬∑ 298th in this Competition arrow_drop_up 1 more_vert –ß–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –ø–µ—Ä–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ –æ—Ç –≤—Ç–æ—Ä–æ–≥–æ? Robert Oganyan Posted 2 years ago ¬∑ 269th in this Competition arrow_drop_up 1 more_vert Congratz, kinda random indeed Demyan Pavlyshenko Posted 2 years ago ¬∑ 471st in this Competition arrow_drop_up 1 more_vert Congratulations for the 14th Place, @zhukovoleksiy ! Sohaib Moradi Posted 2 years ago ¬∑ 285th in this Competition arrow_drop_up 1 more_vert Congratulations for the results. emoji_people Aaradhya Badal Posted 2 years ago ¬∑ 347th in this Competition arrow_drop_up 1 more_vert I will check your notebooks out , thank you for sharing and congrats! Thomas Mei√üner Posted 2 years ago ¬∑ 971st in this Competition arrow_drop_up 1 more_vert Congratulations! Dr. Alvinleenh Posted 2 years ago ¬∑ 295th in this Competition arrow_drop_up 1 more_vert Congrats @zhukovoleksiy , thanks for sharing Suraj Posted 2 years ago ¬∑ 1126th in this Competition arrow_drop_up 1 more_vert Very interesting ! Thanks for sharing your solution @zhukovoleksiy !"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 22 The dataset for this competition (both train and test) was generated from a deep learning model trained on a portion of the Horse Survival Dataset . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Good luck! 3 files 386.6 kB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 386.6 kB sample_submission.csv test.csv train.csv 3 files 59 columns ",
    "data_description": "Predict Health Outcomes of Horses | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Predict Health Outcomes of Horses Playground Series - Season 3, Episode 22 Predict Health Outcomes of Horses Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in September every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Your Goal: Given various medical indicators, predict the health outcomes of horses. Start Sep 12, 2023 Close Oct 3, 2023 Description link keyboard_arrow_up Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Submissions are evaluated on micro-averaged F1-Score between pricted and actual values. Submission File For each id in the test set, you must predict the corresponding outcome . The file should contain a header and have the following format: id,outcome 1235 ,lived 1236 ,lived 1237 ,died etc. content_copy Timeline link keyboard_arrow_up Start Date - September 12, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  October 2, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Predict Health Outcomes of Horses. https://kaggle.com/competitions/playground-series-s3e22, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 4,907 Entrants 1,628 Participants 1,541 Teams 12,287 Submissions Tags Beginner Tabular Multiclass Classification Animals Health F1 Score Table of Contents collapse_all Overview Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e23",
    "discussion_links": [
      "/competitions/playground-series-s3e23/discussion/450315"
    ],
    "discussion_texts": [
      "Binary Classification with a Software Defects Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Software Defects Dataset Playground Series - Season 3, Episode 23 Binary Classification with a Software Defects Dataset Overview Data Code Models Discussion Leaderboard Rules Oscar Aguilar ¬∑ 2nd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 95 more_vert #2 Solution | 8 Models Ensemble First of all, I would like to start with a big thank you to Kaggle for running this episode of the playground series. In this post, I will briefly explain my approach, which most of it can be found in my notebook . Pre-processing Initially, I modeled the data without any transformation, which produce a descent CV and LB score (about 0.793 CV score and 0.790 LB score). Then, I log-transform all of the input features as suggested by @ambrosm in this post . The surprising factor here was that there was a small improvement in model performance in most of the tree-based and boosted-tree models that I considered after the inputs were log-transformed . Models & Ensemble In my notebook , I trained the following models: Random Forest Extra Trees HistGradientBoosting LightGBM XGBoost CatBoost I ensemble those six models with hill climbing ensemble , which gave me a 0.7907 LB score as shown below. Then, in order to increase the diversity of the ensemble, I ensemble the hill climbing ensemble (of the six tree-based models) with the Nystr√∂m LogisticRegression model presented in this notebook . This boost my LB score from 0.7907 to 0.79099 as shown below. Finally, I decided to include neural network model to the ensemble, which was inspired this notebook from @sauravpandey11 . This boost my LB score from 0.79099 to 0.79101 as shown below. Unfortunately, I did not select the above submission. I decided to select another another ensemble that had a slightly higher LB score. What did not work I tried a few different things: Initially I tried PCA as a way to reduce the number of input features to help reduce the model building time. In my notebook, I highlighted that 10 components explain more than 99% of the variability of the data. However, using PCA instead of the log-tranform features did not help with model performance. I also tried t-SNE to see if there was a way to separate the two classes but it did not help. I also tried clustering as presented in my notebook. But it failed. I even tried target encoding with the clusters but there no improvement in model performance. Conclusion In this comp, ensemble and model diversity were the key to victory! 2 Please sign in to reply to this topic. comment 42 Comments 8 appreciation  comments Hotness iMatei Posted 2 years ago arrow_drop_up 1 more_vert Congratulations and thanks for sharing the insights! SaverioMaggese Posted 2 years ago arrow_drop_up 1 more_vert congratulations! Priyavarshini G R Posted 2 years ago ¬∑ 1692nd in this Competition arrow_drop_up 1 more_vert Congratulations DavidAdelG Posted 2 years ago arrow_drop_up 1 more_vert Congrats!!! ArmiinJP Posted 2 years ago arrow_drop_up 1 more_vert Congratulations and thank you for sharing your insight Utkarsh Chaudhary Posted 2 years ago arrow_drop_up 1 more_vert Congratulations on your achievement. Great work Oscar ML_Magic Posted 2 years ago ¬∑ 558th in this Competition arrow_drop_up 1 more_vert congreatulations Samuel Cortinhas Posted 2 years ago arrow_drop_up 1 more_vert Congratulations Oscar, nice job! tazib ahmed Posted 2 years ago ¬∑ 1226th in this Competition arrow_drop_up 1 more_vert Congratulations and thank you for sharing your insight Gizachew Alemu Posted 2 years ago arrow_drop_up 1 more_vert Congratulation on your position and thank you for sharing. Kaggler Posted 2 years ago ¬∑ 1602nd in this Competition arrow_drop_up 2 more_vert Congratulation and thank you for your share. I learnt a lot from you. You lead me to new vision to machine learning. emoji_people Zhuoqun Li Posted 2 years ago ¬∑ 264th in this Competition arrow_drop_up 1 more_vert Great Job!!! Mar√≠lia Prata Posted 2 years ago arrow_drop_up 1 more_vert A huge congratulations Aguilar for your 2nd position on this competition! Felipe Gonzalez Urrego Posted 2 years ago ¬∑ 623rd in this Competition arrow_drop_up 1 more_vert Congratulation and thanks, I forgot to log-transform the data. Eric Exason Posted 2 years ago ¬∑ 257th in this Competition arrow_drop_up 1 more_vert Very well done, thank you for sharing and congratulation! Nice and clean. Modekurthy Venkata Surya Sunil Posted 2 years ago ¬∑ 627th in this Competition arrow_drop_up 1 more_vert Congrats @oscarm524 sun9sun9 Posted 2 years ago ¬∑ 314th in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for sharing your solution. Edmond Kirsch Posted 2 years ago ¬∑ 80th in this Competition arrow_drop_up 1 more_vert Thank you for sharing solution with us. Keep it up aldparis Posted 2 years ago ¬∑ 161st in this Competition arrow_drop_up 1 more_vert Congratulations @oscarm524 ! I regret not having tried Hill Climbing until two days ago. Your public notebook was a very good starter, and your final result is fantastic ! Nikolay Abramov Posted 2 years ago ¬∑ 574th in this Competition arrow_drop_up 1 more_vert Well done!!! I will scrutinize your work - it is drop-dead gorgeous GeorgeCheng97 Posted 2 years ago ¬∑ 31st in this Competition arrow_drop_up 1 more_vert Congratulations, I am reading your shared notebook and that‚Äôs insightful! kerry sun Posted 2 years ago ¬∑ 87th in this Competition arrow_drop_up 1 more_vert Congratulations, i learn a lot of skill from your notebook and discussion Include„ÄåSometimes more is better„Äç. Big thanks BwandoWando Posted 2 years ago arrow_drop_up 1 more_vert Oh wow! Congratulations for placing very high and performing amazingly well @oscarm524 ! Luis Talavera Posted 2 years ago ¬∑ 785th in this Competition arrow_drop_up 1 more_vert Congratulations. Thanks for sharing your findings :D alpimarc Posted 2 years ago ¬∑ 1316th in this Competition arrow_drop_up 2 more_vert Congratulation and thank you for your share and explanations. I tried your Hill climbing implementation with the s3e24 and it gives me good results :)"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 23 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Software Defect Dataset . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 18.63 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 18.63 MB sample_submission.csv test.csv train.csv 3 files 47 columns ",
    "data_description": "Binary Classification with a Software Defects Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Classification with a Software Defects Dataset Playground Series - Season 3, Episode 23 Binary Classification with a Software Defects Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2023 edition of Kaggle's Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! With the same goal to give the Kaggle community a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science, we will continue launching the Tabular Tuesday in October every Tuesday 00:00 UTC, with each competition running for 3 weeks. Again, these will be fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Your Goal: Predict defects in C programs given various various attributes about the code. Start Oct 3, 2023 Close Oct 24, 2023 Description link keyboard_arrow_up Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each id in the test set, you must predict the probability for the target variable defects . The file should contain a header and have the following format: id ,defects 101763 , 0 . 5 101764 , 0 . 5 101765 , 0 . 5 etc . content_copy Timeline link keyboard_arrow_up Start Date - October 3, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  October 23, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Binary Classification with a Software Defects Dataset. https://kaggle.com/competitions/playground-series-s3e23, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 4,140 Entrants 1,798 Participants 1,702 Teams 14,011 Submissions Tags Beginner Tabular Binary Classification Roc Auc Score Table of Contents collapse_all Overview Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e24",
    "discussion_links": [
      "/competitions/playground-series-s3e24/discussion/455248",
      "/competitions/playground-series-s3e24/discussion/455296",
      "/competitions/playground-series-s3e24/discussion/455271",
      "/competitions/playground-series-s3e24/discussion/455268",
      "/competitions/playground-series-s3e24/discussion/462385"
    ],
    "discussion_texts": [
      "Binary Prediction of Smoker Status using Bio-Signals | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Prediction of Smoker Status using Bio-Signals Playground Series - Season 3, Episode 24 Binary Prediction of Smoker Status using Bio-Signals Overview Data Code Models Discussion Leaderboard Rules Ravi Ramakrishnan ¬∑ 3rd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 57 more_vert #3 Private 8 Public approach - simple ensemble with probing Hello all, Firstly wishing one and all a happy Diwali! I hope the festival of lights brings joy, good health and lots of success to one and all! I wish to extend sincere thanks to Kaggle for an interesting episode. It was indeed a great experience with lots of takeaways and learning that we can use outside of Kaggle too. I also wish to thank others participants for their generous contributions with special mention of the below users- @cv13j0 -- his pseudo label notebook was great and offered a slightly different perspective to the assignment in the early stages @arunklenin -- his notebook was one of the most comprehensive work in the episode, especially his EDA and feature engineering sections @oscarm524 -- his notebook provided a good reference at the start of the competition @paddykb -- I appreciate his consistent and valuable contribution to the assignment with this light GBM pipeline I outline my approach as below- Feature engineering I initially started off with my post regarding secondary features but found that not using them was a better alternative with regard to the CV score on the synthetic dataset. I used brute-force features very similar to @arunklenin notebook . My approach was similar to his notebook, but I preferred to restrict myself to 80-120 features only. I eliminated features using permutation importance . This takes a long time to execute, hence kernel time management is key. I used my local PC to engender this task. I used a 10x1 stratified k-fold CV strategy based on the target classes. I tried the 10x3 repeated stratified K-fold strategy too, but it did not give me any added advantage in the challenge, so I reverted to the 10 x 1 stratified K-fold to good effect I used the original dataset for my models but not the adjutant original data mentioned in this post . Using this data did not bode well for me at the start of the competition, so I decided to use the competition synthetic data and the original data only. Model development I used a wide variety of models in my pipeline including the below- Catboost - I used 3 models with varied parameters LightGBM - I used 5 LGBM models with varied parameters XGBoost - I used 3 diverse parameter models Random Forest Logistic Regression TabNet Classifier -- my work without this option was almost equally good, so this gave me a puny advantage Multi-layer perceptron -- the neural network marginally contributed to improving my ensemble Generalized Additive Model Ensemble I used the below ensemble strategies using my single models as above Hill-climbing Optuna Stacking I assessed the effectiveness of the ensemble using my post-ensemble CV score and its relation to the leaderboard. I found that my Optuna framework correlated well to the LB and I was able to fine-tune the weights based on probing, so I decided to go ahead with this approach. My final submission consisted of an Optuna ensemble and probing. I manually modified some model weights in-fold based on the leaderboard score using probing. I found a marginal impact on the CV score using this strategy, this perhaps worked for me. My final submissions consisted of a probed Optuna tuned ensemble and another without the probing. Both submissions worked well, but the probed submission was just a bit better, awarding me the third place. What did not work Using more than 130-140 features -- I tried this in a version to no good CV score improvement and it did not add to my LB score as well, reinforcing the need to prune lowly useful features Neural networks and TabNet -- their weights in my final ensemble were puny. Perhaps not using them in totality would not have made a great difference to the overall model framework, so they could be included in this list Blind probing-- I tried this based on some public approaches but did not go ahead with this considering my past experience with the playground series. A blind ensemble of top public work seldom generalizes, so one may probe with care and caution for the best results. My takeaways Rely on the CV score Evaluate models on a good and appropriate CV strategy Do not over-complicate when not needed- simple models have a lot of power Model parameter tuning is much secondary to feature engineering. Probe with care- one may resist the temptation to probe into the public LB using weights that do not impact the CV positively. Best regards and happy learning! Tabular Binary Classification Please sign in to reply to this topic. comment 32 Comments 1 appreciation  comment Hotness Oscar Aguilar Posted 2 years ago ¬∑ 96th in this Competition arrow_drop_up 3 more_vert Congrats on the 3rd place @ravi20076 . Thanks for the mention! Based on the write up, it shows how much work you put into this! In my case, I gave up almost a week ago when the ensemble of public notebooks got out of control. Anyways, congrats again. Your consistency on the different competitions really show your dedication to the fieldüëç Ravi Ramakrishnan Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 3 more_vert @oscarm524 thank you for the rejoinder. I too was tempted to blindly probe and enjoy the LB jump, but soon resisted the temptation and preferred to stick to my CV. This paid off in the end!! Sarun P M Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 3 more_vert Point taken. I was too much tempted and jumped into it.üòÇ Elad Soffer Posted 2 years ago ¬∑ 1083rd in this Competition arrow_drop_up 1 more_vert Congratulations, and thanks on the explanations, I would like to ask, how did you arrive to so much features , I too tried to add, but I thought only on several? Thanks again. Ravi Ramakrishnan Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert We had a public notebook with a lot of features, I used them to good effect in my work too @eladsoffer Z Chen3 Posted 2 years ago ¬∑ 979th in this Competition arrow_drop_up 1 more_vert Great!!!!! @ravi20076 good notebook and explanationüëè willian  oliveira Posted 2 years ago arrow_drop_up 1 more_vert great! the  datas is perfect about informations Muhammad Dzakwan Alifi Posted 2 years ago arrow_drop_up 1 more_vert Great job! Best regards and continued success in your learning journey! BedynoAG Posted 2 years ago ¬∑ 58th in this Competition arrow_drop_up 1 more_vert Very informative, Thankyou @ravi20076 for this explanation üëè ML_Magic Posted 2 years ago ¬∑ 122nd in this Competition arrow_drop_up 1 more_vert many many congratulations Mehmet ISIK Posted 2 years ago arrow_drop_up 1 more_vert Congrats @ravi20076 good notebook and explanationüëè Rushikesh Dhaigude Posted 2 years ago ¬∑ 848th in this Competition arrow_drop_up 1 more_vert Congrats @ravi20076 Awesome work üòÉ !!! Riyad Mehdi Posted 2 years ago ¬∑ 1026th in this Competition arrow_drop_up 1 more_vert Congrats, great notebook and explanation Tom Wright-Anderson Posted 2 years ago ¬∑ 939th in this Competition arrow_drop_up 1 more_vert Hi Ravi thank you for sharing your insights! May I ask what was your experience like using GAMs in this competition? Which package(s) were you using for them? Thanks Ravi Ramakrishnan Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert I have an R notebook for that @thomaswrightanderson Hriday Posted 2 years ago ¬∑ 554th in this Competition arrow_drop_up 1 more_vert Congratulations Ravi! This was a really helpful resource! Also thanks for your notebooks across different competitions, there is a lot of learning in them! Modekurthy Venkata Surya Sunil Posted 2 years ago ¬∑ 650th in this Competition arrow_drop_up 1 more_vert congrats @ravi20076 ASK Posted 2 years ago ¬∑ 1845th in this Competition arrow_drop_up 1 more_vert @ravi20076 , congratulations on finishing in the top three!  And others as well. Thank you for sharing your expertise. It is quite useful for beginning machine learners like myself. Thanks. Yash Rana Posted 2 years ago ¬∑ 676th in this Competition arrow_drop_up 1 more_vert congratulation @ravi20076 and thanks for giving such a valuable information. Yan Teixeira Posted 2 years ago arrow_drop_up 1 more_vert Congrats @ravi20076 ! Well deserved. Your hard work is inspiring. Sarun P M Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Congrats @ravi20076 for yet another achievement and sharing a very organized notebook. Ashish Kumar Posted 2 years ago ¬∑ 109th in this Competition arrow_drop_up 1 more_vert Congrats @ravi20076 üëè Thomas Mei√üner Posted 2 years ago ¬∑ 437th in this Competition arrow_drop_up 1 more_vert Congratulations! Top! Radhakrishnan Posted 2 years ago ¬∑ 1414th in this Competition arrow_drop_up 1 more_vert Congratulations!! Very great writeup. I did not know much about ensemble strategies till your post. Your post on Domain information, feature ideas and references is very detailed as well. Minato Namikaze Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Congratulations @ravi20076 and Thanks for the mention. Perhaps the most stable LB across Playground series. In terms of the competition, it could have been more challenging. Thank you for all the contributions to the community and wish you a Happy Diwali :) NFoFUW Posted 2 years ago ¬∑ 294th in this Competition arrow_drop_up 2 more_vert Sorry, but I am a beginner, and I just wanted to ask something that may be obvious for you. As you mentioned, 'Catboost - I used 3 models with varied parameters.' I researched this technique and found its efficiency, but I wanted to inquire about how you choose diverse parameters. If you use optune or gridsearch, won't you get almost the same parameters? How can you choose them ? Ravi Ramakrishnan Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert I did not tune parameters with Optuna @justforfun44 NFoFUW Posted 2 years ago ¬∑ 294th in this Competition arrow_drop_up 1 more_vert how did you tune them so? Ravi Ramakrishnan Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert @justforfun44 I did not tune models for this edition. I relied on basic parameters and relied on selecting good features instead. ASK Posted 2 years ago ¬∑ 1845th in this Competition arrow_drop_up 1 more_vert @justforfun44 Sometimes basic parameters are also work better. just do experimentation. Octavi Grau Posted 2 years ago ¬∑ 21st in this Competition arrow_drop_up 2 more_vert Congrats @ravi20076 ! See you in the next PS :) sunny77 Posted 2 years ago ¬∑ 152nd in this Competition arrow_drop_up 2 more_vert Hi Ravi. I'm kind of new around here. May I ask what is Probing? You've also mentioned about appropriate CV strategy. Can you elaborate on that, that is how to build a good CV? Also, how do I test whether my CV is good? I'm sorry if my question is vague. I hope you get what I'm asking. Thanks in advance. Ravi Ramakrishnan Topic Author Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert CV strategy is the base for any competition and emanates from your models. In this case, a stratified CV strategy was suitable and models could be compared using this folding technique. A simple improvement on your CV score occurs when you add/ drop features, change model parameters and include new models in the ensemble. Probing is a situation when a participant fine-tunes the model ensemble based on the public leaderboard performance of ongoing submissions. It is akin to taking feedback from the lb to improve one's leaderboard performance. Hope this helps @sunny7712 sunny77 Posted 2 years ago ¬∑ 152nd in this Competition arrow_drop_up 1 more_vert Got it. Thanks. This comment has been deleted.",
      "Binary Prediction of Smoker Status using Bio-Signals | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Prediction of Smoker Status using Bio-Signals Playground Series - Season 3, Episode 24 Binary Prediction of Smoker Status using Bio-Signals Overview Data Code Models Discussion Leaderboard Rules aldparis ¬∑ 4th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 23 more_vert #4 th place solution - robust Hill Climbing Hi every one, I would like to thank Kaggle for this nice competition, and players community, who have shared so many tricks (see below) and so many good models (see below). Congratulation to the winners ! And congratulation to @kailai who was at the first place of the public LB during almost all this competition, and who should not have overfit the public LB : only 3 submission. You would have deserved to be the winner ! My strategy was to avoid overfitting and chance : public LB was build with only 20% of test data, and this was a 5 digit competition. I wanted to be in the 10% first of the private LB (like I did in the 2 previous PS Competition, but I had not with the same toolkit). My solution is based on a trick presented by @oscarm524 at the beginning of the previous competition available here . He convinced me to try Hill Climbing , BUT with a robust CV strategy . I've read @cdeotte about Hill Climbing here , this was not a 1st place in a private LB but only a first place in a public LB. So I did like I've explained at the beginning of this competition here , by using Hill Climbing with CV , with some little improvement (see below). First I've stored 25 various OOFS predictions, from my own or from public notebooks. With Hill Climbing and CV I've choosen an ensemble of 7 OOFS : @paddykb 's mean OOFS predictions available here . He didn't use any parameter to fit the ensemble, so there was no risk of overfitting the public LB ; all individual models were very strong. @arunklenin 's LGB, CAT and NN predictions in version 26 here . I didn't take later versions of this notebook because I've seen CV ensemble scores of @arunklenin were not better (maybe I'm wrong). I trained all fonds with all original samples, I had OOFS output for every model (and preds on test too), because the oof prediction available in original output had been build without CV. I've used oofs predictions of each model instead of using the optuma oof prediction. I had a personal LGB and 2 personal XGB (no feature engineeing, origin dataset added for training, hearing(left) , hearing(right) , urine protein dropped, random grid search, with GPU for grid search of XGB, grid search with 4 folds and repeatedstratifiedkfold for the 20 best models) I've trained Hill Climbing with 5 folds and by taking care that : when a model improved AUC score on a train fold, there is a score improvement on a validation set too. that the 7 models improves score in all validation folds and i ve repeated this experiment with 5 other folds (another seed) to be sure. The 3 first models (with the strongest coef) in Hill Climbing where always in the same order : @paddykb first, then @arunklenin 's LGB and NN About Hill Climbing improvements during this competition : After I had added parallel computing during Hill Climbiing and CV, @siukeitin had helped me to optimize Hill Climbing AUC computation with numpy here . See you later and have fun ! 1 Please sign in to reply to this topic. comment 12 Comments Hotness Usama7871 Posted 2 years ago arrow_drop_up 1 more_vert amazing work done ML_Magic Posted 2 years ago ¬∑ 122nd in this Competition arrow_drop_up 1 more_vert hi, congratulations NFoFUW Posted 2 years ago ¬∑ 294th in this Competition arrow_drop_up 1 more_vert Can you share your notebook please , i'am a beginner that will help me a lot ! aldparis Topic Author Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert I've worked on local for this competition, so I've not my solution notebook to share yet, but I've shared a notebook at the end of PSS3E23 here which can help to use Hill Climbing by CV. kerry sun Posted 2 years ago ¬∑ 151st in this Competition arrow_drop_up 1 more_vert Congrats üòÅ Octavi Grau Posted 2 years ago ¬∑ 21st in this Competition arrow_drop_up 1 more_vert Thanks for sharing @adaubas ! Incredible detail to learn Thomas Mei√üner Posted 2 years ago ¬∑ 437th in this Competition arrow_drop_up 1 more_vert Congrats, the hill climbing striked once again. Oscar Aguilar Posted 2 years ago ¬∑ 96th in this Competition arrow_drop_up 1 more_vert Congrats @adaubas üëç Minato Namikaze Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Congratulations! Thanks for sharing the approach and glad my notebook added value :) Yan Teixeira Posted 2 years ago arrow_drop_up 1 more_vert Congrats my friend. It's always a pleasure to learn from your kernels. LEVAN KANKADZE Posted 2 years ago ¬∑ 708th in this Competition arrow_drop_up 1 more_vert Congratulations, Thank you for providing insight into your journey throughout this competition. Sarun P M Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 2 more_vert Congratulations. That was a gaint leap to the 4th LB. Thanks for your solution as well.",
      "Binary Prediction of Smoker Status using Bio-Signals | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Prediction of Smoker Status using Bio-Signals Playground Series - Season 3, Episode 24 Binary Prediction of Smoker Status using Bio-Signals Overview Data Code Models Discussion Leaderboard Rules Sarun P M ¬∑ 7th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 17 more_vert #7 Private LB and #2 Public LB solution First, I congratulate all the winners and all the participants in the Binary Prediction of Smoker Status using the Bio-Signal competition. Initially, I used the model with XGBoost and optimized with optuna over a many iterations and no feature addition ( public score: 0.87392 ) Then the pseudo label technique seen in the public notebook of @cv13j0 was added and gave an additional boost in the public score of 0.87901 (landed me in top 100). Tried higher degree of leaking of test dataset to train dataset up to 85 % [Marginal improvement in public score] Then, comes the public notebooks of @zhukovoleksiy and @arunklenin which looks almost similar. But the notebook of @arunklenin was unique because of the added tons of derived features and averaging of prediction probabilities of kaggler having top public scores ( Average Method ). Incorporating these features improves the score a bit in top 20 public score (0.88116) I also observed that the features like, hemoglobin, weight, height, Gtp, serum creatinine and dental caries are important features and added features like hemoglobin x hemoglobin, hemoglobin x height, hemoglobin x weight, weight x height, hemoglobin x Gtp, hemoglobin x serum creatinine , and many more features in combination with hemoglobin which landed me in top 2 public score (0.88126) which I improved to 0.88136 in last day by changing the SEED from 42 to 43 . ( Changing seed has this much impact !!??!! ) Finally after the results my final standing is in 7 th position with private score of 0.87926 . But, I have some past submission with lower public score have better private score than what is showing in the leaderboard. Why?? Anyway, I am happy because of the achievement and I would specially thank the authors, namely, @rukenmissonnier , @paddykb , @arunklenin , @cv13j0 , @alexryzhkov for sharing their notebook whose submission was used to making my weighed submission file. I also appreciate @ravi20076 , @oscarm524 , @yaaangzhou and @armanzhalgasbayev ,  as I learned many new ideas from them and already forked their public notebooks on the present competitions. Thank you all Please sign in to reply to this topic. comment 7 Comments Hotness Thomas Mei√üner Posted 2 years ago ¬∑ 437th in this Competition arrow_drop_up 1 more_vert Congratulations! How many iterations did you use to tune Xgboost? This comment has been deleted. Oscar Aguilar Posted 2 years ago ¬∑ 96th in this Competition arrow_drop_up 1 more_vert Congrats @sarunpm üëç Thanks for the mention! I'm glad my notebook helped a bit. Sarun P M Topic Author Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 2 more_vert Thanks @oscarm524 . I am already very much impressed with your hill-climbing approach in the previous episode (software defects). Ravi Ramakrishnan Posted 2 years ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert @sarunpm I never trust any score improvements from seed changes. Probably this fitted well to the public LB only and took you a bit downwards in the private LB. Perhaps your past submissions were better generalized so they performed better on the private LB. Your final time based submissions may not be the best of all. Sometimes choosing an erstwhile submission becomes prudent. Sarun P M Topic Author Posted 2 years ago ¬∑ 7th in this Competition arrow_drop_up 2 more_vert @ravi20076 - Thanks for your suggestion. Few of my submissions have indeed performed better in private scoring than the highest public scored ones. It is my inexperience in the area played that math. It was a nice excellent experience, and great learning as well from the mistakes. Look at the screenshots given below: 2 Public LB submission score 7 Private LB submission scores Better Performed submission score Minato Namikaze Posted 2 years ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Congratulations! @sarunpm and thanks for the mention:)",
      "Binary Prediction of Smoker Status using Bio-Signals | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Prediction of Smoker Status using Bio-Signals Playground Series - Season 3, Episode 24 Binary Prediction of Smoker Status using Bio-Signals Overview Data Code Models Discussion Leaderboard Rules Minato Namikaze ¬∑ 8th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 14 more_vert #8 Private LB #7 Public - Solution Approach Hello All! Thanks to Kaggle for this episode, the size of the dataset had resulted in a consistent Public to Private LB. I would like to thank the other participants for their contriubtions & the publicly available notebooks. Special mentions to the following individuals for their notebooks & being highly active in this episode: 1) @paddykb , his notebook has contributed in predictions of many other notebooks including mine. 2) @cv13j0 , amazing notebook for reference 3) @ravi20076 , thanks for the contributions as always and congrats again! The appraoch is based on my publicly available notebook My approach: Data Processing Thanks to @paddykb for the data pre-processing Feature Engineering 1) All features with frequency above 2 were treated as discrete and many encoding techniques were applied. The dataset is large enough to enable this approach. 2) New features are created using brute force search for arithmetic combinations of existing features based on performance ciriteria. 3) Selected the union of top N features from CatBoost, XGBoost, LightGBM where N is 50, 100. Higher values of N resulted in \"Run Time Exceed Error\" Modeling Framework 1) XGBoost, Catboost, lightGBM, Artificial Nueral Networks, Logistic Regression, & DecisionTrees are ensembled to maximize the AUC score using Optuna 2) Adding ANNs into ensembling framework was recently added and gives decent performance. 3) My predictions are again ensembled with publicly available predictions with ranks based on public LB as weights. Takeaways 1) Running my notebook took a lot of time and a many submissions are stopped because of runtime limits. 2) A lot of notebooks using predictions from other notebooks and provided weights based on trial & error approach and there are discussions if this is the right approach or not. My thought is simply to use an outcome that is produced from a differently feature engineered data to make a generelized prediction. howvever, I would suggest to use a logical way to assign weights because we only have a part of the information available in the Public LB and anything can happen in the private LB. 3) For users finding difficulty in identifying true notebooks that do not use other's predictions, one way is to look at the number of inputs which is in general the main dataset along with the original dataset in PS series. Thank you all! All the best for the next episode! I wish everyone a Happy Diwali! Please sign in to reply to this topic. comment 5 Comments 1 appreciation  comment Hotness Denis Maltsev Posted 2 years ago arrow_drop_up 1 more_vert Hello @arunklenin , congratulations on the Ranking. C4rl05/V Posted 2 years ago ¬∑ 244th in this Competition arrow_drop_up 1 more_vert Hello @arunklenin , congratulations on the Ranking. Thanks for the mention, I  immensely enjoyed the episode. aldparis Posted 2 years ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Thank you for having shared your notebook during this competition especially your LGB and your NN, and thank you to having shared these 3 takeaways. Congratulations ! Thomas Mei√üner Posted 2 years ago ¬∑ 437th in this Competition arrow_drop_up 1 more_vert Nice solution, congratulations! Appreciation (1) MinhaoLin Posted 2 years ago ¬∑ 247th in this Competition arrow_drop_up 1 more_vert Good jobÔºÅThanks for sharing.",
      "Binary Prediction of Smoker Status using Bio-Signals | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Prediction of Smoker Status using Bio-Signals Playground Series - Season 3, Episode 24 Binary Prediction of Smoker Status using Bio-Signals Overview Data Code Models Discussion Leaderboard Rules Algorex ¬∑ 35th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 3 more_vert #35 - Smoker Status Prediction | Voting and Stacking CLF For This Kaggle Competition, I implemented a sophisticated ensemble learning approach, combining the power of both Voting and Stacking classifiers. By strategically blending diverse base classifiers, I've demonstrated a keen understanding of model diversity and its impact on predictive performance, additionally i have carefully inspected the Competition Data Card to learn more about the Dataset, suprisingly the Dataset is Synthetically-Generated with the Original Dataset here : https://www.kaggle.com/datasets/gauravduttakiit/smoker-status-prediction-using-biosignals . I have took advantage of the sitaution by using the specified dataset as an additional source of data for my AI Model to train on. Behind the Scenes, i have done the extra mile by using Optuna to Finetune the parameters of various Algorithms that are present in the Prediction Model AI which are the following : AL : K-Nearest Neighbors (20%) AL : Logistic Regression (20%) AL : Random Forest (20%) AL : Extra Trees Classifier AL : Histogram Gradient Boosting Classifier AL : Light Gradient Boosting Machine AL : Extreme Gradient Boosting AL : Cat Boost Classifier After that, i collectively group the models together using Voting and Stacking Classifiers. Overall, The heart of my notebook lies in the implementation of ensemble techniques. The Voting classifier, leveraging the collective wisdom of multiple algorithms, showcases the importance of diversity in achieving robust and accurate predictions. Additionally, the Stacking classifier, with its ability to leverage the strengths of different base models. Link to the Top 35th Notebook - https://www.kaggle.com/code/daniellebagaforomeer/smoker-status-prediction-voting-and-stacking-clf Healthcare Health Conditions Health Public Health Public Safety Please sign in to reply to this topic. comment 0 Comments Hotness"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 24 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Smoker Status Prediction using Bio-Signals dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 22.79 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 22.79 MB sample_submission.csv test.csv train.csv 3 files 49 columns ",
    "data_description": "Binary Prediction of Smoker Status using Bio-Signals | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Binary Prediction of Smoker Status using Bio-Signals Playground Series - Season 3, Episode 24 Binary Prediction of Smoker Status using Bio-Signals Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2023 Kaggle Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! Your Goal: For this Episode of the Series, your task is to use binary classification to predict a patient's smoking status given information about various other health indicators. Good luck! Start Oct 24, 2023 Close Nov 14, 2023 Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each id in the test set, you must predict the probability for the target variable smoking . The file should contain a header and have the following format: id ,smoking 159256 , 0 . 5 159257 , 0 . 5 159258 , 0 . 5 etc . content_copy Timeline link keyboard_arrow_up Start Date - October 24, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  November 13, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Binary Prediction of Smoker Status using Bio-Signals. https://kaggle.com/competitions/playground-series-s3e24, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 4,958 Entrants 2,023 Participants 1,908 Teams 14,237 Submissions Tags Beginner Tabular Binary Classification Health Roc Auc Score Table of Contents collapse_all Overview Evaluation Timeline Prizes About the Tabular Playground Series Citation"
  },
  {
    "competition_slug": "playground-series-s3e25",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 25 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Prediction of Mohs Hardness with Machine Learning dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 2.11 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 2.11 MB sample_submission.csv test.csv train.csv 3 files 27 columns ",
    "data_description": "Regression with a Mohs Hardness Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Regression with a Mohs Hardness Dataset Playground Series - Season 3, Episode 25 Regression with a Mohs Hardness Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2023 Kaggle Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far! Your Goal: For this Episode of the Series, your task is to use regression to predict the Mohs hardness of a mineral, given its properties. Good luck! Start Nov 14, 2023 Close Dec 5, 2023 Evaluation link keyboard_arrow_up Submissions are scored on the Median Absolute Error (MedAE). MedAE is defined as: MedAE ( y , ÀÜ y ) = median ( | y i ‚àí ÀÜ y i | , ‚Ä¶ , | y n ‚àí ÀÜ y n | ) where ÀÜ y i is the predicted value and y i is the ground truth for each observation i . Submission File For each id row in the test set, you must predict the value for the target Hardness . The file should contain a header and have the following format: id ,Hardness 10407 , 4 . 647 10408 , 4 . 647 10409 , 4 . 647 etc . content_copy Timeline link keyboard_arrow_up Start Date - November 14, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  December 4, 2023 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Regression with a Mohs Hardness Dataset. https://kaggle.com/competitions/playground-series-s3e25, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 4,133 Entrants 1,705 Participants 1,632 Teams 12,610 Submissions Tags Beginner Tabular Regression Earth Science Median Absolute Error Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s3e26",
    "discussion_links": [
      "/competitions/playground-series-s3e26/discussion/464887",
      "/competitions/playground-series-s3e26/discussion/464863",
      "/competitions/playground-series-s3e26/discussion/464876"
    ],
    "discussion_texts": [
      "Multi-Class Prediction of Cirrhosis Outcomes | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Multi-Class Prediction of Cirrhosis Outcomes Playground Series - Season 3, Episode 26 Multi-Class Prediction of Cirrhosis Outcomes Overview Data Code Models Discussion Leaderboard Rules Hardy Xu ¬∑ 2nd in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 71 more_vert 2nd Place: with help from NNs. With this competition I set out to improve my knowledge of neural networks. I had trouble getting good results with them in past competitions, often finding they performed worse in the private leaderboard, even when added to ensembles of GBDTs. However, this time I think they were the secret sauce that helped me get 2nd place. My solution is pretty straightforward. Just a combination of XGBoost, LightGBM, and neural network predictions, with another neural network used to stack them together. The XGBoost and LightGBM predictions were nothing special, just an average of predictions from 10 different sets of hyperparameters found by Optuna. For the initial neural network predictions, after reading through a bunch of papers and iterating on the techniques described within, I found that the piecewise linear encoding (PLE) technique described in this paper greatly improved my score. I applied PLE to each continuous feature, an embedding layer for the edema and stage features, and fed in the remaining binary features as is (after converting to 0/1). The structure looks sort of like this: The actual network included some additional layers after each PLE input, before they were concatenated together, but to keep the diagram simple I did not include them. With this neural network alone I was able to get a score around .401 on the private LB, good enough for top 10%. Despite its solid individual performance, its addition to the stack only added around a .001 boost to the score, but I'm just glad it improved the stack at all. For the stacking neural network, I started with the idea of getting the network to learn a weighted average of input predictions, which looked something like this: Through some more experimentation, I found that the network achieved better scores by Giving each class probability within a prediction its own weight, rather than computing a single weight and applying it to each class probability, and Calculating each prediction's weights using all 3 predictions, rather than only itself. With these changes the network looked more like this: Again, for the sake of simplicity, there are some additional layers not included here. This stacking architecture gave a boost of .004 compared to taking a simple average. Overall, I had a lot of fun working on this competition. The 2nd place was a nice bonus, but I'm more grateful from the knowledge acquired in the process of researching and trying out neural networks. For anyone else interested in recent tabular neural network innovations, I would recommend taking a look at https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html . There are plenty of ideas using neural networks that I either wasn't able to try or wasn't able to get working for this competition, but definitely look forward to trying these ideas and more in future competitions. Happy new year! 2 Please sign in to reply to this topic. comment 36 Comments 2 appreciation  comments Hotness Benz Nguyen Posted 2 years ago ¬∑ 154th in this Competition arrow_drop_up 9 more_vert Thanks for sharing, that's helpful knowledge! I have a question, why did you only use XGBoost and LightGBM as input for NNs? Hardy Xu Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 8 more_vert If you're talking about the stacking neural network, that used XGBoost, LightGBM, and Neural Network predictions. If you're asking why I didn't use the data features or predictions from other models like Random Forest or Logistic Regression in the stacking neural network, I tried all of those things and they did not improve my CV score. Benz Nguyen Posted 2 years ago ¬∑ 154th in this Competition arrow_drop_up 4 more_vert I got it, thank you so much :) lazy_panda Posted 2 years ago ¬∑ 22nd in this Competition arrow_drop_up 5 more_vert Very impressive solution! I haven't tried NN in this competition but good to see it works. Any chance you can share a notebook to your solution? Congratulations and Thank you üéâ Hardy Xu Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 0 more_vert Thank you! No notebook unfortunately, all my work was done locally. This comment has been deleted. MrSimple Posted 2 years ago arrow_drop_up 3 more_vert Congratulations! I really like the solution! Good job! ML_Magic Posted 2 years ago ¬∑ 703rd in this Competition arrow_drop_up 3 more_vert Congratulations.. wish you a happy new year Jonathan Kao Posted 2 years ago ¬∑ 19th in this Competition arrow_drop_up 3 more_vert Thank you for the interesting solution overview and the links! The diagrams and explanations combined with the log-loss improvement numbers were very helpful in understanding your approach. Hardy Xu Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Thank you, I'm glad you found them helpful! Jonathan Kao Posted 2 years ago ¬∑ 19th in this Competition arrow_drop_up 2 more_vert Actually I did have one question if you don't mind, how much feature engineering did you use in your solution and how helpful was it to your score if you did use it? Hardy Xu Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert For the gbt models I rounded the values of some features, which seemed to help a small amount. Nothing else I tried for feature engineering seemed to help. cattie Posted 2 years ago ¬∑ 335th in this Competition arrow_drop_up 4 more_vert Congratulations, @hardyxu52 ! I was really impressed by your work. I have one question though: I'm struggling to understand the first Neural Network Architecture. How can the input shape be 302, 288, 44, 150, etc.? Hardy Xu Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 3 more_vert Thank you! Each of those inputs are from PLE applied to one continuous feature. In the paper the authors kept the number of bins for the PLE constant but for this competition, I found it more effective to use more bins for features with more unique values. Shum14 Posted 2 years ago ¬∑ 733rd in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for your sharing the knowledge and helpful explanation. Malabh Bakshi Posted 2 years ago arrow_drop_up 1 more_vert Good Solution!!! @hardyxu52 Shahbaz Khan Posted 2 years ago arrow_drop_up 1 more_vert Congratulations, @hardyxu52 ! Quite impressive, I got to learn about On Embeddings through your post! Thanks for sharing the paper link, seems like an interesting read. tcashion Posted 2 years ago ¬∑ 356th in this Competition arrow_drop_up 1 more_vert Very cool! Thanks for sharing your solution including the diagrams to see how it all fits together. Congratulations on the ü•à H.JIMBEAN Posted 2 years ago ¬∑ 251st in this Competition arrow_drop_up 1 more_vert Brilliant! Congrats! Thank for sharing. I want to know if there is anything wrong with my understanding. You use two neural networks, the first one predicts the task, and the second one stacking the prediction results? I feel a little confuse about the label of the stacking network, is that use the label of the training data? Luficer G Posted 2 years ago ¬∑ 39th in this Competition arrow_drop_up 2 more_vert bro I just missed to use NN , but instead used CatBoost in my final model, great to see so similar yet far, Great work @hardyxu52 , btw which Kaggle merchandize  , u want now üòúüòú AaryanSingh729 Posted 2 years ago ¬∑ 314th in this Competition arrow_drop_up 2 more_vert Congratulations on the placement!! I have a very basic understanding on neural networks and ensemble models with them, I could understand the diagrams you shared slightly. Could you recommend me some sources thorough which I can start learning neural networks on tabular data. Thanks a lot and congrats again. Yan Teixeira Posted 2 years ago ¬∑ 20th in this Competition arrow_drop_up 2 more_vert Very nice wrap up. Congrats! May I ask asking what program you used to build these diagrams? Hardy Xu Topic Author Posted 2 years ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Thanks! The diagrams are made using keras's plot_model Terolig Posted 2 years ago ¬∑ 904th in this Competition arrow_drop_up 2 more_vert Awesome work, thanks for sharing! And thanks for the link to Sebastian Raschka's blog post! Hriday Posted 2 years ago ¬∑ 321st in this Competition arrow_drop_up 2 more_vert Thanks for the solution. It was really interesting to read, and more importantly- Congratulations! neolivion Posted a year ago arrow_drop_up 0 more_vert Congratulations , great code! Lukman Aliyu Posted a year ago arrow_drop_up 0 more_vert Awesome. Thank you for sharing. Really helpful. Jianwei Gao Posted a year ago ¬∑ 233rd in this Competition arrow_drop_up 0 more_vert Congratulations! Can you provide a more detailed description of ‚Äúan embedding layer for the edema and stage features‚ÄùÔºåwhat are the specific results after ‚Äúembedding‚Äù and ‚Äúflatten‚ÄùÔºüThank you very much„ÄÇ Amadou Djoulde Barry Posted a year ago arrow_drop_up 0 more_vert I know it's seems easy here for some, but is it possible to see the code implementations ? emoji_people AnubhavMaverick Posted a year ago ¬∑ 617th in this Competition arrow_drop_up 0 more_vert Wow, thanks for sharing this architecture xxxZayH Posted a year ago arrow_drop_up 0 more_vert `class StackingModel(nn.Module): def init (self, input_size, output_size): super(StackingModel, self). init () self.input_size = input_size self.output_size = output_size self .batch_norm = nn.BatchNorm1d( self .input_size) self .dense = nn.Linear( self .input_size* 3 , self .output_size) self .activation = nn.ReLU() def forward ( self, X1, X2, X3 ): # BN x1 = self .batch_norm(X1)\n    x2 = self .batch_norm(X2)\n    x3 = self .batch_norm(X3) # concatenate x = torch.cat((x1, x2, x3), dim= 1 ) #dense, BN, activation x1 = self .activation( self .batch_norm( self .dense(x)))\n    x2 = self .activation( self .batch_norm( self .dense(x)))\n    x3 = self .activation( self .batch_norm( self .dense(x))) #multiply x1 = torch.mul(x1, X1)\n    x2 = torch.mul(x2, X2)\n    x3 = torch.mul(x3, X3) # add out = x1 + x2 + x3 return out` content_copy I'm not sure if this code understands the meaning of your stacking model. If you could take a look, I would greatly appreciate it Kamal Budhathoki Posted a year ago arrow_drop_up 0 more_vert Congrats!!! Kari Posted a year ago ¬∑ 54th in this Competition arrow_drop_up 0 more_vert Congrats on the second place! Learned a lot from your solution. I'll definitely have to try PLE in the future. This comment has been deleted.",
      "Multi-Class Prediction of Cirrhosis Outcomes | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Multi-Class Prediction of Cirrhosis Outcomes Playground Series - Season 3, Episode 26 Multi-Class Prediction of Cirrhosis Outcomes Overview Data Code Models Discussion Leaderboard Rules Kirderf ¬∑ 4th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 25 more_vert 4th place solution - stacking approach with XGB as meta model. Thanks for the PS3 2023 series, always a fun and good area for testing and experiment new models and ideas, benchmark against the old ones! It started well and ending the same with a 1st in PS3E1 and 4th place in this one, the common for all during the series are the knowledge one gets, valuable knowledge and insights. The solution As hinted in this post https://www.kaggle.com/competitions/playground-series-s3e26/discussion/461062 XGB works well in classification, and that was the base idea. The key was using a stacking approach with OOF from many other different top solutions and use XGB as meta model for training the stacked predictions + orginal features. And also try not to tune and optimize every solution to much, reduce the risk of overfitting, instead use the predictions as extra features in the final stacking training. Many solutions shared the same small added feature engineering as below, rest was unchanged in terms of FE. train_df['Age_Group'] = pd.cut(train_df['Age'], bins=[9000, 15000, 20000, 25000, 30000], labels=['A', 'B', 'C', 'D'],) train_df['Log_Age'] = np.log1p(train_df['Age']) scaler = MinMaxScaler() train_df['Scaled_Age'] = scaler.fit_transform(train_df['Age'].values.reshape(-1, 1)) The different solutions and frameworks trained AutoGluon AutoGluon latest pre-release 1.0.1b20231208 with zero-shot HPO as default. The trained framework used below weighted ensemble of models. I also tried distillation and pseudo labeling but didn‚Äôt work better. 0    WeightedEnsemble_L2 -0.436142   log_loss    7.002513    505.384119  0.004281    9.923933    2   True    9 1    XGBoost_r89_BAG_L1  -0.441355   log_loss    0.470275    19.330144   0.470275    19.330144   1   True    6 2    CatBoost_r137_BAG_L1    -0.441618   log_loss    0.138598    115.856482  0.138598    115.856482  1   True    4 3    CatBoost_r50_BAG_L1 -0.442764   log_loss    0.266119    78.536804   0.266119    78.536804   1   True    8 4    LightGBM_r130_BAG_L1    -0.443168   log_loss    1.180046    49.676369   1.180046    49.676369   1   True    7 5    XGBoost_r33_BAG_L1  -0.451191   log_loss    3.457726    68.160608   3.457726    68.160608   1   True    3 6    RandomForestEntr_BAG_L1 -0.475263   log_loss    0.428620    3.049245    0.428620    3.049245    1   True    1 7    NeuralNetTorch_r79_BAG_L1   -0.482747   log_loss    0.301830    66.300480   0.301830    66.300480   1   True    2 8    NeuralNetFastAI_r145_BAG_L1 -0.489281   log_loss    0.755018    94.550054   0.755018    94.550054   1   True    5 LightAutoML Trained the LightAutoML 0.3.8b1 version and the trained frameworked used below weighted model ensemble. [16:19:15] Model description: Final prediction for new objects (level 0) = 0.06558 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LightGBM) + 0.17057 * (5 averaged models Lvl_0_Pipe_0_Mod_1_Tuned_LightGBM) + 0.27900 * (5 averaged models Lvl_0_Pipe_0_Mod_2_CatBoost) + 0.48485 * (5 averaged models Lvl_0_Pipe_0_Mod_3_Tuned_CatBoost) [16:19:15] ================================================== [16:19:15] Blending: optimization starts with equal weights and score -0.4164338072355177 AutoXGB Trained AutoXGB 5 fold CV with standard settings but with the extra features. Public notebook https://www.kaggle.com/code/mattop/ps-s3-e26-lgbm-xgb-preprocessing-fasteda Re-trained the notebook and saved the OOF for the XGB and LGBM. Noticed a good local CV that could be good to use in the stacking and also as diverse model for the owned created. Other trained solutions Trained several other SOTA solutions and ideas, like using old top solutions from old competitions multiclass and binary as well, train every class separately and use the predictions as extra features. It worked well but the four solutions below was enough looking in the mirror. Stacking I then trained the final 20 fold XGB model as meta model with the original features, the extra created and OOF predictions, both the CV score and the leaderboard went better. I also trained a 2 level stacking approach with several of tested stacked XGB models, as an approach instead of ensemble for the second submission. That‚Äôs it! Happy Kaggeling! Please sign in to reply to this topic. comment 9 Comments Hotness Nembot Jules Posted 2 years ago ¬∑ 55th in this Competition arrow_drop_up 0 more_vert Congrats! can't wait to see your solution Amadou Djoulde Barry Posted a year ago arrow_drop_up 0 more_vert how can you see someone solution ? lazy_panda Posted 2 years ago ¬∑ 22nd in this Competition arrow_drop_up 0 more_vert Congratulations on your excellent finish! Looking forward to your solution notebook to see and learn what models worked well for you. Thank you.ü•≥ Ph·∫°m Ng·ªçc Thi√™n √Çn Posted 2 years ago ¬∑ 120th in this Competition arrow_drop_up 0 more_vert Congratulations! Cannot wait to see your solution Brandon Young Posted 2 years ago ¬∑ 739th in this Competition arrow_drop_up 0 more_vert congrats!, cant wait to check it out Chinmaya Posted 2 years ago ¬∑ 281st in this Competition arrow_drop_up 0 more_vert Congratulations. Waiting for your solution. mineweenie Posted 2 years ago ¬∑ 66th in this Competition arrow_drop_up 0 more_vert Congratulations. Waiting for your solution. Yan Teixeira Posted 2 years ago ¬∑ 20th in this Competition arrow_drop_up 0 more_vert Congrats! Hope to see your code. emoji_people Omar Refaie Posted 2 years ago ¬∑ 253rd in this Competition arrow_drop_up 0 more_vert Congratulations brother",
      "Multi-Class Prediction of Cirrhosis Outcomes | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Multi-Class Prediction of Cirrhosis Outcomes Playground Series - Season 3, Episode 26 Multi-Class Prediction of Cirrhosis Outcomes Overview Data Code Models Discussion Leaderboard Rules Luficer G ¬∑ 39th in this Competition  ¬∑ Posted 2 years ago arrow_drop_up 8 more_vert (Rank 39 Upto 250 points added) Why there is so significant difference between private and public scores??? Sup guys and girls, I very excited to share with you , that my rank jumped to 250 points, 39th rank in this competition ü•≥ü•≥ü•≥, here is my solution for this competition , https://www.kaggle.com/code/luficergfree/it-s-me-luficer-g Although I wonder how my rank got so much above than previous may be  because there is a lot of difference in the private and public scores .. if any one has answer plz. reply.. Thank u guys and girls Please sign in to reply to this topic. comment 7 Comments Hotness kailai Posted 2 years ago ¬∑ 1st in this Competition arrow_drop_up 4 more_vert Public dataset has more easy samples than private one. If putting too much weight on easy samples , balance between easy samples and hard samples ruins. Luficer G Topic Author Posted 2 years ago ¬∑ 39th in this Competition arrow_drop_up 3 more_vert that's how you came first ???üòÇüòÇ COngra. man , you are only one who maintained his rank in both public and private set , I am interested to see ur solution also nandomartinez Posted 2 years ago ¬∑ 584th in this Competition arrow_drop_up 0 more_vert In your opinion, how 'penalize' the easy samples, @kailai ? lazy_panda Posted 2 years ago ¬∑ 22nd in this Competition arrow_drop_up 1 more_vert Congratulations on your awesome finish! Luficer G Topic Author Posted 2 years ago ¬∑ 39th in this Competition arrow_drop_up 0 more_vert thanks @byteliberator , you did great too, didn't had model overfitted. Nice job Yan Teixeira Posted 2 years ago ¬∑ 20th in this Competition arrow_drop_up 2 more_vert \"This leaderboard is calculated with approximately 20% of the test data. The final results will be based on the other 80%, so the final standings may be different.\" Luficer G Topic Author Posted 2 years ago ¬∑ 39th in this Competition arrow_drop_up 1 more_vert True words @yantxx .. congratulations for 20th rank üòÅüòÅ"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 3, Episode 26 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Cirrhosis Patient Survival Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 1.39 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 1.39 MB sample_submission.csv test.csv train.csv 3 files 43 columns ",
    "data_description": "Multi-Class Prediction of Cirrhosis Outcomes | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 years ago Late Submission more_horiz Multi-Class Prediction of Cirrhosis Outcomes Playground Series - Season 3, Episode 26 Multi-Class Prediction of Cirrhosis Outcomes Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2023 Kaggle Playground Series! Thank you to everyone who participated in and contributed to Season 3 Playground Series so far. This is our last episode for the Season 3 and we wish you all a Happy New Year! Stay tuned for the new season next year! Your Goal: For this Episode of the Series, your task is to use a multi-class approach to predict the the outcomes of patients with cirrhosis. Good luck! Start Dec 5, 2023 Close Jan 2, 2024 Evaluation link keyboard_arrow_up Submissions are evaluated using the multi-class logarithmic loss. Each id in the test set had a single true class label, Status . For each id , you must submit a set of predicted probabilities for each of the three possible outcomes, e.g., Status_C , Status_CL , and Status_D . The metric is calculated l o g l o s s = ‚àí 1 N N ‚àë i = 1 M ‚àë j = 1 y i j log ( p i j ) , where N is the number of rows¬†in the test set, M is the number of outcomes (i.e., 3), l o g is the natural logarithm, y i j is 1 if row i has the ground truth label j and 0 otherwise, and p i j is the predicted probability that observation i belongs to class j . The submitted probabilities for a given row¬†are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with m a x ( m i n ( p , 1 ‚àí 10 ‚àí 15 ) , 10 ‚àí 15 ) . Submission File For each id row in the test set, you must predict probabilities of the three outcomes Status_C , Status_CL , and Status_D . The file should contain a header and have the following format: id ,Status_C,Status_CL,Status_D 7905 , 0 . 628084 , 0 . 034788 , 0 . 337128 7906 , 0 . 628084 , 0 . 034788 , 0 . 337128 7907 , 0 . 628084 , 0 . 034788 , 0 . 337128 etc . content_copy Timeline link keyboard_arrow_up Start Date - December 5, 2023 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  January 1, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Multi-Class Prediction of Cirrhosis Outcomes. https://kaggle.com/competitions/playground-series-s3e26, 2023. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 4,627 Entrants 1,718 Participants 1,661 Teams 15,115 Submissions Tags Beginner Time Series Analysis Tabular Multiclass Classification Log Loss Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s4e1",
    "discussion_links": [
      "/competitions/playground-series-s4e1/discussion/472496",
      "/competitions/playground-series-s4e1/discussion/472413",
      "/competitions/playground-series-s4e1/discussion/472636"
    ],
    "discussion_texts": [
      "Binary Classification with a Bank Churn Dataset  | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Binary Classification with a Bank Churn Dataset Playground Series - Season 4, Episode 1 Binary Classification with a Bank Churn Dataset Overview Data Code Models Discussion Leaderboard Rules lukaszl ¬∑ 2nd in this Competition  ¬∑ Posted a year ago arrow_drop_up 82 more_vert 2nd place solution Hello everyone. My solution may be a ray of hope for those who don't always have time for many experiments ;) The Idea The main idea (or if you prefer, the only one ;)) behind my solution is to use the 'syntheticity of the dataset' and mostly ignore the topic of the competition. The first step was to automatically generate all 1,2,3‚Ä¶10 element subsets from the set: ['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember' ','EstimatedSalary','Balance']. Then, for each such subset, checking whether it appears unchanged in the original dataset. If so, the feature received a value of 1, if not 0. Example (pseudocode): subset = [ 'CreditScore' , 'Geography' , 'Balance' ]\nx1 = 1 if subset in original_dataset else 0 content_copy Why ? why so? I noticed that the target distribution on the features constructed in this way is completely different than the basic 0.21 for the entire dataset. Using the above feature as an example: df_train.groupby( 'x1' ).Exited.mean() gives the result: False 0.171026 True 0.269552 or for [ 'CreditScore' , 'Gender' , 'NumOfProducts' ]: False 0.203034 True 0.707857 content_copy I also added subset [CustomerId, Surname] to this set of ~1000 features. There was actually nothing else that needed to be done :) Models The first model was: lgbm : 50 selected features (including basic features), without excessive hyperparameters tuning, on private leaderboard it got 0.90203 autogluon : all features included, presets='best_quality', time_limit=14400, on private leaderboard it got  0.90378 simple ensamble : (lgbm+autogluon)/2, on private leaderboard it got my final - 0.90462 In the latest submission some of the publicly available features were included, but they didn't change much. Of course, I also added the @paddykb \"Feeling lucky trick\" :), in my case it's +0.01 to the result, a little bit less then 0.02, this is probably due to the features that partially explained this 'strange' behavior of the dataset. and that's it :) Thank you everyone who has participated in this competition and remember, if you are asked to predict churn in real life, do not ask the client for the original dataset from which yours was generated :) Please sign in to reply to this topic. comment 40 Comments 6 appreciation  comments Hotness Nick Erickson Posted a year ago arrow_drop_up 11 more_vert Congrats on the solution! As the lead developer of AutoGluon, I'm really happy to see it being used so widely in this competition and to such great success! Cheers! lukaszl Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert I've only scratched the surface of your library, but I must say that from what I've seen.. great job :) Iqbal Syah Akbar Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 4 more_vert Thanks for sharing! So from what I understand here, the trick is basically a (somewhat) generalization of leakage trick into all subsets of features instead of just one full set of features. lukaszl Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 5 more_vert I think it's a bit like that. When I thought about it, I treated it as a 'problem' in the work of the algorithm generating the synthetic dataset. It could not maintain the same target distribution across so many levels of feature combinations. It seems to me that this approach may turn out to be very generic and in the case of access to the original dataset in future competitions, it may also give very good results with a relatively low effort :) Craig Thomas Posted a year ago ¬∑ 252nd in this Competition arrow_drop_up 0 more_vert @lukaszl nice! Harshit Sharma Posted a year ago ¬∑ 305th in this Competition arrow_drop_up 1 more_vert Your innovative approach is greatly appreciated. Congratulations on your remarkable achievement @lukaszl ! Thermostatic Posted a year ago ¬∑ 2841st in this Competition arrow_drop_up 1 more_vert Funny but good solution haha :) Hikmat Ullah Posted a year ago ¬∑ 3301st in this Competition arrow_drop_up 1 more_vert Thank You! @lukaszl for your tricky solution. but sorry to say, I haven't got your point here \"Remember, if you are asked to predict churn in real life, do not ask the client for the original dataset from which yours was generated\" means why so that? lukaszl Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert it's a joke :) my solution is based on the availability of the original dataset and the fact that the training and test datasets are synthetic. In real life, you simply cannot use this solution, because there will be no 'original dataset'. Hikmat Ullah Posted a year ago ¬∑ 3301st in this Competition arrow_drop_up 0 more_vert ok, Thanks! llorenzo Posted a year ago ¬∑ 233rd in this Competition arrow_drop_up 1 more_vert Thanks for sharing this brilliant idea. I wonder how you select ~50 features from the potential ~1024 combinations of features. lukaszl Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 3 more_vert Good question. For LGBM I wrote a simple genetic algorithm and let it run for a few hours and it chose them for me :) In the autogluon part I put everything inside and the autogluon decided what was important. Jordi Rosell Posted a year ago ¬∑ 287th in this Competition arrow_drop_up 2 more_vert \"if you are asked to predict churn in real life, do not ask the client for the original dataset from which yours was generated\" Thanks!! kailai Posted a year ago ¬∑ 5th in this Competition arrow_drop_up 2 more_vert Brilliant! Thanks and congratulation! lukaszl Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Big thank You :) jiyuebo Posted 10 months ago arrow_drop_up 0 more_vert Thanks for sharing! But I can not get your point. I'm confused, What is \"original dataset.\"? And the \"subset\"  is just the List like data structure like: subset = ['CreditScore', 'Geography', 'Balance']? I hope to get your guidance! muhd sheikh Posted a year ago arrow_drop_up 0 more_vert wow great job Jordi Rosell Posted a year ago ¬∑ 287th in this Competition arrow_drop_up 0 more_vert Please, can you share what parameters have you used to tune your lgbm model? Viktor Posted a year ago arrow_drop_up 0 more_vert it was helpful Kyle Walker Posted a year ago arrow_drop_up 0 more_vert Thanks for sharing your idea and congrats on your winning! nasan Posted a year ago ¬∑ 2742nd in this Competition arrow_drop_up 0 more_vert This is a innovative solution, congratulations! afsar701 Posted a year ago arrow_drop_up 0 more_vert sorry to say, but didnt get it at all will any body please explain me what lukaszl is trying to convey? Rajavel KS Posted a year ago ¬∑ 3407th in this Competition arrow_drop_up 0 more_vert That easy. Wow. This is an example where wisdom trumps intelligence. Harald Brinck Posted a year ago ¬∑ 1067th in this Competition arrow_drop_up 0 more_vert Congratulations, I could have worked on this competition for the rest of my life and had never thought of a solution like this. I hope I will understand why it works within my lifetime though. ü§™ So you have some knowledge on how the ‚Äúsynthetic data generator‚Äù works? It only uses field values from the original dataset? Then shuffles them to create new rows? Ok, but then what, it can't use random values for ‚ÄúExited‚Äù. ü§î Jarko (Manuel) Posted a year ago ¬∑ 3543rd in this Competition arrow_drop_up 0 more_vert Amazing, congratulations nevroHelios Posted a year ago ¬∑ 1877th in this Competition arrow_drop_up 0 more_vert Thanks‚Ä¶ This helped in a lot in understanding the process‚Ä¶ Nepal Singh Posted a year ago ¬∑ 3506th in this Competition arrow_drop_up 0 more_vert Congratulation @lukaszl for securing the top stop, very encouraging Chagin Posted a year ago ¬∑ 241st in this Competition arrow_drop_up 0 more_vert Congratulations! Very interesting way to handle the problem, Thank you. tdoh86 Posted a year ago ¬∑ 12th in this Competition arrow_drop_up 0 more_vert Clever feature engineering idea, appreciated. Bobo Jamson Posted a year ago ¬∑ 1129th in this Competition arrow_drop_up 0 more_vert Great work and clever solution! Congrats on the result and thanks for sharing. LuminousC Posted a year ago ¬∑ 79th in this Competition arrow_drop_up 0 more_vert Congratulations and thanks for sharing the innovation in the solution üëç Thiago Mantuani Posted a year ago arrow_drop_up 0 more_vert Congratulations, about autogluon, did you use all the models or did you exclude any from the list? lukaszl Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 0 more_vert I haven't excluded anything",
      "Binary Classification with a Bank Churn Dataset  | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Binary Classification with a Bank Churn Dataset Playground Series - Season 4, Episode 1 Binary Classification with a Bank Churn Dataset Overview Data Code Models Discussion Leaderboard Rules Iqbal Syah Akbar ¬∑ 3rd in this Competition  ¬∑ Posted a year ago arrow_drop_up 98 more_vert 3rd Place Solution: CatBoost Encoding Galore Welp, I have finally reached top 3 in Playground Series after 11 months. I never imagined that my first competition of this year would end in such an insane way. After all, getting 3rd place out of 3600 teams is something that's on much higher level than what I've achieved last year. So without further ado, here's what I've done. Feature Engineering I applied TF-IDF vectorization on some features and then decompose the result with TruncatedSVD (credit to @arunklenin ). However, unlike most people, I tried to build a class so I can implement it within a pipeline and do different type of vectorizations and decompositions on different set of features. I created new features based on this notebook by @aspillai . However, scaling and IsSenior features aren't included. I also modified Sun_Geo_Gend_Sal for my own purposes. I also will refer to it as AllCat from now on. I created a new feature named ZeroBalance as indicator whether a customer has zero balance or not. Finally, I casted both EstimatedSalary and Age as integer by multiplying them by 100 and 10 respectively first. Why? You will see it the main reason soon. However, one funny side effect is that just multiplying the Age by 10 will give you a boost when paired with the code for binning from @aspillai notebook (it's equivalent of binning Age by dividing it by only 2). Encoding Before we're getting into ensembling, I want to talk about encoding first, which is the key to getting high performance. There are three types of encoders I used in this competition: CatBoost's built-in encoder, CatBoost Encoder from category-encoders library, and M-Estimate Encoder. The first one is obvious why, but the second one is because I also wanted to use apply CatBoost encoding on other estimators. As for the third, it's because XGBoost and LightGBM doesn't like too much CatBoost encoding. I also won't explain much about it as it's not the main contributor to the high performance. Now, let's get into what features I had encoded. Actually, let me rephrase the sentence. Let's get into what features I did not encode . In the original set of features, there are only Balance and HasCrCard as the unencoded features. The rest? Pretty much all encoded. This includes float features such as EstimatedSalary and Age , and now you know why I casted them as integer. Also, do you remember that I referred to one of feature engineering as AllCat ? That's because I concatted almost all features I planned to encode in that feature, with exception of IsActiveMember because I also encoded IsActive_by_CreditCard . In total, there are 12 features I had encoded‚Ä¶ or so you thought. Remember TF-IDF vectorization and SVD decomposition? Well, you can encode them too! Just do the same thing as what I had done to EstimatedSalary and Age to the decomposition result. As a note, I only did encoding on SVD decomposition of Surname with 4 components, even though I also did vectorization and decomposition on AllCat and some other features. Another important thing about the encoding here is that, CatBoost encoding actually cares about the order of your dataset so much. Well, not by default but, you can set it as such. In fact, category-encoders CatBoostEncoder treats different orders of data differently. In order for CatBoost to disallow permutation of dataset when encoding features, you have to set has_time parameter to True . And the best order of the dataset? When concatting the original dataset and the competition training dataset, you have to put original dataset before the competition dataset. This will give you the best result for this competition. Ensembling I used 7 models on this competition. Logistic Regression is the lowest performing model but also the greatest one for experimenting which features you need to encode with category-encoders CatBoostEncoder. In a way, this can function as indicator on which features you need to encode for CatBoost. For Neural Network, I used Input -> 32 LeakyReLU -> 64 LeakyReLU -> 16 LeakyReLU -> 4 LeakyReLU -> 1 Sigmoid architecture, with AdamW optimizers. It has the same encoding as both Logistic Regression. Also, this is the only model where I didn't apply any vectorization. XGBoost has both CatBoost Encoder and M-Estimate Encoder for different set of features, and I used Optuna for HPO. Vectorization is also applied to 4 features with 500 max features and 3 decomposition components. LightGBM is somewhat similar to XGBoost when it comes to pre-processing. 3 CatBoost with different bootstrap type each: no bootstrap, Bayesian, and Bernoulli, with same exact preprocessing: vectorization on 2 features with 1000 max features and 4 decomposition components as one of them. All of them have +0.902 CV score. I didn't do any HPO because they're already as slow as snail. The weights are defined with Ridge Classifier. And if you're curious, I implemented all preprocessing within each model pipelines because I'm a freak when it comes to leakage preventation in cross-validation. Additional Note One way to boost the score from what I have noticed is to increase the number of folds. Therefore, I used 5-folds for experimentation and 30-folds for submission, which almost took 12 hours. One thing I wished to discover and do earlier was to do multiple concatenation on the same original dataset, as concatenating twice actually gave me the best private LB score (maybe this is the black magic that the top 2 had done idk). I also applied postprocessing related to the data leakage by @paddykb . Finally, you can read my notebook here . Thank you everyone who has participated in this competition. I hope you will learn a lot from this write-up. Also, I will try to participate in the discussion next time as I have no more reason to stay silent after getting the prize (unless I have a very big chance on reaching 1st) :) Please sign in to reply to this topic. comment 48 Comments 4 appreciation  comments Hotness paddykb Posted a year ago ¬∑ 152nd in this Competition arrow_drop_up 5 more_vert Congratulations! class TensorFlower ^-- I will be calling my nn models flowers üåª and gbms boosted blossom from now on :) Thiago Mantuani Posted a year ago arrow_drop_up 5 more_vert Congratulations on the placement, and thanks for sharing the solution. One question, why do xgboost and lightgbm not like the catboost encoder? Iqbal Syah Akbar Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 5 more_vert @thiagomantuani Specifically category-encoders' CatBoost Encoder, because CatBoost itself doesn't like it when that encoder encode too many features, unlike its built-in encoder. As for the reason, probably because of how category-encoders' CatBoost Encoder tend to cause underfitting when applied to categorical features with low unique values (say <100 unique values) in most cases. This competition is honestly a bizarre exception because I can use CatBoost Encoder on as many features as possible, like how I use Logistic Regression as experiment subject for this. Aravind Pillai Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 3 more_vert Superb! Congrats @iqbalsyahakbar . Thanks for sharing. This competition was a good learning experience for me. lazy_panda Posted a year ago ¬∑ 242nd in this Competition arrow_drop_up 3 more_vert Congratulations @iqbalsyahakbar on your stellar achievement in this competition. Your post is quite informative, taking a few learnings from here for sure. All the best! Galdir Reges Posted a year ago ¬∑ 235th in this Competition arrow_drop_up 3 more_vert Congratulations! Thomas Mei√üner Posted a year ago ¬∑ 460th in this Competition arrow_drop_up 4 more_vert Congratulations! Fantastic work! Akira Posted a year ago ¬∑ 237th in this Competition arrow_drop_up 4 more_vert congrats! I'm curious why increasing the number of folds can boost the score. Do you have any ideas? Iqbal Syah Akbar Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 7 more_vert @stellarhymia It only applies to big dataset like this honestly but, you can think of it as taking 30 different subsets of a dataset and then your ensemble treat each subset differently, as if you have artificially increased your dataset size even if you average the result from all the subsets in the end. It acts as last resort for me though when I have no more idea on how to improve the CV score. Akira Posted a year ago ¬∑ 237th in this Competition arrow_drop_up 3 more_vert Wow, thank you very much. Maybe it's also a good way to use validate sets and early stopping in training. Chagin Posted a year ago ¬∑ 241st in this Competition arrow_drop_up 1 more_vert Congratulations! Oh man, 30 folds and i thought i was on the wild side for doing 10 for CatBoost HPO. By the way, I really enjoy reading your solution, very informative and I can feel your dedication seeping through the screen. Thank you! Harshit Sharma Posted a year ago ¬∑ 305th in this Competition arrow_drop_up 1 more_vert Congratulations on your phenomenal achievement @iqbalsyahakbar ! Showcasing your dedication and expertise, well-deserved success after 11 months of hard work. Sheikh Muhammad Abdullah Posted a year ago ¬∑ 271st in this Competition arrow_drop_up 1 more_vert Congratulations . and Good Effort bro . Keep the Good Work Up . Thiago Lima Santos Posted a year ago ¬∑ 104th in this Competition arrow_drop_up 1 more_vert Congrats! Great notebook, I have a lot to learn with it Nat Apel Posted a year ago ¬∑ 2627th in this Competition arrow_drop_up 1 more_vert Congrats and thank you for sharing code with us! I have one question - have you check feature importance upon the model have been taught? just interesting which features finally effected result. As for me, for example, I initially dropped Surname, as I thought that it couldnot effect churn task. Your experience are interesting! Iqbal Syah Akbar Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert @natapelysynka I did early on, and I don't think there is a feature with negative permutation importance. I also tried to see the impact of dropping feature with lowest importance (I forgot what it was) and the result was negative performance. That's why I don't drop anything. Also, this competition uses synthetic dataset. Domain knowledge doesn't matter much. Nat Apel Posted a year ago ¬∑ 2627th in this Competition arrow_drop_up 0 more_vert Thanks for reply! very interesting! Good luck to you! MrSimple Posted a year ago arrow_drop_up 1 more_vert Congratulations! Great achievement! Thanks for sharing! Jordi Rosell Posted a year ago ¬∑ 287th in this Competition arrow_drop_up 1 more_vert Thanks for sharing. I also have seen that the number of folds have great impact but I used only 10 because I lost patience. I hope your coding style with sklean pipelines will become standard :) Iqbal Syah Akbar Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert @jordirosell and also @gunhyunjeong Protip: You can use Kaggle Notebook and instead of running the draft, do Save Version so you can sleep while the notebook is doing its job. Just make sure that you've calculated the runtime so it won't hit the 12 hours limit. Jordi Rosell Posted a year ago ¬∑ 287th in this Competition arrow_drop_up 0 more_vert Is there a way save objects of a notebook and explore results later from another notebook? For example, multiple encodings, multiple hyperparameters, mutiple feature extractions, etc I mostly work locally because of that. Iqbal Syah Akbar Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert I think you can use Pickle for that? I haven't used it at all though, so I can't give you opinion about it. @jordirossell winmean Posted a year ago arrow_drop_up 0 more_vert maybe you can copy several backup to finish  the task enizzzz Posted a year ago ¬∑ 2802nd in this Competition arrow_drop_up 1 more_vert Congratulations! Very detailed explanation! Thanks for sharing! mirko ferretti Posted a year ago ¬∑ 591st in this Competition arrow_drop_up 2 more_vert Congratulations! And thanks for the notebook. As a beginner, studying notebooks of people such as yourself helps a lot! lukaszl Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Congratulations! Thanks for sharing, great notebook, great solution description :) GunHyun Jeong Posted a year ago ¬∑ 35th in this Competition arrow_drop_up 2 more_vert Congrats! @iqbalsyahakbar Wow, 12 hours for 30 folds. I went up to 20 and thought that was crazy haha Thank you for sharing your ins and outs! Lots to learn from Goutham Posted a year ago ¬∑ 2650th in this Competition arrow_drop_up 2 more_vert Congrats! Khemkaran Sevta Posted a year ago ¬∑ 117th in this Competition arrow_drop_up 2 more_vert congratulations @iqbalsyahakbar . great work. Thanks for sharing this !! Ubaydullo Asatullaev Posted a year ago ¬∑ 50th in this Competition arrow_drop_up 2 more_vert Congratulations !! Bobo Jamson Posted a year ago ¬∑ 1129th in this Competition arrow_drop_up 2 more_vert Congrats! Great work and thanks for sharing your write up and code. There's much to learn from this, and I think it's insightful that you shared a few things that didn't work or were nuanced. Congrats again! Priti P Posted a year ago ¬∑ 2331st in this Competition arrow_drop_up 2 more_vert Congratulations!!!! Thank you for the great insight! As a beginner, I believe I have lot more to learn. Thank you! Shane Simon Posted a year ago arrow_drop_up 2 more_vert Great work and so awesome to get some Kaggle merch! INDHRA KIRANU Posted a year ago ¬∑ 1485th in this Competition arrow_drop_up 2 more_vert The approach of using 7 models and ensembling them to get better results is really a wow factor and tells that there is always a possibility for the results improvement",
      "Binary Classification with a Bank Churn Dataset  | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Binary Classification with a Bank Churn Dataset Playground Series - Season 4, Episode 1 Binary Classification with a Bank Churn Dataset Overview Data Code Models Discussion Leaderboard Rules Samvel Kocharyan ¬∑ 17th in this Competition  ¬∑ Posted a year ago arrow_drop_up 11 more_vert 17th Place Solution| AutoML + Unicorn's pollen + Lack of sleep Context S4E1 Playground \"Binary Classification with a Bank Churn Dataset\". Business context: https://www.kaggle.com/competitions/playground-series-s4e1/overview Data context: https://www.kaggle.com/competitions/playground-series-s4e1/data Overview of the approach Our final submission was a combination of AutoGluon 3-level stack we called \"Frankenstein II\" and set of  averages from our previous models and some public notebooks. Final submission was trained on the reduced set of features we got from OpenFE. Features were eliminated by BorutaSHAP and RFECV. Final model used 103 features. Detail of the Submissions We selected 2 submissions: WeightedEnsemble_L3 0.89372 Public | 0.89637 Private | 0.898947 CV Winning solution  0.90106 Private | 0.89687 Public. We got it from averaging 0.89673 and 0.89565 in last hours of the competition. Frankenstein II schema What worked for us? Feature generation - 470 and Feature Elimination - 103 Data-Centric Approach (CleanLab) Relabeling AutoGluon 1.0.1 (thanks to @innixma ) BorutaSHAP framework and Skleran - RFECV Ideas published by @paddykb , @thomasmeiner and respected community Merging, Stacking, Ensembling, Averaging Tons of experiments. Mainly for educative purposes üî• Kaggle Alchemists Secret Society named after Akka fr√•n Kebnekajse ü¶Ñ Unicorn's pollen What doesn't work for us this time? PCA / ICA Standalone Boosting models TabPFN Surnames features Original dataset Sources https://www.kaggle.com/competitions/playground-series-s4e1/discussion/470363 https://www.kaggle.com/competitions/playground-series-s4e1/discussion/471164 https://www.kaggle.com/competitions/playground-series-s4e1/discussion/469859 https://www.kaggle.com/competitions/playground-series-s4e1/discussion/465192 https://www.kaggle.com/competitions/playground-series-s4e1/discussion/470610 https://www.kaggle.com/code/arunklenin/ps4e1-advanced-feature-engineering-ensemble https://www.kaggle.com/code/thomasmeiner/ps4e1-eda-feature-engineering-modelling Please sign in to reply to this topic. comment 5 Comments Hotness Thiago Mantuani Posted a year ago arrow_drop_up 1 more_vert Congratulations!!! Nick Erickson Posted a year ago arrow_drop_up 1 more_vert Great job!! Harshit Sharma Posted a year ago ¬∑ 305th in this Competition arrow_drop_up 2 more_vert Congratulations on your impressive 17th place finish @samvelkoch and the successful combination of AutoML techniques, averaging from previous models, and feature selection strategies! Thomas Mei√üner Posted a year ago ¬∑ 460th in this Competition arrow_drop_up 2 more_vert Congratulations! Really good final place!!! Samvel Kocharyan Topic Author Posted a year ago ¬∑ 17th in this Competition arrow_drop_up 0 more_vert Thank you Thomas"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 4, Episode 1 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Bank Customer Churn Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 21.65 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 21.65 MB sample_submission.csv test.csv train.csv 3 files 29 columns ",
    "data_description": "Binary Classification with a Bank Churn Dataset  | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Binary Classification with a Bank Churn Dataset Playground Series - Season 4, Episode 1 Binary Classification with a Bank Churn Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2024 Kaggle Playground Series! Happy New Year! This is the 1st episode of Season 4. We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: For this Episode of the Series, your task is to predict whether a customer continues with their account or closes it (e.g., churns). Good luck! Start Jan 2, 2024 Close Feb 1, 2024 Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each id in the test set, you must predict the probability for the target variable Exited . The file should contain a header and have the following format: id ,Exited 165034 , 0 . 9 165035 , 0 . 1 165036 , 0 . 5 etc . content_copy Timeline link keyboard_arrow_up Start Date - January 2, 2024 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  January 31, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Binary Classification with a Bank Churn Dataset . https://kaggle.com/competitions/playground-series-s4e1, 2024. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 8,898 Entrants 3,777 Participants 3,632 Teams 28,457 Submissions Tags Beginner Tabular Binary Classification Banking Roc Auc Score Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s4e2",
    "discussion_links": [
      "/competitions/playground-series-s4e2/discussion/480939",
      "/competitions/playground-series-s4e2/discussion/480795",
      "/competitions/playground-series-s4e2/discussion/482075",
      "/competitions/playground-series-s4e2/discussion/480927"
    ],
    "discussion_texts": [
      "Multi-Class Prediction of Obesity Risk | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Multi-Class Prediction of Obesity Risk Playground Series - Season 4, Episode 2 Multi-Class Prediction of Obesity Risk Overview Data Code Models Discussion Leaderboard Rules Kirderf ¬∑ 4th in this Competition  ¬∑ Posted a year ago arrow_drop_up 75 more_vert 4th place solution: Stacking with XGB + Pseudo labeling + metric optimizing. Thanks again for an interesting series challenge, always a fun and good area for testing and experiment new models and ideas, benchmark against the old ones! Big shakeup in this competition, personally I moved 255 places. But this is expected with a 20/80 ratio in the test set, and one should heavily weight the best local cross-validation in such scenario. In this challenge we had a multi-class problem, we had similar not so long ago in the Multi-Class Prediction of Cirrhosis Outcomes so I picked up some ideas from there from my 4th place solution. https://www.kaggle.com/competitions/playground-series-s3e26/discussion/464863 Alright it‚Äôs not the same, different features, metric, data etc. but still the concept can be reused. And what a positive result, I landed in 4th place here as well. Solution Summary A stacking approach with XGB as meta learning and different SOTA solutions as extra added stacking features from each prediction. Final inference includes optimized accuracy metric and 9 predictions of different version of the stacking code and other diverse solutions for a count of max class per row. Data and Feature Engineering Both the competition dataset and the extra dataset was used. I used two different FE for training: train_df = pd.read_csv( '/kaggle/input/playground-series-s4e2/train.csv' )\noriginal = pd.read_csv( '/kaggle/input/obesity-or-cvd-risk-classifyregressorcluster/ObesityDataSet.csv' )\ntrain_df = pd.concat([train_df, original]).drop([ 'id' ], axis= 1 ).drop_duplicates().reset_index(drop= True ) # Advanced feature engineering for training data train_df[ 'Age_Group' ] = pd.cut(train_df[ 'Age' ], bins=[ 20 , 30 , 40 , 50 , 55 ], labels=[ 'A' , 'B' , 'C' , 'D' ],)\ntrain_df[ 'Log_Age' ] = np.log1p(train_df[ 'Age' ])\nscaler = MinMaxScaler()\ntrain_df[ 'Scaled_Age' ] = scaler.fit_transform(train_df[ 'Age' ].values.reshape(- 1 , 1 )) #train_df = train_df.drop(columns=['id']) test_df = pd.read_csv( '/kaggle/input/playground-series-s4e2/test.csv' ) # Advanced feature engineering for test data test_df[ 'Age_Group' ] = pd.cut(test_df[ 'Age' ], bins=[ 20 , 30 , 40 , 50 , 55 ], labels=[ 'A' , 'B' , 'C' , 'D' ],)\ntest_df[ 'Log_Age' ] = np.log1p(test_df[ 'Age' ])\ntest_df[ 'Scaled_Age' ] = scaler.transform(test_df[ 'Age' ].values.reshape(- 1 , 1 ))\n\ntest_df = test_df.drop(columns=[ 'id' ])\n\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(train_df[ 'NObeyesdad' ].unique())\ntrain_df[ 'NObeyesdad' ] = label_encoder.transform(train_df[ 'NObeyesdad' ]) content_copy And train[ 'Age group' ] = pd.cut(train[ 'Age' ], bins=[ 0 , 18 , 30 , 45 , 60 , train[ 'Age' ]. max ()], labels=[ '0-18' , '19-30' , '31-45' , '46-60' , '60+' ])\ntrain[ 'BMI' ] = train[ 'Weight' ] / (train[ 'Height' ] ** 2 )\ntest[ 'Age group' ] = pd.cut(test[ 'Age' ], bins=[ 0 , 18 , 30 , 45 , 60 , test[ 'Age' ]. max ()], labels=[ '0-18' , '19-30' , '31-45' , '46-60' , '60+' ])\ntest[ 'BMI' ] = test[ 'Weight' ] / (test[ 'Height' ] ** 2 )\noriginal[ 'Age group' ] = pd.cut(original[ 'Age' ], bins=[ 0 , 18 , 30 , 45 , 60 , original[ 'Age' ]. max ()], labels=[ '0-18' , '19-30' , '31-45' , '46-60' , '60+' ])\noriginal[ 'BMI' ] = original[ 'Weight' ] / (original[ 'Height' ] ** 2 )\n\ntrain[ 'Age * Gender' ] = train[ 'Age' ] * train[ 'Gender' ]\ntest[ 'Age * Gender' ] = test[ 'Age' ] * test[ 'Gender' ]\noriginal[ 'Age * Gender' ] = original[ 'Age' ] * original[ 'Gender' ]        \n\ncategorical_features = [ 'Gender' , 'family_history_with_overweight' , 'Age group' , 'FAVC' , 'CAEC' , 'SMOKE' , 'SCC' , 'CALC' , 'MTRANS' ]\ntrain = pd.get_dummies(train, columns=categorical_features)\ntest = pd.get_dummies(test, columns=categorical_features)\noriginal = pd.get_dummies(original, columns=categorical_features)\n\npolynomial_features = PolynomialFeatures(degree= 2 )\nX_poly = polynomial_features.fit_transform(train[[ 'Age' , 'BMI' ]])\ntrain = pd.concat([train, pd.DataFrame(X_poly, columns=[ 'Age^2' , 'Age^3' , 'BMI^2' , 'Age * BMI' , 'Age * BMI2' , 'Age * BMI3' ])], axis= 1 )\nX_poly = polynomial_features.transform(test[[ 'Age' , 'BMI' ]])\ntest = pd.concat([test, pd.DataFrame(X_poly, columns=[ 'Age^2' , 'Age^3' , 'BMI^2' , 'Age * BMI' , 'Age * BMI2' , 'Age * BMI3' ])], axis= 1 )\nX_poly = polynomial_features.transform(original[[ 'Age' , 'BMI' ]])\noriginal = pd.concat([original, pd.DataFrame(X_poly, columns=[ 'Age^2' , 'Age^3' , 'BMI^2' , 'Age * BMI' , 'Age * BMI2' , 'Age * BMI3' ])], axis= 1 ) content_copy Models and frameworks used to the stacking approach. Competition metric is Accuracy but for training log_loss was set and probability per solution was saved for later use. AutoXGB AutoGluon with the new zero-shot HPO training. Ensemble Weights: {'CatBoost_r9_BAG_L1': 0.363, 'LightGBM_r131_BAG_L1': 0.253, 'XGBoost_BAG_L1': 0.099, 'XGBoost_r33_BAG_L1': 0.099, 'NeuralNetTorch_BAG_L2': 0.077, 'ExtraTreesEntr_BAG_L2': 0.033, 'NeuralNetFastAI_BAG_L2': 0.022, 'CatBoost_BAG_L2': 0.022, 'NeuralNetTorch_BAG_L1': 0.011, 'RandomForestGini_BAG_L2': 0.011, 'ExtraTreesGini_BAG_L2': 0.011} LightAutoml with LGBM and Catboost. Custom XGB + LGBM training code with and without Pseudo Label training. Stacking Used the features from training and added the probability from the different solution as extra features. The meta model was XGB. Stacking is a great approach in classification problem versus other techniques. Metric optimizing and final inference. I used the public code for optimizing the metric and used earlier saved probabilities per class for it. The public code for optimizing can be find here https://www.kaggle.com/code/samlakhmani/easy-92-196-single-model . For the final inference I used a diverse 9 prediction collection of both own trained solutions and 2 public notebooks and calculated the max class per row. That‚Äôs it!! Happy Kaggling! 7 Please sign in to reply to this topic. comment 28 Comments 9 appreciation  comments Hotness Harshit Sharma Posted a year ago ¬∑ 797th in this Competition arrow_drop_up 3 more_vert Congratulations @kirderf on the impressive 4th place finish and leveraging innovative stacking techniques along with advanced feature engineering! benthebest Posted a year ago ¬∑ 368th in this Competition arrow_drop_up 3 more_vert yes, i appreciate your work a lot. hope u have a good result in the competition borhanitrash Posted a year ago arrow_drop_up 2 more_vert Thanks for the detailed summary of your approach. Stacking diverse models and optimizing for the competition metric shows good problem solving skills. Leveraging ideas from past competitions is smart reuse. Congratulations on placing 4th again - it shows your systematic methodology works. These types of stacking/ensembling techniques combined with pragmatic FE are always useful to see. Best of luck in future challenges! @kirderf dideagoc Posted a year ago arrow_drop_up 1 more_vert Nicely done! dideagoc Posted a year ago arrow_drop_up 1 more_vert Great work. Filip Posted a year ago ¬∑ 2733rd in this Competition arrow_drop_up 1 more_vert Very interesting approach! Really nice write up Vivek Revi Posted a year ago ¬∑ 724th in this Competition arrow_drop_up 1 more_vert Great Work! Will try implementing this. Thank you. @kirderf Sudhir R. Pradhan Posted a year ago arrow_drop_up 1 more_vert Congratulations @kirderf , for a fascinating submission that encompasses the stacking technique beautifully. Kudos !!! Neeraj Mehta Posted a year ago ¬∑ 2278th in this Competition arrow_drop_up 2 more_vert Thanks for sharing your approach. Really helpful. zephyrus1 Posted a year ago ¬∑ 1684th in this Competition arrow_drop_up 2 more_vert The things you share are very precious, Thanks again. ü§© Khoa Tran Posted a year ago ¬∑ 1095th in this Competition arrow_drop_up 2 more_vert Very well designed! Congratulations! This was very well deserved! ËÄ∂‚úå Posted 8 months ago arrow_drop_up 0 more_vert XGBÁ´üÁÑ∂‰πüÂèØ‰ª•ËøõË°åÂ§öÂàÜÁ±ªÔºågood job xy1694030 Posted a year ago arrow_drop_up 0 more_vert Â§™Â•Ω‰∫ÜÔºåÊÑüË∞¢ÂàÜ‰∫´ üòÉ KAMALJIT_SINGH Posted a year ago arrow_drop_up 0 more_vert Great Work Navanish Pandey Posted a year ago ¬∑ 2639th in this Competition arrow_drop_up 0 more_vert Wow, nice Harsh Gupta Posted a year ago arrow_drop_up 0 more_vert great work! Azamat Posted a year ago arrow_drop_up 0 more_vert Great Work! Code is so clean and concise ASK Posted a year ago ¬∑ 651st in this Competition arrow_drop_up 0 more_vert @kirderf Congratulations on‚Ä¶ Great work. Great explanation. This comment has been deleted. Appreciation (9) NDIAYE MOUSSA Posted a year ago arrow_drop_up 1 more_vert Thanks for the well explained approach Filip Kozal Posted a year ago arrow_drop_up 1 more_vert Thanks for sharing! I learned a lot. T_Py09 Posted a year ago arrow_drop_up 1 more_vert Great Work. Thanks for sharing. ü§ù Willy Wang Posted a year ago ¬∑ 496th in this Competition arrow_drop_up 1 more_vert Thanks for sharing! I learn a lot~ mineweenie Posted a year ago ¬∑ 90th in this Competition arrow_drop_up 1 more_vert Thank you for sharing !!! Cascarino Posted a year ago ¬∑ 1702nd in this Competition arrow_drop_up 1 more_vert Thanks for sharing !",
      "Multi-Class Prediction of Obesity Risk | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Multi-Class Prediction of Obesity Risk Playground Series - Season 4, Episode 2 Multi-Class Prediction of Obesity Risk Overview Data Code Models Discussion Leaderboard Rules Nirmal Falzoni ¬∑ 6th in this Competition  ¬∑ Posted a year ago arrow_drop_up 27 more_vert 6th place solution first thanks to @divyam6969 for your helpful notebook. I edited it to make it a massive mess so not going to post it. The main part of the model was from the notebook (ensemble of the classic tree algorithms ect). I then added a simple neural network into it where I optimised the parameters with optuna. I am very new to data science, but my thinking was that a neural network would learn differently to the tree based algorithms, so it would be good to keep it in for stability purposes, even if it didn't improve the cv much. Also the data was generated from a deep learning model, so it would probably be a good idea to incorporate one. The second thing i changed was optimising the weights of the ensemble with a simple grid search(i'm not sure if this is good practise but went with it because it increased my cv). I had a lot of models with higher LB scores but I followed @thomasmeiner advice and treated it as a fold in cv, so submitted models with high cv, but cv that didn't deviate when submitted to public LB. The last and most important thing was luck :) Please sign in to reply to this topic. comment 26 Comments Hotness HiMayankGour Posted a year ago ¬∑ 181st in this Competition arrow_drop_up 5 more_vert Hello, Nirmal Falzoni,  I too made a deep learning model at first but it never got past the validation accuracy of ~88%, i tried multiple architectures, so i ended up using trees instead. I am new to deep learning, and most notebooks of this competetion used Tree but if you managed to make a better model via deep learning would you mind sharing, strictly for learning purposes. I would really appreciate it. Nirmal Falzoni Topic Author Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 2 more_vert Hi, my cv for my neural network never got above 0.89 ish if i remember correctly. I remember adding in a learning rate scheduler helped a bit but no way near enough to catch up to the tree algorithms. I decided to still keep it in though, (with lower weights than the tree algorithms) for the reasons mentioned above. AmanSingh2002 Posted a year ago ¬∑ 706th in this Competition arrow_drop_up 3 more_vert Congratulations!! I'm begginer too, I also tried ANN but couldn't get better accuracy than ~87% on validation data, can you please share your ANN model architecture and by cv do you mean cross velidation? Nirmal Falzoni Topic Author Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 3 more_vert by cv I mean cross validation yes. As mentioned in another comment i couldn't get my cv above 0.89 for the neural network. Here is the code for it if your still interested though: def create_neural_network(input_dim,lr_schedule):\n    model = Sequential([\n    Dense(64, activation='sigmoid', input_dim=16),\n    Dropout(0.05),\n    Dense(128, activation='relu'),\n    Dropout(0.05),\n    Dense(256, activation='relu'),\n    Dropout(0.1),\n    Dense(512, activation='relu'),\n    Dropout(0.15),\n    Dense(256, activation='relu'),\n    Dropout(0.15),\n    Dense(128, activation='relu'),\n    Dropout(0.1),\n    Dense(64, activation='relu'),\n    Dropout(0.1),\n    Dense(32, activation='relu'),\n    Dropout(0.05),\n    Dense(16, activation='relu'),\n    Dense(7, activation='softmax')\n    ])\n    optimizer = Adam(learning_rate=lr_schedule)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model def step_decay(epoch):\n    initial_lr = 0.01\n    drop = 0.5\n    epochs_drop = 10\n    lr = initial_lr * (drop ** (epoch // epochs_drop))\n    return lr keras_classifier = KerasClassifier(build_fn=create_neural_network(16,step_decay(50)), epochs=50, batch_size=32, verbose=0) AmanSingh2002 Posted a year ago ¬∑ 706th in this Competition arrow_drop_up 2 more_vert Thanks I will give it a try Priti P Posted a year ago ¬∑ 993rd in this Competition arrow_drop_up 3 more_vert Congratulations to you! I am too a beginner and would like to see your notebook to try out few new things and possibly learn. If you don't mind, requesting you to kindly share your work (please don't worry about the messy notebook). Thank you! Ravi Ramakrishnan Posted a year ago ¬∑ 705th in this Competition arrow_drop_up 3 more_vert @nirmalfalzoni what was your CV scheme and score? Nirmal Falzoni Topic Author Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 2 more_vert 10 fold stratifiedKfold which got me a score of 0.9184 Ravi Ramakrishnan Posted a year ago ¬∑ 705th in this Competition arrow_drop_up 1 more_vert Thanks a lot for the rejoinder and congrats! @nirmalfalzoni Thomas Mei√üner Posted a year ago ¬∑ 860th in this Competition arrow_drop_up 4 more_vert Congratulations! Pai Posted a year ago ¬∑ 997th in this Competition arrow_drop_up 1 more_vert Congratulations üòÄ Thiago Mantuani Posted a year ago arrow_drop_up 1 more_vert Congratulations! Zain Faisal Posted a year ago arrow_drop_up 1 more_vert Good job. Keep doing this amazing work. Help others as well to learn and grow Paulo Robinson Giaciani (pt-BR) Posted a year ago ¬∑ 202nd in this Competition arrow_drop_up 2 more_vert Congratulations! Good Job! üîùüèÜüòä Henrique Machado Posted a year ago ¬∑ 126th in this Competition arrow_drop_up 2 more_vert Congratulations! Md. Sadman Sakib Posted a year ago ¬∑ 2599th in this Competition arrow_drop_up 2 more_vert Congratulations! Ivan Zadorozniy Posted a year ago ¬∑ 591st in this Competition arrow_drop_up 2 more_vert Good job and good luck in the future Meagan Burkhart Posted a year ago ¬∑ 327th in this Competition arrow_drop_up 2 more_vert Congratulations! I noted in my solution writeup that I probably should have optimized the weights for my voting classifier, so good for you for taking the time to do it! Nat Apel Posted a year ago ¬∑ 994th in this Competition arrow_drop_up 2 more_vert Congratulations! What data did you use for your work,  I have you added origonal data? I noticed that goos results have been on combinded results train data + original data, but all fell in private contest. Nirmal Falzoni Topic Author Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert I used both the original and train data SAI LIKITH KUMAR REDDY D Posted a year ago ¬∑ 3573rd in this Competition arrow_drop_up 2 more_vert congratulation bro Divyam6969 Posted a year ago ¬∑ 103rd in this Competition arrow_drop_up 1 more_vert Congratulations buddy :))))))  and good luck in upcoming contests Angad Singh Posted 24 days ago arrow_drop_up 0 more_vert The code is fantastic! AHMED AIT HADJ IAICH Posted a year ago ¬∑ 495th in this Competition arrow_drop_up 0 more_vert Good job! Congratulations Samudra Roy Posted a year ago ¬∑ 2869th in this Competition arrow_drop_up 0 more_vert Congrats!!! üèÜ Sam Posted a year ago ¬∑ 19th in this Competition arrow_drop_up 0 more_vert Congratulations üëçüèª kerry sun Posted a year ago ¬∑ 129th in this Competition arrow_drop_up 0 more_vert Congratulations! This comment has been deleted.",
      "Multi-Class Prediction of Obesity Risk | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Multi-Class Prediction of Obesity Risk Playground Series - Season 4, Episode 2 Multi-Class Prediction of Obesity Risk Overview Data Code Models Discussion Leaderboard Rules Sam ¬∑ 19th in this Competition  ¬∑ Posted a year ago arrow_drop_up 7 more_vert 19th Place Solution Notebook : link I kept the original data as part of the training set only The idea was the in the CV score, I wanted the original data to not influence the accuracy of the CV. Thus I appended the original data. You can observe the model function in the above code. Choosing Folds I experimented with optimization for 10CV 5CV performed better than 10CV and thus I choose 5CV Looks like this remains the only difference between the 19th place and 2nd palace :P \n2nd place used 20CV content_copy Choosing no of time to append Original data to training If you look at these two notebooks you will be able to observe experiments I had done with the multiplier. Even My experiments gave 4 as the best multiplier for LGBM, and 1 as a multiplier to the XGB model. Thresholding Optimization This is the notebook where I introduced threshold optimization to this competition, post which a lot of people started implementing it. My intention of making it public was to learn on how the implementation can be improved. It would me a lot to know the impact of the model without the optimization. Please share. Requesting to Upvote the Notebook ^ Please sign in to reply to this topic. comment 1 Comment 1 appreciation  comment Hotness Appreciation (1) emoji_people MrSimple Posted a year ago ¬∑ 746th in this Competition arrow_drop_up 2 more_vert Really good job! thanks for sharing!",
      "Multi-Class Prediction of Obesity Risk | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Multi-Class Prediction of Obesity Risk Playground Series - Season 4, Episode 2 Multi-Class Prediction of Obesity Risk Overview Data Code Models Discussion Leaderboard Rules Carlos Junior ¬∑ 24th in this Competition  ¬∑ Posted a year ago arrow_drop_up 14 more_vert 24th Place Solution Even though 24th place is not like top 3 or something, I am so happy to have accomplished this. I've always wanted to dedicate some time to Kaggle to enhance my skills and this time not only I managed to do it, but also got rewarded with 24th place. Ok, enough about my feelings, and let's go to the solution explanation. Main idea The idea behind the solution was based on code from @divyam6969 [1]. It is pretty much an ensemble of XGBoost and LightGBM. I didn't add any features and used StdScaler for the numerical ones. For the categorical features, in XGBoost I used MEstimateEncoder (didn't know until this competition) and for the LightGBM I used a OneHotEncoder. The CV consisted of a 10-split Stratified K-fold. The crucial part was tweaking the weights of the ensemble, which led to the difference in LB. What didn't work Additional features such as BMI Catboost didn't contribute to the CV Other types of scalars and categorical encoding Pruning the dataset What I've learned The classic: Trust in your CV over LB Experimentation tracking (for this particular competition I used MLFlow and logged the important parameters and metrics) One can always look up to other solutions, be it to extend them or incorporate the ideas into your own solution. Kernel https://www.kaggle.com/code/nicowxd/ps4e2-top-25-simple-ensemble-private-0-91067 References used (Thanks to the authors) [1] https://www.kaggle.com/code/divyam6969/best-solution-multiclass-obesity-prediction [2] https://www.kaggle.com/code/ddosad/ps4e2-visual-eda-lgbm-obesity-risk 2 Please sign in to reply to this topic. comment 8 Comments Hotness Ambigapathi Posted a year ago arrow_drop_up 1 more_vert Thanks for your code it helps me to know several methods as I am new to ml which attack to learn more in ML and AI Carlos Junior Topic Author Posted a year ago ¬∑ 24th in this Competition arrow_drop_up 0 more_vert Glad you found it helpful! üòÉ Divyam6969 Posted a year ago ¬∑ 103rd in this Competition arrow_drop_up 1 more_vert Congratulations on your impressive 24th place solution! While it may not be in the top 3, achieving 24th place is definitely something to be proud of. Your dedication to enhancing your skills on Kaggle has truly paid off, and it's fantastic to see your hard work recognized :)))) Carlos Junior Topic Author Posted a year ago ¬∑ 24th in this Competition arrow_drop_up 0 more_vert Thank you for the kind words and also for the notebook that inspired me to reach 24th place! üôÇ Bobo Jamson Posted a year ago ¬∑ 491st in this Competition arrow_drop_up 1 more_vert Congrats on the placement and surviving the shakeup. It's difficult not to get pulled into the public leaderboard scores and loose sight of the real goal. When I was choosing which two submissions to submit, I would keep telling myself to trust your CV. This cannot be overstated. \"The classic: Trust in your CV over LB\"  +10000 Carlos Junior Topic Author Posted a year ago ¬∑ 24th in this Competition arrow_drop_up 1 more_vert Thank you! You're absolutely right. I was also so tempted to submit based only on public leaderboard, that one of my submissions was exactly my top leaderboard score. The other submission, however, I gave a long shot of trusting in the CV and was the one that landed me to 24th place. Angad Singh Posted 24 days ago arrow_drop_up 0 more_vert Game changing code! Banda Sairam Reddy Posted a year ago arrow_drop_up 0 more_vert niceee notebook very informative"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 4, Episode 2 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Obesity or CVD risk dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note: This dataset is particularly well suited for visualizations, clustering, and general EDA. Show off your skills! 3 files 4.59 MB csv CC BY-SA 4.0 Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 4.59 MB sample_submission.csv test.csv train.csv 3 files 37 columns ",
    "data_description": "Multi-Class Prediction of Obesity Risk | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Multi-Class Prediction of Obesity Risk Playground Series - Season 4, Episode 2 Multi-Class Prediction of Obesity Risk Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: The goal of this competition is to use various factors to predict obesity risk in individuals, which is related to cardiovascular disease. Good luck! Start Feb 1, 2024 Close Mar 1, 2024 Evaluation link keyboard_arrow_up Submissions are evaluated using the accuracy score. Submission File For each id row in the test set, you must predict the class value of the target, NObeyesdad . The file should contain a header and have the following format: id,NObeyesdad 20758 , Normal_Weight 20759 , Normal_Weight 20760 , Normal_Weight etc. content_copy Timeline link keyboard_arrow_up Start Date - February 1, 2024 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  February 29, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Multi-Class Prediction of Obesity Risk. https://kaggle.com/competitions/playground-series-s4e2, 2024. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 8,346 Entrants 3,746 Participants 3,587 Teams 29,669 Submissions Tags Beginner Time Series Analysis Tabular Multiclass Classification Accuracy Score Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s4e3",
    "discussion_links": [
      "/competitions/playground-series-s4e3/discussion/488106",
      "/competitions/playground-series-s4e3/discussion/488127"
    ],
    "discussion_texts": [
      "Steel Plate Defect Prediction | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Steel Plate Defect Prediction Playground Series - Season 4, Episode 3 Steel Plate Defect Prediction Overview Data Code Models Discussion Leaderboard Rules Moonlit ¬∑ 2nd in this Competition  ¬∑ Posted a year ago arrow_drop_up 54 more_vert 2nd place solution: OOF Ensemble Hello everyone, It never comes to me that I would be sharing my solution here. As a student, this is my first time fully participating in a Kaggle competition. Context: Steel Plate Defect Prediction Competition (Playground Series - Season 4, Episode 3) Business Context: https://www.kaggle.com/competitions/playground-series-s4e3 Data Description: https://www.kaggle.com/competitions/playground-series-s4e3/data My Solution Code Link: https://www.kaggle.com/code/yunqicao/2nd-place-solution-steel-plate-defect-prediction Here is my solution: 1. Data I combine the provided data with the original dataset and exclude rows with multiple labels, as they constitute only a small portion. 2. Feature Engineering I create three features and remove some features. My decisions on feature selection are primarily guided by cross-validation scores, feature importance, and pairwise correlation. def feature_engineering(data):\n\n    data[ 'Ratio_Length_Thickness' ] = data[ 'Length_of_Conveyer' ] / data[ 'Steel_Plate_Thickness' ]\n    data[ 'Normalized_Steel_Thickness' ] = (data[ 'Steel_Plate_Thickness' ] -data[ 'Steel_Plate_Thickness' ].min()) / (data[ 'Steel_Plate_Thickness' ].max() - data[ 'Steel_Plate_Thickness' ].min())\n    data[ 'X_Range*Pixels_Areas' ] = (data[ 'X_Maximum' ] - data[ 'X_Minimum' ]) * data[ 'Pixels_Areas' ]\n\n    return data\n\nfeatures_to_drop = [ 'Y_Minimum' , 'Steel_Plate_Thickness' , 'Sum_of_Luminosity' , 'Edges_X_Index' , 'SigmoidOfAreas' , 'Luminosity_Index' , 'TypeOfSteel_A300' ] content_copy Thanks to: https://www.kaggle.com/code/thomasmeiner/ps4e3-eda-feature-engineering-model https://www.kaggle.com/competitions/playground-series-s4e3/discussion/482401 3. Cross Validation and Tuning Parameters I choose four multi-class models(xgb, lgbm, cat, hgbc), implement a 10-fold cross-validation and tune hyperparameters using Optuna. For the final predictions, I calculate the average of the predictions across the 10 folds. Thanks to: https://www.kaggle.com/code/ankurgarg04/steel-plate-defect-detection-technique-exploration 4. Out-of-Fold Ensemble I create an ensemble of three models by utilizing out-of-fold (OOF) files. To ensure consistency, I replicate or adapt their publicly shared notebooks to generate OOF files with an identical train-validation split. The weights for the ensemble are optimized for cross-validation score using Nelder-Mead optimization. Model 1: My model Model 2: Replicated from https://www.kaggle.com/code/noepinefrin/0-89534-clustered-feature-lgbm-xgb-cat Model 3: Adapted from https://www.kaggle.com/code/arunklenin/ps4e3-steel-plate-fault-prediction-multilabel Thanks to: https://www.kaggle.com/code/lucamassaron/steel-plate-eda-xgboost-is-all-you-need https://www.kaggle.com/competitions/siim-isic-melanoma-classification/discussion/175614 5. Simple Ensemble At the final step, I combine my submission with outstanding public submissions. In addition to the notebooks previously referenced, I also incorporate output from https://www.kaggle.com/code/cyrilbourgeois/playground-s4e03-xgboost-ensembler . I had limited opportunities to submit in the final step, so I assigned weights based on intuition. As it turned out, using average weights is effective. 6. Other Thoughts Other approches I tried but not work: Pseudo labeling; Stacking approach with XGB as the meta model; Change eval_metric to 'roc_auc' Thanks to: https://www.kaggle.com/code/arnogils/basic-xgb-for-steel-plate-defects-0-8948 https://www.kaggle.com/competitions/playground-series-s3e26/discussion/464863 7. Code My Solution Code Link: https://www.kaggle.com/code/yunqicao/2nd-place-solution-steel-plate-defect-prediction By the way, simply ensembling the outputs from the public notebooks I mentioned with equal weights can get an excellent result. This approach achieves a public leaderboard score of 0.89684 and a private leaderboard score of 0.88923, marking my highest score on the public leaderboard. Although I choose not to use this version as my final submission, it shows that these public notebooks are distinguished. Many Thanks to Everyone! Thank you for reviewing my solution. I'm grateful for all the public notebooks and discussions. They are really valuable.  I look forward to sharing my insights during the next competition. Please sign in to reply to this topic. comment 21 Comments 3 appreciation  comments Hotness e_landeros Posted a year ago ¬∑ 1228th in this Competition arrow_drop_up 3 more_vert awesome! thanks for sharing. also very cool you shared all the notebooks. Yousif Hajajj Posted a year ago arrow_drop_up 2 more_vert Congratulations on achieving 2nd  place Stanciu Rares Stefan Posted 10 months ago arrow_drop_up 0 more_vert Congratulations! I really liked the explanation to your solution, thank you! Himanshu Yadav Posted a year ago arrow_drop_up 0 more_vert congratulations on your achievement. MarianoYe Posted a year ago arrow_drop_up 0 more_vert congratulation kerry sun Posted a year ago ¬∑ 48th in this Competition arrow_drop_up 0 more_vert Congratulations! Daniel Hoyos Ospina Posted a year ago arrow_drop_up 0 more_vert Nice work! Aiden Neal Posted a year ago arrow_drop_up 0 more_vert This is some really cool stuff! Optimistix Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Congratulations, and thanks for sharing your approach, and notebook. Public notebooks and discussions really provide a wealth of knowledge and insights. Aditya kishor Posted a year ago arrow_drop_up 0 more_vert Congratulations! Thank you for that tip. Matthew S Farmer Posted a year ago ¬∑ 928th in this Competition arrow_drop_up 0 more_vert great work. thanks for sharing your insights. Samvel Kocharyan Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Congrats @yunqicao ! Well done! You can make this topic as your solution marked at the leaderboard. To do it - just copy topic URL to the \"Share your solution\" under your Team tab. Moonlit Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for that tip. Really helpful. By the way, do you know how to get the kaggle merchandise? Samvel Kocharyan Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert This is also my 1st Kaggle Swag prize competition. But as I know the leaderboard should be finalized in 1-2 days and after that Kaggle team will send to your email a personal link to the virtual treasury where you'll choose your   swag item(s). Everything will be mailed to the postal address. Moonlit Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 0 more_vert Got it! Thank you and congrats again. Thiago Mantuani Posted a year ago arrow_drop_up 0 more_vert very good, good approach. kerry sun Posted a year ago ¬∑ 48th in this Competition arrow_drop_up 0 more_vert Congratulations! Nice approach! ankur garg Posted a year ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Congratulations on 2nd place @yunqicao ! Great work! Minato Namikaze Posted a year ago ¬∑ 8th in this Competition arrow_drop_up 0 more_vert Congratulations! Great Approach @yunqicao Moonlit Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Thank you! Your notebook and approch are quite enlightening. I just got lucky this time when choosing the final submission version :) Appreciation (3) huzhonglei Posted a year ago ¬∑ 7th in this Competition arrow_drop_up 0 more_vert Congrats! Thank you!!! Yeshua T Posted a year ago ¬∑ 29th in this Competition arrow_drop_up 0 more_vert great work. thanks for sharing atrbyg24 Posted a year ago ¬∑ 2075th in this Competition arrow_drop_up 0 more_vert Thanks for sharing. Congrats!",
      "Steel Plate Defect Prediction | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Steel Plate Defect Prediction Playground Series - Season 4, Episode 3 Steel Plate Defect Prediction Overview Data Code Models Discussion Leaderboard Rules Samvel Kocharyan ¬∑ 3rd in this Competition  ¬∑ Posted a year ago arrow_drop_up 21 more_vert 3rd Place Solution - Mediocres et Impera Context: Steel Plate Defect Prediction Competition (Playground Series - Season 4, Episode 3) Business Context: https://www.kaggle.com/competitions/playground-series-s4e3 Data Description: https://www.kaggle.com/competitions/playground-series-s4e3/data Overview of the approach: Due to data and targets nature of this competition I considered competition as a multi-label problem. I've tried also multi-class approach but CV and public LB scores were a bit lower comparing to mutli-label. Final submission is a combination of the several models - PyBoost, AutoGluon and some public bests submissions. My simple secret souce in this competition is \"Mediocres et Impera\" (average and rule) strategy: which here means - take something you trust the most and average the results. Overview of the submissions: Selected submission: 0.88936 (Private, 3rd place), 0.89674 (Public, 8th place) - Mix Selected submission: 0.88763 (Private), 0.89476 (Private) - PyBoost + OpenFE (features) Unselected BEST: 0.88944 (Private, 2nd place), 0.89670 (Public, 8th place) - Mix What works? Multi-Label approach PyBoost framework for really quick multi-label approach (thanks to @btbpanda ) AutoGluon AutoML (thanks to @innixma ) Ideas, notebooks and submissions published by @thomasmeiner , @arunklenin , @ravi20076 , @lucamassaron , @liudacheldieva , @ivanblch , @oscarm524 OpenFE framework for features generation Merging, Stacking, Ensembling, Averaging ü¶Ñ Unicorn's pollen What doesn't work this time? Standalone models Original Data Sources: https://github.com/sb-ai-lab/Py-Boost https://github.com/autogluon/autogluon https://github.com/IIIS-Li-Group/OpenFE https://www.kaggle.com/competitions/playground-series-s4e3/discussion/481015 https://www.kaggle.com/competitions/playground-series-s4e3/discussion/481098 https://www.kaggle.com/code/arunklenin/ps4e3-steel-plate-fault-prediction-multilabel https://www.kaggle.com/code/ravi20076/playgrounds4e03-eda-binaryclassifier Thanks to the authors üôè BONUS Leaderboard Visualisation https://www.kaggle.com/code/samvelkoch/s4e3-shake-up-plot Please sign in to reply to this topic. comment 9 Comments Hotness kartikeya kotnala Posted a year ago arrow_drop_up 1 more_vert Awesome, you guys are impressive‚Ä¶.some people on kaggle are so good they scare me Nick Erickson Posted a year ago arrow_drop_up 1 more_vert Congrats on 3rd place! I've added you to the AutoGluon Hall of Fame ! Samvel Kocharyan Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thank you Nick! AutoGluon is great! Minato Namikaze Posted a year ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Congratulations for securing 3rd! Great Approach @samvelkoch Samvel Kocharyan Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thank you Arun @arunklenin ! You notebook leads this competition and I'm very grateful for that. Sorry for small shake-up for you and Ravi. I'm curious was there better unselected submission? Minato Namikaze Posted a year ago ¬∑ 8th in this Competition arrow_drop_up 2 more_vert Thank you! there were about 20 better unselected results with 0.88951 as the highest obtained from Geometric Mean of selected results. @samvelkoch Ravi Ramakrishnan Posted a year ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Of course @samvelkoch We had some old submissions that could have given us 2nd place! Igor Volianiuk Posted a year ago ¬∑ 105th in this Competition arrow_drop_up 2 more_vert Thanks for sharing @samvelkoch . AutoGluon new meta? Nick Erickson Posted a year ago arrow_drop_up 0 more_vert I hope so üòÅ Thiago Mantuani Posted a year ago arrow_drop_up 2 more_vert Congratulations on the placement @samvelkoch , share your notebook with us if possible. Samvel Kocharyan Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Thank you Thiago. I will share it. Need some timeout."
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 4, Episode 3 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Steel Plates Faults dataset from UCI. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 5.49 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 5.49 MB sample_submission.csv test.csv train.csv 3 files 71 columns ",
    "data_description": "Steel Plate Defect Prediction | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Steel Plate Defect Prediction Playground Series - Season 4, Episode 3 Steel Plate Defect Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: Predict the probability of various defects on steel plates. Good luck! Start Mar 1, 2024 Close Apr 1, 2024 Evaluation link keyboard_arrow_up Submissions are evaluated using area under the ROC curve using the predicted probabilities and the ground truth targets. To calculate the final score, AUC is calculated for each of the 7 defect categories and then averaged. In other words, the score is the average of the individual AUC of each predicted column. Submission File For each id in the test set, you must predict the probability for each of 7 defect categories: Pastry , Z_Scratch , K_Scatch , Stains , Dirtiness , Bumps , Other_Faults . The file should contain a header and have the following format: id ,Pastry,Z_Scratch,K_Scatch,Stains,Dirtiness,Bumps,Other_Faults 19219 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 19220 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 19221 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 , 0 . 5 etc . content_copy Timeline link keyboard_arrow_up Start Date - March 1, 2024 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  March 31, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Steel Plate Defect Prediction. https://kaggle.com/competitions/playground-series-s4e3, 2024. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 6,473 Entrants 2,303 Participants 2,199 Teams 17,300 Submissions Tags Beginner Tabular Multiclass Classification Binary Classification Manufacturing Mean Columnwise Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s4e4",
    "discussion_links": [
      "/competitions/playground-series-s4e4/discussion/499174",
      "/competitions/playground-series-s4e4/discussion/499698",
      "/competitions/playground-series-s4e4/discussion/499747",
      "/competitions/playground-series-s4e4/discussion/499341",
      "/competitions/playground-series-s4e4/discussion/499204",
      "/competitions/playground-series-s4e4/discussion/499258"
    ],
    "discussion_texts": [
      "Regression with an Abalone Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Regression with an Abalone Dataset Playground Series - Season 4, Episode 4 Regression with an Abalone Dataset Overview Data Code Models Discussion Leaderboard Rules Johannes Heller ¬∑ 1st in this Competition  ¬∑ Posted a year ago arrow_drop_up 121 more_vert 1st Place Solution for the Regression with an Abalone Dataset Competition Thanks for hosting the Playground series, it's a great place for experimenting and focusing on model building üëèüèΩ Context Business context: https://www.kaggle.com/competitions/playground-series-s4e4 Data context: https://www.kaggle.com/competitions/playground-series-s4e4/data Overview of the Approach With this competition, I tried to max out the heavy ensembling GBDT strategy. In an earlier tabular data competition I tried different neural network architectures only to observe that none of them is competitive. I guess the papers claiming ANNs are finally on a par with GBDTs for tabular data haven't done proper preprocessing and fine-tuning. So here I focused mainly on GBDTs. AutoGluon indirectly adds some ANN to the ensemble, though. The solution in a nutshell: Feature Engineering using an AutoML solution (OpenFE, Zhang et al. (2022) OpenFE: Automated Feature Generation with Expert-level Performance , https://github.com/IIIS-Li-Group/OpenFE ) Selecting ca. 20 features (in addition to the original columns) using Sequential Feature Selection, individually for LGBM, XGB, Catboost. Examples: 'freq(Shell_weight)', '(Length-Shell_weight)', '(Whole_weight/Shucked_weight)', 'freq(Whole_weight)', '(Whole_weight+Shell_weight)', '(Length/Shell_weight)', 'residual(Whole_weight)', 'log(Whole_weight)', 'max(Whole_weight,Height)' Find some good hyperparam combinations using Bayesian Optimization (WandB Sweeps) for LGBM, XGB, CatBoost, HistGB and RandomForest Regression. Attempts with custom MLP were quickly abandoned, see my comment above üòû Train those models (10-folds CV), save OOF predictions. Use complete original Abalone dataset for training in each fold, never for validation. AutoGluon (first I had some failed attempts that exceeded the Kaggle time limit or didn't return proper OOF predictions) Two-step ensemble: The ensemble itself consists of 49 models. I optimized the weights using the OOF predictions and Nelder-Mead algorithm. The best results included negative coefficients and summed up to 0.997. That seemed cheesy to me but consistently outperformed coefficients that were clipped at min=0.0 and scaled to sum up to 1.0. Thus, I kept the optimized coefficients to blend the models' predictions. Tried adding some well-performing public notebook submissions to the ensemble. In the absence of OOFs I used some submissions against public LB to find an optimal weight of 17% and ended up with using only one ANN-based submission by endofnight17j03 üëã My second submission skipped the external public notebook and scored 0.14372/0.14379 (public/private LB), i.e. it would have finished first, too. Details of the submission Handling of the RMSLE metric ‚Ä¶by using proper loss fn. and eval. metric if possible: LGBM: custom MSLE loss (similar to proposal by broccoli beef in this discussion üëã) - btw 'gamma' objective scored only slightly worse XGB: 'reg:squaredlogerror' np.log1p / np.expm1 for other models Best individual models with CV RMSLE: LGBM: 0.14611 CatBoost: 0.14620 XGB: 0.14616 XGB Classifier with Regression Head: 0.14680 HistGB: 0.14648 (AutoGluon: 0.14592) (Ensemble: 0.14514) Classification with custom Regression Head Prajwal Anagani introduced an unusual approach üëã: Using an XGB Classifier with 'multi:softprob' objective but adding a Softmax-based regression head. I still don't know why it works (maybe due to the synthetical nature of the dataset and the discrete regression target) but the optimized ensemble weights include it despite worse individual CV results in comparison to (traditional) XGB regressors. What did not work Polynomials, more sophisticated stacking/blending/meta-model approaches, classic ML approaches, Stratified KFold, any kind of encoding of 'Sex' feature (except OHE for models that don't accept categorical features natively). High Value Targets My models never predicted more than a maximum of 20 Rings (with 29 being the maximum in the train dataset). Any approach to tackle that gap (SMOTE, augmentation) worsened CV significantly. So did excluding the outlier samples from training. What I learned in the competition: AutoML approaches work (AutoFE, AutoGluon) but require manual post-processing to achieve competitive results. The Kaggle API is awesome for writing code locally in PyCharm and having it executed at Kaggle with a few clicks. Wrap often-used code (e.g. Trainer for CV and prediction) into a custom Python Package to guarantee comparable results when experimenting. Kaggle API helps here, too, to automate deployment. Thanks to the authors and other Kagglers who shared lots of great insightsüëç Ensembling Feature Engineering Gradient Boosting Optimization Regression 51 15 1 6 Please sign in to reply to this topic. comment 47 Comments 3 appreciation  comments Hotness ms_island Posted 10 months ago arrow_drop_up 7 more_vert wow very good !! i'm korean ms_island Posted 10 months ago arrow_drop_up 3 more_vert wow good idea George Papachristou Posted a year ago ¬∑ 137th in this Competition arrow_drop_up 10 more_vert Congratulations for your winning üéä and thanks for sharing!! I just have two questions, why you said that stratified CV didn't work? Tha abalone dataset have Rings with very few samples, this target skewness will effect the CV scores by creating significant deviation. In my mind, either use CV with many folds (e.g 10+) or with few folds (e.g <4) and stratified on Rings will be equivalent. And the second part of my question is about the following sentence \"Use complete original Abalone dataset for training in each fold, never for validation.\". Can you elaborate more? It sounds weird to me to train with whole dataset for each fold, so maybe I'm missing something ü•π Prajwal Anagani Posted a year ago ¬∑ 339th in this Competition arrow_drop_up 2 more_vert I had tried Stratified too, and i my cv and lb scores were matching up very well. I think it might be because of lack of the less occuring labels in the test set. George Papachristou Posted a year ago ¬∑ 137th in this Competition arrow_drop_up 0 more_vert Thanks! @inagana what about  train with whole dataset for each fold? Prajwal Anagani Posted a year ago ¬∑ 339th in this Competition arrow_drop_up 0 more_vert I think it means that when training we use the original dataset, doesn't make sense to use a part of the original dataset for validation as the LB dataset will have a distribution similar to the synthetic dataset generated than the original one. So, maybe it's better used for training, but not a good validation data Prajwal Anagani Posted a year ago ¬∑ 339th in this Competition arrow_drop_up 4 more_vert Congratulations @stopwhispering ! Also, the XGB with softmax regression head ended up in my ensemble with a decent weightage (which I was not expecting because of the CV) as well I was always reluctant to use Wandb sweeps, but since it worked well for you, I'm going to give it a try sometime :) Johannes Heller Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert I prefer WandB Sweeps to Optuna & Hyperopt mainly because of the easy parallelization: Just copy your notebook up to 5 times, use same Sweep ID, and run it. Then check the interim results at WandB every now and then and when scores are no longer improving, cancel the kernels. Then try some good combinations either via copy&paste, API, or Excel export. The feature list, however, is somewhat smaller in comparison to Optuna. Still more than enough for a competition like this. Prajwal Anagani Posted a year ago ¬∑ 339th in this Competition arrow_drop_up 0 more_vert That's very intresting! It'd be great to speed up optimization tbh, a good amount of time goes in me optimising via Optuna. Will try that out in the current playground series :) Thanks! danzoda Posted a year ago ¬∑ 129th in this Competition arrow_drop_up 1 more_vert a huge number of models! great job Kaius de Paula Correa Posted a year ago ¬∑ 1431st in this Competition arrow_drop_up 1 more_vert Congratulations!! I'll be conducting a few changes to my solution inspired by what you've said here, so thank you for your insights!! Isaac Moura Posted a year ago ¬∑ 1570th in this Competition arrow_drop_up 1 more_vert Congratulations @stopwhispering , thanks for sharing Optimistix Posted a year ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert Congratulations on achieving the 1st place, and thanks for sharing your solution! I tried most of the same things, but model-specific feature selection is a simple yet brilliant idea that would have probably gotten me close to 0.14440, or under. I should look into WandB Sweeps and using AutoML more extensively in the future. Congrats once again! Mubashir Ahmed Siddiqui Posted a year ago ¬∑ 1606th in this Competition arrow_drop_up 1 more_vert Congratulations @stopwhispering ! Guilherme P Oliveira Posted a year ago ¬∑ 1227th in this Competition arrow_drop_up 1 more_vert Congratulations on the great results @stopwhispering ! Thank you very much for sharing your solution :) Kumar S Posted a year ago ¬∑ 1894th in this Competition arrow_drop_up 1 more_vert yes,  I also tried different neural network architectures only to observe that they could not reach beyond .15xx Cony Kuo Posted a year ago ¬∑ 228th in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for the informative sharing. Amaan Akhtar Posted a year ago ¬∑ 2005th in this Competition arrow_drop_up 1 more_vert Congratulations @stopwhispering ! Amal A P Posted a year ago ¬∑ 1980th in this Competition arrow_drop_up 1 more_vert Congratulations @stopwhispering !!!!! rrr584466 Posted a year ago arrow_drop_up 1 more_vert üëçwow very nice Minato Namikaze Posted a year ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Congratulations @stopwhispering Thanks for sharing the approach :) Mart Preusse Posted a year ago ¬∑ 130th in this Competition arrow_drop_up 2 more_vert @stopwhispering thanks for sharing your strategic approach, which involved a lot of effort. It is a well earned victory. Congratulations! MaxUhl98 Posted a year ago arrow_drop_up 0 more_vert Hi Johannes, Congratulations on your victory! Regarding your write-up, I have some questions: If you train the 5 models LGBM, XGB, CatBoost, HistGB, and RandomForest for 10 folds each, plus probably 1 AutoGluon model and a classificator with a custom regression head, how do you end up with only 49 models in the final ensemble? How did you generate or access the AutoGluon OOF predictions and ensure they predict the values for the same columns as the OOF predictions from the other models? (I assume that you did not train multiple AutoGluon models since AutoGluon does CV by itself.) Did you use scipy.minimize for Nelder-Mead and sklearn.SequentialFeatureSelector for SFS or are there better tools for the job? Thank you for sharing, and good luck in your future endeavors. Best regards, Max Logan Posted a year ago arrow_drop_up 0 more_vert Congratulations and thanks for sharing your approach. Zhousi Chen Posted a year ago ¬∑ 2352nd in this Competition arrow_drop_up 0 more_vert LightGBM is handy. Thanks for sharing the winning methodology! Ayush Kumar Posted a year ago arrow_drop_up 0 more_vert Great Work Kaizo ML World Posted a year ago arrow_drop_up 0 more_vert Hi Johannes, congratulations for this achievement and lots of success for your future competitions. You mentioned using OpenFE to get on fast lane for FE. Unfortunately they do not provide a lot of parametrization options or documentation, e.g. the number of estimators is hard coded and fixed. Did you just let it run for several hours or did you adjust the setup, for example to test the outputs of OpenFE. Can you provide the setup of OpenFE or some links in regards of this matter? Johannes Heller Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Hi Fb, in the absence of any proper API documentation I just ran OpenFE with the default setting. For the 8 original features, it generated 192 additional features. I was able to exclude some of them due to very high correlation to other features. For the rest, I ran sequential feature selection over night. With the competition's little amount of data this was perfectly feasible. For a real-world project with more original features and more sample rows, some other approach would probably be better. Generally it seems to be a suitable tool for a small project like this with little, flat, high-quality data. For everything else there are better alternatives. OpenFE seems to be no longer maintained, the last commits being from last year . Kaizo ML World Posted a year ago arrow_drop_up 0 more_vert Thank you for taking your time to respond. They seem to have published the code for the paper only. But it sure sounded promising. Have you used any other modules, e.g. Featuretools? Sheema Zain Posted a year ago arrow_drop_up 0 more_vert Congratulations for your winning ! uday guntupally Posted a year ago arrow_drop_up 0 more_vert Congratulations for winning thanks for sharing!!!! Nick Erickson Posted a year ago arrow_drop_up 0 more_vert Congrats on the win and thanks for sharing your approach!! I have added you to the AutoGluon Hall of Fame :) https://github.com/autogluon/autogluon/blob/master/AWESOME.md#2024",
      "Regression with an Abalone Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Regression with an Abalone Dataset Playground Series - Season 4, Episode 4 Regression with an Abalone Dataset Overview Data Code Models Discussion Leaderboard Rules Lennart Purucker ¬∑ 2nd in this Competition  ¬∑ Posted a year ago arrow_drop_up 11 more_vert 2nd Place Solution for the Regression with an Abalone Dataset Competition Heyho everyone, thanks for hosting the competition , and shoutouts to the active community ! Context for My Submissions I entered the competition close to the deadline to test my automated machine learning (AutoML) workflow, which I intended to use for the AutoML Grand Prix that started right after this competition ended. As it turns out, and to my surprise, my AutoML workflow performed really well without adjustments in this competition. In fact, it worked much better here than on the actual first Grand Prix Dataset because this dataset actually benefited from automated feature engineering. I will write up and share the code for the major parts of my workflow as part of my write-up for the Grand Prix. If possible, I will link my write-up here or post it as a comment later. EDIT : here is the write-up. Summary of the AutoML Workflow Automated Two-Sample Tests For Training Data Automated Feature Engineering (OpenFE + AutoGluon) Automated Model Selection (Customzied AutoGluon) 1. Automated Two-Sample Tests To Optimize Training Data Since I am rather new to actively participating in Kaggle competitions, I found the concept of having an additional potentially unreliable data source quite interesting. Thus, to determine whether I wanted to include the original data in my training data, I created several two-sample tests for original data, training from the competition, and test data from the competition. These tests are supposed to determine if a distribution shift exists. If any shift exists, including the original data in the training data might be harmful. I used AutoGluon to predict whether it can detect which dataset a sample is from and used a Mann-Whitney U Test to determine significance. I tested: (Kaggle+Original) Train Data - (Kaggle) Test Data (Kaggle) Train Data - (Original) Train Data (Kaggle) Train Data - (Kaggle) Test Data (Original) Train Data - (Kaggle) Test Data Based on my observations from these tests, I included the original data. I still need to formalize a way of automatically deciding this. The setup can be improved, and I want to improve it in the future by including tests assessing whether including data is estimated to improve performance. 2. ¬†Automated Feature Engineering (OpenFE + AutoGluon) I suspect this part was by far the most important in obtaining second place in this submission. In essence, what I did was: A) run a customized¬†version of OpenFE , and then B) use AutoGluon's feature selection to prune features. I played around with B) and used a LightGBM model as a proxy. For A), I first selected the potential feature candidates by hand to control what might be searched over by OpenFE and added a new type of feature generator. Then, I ran OpenFE with default settings, which got me to ca. 200 features. Finally, I automatically pruned them down using B) with a time limit of 1 hour. Sadly, I do not have all the details for the final feature generators, as my pipeline did not log their generator function. Moreover, while trying to make automated feature engineering work for the first AutoML Grand Prix dataset (spoiler: it was not worth the effort), my code base got a bit too messy to trace back my steps. I used the following code to control the feature generators: all_features = list (train_data.columns)\nsoft_ordinal = [f for f in all_features if (train_data[f].nunique() <= 100 ) and (f not in ordinal_columns)]\nnumerical_features = [f for f in all_features if f not in categorical_columns]\ncandidate_features = get_candidate_features(\n    numerical_features=numerical_features,\n    categorical_features=categorical_columns,\n    ordinal_features=ordinal_columns + soft_ordinal,\n    order= 1 , # 2 is likely impossible to use w/o time estimate. ) # Restrict Search Space of Candidate Features candidate_features = [\n    f for f in candidate_features if f.name in [ # \"abs\" -> dataset specific, not useful in most cases # \"log\" -> can be done by scalers, no need for GBDTs # \"sqrt\", -> see above (s.a.) # \"square\" , -> s.a. # \"sigmoid\" , -> s.a. \"freq\" , -> # do not like it but giving it a shot. \"round\" , \"residual\" , # \"max\", -> IMO, trivial to model for first-order features # \"min\", -> s.a. # \"+\", -> s.a. # \"-\", -> s.a. \"/\" , \"*\" , # \"GroupByThenMin\",  -> &nbsp;The essential benefit of GroupBy is captured with any of these, so I filtered this to reduce the search space. # \"GroupByThenMax\", -> s.a. # \"GroupByThenMean\", -> s.a. \"GroupByThenMedian\" , \"GroupByThenStd\" , # \"GroupByThenRank\", -> s.a. \"GroupByThenFreq\" , \"GroupByThenNUnique\" , \"Combine\" , # New Generators #   - Hacked into OpenFE by adding `new_data = int(d < d.quantile(X).max())` to the generator options. \"<p0.2\" , # X = 0.2 \"<p0.4\" , \"<p0.6\" , \"<p0.8\" ,\n    ]\n] content_copy 3. Automated Model Selection (Customzied AutoGluon) Finally, I used a \"tuned\" version of AutoGluon to select the final model. I will share all the details in my write-up for the AutoML Grand Prix. EDIT : here is the write-up. Of importance here was that I noticed that stacking worked really well for this dataset (with code related to dynamic stacking ). Thus, I set AutoGluon to fit up to 6 Layers for Stacking. In the end, it used 5 Layers, but looking at the final¬†scores of my submissions, I got the same score with only 3 layers, but layer 5 did not overfit either! I hope this (and the more detailed future write-up) provides a good overview of my solution. Moreover, I hope you are as excited for more and better automated feature engineering as I am ;) Best, Lennart 5 Please sign in to reply to this topic. comment 5 Comments 1 appreciation  comment Hotness MaxUhl98 Posted a year ago arrow_drop_up 1 more_vert Hi Lennart, congratulations for reaching 2. place! Since the book 'Get better at anything' suggests that beginners learn faster when copying I am trying to use this technique to improving my kaggling. Currently I am attempting to reproduce your solution, but trying to  copy your techniques made my results actually worse(.14517 is my best autogluon model which uses ALL openfe features without hack, .14555 with selected hacked features as described in your post, .14632 when i use your feature generation and a vanilla lgbm for feature selection). Is it possible for me to take a look at your code for this competition? This would greatly help me in my learning efforts, since I won't have to manually find out what exactly makes my solutions worse. Thanks in advance and keep up the great work, Max Lennart Purucker Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Heyo Max, I do not have a code snippet to replicate my results. Based on your description, I am unsure if you have used feature selection after OpenFE as I did for this challenge. This was quite important for this dataset. Moreover, feature selection can be done in many different ways and needs some tuning (besides a good validation loop). Here is a short code snippet for feature pruning, maybe that can help you: def run_autogluon_feature_pruning ( *,\n    train_data: pd.DataFrame, ) -> list [ str ]: from autogluon.core.models import BaggedEnsembleModel from autogluon.core.utils.feature_selection import FeatureSelector from autogluon.tabular.models.lgb.lgb_model import LGBModel # -- Input Data train_data = train_data.copy()\n    target_column = \"label\" n_features_original = train_data.shape[ 1 ] - 1 # -1 for target column y = train_data[target_column] # -- Problem Settings problem_kwargs = dict (\n        problem_type=, # TODO fill out eval_metric=, # TODO fill out )\n    time_limit = 60 * 5 # TODO n_sub_sample = 300000 # Get Proxy Model proxy_model = LGBModel\n    model_hps = dict (extra_tees= True )\n\n    proxy_model_kwargs = dict (\n        path=output_path,\n        **problem_kwargs,\n        hyperparameters= dict (\n            **model_hps,\n        ),\n    )\n    proxy_model = BaggedEnsembleModel(\n        path=output_path,\n        model_base=proxy_model,\n        model_base_kwargs=proxy_model_kwargs,\n        random_state= 42 ,\n    )\n    fit_kwargs = dict (\n        sample_weight=sample_weight,\n        k_fold= 4 ,\n        n_repeats= 1 ,\n        replace_bag= False ,\n    )\n\n    fs = FeatureSelector(\n        model=proxy_model,\n        time_limit= time_limit,\n        problem_type=problem_kwargs[ \"problem_type\" ],\n        seed=conf.run.seed,\n        raise_exception= True ,\n    )\n    candidate_features = fs.select_features(\n        X=train_data.drop(columns=[arget_column]),\n        y=y,\n        n_train_subsample=n_sub_sample, # Play around with these variables: prune_threshold= \"noise\" ,\n        prune_ratio= 0.15 ,\n        stopping_round= 4 ,\n        **fit_kwargs,\n    ) print ( f\"Original #feat: {n_features_original} , #feat after Autogluon Feature Pruning: { len (candidate_features)} \" ) print ( f\"Features:\" , candidate_features) return candidate_features content_copy Best, Lennart Reynard S Posted a year ago arrow_drop_up 1 more_vert Hey Lennart, Congratulations on getting the podium position in this competition. Thank you for explaining the workflow, I really appreciated it. As someone new to machine learning, this is my first encounter with AutoML, it seems to be very interesting, especially the Automated Feature Engineering part. I may not be all too familiar with the concepts yet, but, I will be looking forward to your AutoML Grandprix write up for more details. Best of luck for the next competition! Lennart Purucker Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Hey Lennart, Congratulations on getting the podium position in this competition. Thank you for explaining the workflow, I really appreciated it. As someone new to machine learning, this is my first encounter with AutoML, it seems to be very interesting, especially the Automated Feature Engineering part. I may not be all too familiar with the concepts yet, but, I will be looking forward to your AutoML Grandprix write up for more details. Best of luck for the next competition! Thank you! Appreciation (1) Wijayanto, Arvin Posted 9 months ago arrow_drop_up 0 more_vert thankyou for your sharing!",
      "Regression with an Abalone Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Regression with an Abalone Dataset Playground Series - Season 4, Episode 4 Regression with an Abalone Dataset Overview Data Code Models Discussion Leaderboard Rules LuminousC ¬∑ 3rd in this Competition  ¬∑ Posted a year ago arrow_drop_up 11 more_vert 3rd Place Solution for the Regression with an Abalone Dataset Competition Thanks for holding this excellent machine learning game. I'm happy here to share my learning and experience during the match. Context Business context: https://www.kaggle.com/competitions/playground-series-s4e4 Data context: https://www.kaggle.com/competitions/playground-series-s4e4/data Overview of the approach Two key factors that contribute to higher position in the final leaderboard are: Feature engineering strategy Model ensemble way For feature engineering, I applied a customized version of OpenFE in several of final sub-models. More details will be covered in later section. The final model was a ensemble of 6 models as below: AutoGluon selected model with raw feature AutoGluon selected model with OpenFE* 30 features AutoGluon selected model with OpenFE* 30 features plus pseudo label data Ensemble of 3 sub-models(XGB,LGB,CAT) with Optuna tuned weights Ensemble of 3 sub-models(XGB,LGB,CAT) with Voting regressor Best ensemble of LGB models from this post The NN ones published in \"Code\" forum is good but finally I decide to not include it as it doesn't perform significantly better than other candidates in local CV set. Details of the submission Cross-validation is first key In this playground series, the public leaderboard is tested on approximately 20% of the test data while the private leaderboard is tested on approximately 80% test data. This usually lead to different performance on both boards. Hence, to build a reliable cross-validation way is almost first key in solutions. Start with AutoFE and end with careful feature selection In feature engineering part, I made some modification on OpenFE for the initial feature engineering to handle the RMSLE metric. Of course, you could apply the np.log1p and np.expm1 to achieve the same effect. After having initial feature candidates, it'd better to do careful feature selection because: reduce feature dimension to avoid overfitting avoid long training time make model more general to unseen samples In my solution, I applied RFE (Recursive Feature Elimination) to select important features with a limited size (30 in my case) with Logistic Regression. The most important thing is to apply the feature selection process on a cross-validation way. If only applied on a single fold, the selected feature set could be not generalized enough on different data. It could be more stable by average the feature importance on multiple folds. Ideas Tried But Not Working Classification model instead of regression model More weight on original data instead of synthetic data Some Suggestions To Beginners Everyone could have chance to win in the competition once you're well-prepared. Below are some general guidance that may help you improve the model performance: Data study: methods like feature correlation analysis, model error analysis could bring you new feature or ideas. Model tuning: even with same model, different hyper parameter may lead to huge performance gain. Selection and ensemble: trying different model combination is one direct way to improve the result. Learn from others: engage with Kaggle community, participate in forums and learn from others' posts or codes. Wish all of you have a good journey in those competitions. 2 3 2 Please sign in to reply to this topic. comment 2 Comments 1 appreciation  comment Hotness Abdullah An Nu'man Shanto Posted a year ago arrow_drop_up 0 more_vert Thanks for giving such a clear understanding. Appreciation (1) Willy Wang Posted a year ago ¬∑ 342nd in this Competition arrow_drop_up 0 more_vert Very detail ! thanks for ur sharing~",
      "Regression with an Abalone Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Regression with an Abalone Dataset Playground Series - Season 4, Episode 4 Regression with an Abalone Dataset Overview Data Code Models Discussion Leaderboard Rules Bertan Pank ¬∑ 4th in this Competition  ¬∑ Posted a year ago arrow_drop_up 13 more_vert 4th Place Solution for the Regression with an Abalone Dataset My approach was pretty simple. So, I will keep it short. All I did was to improvise this notebook : OpenFE Target log transformation AutoGluon (with custom RMSLE metric and only tree-based models) Average of this notebook's output and mine (yeah, it's 50-50) That's about it. To be honest, I didn't expect it to perform this well.  From what I have observed so far, AutoGluon performs really well on the datasets that has at least 50-60k observations. 1 Please sign in to reply to this topic. comment 2 Comments Hotness Thiago Mantuani Posted a year ago ¬∑ 244th in this Competition arrow_drop_up 0 more_vert Congratulations on the placement, if possible share with us your notebook with the solution @bertanpank Aaradhya Badal Posted a year ago ¬∑ 881st in this Competition arrow_drop_up 0 more_vert I think the author shared the notebook in the post itself as a hyperlink, here it is: https://www.kaggle.com/code/mustafakeser4/ps-s4e4-autogluon Too many requests error Too many requests",
      "Regression with an Abalone Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Regression with an Abalone Dataset Playground Series - Season 4, Episode 4 Regression with an Abalone Dataset Overview Data Code Models Discussion Leaderboard Rules Minato Namikaze ¬∑ 5th in this Competition  ¬∑ Posted a year ago arrow_drop_up 32 more_vert 5th Place Solution | Learnings Hello All, This was a stable competition with less shake up because of the large volume of data and the metric RMSLE. I'm happy to share a note of my work in this competition. Firstly, the competition was very similar to Crab Age Prediction , Interestingly I was Public LB 1 in that competition and fell to 40 in private. Hence I used the similar approach to estimate new features. I'd like to thank two public notebooks for their amazing work that helped me in this competition: 1) Notebook by mfmfmf3 , replacing the work with my feature engineering resulted in a really good score that was used to blend later. 2) Notebook by igorvolianiuk Many high scoring public notebooks were available but their score were a result of a blend from some other work and that is the only reason for not using them. Approach Methodology Details New Features Top Surface Area, Water Loss, Measurement Ratios, Abalone Density, BMI Feature Engineering Discrete & Categorical Encoding, Transformations on Numerical Features Models XGBoost, CatBoost, LightGBM, ANNs Learnings 15 fold & 20 fold Cross validation increased my score than 5 fold. A lot of importance was given on tuning hyper-parameters and that worked well. I have improved a lot in using ANNs in ensemble modeling. I have only used Harmonic Mean to blend results, this is really important to give an edge in the private LB and avoid overfitting. Thank you all for the support & Wish you all the best in the next competition! Happy Learning! Regression Beginner Intermediate 1 1 Please sign in to reply to this topic. comment 3 Comments Hotness Guilherme P Oliveira Posted a year ago ¬∑ 1227th in this Competition arrow_drop_up 2 more_vert Thank you very much for your contribution @arunklenin , I am learning a lot from you!! Ravi Ramakrishnan Posted a year ago ¬∑ 21st in this Competition arrow_drop_up 2 more_vert All the best for the auto ML grand prix and upcoming playground episodes @arunklenin Minato Namikaze Topic Author Posted a year ago ¬∑ 5th in this Competition arrow_drop_up 2 more_vert Thank you @ravi20076 , wish you the best :) Ravi Ramakrishnan Posted a year ago ¬∑ 21st in this Competition arrow_drop_up 2 more_vert Considering the nature of the auto ML grand prix and the time factor, I wonder if we could witness regular and concerted participation there! It seems a bit difficult for most working professionals and students alike to reserve substantial time in 24 hours to make a meaningful solution. The start of month date is usually a busy period at work as well! I shall evaluate my time availability and make a personal decision for the auto ML competition @arunklenin Minato Namikaze Topic Author Posted a year ago ¬∑ 5th in this Competition arrow_drop_up 2 more_vert I agree, luckily it's a holiday in India today but might not be the case every month. I have personally avoided using auto ML approaches just for the sake of learning and getting my hands dirty, although I understand it's prominence and time saving factors. I shall give a try @ravi20076 Too many requests error Too many requests",
      "Regression with an Abalone Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Regression with an Abalone Dataset Playground Series - Season 4, Episode 4 Regression with an Abalone Dataset Overview Data Code Models Discussion Leaderboard Rules EISLab_hwlee ¬∑ 8th in this Competition  ¬∑ Posted a year ago arrow_drop_up 9 more_vert 8th Place Solution | So simple! feature engineering https://www.kaggle.com/code/arunklenin/ps4e4-abalone-age-prediction-regression training Using Autogluon with hyperparameter tuning hyperparameter_tune_kwargs = {\n'num_trials': 5,\n'scheduler' : 'local',\n'searcher': 'auto',\n} And training setting predictor = TabularPredictor(\nlabel='Rings',\neval_metric='root_mean_squared_error',\nproblem_type='regression',\n)\npredictor.fit(\ntrain,\npresets='best_quality',\ndynamic_stacking=False,\nnum_stack_levels=2,\nexcluded_model_types=['KNN', 'NN_TORCH', 'FASTAI'],\ntime_limit=3600*24,\nhyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\nkeep_only_best=True,\n) ensemble notebook from : https://www.kaggle.com/code/trupologhelper/ps4e4-lightgbm-only autogluon 1 Please sign in to reply to this topic. comment 0 Comments Hotness Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 4, Episode 4 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Abalone dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 8.4 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 8.4 MB sample_submission.csv test.csv train.csv 3 files 21 columns ",
    "data_description": "Regression with an Abalone Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Regression with an Abalone Dataset Playground Series - Season 4, Episode 4 Regression with an Abalone Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: The goal of this competition is to predict the age of abalone from various physical measurements. Start Apr 1, 2024 Close May 1, 2024 Evaluation link keyboard_arrow_up The evaluation metric for this competition is Root Mean Squared Logarithmic Error . The RMSLE is calculated as: ‚àö 1 n n ‚àë i = 1 ( log ( 1 + ÀÜ y i ) ‚àí log ( 1 + y i ) ) 2 where: n is the total number of observations in the test set, ÀÜ y i is the predicted value of the target for instance (i), y i is the actual value of the target for instance (i), and, log is the natural logarithm. Submission File For each id row in the test set, you must predict the target, Rings . The file should contain a header and have the following format: id ,Rings 90615 , 10 90616 , 10 90617 , 10 etc . content_copy Timeline link keyboard_arrow_up Start Date - April 1, 2024 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  April 30, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Regression with an Abalone Dataset. https://kaggle.com/competitions/playground-series-s4e4, 2024. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 5,859 Entrants 2,719 Participants 2,606 Teams 21,531 Submissions Tags Beginner Tabular Regression Mean Squared Log Error Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation"
  },
  {
    "competition_slug": "playground-series-s4e5",
    "discussion_links": [
      "/competitions/playground-series-s4e5/discussion/509043",
      "/competitions/playground-series-s4e5/discussion/509410",
      "/competitions/playground-series-s4e5/discussion/509042",
      "/competitions/playground-series-s4e5/discussion/509044"
    ],
    "discussion_texts": [
      "Regression with a Flood Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Regression with a Flood Prediction Dataset Playground Series - Season 4, Episode 5 Regression with a Flood Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules aldparis ¬∑ 1st in this Competition  ¬∑ Posted a year ago arrow_drop_up 106 more_vert #1st place solution Thank's to Kaggle for organizing tabular competitions like this one. I learned a lot from those competitions. I learned a lot also from public notebooks and discussions, thank you everyone for sharing so much. Because this dataset contains a sum of Poisson distributions, the winner of this competition had to be from France. Thank you M. Simeon Denis Poisson for giving us such a tool, and thank you for giving me a extra motivation during this competition. Thank you to all competitors, you are really challenging, playing with you is a pleasure. Congrat's to @mdoroch (I will contact you, you were very impressive), @lashfire , @mattop , @tilii7 : your submissions during last days  stressed me so much ! My solution is  : an ensemble ajusted with a ridge regression positive = False, fit_intercept = False . and ajusted with 3 repeated oof predictions of a variety of models produced with catboost, xgboost, lightgbm and 2 public notebooks Auto Gluon Starter by @mfmfmf3 available https://www.kaggle.com/code/mfmfmf3/autogluon-starter Flood Prediction LGBM by @igorvolianiuk https://www.kaggle.com/code/igorvolianiuk/flood-prediction-lgbm EDA helped me to understand that original dataset was completly different from train dataset. EDA helped me to understand that the sum of 16 of 17 orginal features was still a Poisson distribution in train dataset, but sum of 18, 19, and 20 original features was not (neither in test dataset). And EDA helped me to find interesting features, correlated to target (thank you @ambrosm for the most important feature) I took care for my own GBMs to feature selection : usefull features : sum of each lign, std of each lign, max of each lign, sorted original features (thank you @siukeitin for Sorting along the feature axis as \"feature engineering\" 's discussion, number of variables which have a value higher than 6, or than 7, or than 8 (count features) : useless features : original features, skewness of each lign, kurtosis of each lign, redondant features. I dropped them. magic feature : because target could be seen as a discrete variable, I used a target encoder , and train.groupby(\"sum\")[\"FloodProbability\"].std() was an interesting feature (where train[\"sum\"] = train[test.columns].sum(axis=1) ) I used permutation feature importance technique explained in scikit-learn or in Kaggle to detect useless features. I used backward technique too. Variety of models I trained more than 30 GBM's, with various sets of features : sometimes with sorted original features, or sometimes with count features \"nb_inf4\", \"nb_inf3\", \"nb_inf2\", \"nb_sup6\", \"nb_sup7\", \"nb_sup8\" , sometimes with target encoder, sometimes without. I used also a target transformation sometimes : there was a strong signal (sum), and a noisy signal (deviance from sum). My transformed target was : train[\"target_transf\"] = train[\"FloodProbability\"] - df[original_features].mean(axis=1) * 0.1 or train[\"target_transf\"] = (train[\"FloodProbability\"]*400 - train[original_features].mean(axis=1) * 40).astype(np.int16) Thank's to @act18l for https://www.kaggle.com/competitions/playground-series-s4e5/discussion/499263#2787165 During the first 10 days, I used only default parameters (with early stopming) and didn't optimize any GBM, concentrated on my features and my ensemble loop. After, hyperparameters were optimized with optuna by using only my kaggle quota of GPU. I optimized only the following hyper parameters (with max timeout = 5400 seconds, it's a lot) : depth for catboost and xgboost, num_leaves for lightgbm, alpha, lambda, min_child_weight or min_child_samples for xgboost and ligntgbm bagging_temperature, random_strength for catboost subsample, colsample_by_node for xgboost and lightgbm I used default grow_policy of each GBM method (xgboost, catboost and lightgbm have each a different default grow_policy value, it gives variety of predictions), I used default learning_rate or .1, I used early_stopping (od_wait in catboost) to fit n_iterations. Ensemble During the first two weeks, I trained my ensemble with LinearRegression and positive = True and fit_intercept = False. Until I tried Ridge and positive = False (still fit_intercept = False) which gave me a little improvement. To have a robust ensemble, I trained all my models on 3 repeated kfold, and select by CV the features for my ensemble. At the end of week 2, I made a submssion with a blind blend using https://www.kaggle.com/code/mfmfmf3/autogluon-starter output. I had a .86939 public score better than .96934 my previous one. Few days later, I had a first complete oofs file from AutoGluon. Then I fitted carefully a new ensemble, and get .86941 score at the end of week 3. Then I decided to get 3 oofs AutoGluon files and to fit AutoGluon with some other features (3 more oofs). I trained in parallel autogluon here to get 6 oofs files. And then I fitted my final ensemble which score .86943 on public LB. In conclusion, I have success by combining : EDA feature engineering variety of GBM AutoGluon a strong CV for my Ridge ensemble I'm proud to have made only 2 submissions between the 18th of may and the end, to win on public and private LB. My CV was highly reliable to public LB since the first day, I trusted my CV and didn't need to submit. My final ensemble is available here https://www.kaggle.com/code/adaubas/pss4e05-ensemble-with-ridge . I'm so happy ! Waouh 46 6 4 1 Please sign in to reply to this topic. comment 56 Comments 1 appreciation  comment Hotness Vikram12301 Posted a year ago arrow_drop_up 3 more_vert Hey, Can you explain how finding sum of the features as a single poisson distribution help you? Krens Posted a year ago ¬∑ 247th in this Competition arrow_drop_up 3 more_vert Congratulations on your 1st place win! Your detailed explanation and transparency about the process are incredibly valuable for the community. Your approach of using an ensemble adjusted with ridge regression, and combining models from CatBoost, XGBoost, LightGBM, and public notebooks, clearly highlights the power of diverse methodologies. The feature engineering techniques, especially understanding the Poisson distribution characteristics of the dataset, are impressive. I found your insights into the EDA particularly useful. Identifying that the sum of certain features maintained a Poisson distribution while others did not, and leveraging that for feature selection and engineering, is a brilliant strategy. The use of permutation feature importance and backward techniques for feature selection is also a great takeaway. Your strategy of focusing on features and ensemble loop during the initial days, and only later optimizing hyperparameters using Optuna, showcases a disciplined and effective approach to problem-solving. The final ensemble strategy, incorporating multiple OOF files and a strong CV approach, is clearly a winning formula. It's amazing to see how trusting your CV paid off, allowing you to make minimal submissions and still secure the top spot. Nick Erickson Posted a year ago ¬∑ 9th in this Competition arrow_drop_up 3 more_vert Congrats @adaubas on the win, and welcome to the AutoGluon Hall of Fame ! aldparis Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert Hi @innixma , Thank you for the link !  and the citation : it's you of course ? üòÄ Even if AutoGluon kept me busy the last 2 weeks (how to get oof predictions ?), and even if I admit that I am changing my mind about automl after having experimented with AutoGluon in this competition, I'd rather to say that's a mix of good models which made me win : final parsimonious ridge regression, many GBM, and AutoGluon (close to the top) : I was opportunistic by using a copy of a @mfmfmf3 's notebook (my copy is available here ). @mfmfmf3 was the real promoter of AutoGluon in this competition. Tilii Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 3 more_vert Great job and thank you for a detailed explanation. I don't think it was Ridge or positive=True that were particularly important here. We are talking about miniscule differences on the 5th decimal place. I think you likely had a couple of better individual models, and that was it. aldparis Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Thank you @tilii7 , I really got CV improvement by using Ridge instead of LinearRegression, by using positive = False instead of positive = True, in this competition for this dataset. I can't explain why, that's my CV which told me that. Sergey Saharovskiy Posted a year ago ¬∑ 149th in this Competition arrow_drop_up 4 more_vert That's a lot of sweat, literally squeezed atoms. Fact check:  the man from France predicts floods 0.00007 (actually fits R2) more accurate than anybody else. :) kartikey bartwal Posted a year ago ¬∑ 1201st in this Competition arrow_drop_up 1 more_vert You're amazing too!!! The best part is this healthy competition on Kaggle. The competition is tight (everyone competing to increase R2 by 0.002) but everyone is regardless are so heartwarming and collaborative. Best wishes to you :) aldparis Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert thank you @kartikeybartwal , aldparis Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Thank you @sergiosaharovskiy , you are right ! about sweat and sqeezed atoms, Ivan Radko Posted a year ago arrow_drop_up 1 more_vert Hi, thank you very much for this post! I want to ask about Poisson distribution. How did you detect that sum of features have Poisson distribution? I used the Kolmogorov-Smirnov test and p-value <<0.05 and this rejects the hypothesis that distribution is Poissonian , but more of features have mean ~ std^2 this may indirectly indicate that the distribution is indeed Poissonian‚Ä¶ aldparis Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Hi @ivanrad17 , It was admitted since the beginning of the competition that columns looked like Poisson : https://www.kaggle.com/competitions/playground-series-s4e5/discussion/499244 Pran Posted a year ago ¬∑ 910th in this Competition arrow_drop_up 1 more_vert Congrats!!! Muhammad Azeem Posted a year ago arrow_drop_up 1 more_vert Congratulations on your win @adaubas Fatima Azfar Ziya Posted a year ago arrow_drop_up 1 more_vert Congratulations! This writeup was very insightful and helpful. Abdulaziz Ergashev Posted a year ago arrow_drop_up 1 more_vert Congrats on your win @adaubas Muhammad Azeem Posted a year ago arrow_drop_up 1 more_vert Congratulations on your 1st place win! Your detailed breakdown of the process is incredibly insightful. Your strategy of using an ensemble adjusted with ridge regression, and combining models from CatBoost, XGBoost, LightGBM, and public notebooks, showcases the power of diverse methodologies. The way you identified and leveraged the Poisson distribution characteristics of the dataset for feature selection is particularly impressive. Your disciplined approach to focusing on feature engineering and ensemble loop before hyperparameter optimization with Optuna clearly paid off. The final ensemble strategy, incorporating multiple OOF files and a strong CV approach, is a testament to your skill and meticulousness. Well done on a well-deserved victory! sihe blu Posted a year ago arrow_drop_up 1 more_vert Congratulations! üéâüéâ George Papachristou Posted a year ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert CongratulationsüéâüèÜ @adaubas !! Nice explaination!! I have a question about the following Few days later, I had a first complete oofs file from AutoGluon. Then I fitted carefully a new ensemble, and get .86941 score at the end of week 3. . Can you elaborate how you create the oofs file with AutoGluon and what do you mean with fitted carefully a new ensemble ? Did you fitted in the oof files? aldparis Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Hi @mscgeorges I made public my autogluon notebook, I hope everything is explained inside : https://www.kaggle.com/code/adaubas/pss4e05-copy-autogluon-starter Willy Wang Posted a year ago ¬∑ 399th in this Competition arrow_drop_up 1 more_vert I first learned about permutation feature importance. Thanks for sharing~~ Light_Shot Posted a year ago ¬∑ 2073rd in this Competition arrow_drop_up 1 more_vert Very nice. Congratulations. It requires lots of efforts. Mubashir Ahmed Siddiqui Posted a year ago arrow_drop_up 1 more_vert congratulations! sun9sun9 Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for sharing your solution.  I believe your effort is enough to take first place. Paulo Robinson Giaciani (pt-BR) Posted a year ago ¬∑ 205th in this Competition arrow_drop_up 1 more_vert Congratulations! üëèüèªüëèüèªüëèüèªüëèüèª Muhammed Tausif Posted a year ago arrow_drop_up 1 more_vert Congratulations indeed. @adaubas Yan Teixeira Posted a year ago ¬∑ 74th in this Competition arrow_drop_up 1 more_vert Congratulations, my friend! You were very strong throughout the competition. Thank you for the amazing wrap-up. aldparis Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks @yantxx for the compliment, Mart Preusse Posted a year ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Thanks for sharing your solution in such detail, @adaubas . The victory is well earned. bhaskar9221 Posted a year ago ¬∑ 1390th in this Competition arrow_drop_up 1 more_vert Congratuations for the win :) ! I am currently a newbie in ML, and after participation in this competition was really helpful for me. I learned a lot from people like you sharing and discussing and all. btw my score was 0.85806 turtle Posted a year ago ¬∑ 161st in this Competition arrow_drop_up 1 more_vert @adaubas Congratulations for securing 1st! Thanks for sharing your solution. Cristhian Ccala H. Posted a year ago ¬∑ 1200th in this Competition arrow_drop_up 1 more_vert Congratulations on your victory! Your dedication and focus on EDA and feature engineering are truly inspiring. Thank you for sharing your process; it will be very helpful to the community. Enjoy your success! aldparis Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 4 more_vert Thank you @cristhianccalah , happy that my words are inspiring, i'm really convinced, I'd rather spend my time on EDA and feature engineering and CV than waste time to try many several blind blends on public LB ! (even if I submit one very important 2 weeks ago) And pandas, numpy, scipy, mathplotlib / seaborn, scikit are so powerfull tools to do EDA and to understand data ! lash_fire Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Firstly,Congratulationsü•á! Thanks for this detailed and comprehensive solution. I particularly love exploring the EDA and Feature Engineering parts; they were truly fascinating. I wish I had spent more time on feature engineering and EDA. It was a pleasure competing against you! üòÑ aldparis Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert thank you @lashfire it was a pleasure too to compete against you, you and @mdoroch were really challenging ! I hesitated to contact @mdoroch when he was looking for a teammate ! You two were a great team ! Congratulations ! Too many requests error Too many requests",
      "Regression with a Flood Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Regression with a Flood Prediction Dataset Playground Series - Season 4, Episode 5 Regression with a Flood Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules lash_fire ¬∑ 2nd in this Competition  ¬∑ Posted a year ago arrow_drop_up 27 more_vert #2nd Place Solution(Team Peaky Blenders): Blends Of Blends. Thank you to Kaggle for organizing tabular playground competitions. I've learned a great deal from participating in these competitions. I also gained valuable insights and modeling from public notebooks and discussions. Thank you to everyone for sharing so much knowledge. Thank you @mdoroch for teaming up , I had wonderful time working with you and your contribution were incredible for our teamüòÄ! Our Solution: For feature engineering, Features suggested by @ambrosm and @siukeitin were used and features from top notebook Team Oxygen: AutoML 2nd Place Our solution contained of total of 56(excluding autogluon) .These 56 models contained models hyperparamter from public notebooks to be specific, (hyperparamters from these notebooks) - OpenFE + Blending + Explain - S04E05 | Flood Prediction | Ensemble Flood Prediction Regression LGB XGB CAT [0.86933] PG S4E05 - Cross-Validation madness v1 Thanks for you contributions @trupologhelper ,@suharkov ,@ravaghi ,@aspillai.They were truly helpful! Apart from that, all others models(XGB,CATBoost,LGBM) were fine tuned with different combination of grow_policy , tree_method , objective , sampling_method ,etc. Our best single model score was around 0.86933 ,which was not much so I thought ensembling is a way to go. - Ensembling: The ensemble model used was LinearRegression() . To increase the score further, we created different subsets of features/models and determined weights using LinearRegression(), adding them to the pool of OOF predictions. Finally, we performed Forward Feature Selection on all features, including all the engineered subset features. In this solution, we couldn't get OOF predictions from Autogluon because we had less time to get it and also we were facing some version file naming error while loading the autogluon model. I wish we had OOF predictions from autogluon. Nevertheless, we did blended it with best solution from @mdoroch (0.86940) which in turn had autogluon in it. Again, I want to thank @mfmfmf3 and @meloncc for providing high scoring autogluon solution. Our final ensemble weight was 0.6* Best solution from above + 0.4* @mdoroch solution= 0.86943 ,(Private LB: 0.86902 ) @mdoroch solution contained weighted ensemble of his own tuned ensemble of XGB,LGBM,CATBoost and PyBoost and two top public autogluons. Ensemble weights\nngb                 - 0.011743 xgb_params8         - 0.058489 lgb_params_bestcv 0.064538 lgb_params_serial   - 0.112808 lgb_et_params       - 0.024884 lgb_params4         - 0.032236 lgb_dart            - 0.064403 lgb_params6_goss 0.027780 cat_params_t1       - 0.024164 gb_params1          - 0.061639 weighted_sum 0.302751 lgbm_oof_preds 0.078312 xgbrf_oof_preds     - 0.027127 model3              - 0.067031 model5 0.056901 p_xgb_              - 0.061300 gamma_xgb           - 0.073125 weighted0 0.500579 weighted2 0.122773 weighted3 0.279514 weighted1 0.187362 content_copy weighted0,weighted1‚Ä¶etc are the weighted ensemble of subsets of features. ngb(Natural Gradient Boosting),Linear Tree Regression,Linear Forest Regression and Linear Boosting Regression these are some new model which I used , without tunning and they had small contribution overall. Linear Tree . Thanks for everyone !!! Please sign in to reply to this topic. comment 2 Comments Hotness Chinmoy Dutta Posted a year ago arrow_drop_up 3 more_vert Hii @lashfire mind connecting on Linkedin? U dont have urs mentioned on profile so asked here. Sorry for the trouble! lash_fire Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Hey, I would be happy to connect!(It is mentioned in my kaggle profile too) LinkedIn Profile Chinmoy Dutta Posted a year ago arrow_drop_up 2 more_vert So sorry i must have missed it‚Ä¶ Too many requests error Too many requests",
      "Regression with a Flood Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Regression with a Flood Prediction Dataset Playground Series - Season 4, Episode 5 Regression with a Flood Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules Tilii ¬∑ 3rd in this Competition  ¬∑ Posted a year ago arrow_drop_up 25 more_vert #3 solution - A blend of 57 models My solution was a Lasso blend of 57 models, some of which were AutoGluon ensembles. See model coefficients below. After about 5 models there were really no changes except on the 5th or 6th decimal place. The two best individual method models (LigtGBM and CatBoost) were at private LB score 0.86890, while the best AutoML ensemble was created by AutoGluon (private score 0.86893). The master ensemble is barely better at private LB score 0.86902. But hey, we are competing here. Good correlation between CV scores and public/private LBs. 1 Please sign in to reply to this topic. comment 22 Comments Hotness Krens Posted a year ago ¬∑ 247th in this Competition arrow_drop_up 3 more_vert Congratulations on securing the #3 spot with your impressive solution! Your approach of blending 57 models using Lasso regression demonstrates the power of ensemble methods. The visualization of model coefficients is particularly insightful. It clearly shows the contributions of each model in your ensemble, highlighting the importance of models like LightGBM and CatBoost, as well as the effectiveness of the AutoGluon ensemble. It's interesting to see that after about 5 models, further additions only made marginal differences. This emphasizes the importance of a strong core set of models in an ensemble. Your private leaderboard score of 0.86900, slightly above the individual best models, showcases the precision and fine-tuning of your approach. George Papachristou Posted a year ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert Congrats @tilii7 üéâ!!! Thanks for sharing, its more important to me to learn from the forum rather than the final position and I really appreciate solution insights!! For my end, I have two questions, as I'm new in this \"Kaggle style ensembling\", because in real-life it's not applicable this kind of modeling: Did you include also the models with negative coefficients in your final ensemble? What were the difference between AutoGluon ensembles? Did you mean you train different AutoGluon models for different folds? Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Did you include also the models with negative coefficients in your final ensemble? Yes. If you look through the graph at the top of this page, about 40% of models were given negative coefficients. What were the difference between AutoGluon ensembles? The main difference in AugoGluon ensembles was the starting dataset. I explained that in detail here . George Papachristou Posted a year ago ¬∑ 13th in this Competition arrow_drop_up 0 more_vert @tilii7 Thanks for the explaination! lash_fire Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Congratulations! üéâ,It was a close competition. I guess their was very slightly difference in our scores. Thank you for providing the solution, we too had similar number of models! Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Well done in the last week to make leaderboard jumps. Glad to hear that you managed to navigate through many models. Chagin Posted a year ago ¬∑ 170th in this Competition arrow_drop_up 1 more_vert Oh god‚Ä¶57!! I stopped at 5 üòÇ. Congratulations! üòÅ @tilii7 Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Computers do all the ensembling work, whether it is 5 of 50 models. I was running another blend in parallel with 26 models that scored 0.86900. One can easily argue that it wasn't worth adding ~30 models to get a 0.00002 improvement, but on the private LB that meant going from somewhere between positions 8-12 to #3. Chagin Posted a year ago ¬∑ 170th in this Competition arrow_drop_up 1 more_vert it wasn't worth adding ~30 models to get a 0.00002 improvement I think it's worth it, consider the gap between higher ranks and lower ones are pretty small in this competition. Computers do all the ensembling work, whether it is 5 of 50 models. I do get what you mean, probably I do the ensembling part poorer than you. I always do fine-tuning for each model first before doing ensemble. I often run out of idea at around 5 models, lol. Ensemble is like the final thing i throw in after that. Mind giving hint how you get all those 57 models @tilii7 ???? 3 more replies arrow_drop_down √ñzg√ºr Deniz √áelik Posted a year ago ¬∑ 385th in this Competition arrow_drop_up 1 more_vert Congratulations!! I'm quite a beginner in Kaggle and ML too. I have to ask: how does blending so many models improve performance further? Doesn't it take more time? Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert Adding more models didn't improve the final score by much. The master blend had a better score by 0.00012 compared to best individual models. It would never be worth the effort in real life to make so many models for such a small gain. But we had a month to go, so why not? emoji_people melon-cc Posted a year ago ¬∑ 29th in this Competition arrow_drop_up 1 more_vert Congratulations, very impressive! It seems this competition was indeed very tough, and the differences between everyone were minimal. Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert The scores were close indeed. Having never worked with AutoML packages before, I learned in this competition that they work well on this type of data. Nice move up on the LB! Matt OP Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Nice work on 3rd place @tilii7 , this competition eventually ended up just being \"Will It Blend? That is the question.\" https://www.youtube.com/watch?v=lAl28d6tbko Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Yes, ensembling was the key. Too bad neural network solutions weren't more helpful. I had several of them in my final ensemble, but they didn't make much of a difference. NN models also had better LB scores than CV, which was unusual compared to others. Sergey Saharovskiy Posted a year ago ¬∑ 149th in this Competition arrow_drop_up 1 more_vert ahahahahahahha 57 Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Is something funny about the number 57 or that I put so much effort into it? The models were so uniform that almost no individual model made a difference. I say almost, because there was a feature that added a small improvement, but I will write about that separately. Sergey Saharovskiy Posted a year ago ¬∑ 149th in this Competition arrow_drop_up 1 more_vert It is an unconventional weapon code number 57, I mean ü§£, is it even legal?! Christian Barreto Posted a year ago ¬∑ 391st in this Competition arrow_drop_up 1 more_vert Congratulations!! Mart Preusse Posted a year ago ¬∑ 12th in this Competition arrow_drop_up 2 more_vert congratulations @tilii7 ! After about 5 models there were really no changes except on the 5th or 6th decimal place. That is what I observed to. I ended up with only 9 models and could find no model for further improvement. The funny thing is: not even my carefully and amazingly tuned Catboost predictions were in the ensemble, I ended with only autogluon and lgbm and one xgb. Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert Nice move upward on private LB! aldparis Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Congratulations @tilii7 and thank you for having shared your lasso solution. I tried Lasso too but without success, got -66 R2 with Lasso, I should have made an error ! I will try in another competition‚Ä¶ Your discussions were very challenging ! Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert It doesn't matter in the end what type of blending one used as long as something worked. There were many good discussions, and I learn something new in each competition. Muhammed Tausif Posted a year ago arrow_drop_up 0 more_vert Congratulations, thanks for sharing. It is very helpful. Mubashir Ahmed Siddiqui Posted a year ago arrow_drop_up 0 more_vert 57 models, wow! That's some serious dedication. Nice work! üòÅ Too many requests error Too many requests",
      "Regression with a Flood Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Regression with a Flood Prediction Dataset Playground Series - Season 4, Episode 5 Regression with a Flood Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules Matt OP ¬∑ 4th in this Competition  ¬∑ Posted a year ago arrow_drop_up 22 more_vert 4th Place Solution: Hill climbing through the noise Hi everyone, Thank you for another exciting Playground Series competition! Before presenting my solution, I'd like to suggest that a two-week duration might have been more appropriate for this competition. The final stretch felt like trying to squeeze out the last few drops of optimization. Now, onto my solution: The crucial strategy for this competition was to blend as many models as possible. This approach was necessary because we were essentially predicting noise. hillclimbers I experimented with various model blending methods, including Lasso and Ridge regression, but ultimately achieved the best results with hill climbing. I used my Python package, hillclimbers , to determine the optimal weights. Feature Engineering I utilized 3 sets of features to train my models, denoted by FE2 and FE3 seen in the bar chart of selected weights. These sets included the following features in different variations: Statistical features: mean, median, mode, max, min, standard deviation, skewness, kurtosis, quantiles, etc. I also included features for the counts of each unique value in a row which was originally proposed in the [AutoML Grand Prix] 1st place solution write-up . Hyperparameter turning I employed various sets of hyperparameters, some manually tuned and others optimized using Optuna. For example, my DecisionTreeRegressor ensemble utilized multiple hyperparameter sets, and the resulting predictions were then blended together. Link to the notebook. Please sign in to reply to this topic. comment 7 Comments Hotness Tilii Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 3 more_vert Nicely done! From the same set of models, I got 0.86901 with your hill climbing package vs 0.86902 with Lasso. It is not always this close, and I am talking about hill climbing in general rather than your approach to it. Yet I have been saying for a while now that hill climbing was a viable approach in this competition, and that proved to be true. Congratulations! lash_fire Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Congratulations, @mattop ! üéâ I also experimented with HillClimbing in the early stages of the competition and found it to be quite reliable. Your HillClimbing Library really streamlines the process, making it much more convenient to use. I wish I had given it a shot during the final stages. Great job! Frederic Nicholson Posted a year ago ¬∑ 1144th in this Competition arrow_drop_up 1 more_vert thank you very much for sharing your strategy. Thiago Mantuani Posted a year ago ¬∑ 171st in this Competition arrow_drop_up 1 more_vert @mattop Congratulations on the position, share the notebooks with us if possible. Matt OP Topic Author Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Hi @thiagomantuani , thank you. Unfortunately, my models are spread across many notebooks and it would not be feasible to publish them all. However, I have added a link to the notebook where you can view the ensembling process. aldparis Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Congratulations @mattop , When we discussed about Hill Climbing 2 weeks ago with @tilii7 , I was (secretly) convinced that Hill Climbing will lead to overfit in this competition, because there are too many parameters to estimate, although I had good results with Hill Climbing in some others competitions. Your positions on public and private LB prooves I was wrong ! Thank you ! Temitayo Raymond Adedipe Posted a year ago arrow_drop_up 0 more_vert congratulations @ do you accept mentees Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 4, Episode 5 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Flood Prediction Factors dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note: This dataset is particularly well suited for visualizations, clustering, and general EDA. Show off your skills! 3 files 104.17 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 104.17 MB sample_submission.csv test.csv train.csv 3 files 45 columns  Too many requests",
    "data_description": "Regression with a Flood Prediction Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Regression with a Flood Prediction Dataset Playground Series - Season 4, Episode 5 Regression with a Flood Prediction Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: The goal of this competition is to predict the probability of a region flooding based on various factors. Start May 1, 2024 Close Jun 1, 2024 Evaluation link keyboard_arrow_up Submissions are evaluated using the R2 score . Submission File For each id row in the test set, you must predict the value of the target, FloodProbability . The file should contain a header and have the following format: id ,FloodProbability 1117957 , 0 . 5 1117958 , 0 . 5 1117959 , 0 . 5 etc . content_copy Timeline link keyboard_arrow_up Start Date - May 1, 2024 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  May 31, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Regression with a Flood Prediction Dataset. https://kaggle.com/competitions/playground-series-s4e5, 2024. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 7,193 Entrants 2,932 Participants 2,788 Teams 21,519 Submissions Tags Beginner Tabular Logistic Regression R2 Score Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s4e6",
    "discussion_links": [
      "/competitions/playground-series-s4e6/discussion/515989",
      "/competitions/playground-series-s4e6/discussion/515980"
    ],
    "discussion_texts": [
      "Classification with an Academic Success Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Classification with an Academic Success Dataset Playground Series - Season 4, Episode 6 Classification with an Academic Success Dataset Overview Data Code Models Discussion Leaderboard Rules Matt OP ¬∑ 6th in this Competition  ¬∑ Posted a year ago arrow_drop_up 54 more_vert 6th Place Solution: Many model ensembles were detrimental? Hi everyone, My solution is simple: 5-fold StratifiedKFold for validation. Included the original dataset. Ensemble of XGB and LGBM. Optuna hyperparamter  tuning. I didn't dedicate as much time to this competition as I usually would because I noticed a significant amount of randomness in the outcomes. My assessment appears to be validated due to a new Kaggler @furgalhachaimajhi achieving 2nd place (congratulations btw!) with only a single submission. The most interesting aspect of this competition for me was that single models or very small ensembles seemed to perform exceptionally well. It reminds of a Playground competition over a year ago where I achieved 54th place with a single CatBoost model. Single Model Catboost PS S3 E4 My solution was heavily inspired by @rzatemizel 's notebook LGBM + CATB+ XGB + NN: Voting or Stacking? (make sure to upvote) which employs recursive feature elimination with cross-validation to determine the models to include in the ensemble. 26 Please sign in to reply to this topic. comment 21 Comments 1 appreciation  comment Hotness Sergei Tsibikov Posted a year ago ¬∑ 807th in this Competition arrow_drop_up 3 more_vert Thank you for posting your solution. I have also noticed this - I could not beat my Xgboost baseline without any hyperparameter tuning with stacking models made with 4-6 base learners. Anupam Yadav Posted a year ago ¬∑ 125th in this Competition arrow_drop_up 3 more_vert In my case single LightGBM has the best private score, better than the ensemble of 8 models that I chose. Also catboost performs worse than LightGBM though it score more on Public LB. The feature importances discovered by LightGBM and catboost also differ quite much. Not sure whether feature engineering improved or degraded the score. Shivam Jaiswal Posted a year ago ¬∑ 950th in this Competition arrow_drop_up 1 more_vert I tried feature engineering, but the result in reduced accurcy. Optimistix Posted a year ago ¬∑ 113th in this Competition arrow_drop_up 4 more_vert Congratulations! I too benefited from @rzatemizel 's wonderful notebook, & it helped me generate a solution which I didn't submit because the CV score was \"only\" 0.83568 - just found out via late submission that it would have scored 0.83959 on the private leaderboard. It used 14 models. Matt OP Topic Author Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 3 more_vert Thank you @optimistix and wow 0.83959 would have secured 3rd place. It's unfortunate, but it's a scenario that happens to everyone who participates in enough competitions. Optimistix Posted a year ago ¬∑ 113th in this Competition arrow_drop_up 2 more_vert I'm taking it as a coming of age moment, though time will tell :-) I also remembered you & @tilii7 these past two days as I finally played around with hill climbing (though not with your package) - I had an 18 model ensemble which gave me 0.835359 with hill climbing, and 0.83552 on the public LB. I didn't choose it, and it would have been 0.83849 on the private LB (so 8th or 9th place). In hindsight, it's plain to see that my best private LB results (none of which I chose) are ones that had a great CV-LB agreement in the 0.8350-0.8365 range, and were obtained with proven methods like stacking with a ridge classifier, hill climbing etc. ReyadGH Posted a year ago arrow_drop_up 1 more_vert congratulations, and thank you for discussing your solution! John Doe Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert Well, in my case my best submission was a 30-model ensemble. Best single model had 0.83471 CV, 30-model ensemble had 0.83571 CV. I'm not certain if I tried uploading that best single model but I doubt it would do better. I suppose using more models in ensemble would be even more effective. This ensemble went from 0.8359 public to 0.83903 private (that is about +750 places). rƒ±za temizel Posted a year ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Congratulations @mattop , and thank you for mentioning my notebook:) Harshit Sharma Posted a year ago ¬∑ 221st in this Competition arrow_drop_up 2 more_vert Great insights! It's fascinating how simpler models or small ensembles can sometimes outperform more complex approaches. Thanks for sharing your solution and congrats @mattop on your 6th place finish! Tazaki Posted a year ago ¬∑ 667th in this Competition arrow_drop_up 2 more_vert Thank you for the post. Barring the use of LGBM, all the points you mentioned worked for me, to a good extent I think. Most of the top rankers I noticed entered fewer submissions than the rest. Khadidja Brakta Posted a year ago ¬∑ 1069th in this Competition arrow_drop_up 2 more_vert i used lightGBM , it preforms better than other models , even making some feature engineering reduced the accuracy congratulations , thank u for explaining your solution Tilii Posted a year ago ¬∑ 62nd in this Competition arrow_drop_up 2 more_vert Good job with selecting your model for ensembling and congratulations on a nice jump upwards! The most interesting aspect of this competition for me was that single models or very small ensembles seemed to perform exceptionally well. Not necessarily. I had ensembles that were combining 3, 4, and 6 models and all of them bested individual models (CVs for 6 members were 0.832131-0.835529 and for the ensemble it was 0.836134). It is not a given that any two models will ensemble well, and that was especially difficult in this competition because of multiple classes. Obviously, it is even more difficult to select many models that will ensemble well, but I gave some pointers mid-way through the competition. Matt OP Topic Author Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Thank you as always @tilii7 , you provided some excellent information regarding ensembling in this particular competition. Ensembling is generally a great approach, but when discussing the performance of single models or very small ensembles, I should have made the distinction that they perform relatively very well. Itachi Uchiha Posted a year ago ¬∑ 1079th in this Competition arrow_drop_up 2 more_vert Hey @mattop , Congrats for 6th position, Quick Question for me, How did you merge your predictions on test set during your 5 folds validation? Matt OP Topic Author Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thanks @blade007 ! I retrained the model using the entire dataset, incorporating the original data, and subsequently generated final predictions on the test set, as demonstrated in @rzatemizel 's linked notebook. omid sakaki ghazvini Posted 9 months ago arrow_drop_up 0 more_vert With Outliers: Accuracy = 84% üëé Feature Engineering & Remove Outliers: Accuracy = 95% üëç NoteBook Tilii Posted 9 months ago ¬∑ 62nd in this Competition arrow_drop_up 1 more_vert It would be nice if that kind of score was possible on this dataset, but I don't think so. It is easy to overfit to train data and get high accuracy. Why don't you submit your prediction and tell us the score on private test data? omid sakaki ghazvini Posted 9 months ago arrow_drop_up 0 more_vert Thank you for your guidance üåª done. Metin Amedi Posted a year ago ¬∑ 615th in this Competition arrow_drop_up 0 more_vert Congratulations on your achievement, @furgalhachaimajhi ! It's impressive to secure 2nd place with just a single submission. I'm curious to know, what inspired you to go with an ensemble of XGB and LGBM models? Did you encounter any challenges or interesting observations while tuning the hyperparameters with Optuna? Fabien Roduit Posted a year ago ¬∑ 20th in this Competition arrow_drop_up 0 more_vert Thanks for sharing. Very interesting concepts. I ended up submitting a simple single LightGBM model, which I intended to use as baseline only, because I got discouraged by the public ladder score (+a relatively tight schedule) even though I had some promising ideas to test out. emoji_people tronDataAna Posted a year ago ¬∑ 833rd in this Competition arrow_drop_up 0 more_vert Can You please Share your submission. I want to look at number of evaluations and your hyperparameter grid. I used the same approach but did'nt place high. Appreciation (1) Naveen Malla Posted a year ago arrow_drop_up 0 more_vert Congrats and thank u for sharing. Too many requests error Too many requests",
      "Classification with an Academic Success Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Classification with an Academic Success Dataset Playground Series - Season 4, Episode 6 Classification with an Academic Success Dataset Overview Data Code Models Discussion Leaderboard Rules lash_fire ¬∑ 10th in this Competition  ¬∑ Posted a year ago arrow_drop_up 23 more_vert 10th Place Solution Firstly, I want to express my gratitude to the Kaggle team for organizing this competition. Ironically, Our Final Submission is same submission from AutoML GrandPrix Solution, which was Bagged Tunned LGB + Bagged Tunned XGB(Voting) which scored 0.83699 on Public LB and 0.83905 on Private LB. I applied Bagging with n_estimators=100 and predictions were obtained with full fit something like this, estimators = [\n    ( 'lgb' ,BaggingClassifier(estimator=lgb,n_estimators= 100 )),\n   ( 'xgb' ,BaggingClassifier(estimator=xgb,n_estimators= 100 )),\n]\n\nVotingClassifier(estimators=estimators,voting= 'soft' ) content_copy Detailed Solution I wish they considered the private leaderboard scores for the AutoML GrandPrix ü•≤. Please sign in to reply to this topic. comment 6 Comments Hotness Tilii Posted a year ago ¬∑ 62nd in this Competition arrow_drop_up 4 more_vert Well done. Due to the recency bias, many competitors tend to \"forget\" their early solutions and pick something that was created in the last few days. Good for you for having the conviction to stick with your best model even if it was made on day 1. Sanyam Jain Posted a year ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Exactly, brother, we would have won :) tanaka Posted a year ago ¬∑ 139th in this Competition arrow_drop_up 1 more_vert Congratulations! Did it ultimately achieve the highest score on the private leaderboard as well? Additionally, n_estimators is set to 100, but what do you think would happen if these values were adjusted? estimators = [\n    ( 'lgb' ,BaggingClassifier(estimator=lgb,n_estimators= 100 )),\n   ( 'xgb' ,BaggingClassifier(estimator=xgb,n_estimators= 100 )),\n]\n\nVotingClassifier(estimators=estimators,voting= 'soft' ) content_copy lash_fire Topic Author Posted a year ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Thank you! Did it ultimately achieve the highest score on the private leaderboard as well? Well, it scored 0.83905 on private LB.(Not the highest score ! ) Additionally, n_estimators is set to 100, but what do you think would happen if these values were adjusted? As of my experiments, Increasing n_estimators tends to Increase the score to a certain point , after then score difference becomes negligible (in some cases maybe even decrease) and it even becomes computationally expensive too. supermiojo Posted a year ago arrow_drop_up 1 more_vert share your notebook. @lashfire lash_fire Topic Author Posted a year ago ¬∑ 10th in this Competition arrow_drop_up 0 more_vert Hey, sorry for late response. here's the Code Link Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 4, Episode 6 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Predict Students' Dropout and Academic Success dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Please refer to the original dataset for feature feature explanations. 3 files 16.2 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 16.2 MB sample_submission.csv test.csv train.csv 3 files 77 columns  Too many requests",
    "data_description": "Classification with an Academic Success Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Classification with an Academic Success Dataset Playground Series - Season 4, Episode 6 Classification with an Academic Success Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: The goal of this competition is to predict academic risk of students in higher education. Start Jun 1, 2024 Close Jul 1, 2024 Evaluation link keyboard_arrow_up Submissions are evaluated using the accuracy score. Submission File For each id row in the test set, you must predict the class value of the Target , which is a categorical academic risk assessment. The file should contain a header and have the following format: id ,Target 76518 ,Graduate 76519 ,Graduate 76520 ,Graduate\netc. content_copy Timeline link keyboard_arrow_up Start Date - June 1, 2024 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  June 30, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Classification with an Academic Success Dataset. https://kaggle.com/competitions/playground-series-s4e6, 2024. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 7,152 Entrants 2,858 Participants 2,684 Teams 20,893 Submissions Tags Beginner Tabular Education Accuracy Score Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s4e7",
    "discussion_links": [
      "/competitions/playground-series-s4e7/discussion/523404",
      "/competitions/playground-series-s4e7/discussion/523489",
      "/competitions/playground-series-s4e7/discussion/523661",
      "/competitions/playground-series-s4e7/discussion/523698",
      "/competitions/playground-series-s4e7/discussion/523484",
      "/competitions/playground-series-s4e7/discussion/523486",
      "/competitions/playground-series-s4e7/discussion/523403"
    ],
    "discussion_texts": [
      "Binary Classification of Insurance Cross Selling  | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Binary Classification of Insurance Cross Selling Playground Series - Season 4, Episode 7 Binary Classification of Insurance Cross Selling Overview Data Code Models Discussion Leaderboard Rules Ravi Ramakrishnan ¬∑ 1st in this Competition  ¬∑ Posted a year ago arrow_drop_up 130 more_vert Winning approach- Team Cross Sellers Hello all, I am extremely thankful to Kaggle for the playground series episode. I wish to thank all my fellow participants and and my team member @arunklenin for a successful team building. I also wish to applaud @uryednap @tilii7 @optimistix for their tough and healthy competition over the course of the month's episode. Please find below our approach for the assignment- Strategy overview We realized since the start of the episode that this dataset has a near-perfect CV-LB relation and the size of the data makes it almost perfectly insulated from overfitting risks. This assignment necessitated baroque GPU resources for efficient model development and experimentation. We decided not to use Kaggle free resources and relied on better GPU hardware including A6000x2, A6000Adax2, A100 and A5000x2, RTX4090, RTX3090 (local PC) for our interim experiments We were well aware of the advantages of using neural networks in this assignment. We used a number of neural networks herewith and all our attempts at this were highly successful Time management and resource management were key in this assignment. Model training had to be on-point and ensembles had to be managed well for memory resources and time to completion as well. We planned to blend a number of diverse models in our final ensemble and rely on the same CV scheme throughout the competition to foster effective model comparison Data organization was key to our success herewith. We resorted to a standard file and model naming convention, tracked experiments regularly and were in-control of individual CV-LB scores as well, enabling us to choose the right mix of models in our final ensemble. Small, but pertinent steps like file naming standardization, code automation, usage of a well organized GitHub repository and a planned feature-store design with CV-oriented feature inclusion helped us with a robust, quick, effective and meaningful model training regime We believed that using diverse training datasets, multiple feature combinations, multiple model parameters and options and a diverse combination of neural network architectures with boosted tree models will yield results and perhaps our belief prevailed! We resorted to 3 training dataset ideas in this assignment - Training data from the competition only (fully ignored the supplementary dataset) Training data created from the competition and supplementary dataset Training data from the competition, with the supplementary dataset added to each fold in entirety- this approach boosted our CV to the maximum We consistently used the below cv scheme - StratifiedKFold(n_splits = 5, shuffle = True, random_state =42) Feature engineering We borrowed ideas from the topper posts in the AutoML component of the competition, but tested the features on the competition synthetic training data only (to avoid bias from the supplementary data). We resorted to testing the feature importance on a single fold only using a simple catboost model consistently. We designed feature stores to collate our feature ideas and created ready-to-use datasets for all further experiments at the end of this step. An illustration of this store is in the kernel here . Our actual feature store consists of 12 versions (Version V1-V12), this kernel is an illustration only. Using such a centralized feature store with tried-and-tested features helped us iterate through our brute-force experiments well and quickly Model training process We resorted to a 3-step training process for the assignment illustrated as below- Model stage Included models Model training options and strategies Stage 1 models 1. LightGBM (manual) 2. LAMA lgb 3. Catboost 4. Denselight NN and  MLP 5. Tab-Resnet 6. Tab-Transformer 7. Autoint * Complete training data * Component models on previously insured (2 models), vehicle damage (2 models), previously insured + vehicle damage (4 models) Stage 2 models 1. LightGBM (manual) 2. LAMA lgb 3. Catboost 4. Denselight NN and MLP 5. Tab-Resnet 6. Tab-Transformer 7. Autoint * Complete training data * Component models on previously insured (2 models), vehicle damage (2 models), previously insured + vehicle damage (4 models) Stage 3 models XgBoost * Selected OOF predictions from across Stage 1 and 2 model options This can be visualized as below - Key notes We used the public kernel outputs from the work here We did not use XgBoost and max_bins in our weak learners We stacked tree models with neural networks and vice-versa XgBoost stacker as the last step was the most effective option for CV-LB relations We did not use max_bins anywhere in our process, except for the public kernel outputs We used 78 weak learners in our final model and trained more than 125 experiments through the month. Adjutant artefacts We have made our single models and a few selected stacks and blends public. Feel free to use the kernels and datasets as below- Kernel/ Dataset Contents Link PlaygroundS4E07-ModelPP * OOF scores for all my component models * Data preparation for the final stacker model https://www.kaggle.com/code/ravi20076/playgrounds4e07-modelpp PlaygroundS4E07-ModelCollation * Single models and a few blends and stacks https://www.kaggle.com/datasets/ravi20076/playgrounds4e07modelcollation PlaygroundS4E07- * Example of stacking from selected private experiments * Includes post-processing dataset containing duplicates between train-original and test-original for reversing the labels https://www.kaggle.com/datasets/ravi20076/playgrounds4e07privatefiles Post-processing We discovered the target reversals discussed here on day 1 and used it in the auto ML submission as well. All our submissions included the target reversal post-processing throughout Both our final submissions retained the post-processing elements What worked for us Effective time management and resource allocation across single models and ensembles Catboost -this was the best single model and in conjunction with category_features, proved to be the best single model option All neural network options worked well for us. Stacking them with Catboost models boosted our CV-LB together LightGBM was also highly effective here and provided a good boost to the final stack with diversity Using the public kernel helped us a bit towards the end of the competition What did not work for us Catboost stacker Linear approaches, Optuna, Hill Climb XgBoost weak learners -they were good in isolation but did not improve the final CV score when we included them in the final model Harmonic mean/ GM generalization LAMA dense model - they caused memory overflows with stacking approaches Our key takeaways Time and resource management is key in such assignments- effective experiment tracking is key. MLOps is important for Kaggle as well, though this is not directly tested and evaluated Committing resources to a competition is necessary when required. Using better resources enabled us to experiment better Sometimes deviating from the norm is important- not using XgBoost stage 1 model benefitted us a lot Concluding remarks We extend sincere wishes to one and all and hope to see you all in the next episode! We may opine that we have a lot of untapped signal in the dataset even now, despite our collective effort over a month. Perhaps one may consider working on the dataset should time prevail and improve the score from here onwards Wishing you all the best and happy learning! Regards, Ravi Ramakrishnan Insurance Tabular Classification 29 27 26 2 Please sign in to reply to this topic. comment 91 Comments 3 appreciation  comments Hotness Arup Posted 8 months ago arrow_drop_up 1 more_vert Thanks for sharing your strategy and the extensive write-up. Frederic Nicholson Posted a year ago ¬∑ 1270th in this Competition arrow_drop_up 5 more_vert @ravi20076 amazing! thanks for explaining your work. the approach deserved to win for the structured process. how much time (human time, not GPU) did you put into this competition? Ravi Ramakrishnan Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 5 more_vert A lot @fredericnicholson I spent about 3-4 hours daily through the month for this competition Tilii Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 5 more_vert Well done team Cross Sellers! Your approach was systematic and in the end clearly the best. Mine overlapped with yours quite a bit except for different neural networks. Hopefully I get to write more about it tomorrow. Ravi Ramakrishnan Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 5 more_vert Congrats to you too @tilii7 for 3rd rank We consider your work as an inspiration! Saad Sayed Posted 9 months ago ¬∑ 1759th in this Competition arrow_drop_up 2 more_vert I am scared yet fascinated by this . What is the maximum score you can achieve by only using free resources of GPUs . I mean at what level of ensemble learning should 1 give up with free resources ?? Ravi Ramakrishnan Topic Author Posted 8 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert @saadsayed123 unfortunately Kaggle GPUs cannot be used here. I used A100, A6000Ada x2 and A6000 for my models Marcos Vinicius Lopes Posted 10 months ago arrow_drop_up 1 more_vert I'm glad to read it! Mahdieh Mortazavi Posted a year ago ¬∑ 39th in this Competition arrow_drop_up 3 more_vert Congrats! Great ideas! sethpointaverage Posted a year ago arrow_drop_up 3 more_vert What is the intuition behind component models? Is it that they are faster to train since they can generalize at lower depths (or lower parameterization for NNs) and when combined represent the same data distribution? Ravi Ramakrishnan Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Yes you are right @sethpointaverage shan007007 Posted a year ago ¬∑ 34th in this Competition arrow_drop_up 3 more_vert Great work. I learned a lot here which would help me in my future competitions. Thanks!! Mart Preusse Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 3 more_vert Congratulations for winning! I realized at some point that my chances for a better place do not stand well as I do not have the computational ressources at home. Besides that I am really impressed by your work and I think your victory is well earned. Two questions: Component model on previously insured means that you diveded the training data according to previously insured in two sets and trained two models, one on the set previously insured == yes and previously insured == no and combined the oof predictions afterwards? How did you handle the different cv-folds? Do I understand the visualization correctly that you only fed the oof predictions of LightGBM and Catboost to the neural networks (without the training data)? Thanks for sharing your solution in such detail. It is a great inspiration! Ravi Ramakrishnan Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Yes you are right on both accounts @martinapreusse Optimistix Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 2 more_vert You were a major contributor to this month's competition, with your work on under-sampling. Thanks for sharing your research and code - they were very valuable! jimmyc891 Posted a year ago ¬∑ 74th in this Competition arrow_drop_up 3 more_vert Congrats on 1st place! Could you please elaborate on what component models are? Ravi Ramakrishnan Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Sure @jimmyc891 Kindly look into the figure We have catboost, light gbm and a number of neural networks. These are my base models. Stacker models are also composed of the same algorithms, but I took tree base learners for neural network stacks and vice versa. My meta model was a single XgBoost that included 78 selected single models and their stacks. jimmyc891 Posted a year ago ¬∑ 74th in this Competition arrow_drop_up 2 more_vert Sorry I should have been more specific. In the 'Model training process' section you mentioned using the Complete training data and also component models on previously insured, vehicle damage and so on. Are the component models just a model where its just the one feature being used as the training data? i.e only 'previously insured' and all other features removed? Ravi Ramakrishnan Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert @jimmyc891 We built separate models for previously insured = 0, previously insured =1, vehicle damage =0 and vehicle damage =1 jimmyc891 Posted a year ago ¬∑ 74th in this Competition arrow_drop_up 1 more_vert ohh okay gotcha! thanks DANISH YOUSUF Posted a year ago ¬∑ 517th in this Competition arrow_drop_up 4 more_vert Congratulations great  work Davi N. Posted 10 months ago arrow_drop_up 1 more_vert Very good work, congrats! sc0v0ne Posted a year ago arrow_drop_up 1 more_vert Congrats! Great ideas! MrDAzh Posted a year ago ¬∑ 2209th in this Competition arrow_drop_up 1 more_vert I always wanted to try stacking machine learning models, but still struggle to find the optimal one, this discussion helps a lot. Ravi Ramakrishnan Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert I am happy if this helped you @mrdazh pubupka Posted a year ago ¬∑ 854th in this Competition arrow_drop_up 1 more_vert Great job! Tanishk Patil Posted a year ago ¬∑ 505th in this Competition arrow_drop_up 1 more_vert Congratulations! This post is very informative for me as a beginner. I feel enriched by the knowledge you've shared. Thank you so much for posting! Roya Jamshidi Posted 10 months ago arrow_drop_up 1 more_vert üü†> Congratulations! This post is very informative for me as a beginner. I feel enriched by the knowledge you've shared. Thank you so much for posting! Amrish Careem Posted a year ago arrow_drop_up 1 more_vert great work Lucas Morin Posted a year ago arrow_drop_up 1 more_vert Sorry I haven't followed this competition. How did you choose to split the model on previsouly insured v.s. non insured ? Ravi Ramakrishnan Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert We designed 2 separate models with 5 fold CV on the below- Previously_insured = 0 Previously_insured = 1 Similarly we designed 2 separate models with 5 fold CV on the below- Vehicle_damage = 0 Vehicle_damage = 1 Finally we extended this to 5-fold CV on 4 component models- Previously_insured = 0 and Vehicle_damage = 0 Previously_insured = 1 and Vehicle_damage = 0 Previously_insured = 0 and Vehicle_damage = 1 Previously_insured = 1 and Vehicle_damage = 1 @lucasmorin ASHUTOSH BHAYDE Posted 10 months ago arrow_drop_up 2 more_vert Thanks for the explanation, definitely provide the good insights for the beginner. KennyT Posted a year ago ¬∑ 695th in this Competition arrow_drop_up 1 more_vert Thanks for share the insights of your work. I have learnt a lot. Especially how to manage features and models (as well as naming convention) to be able to experiment faster. The way you manage feature store is excellent, too. I struggled a lot with manage different versions of features. I have several questions. you mentioned \"supplementary dataset\". So what supplementary dataset did you use? you said that \"We resorted to testing the feature importance on a single fold only using a simple catboost model consistently.\" The aim is to use important features which are shared between folds? What does \"Component models on previously insured (2 models)\" mean? The reason to use \"public kernel outputs\" from other notebook is to increase the variance of OOF predictions? The reason not to use \"max_bins\" is due to the result of your experiment or something else? Thanks and congrats again for the gold medal. Ravi Ramakrishnan Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert @cdthinh please find below my rejoinder- Supplementary dataset - https://www.kaggle.com/datasets/annantkumarsingh/health-insurance-cross-sell-prediction-data Feature importance - We used a single fold permutation importance score to assess feature importances Component models Separate models for previously insured = 0 and previously insured  = 1 Public kernels We used the out of fold predictions from 1 kernel in our model stack as a diversity measure Max bins We did not use XgBoost in our single models, except for the public kernel outputs (the kernel writer used max bins and XgBoost) We tried to use XgBoost and stack it to our models, but it did not improve the CV score, so we dumped the XgBoost models from our set of chosen models for the final submission KennyT Posted a year ago ¬∑ 695th in this Competition arrow_drop_up 1 more_vert Thank you very much to make things clear. Ravi Ramakrishnan Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert You are welcome Good luck @cdthinh Lucas Morin Posted a year ago arrow_drop_up 1 more_vert A lot to unpack here. Congratulations to you and your team. Mohammed Salem Lanqes Posted a year ago ¬∑ 1742nd in this Competition arrow_drop_up 1 more_vert Great work Congratulation , But sorry I still confusing how you overcome the problem of class imbalance.  Thank you in advance Ravi Ramakrishnan Topic Author Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert We never considered handling it due to the large data size @mohammedsalem2011 Patel Manthan17 Posted a year ago arrow_drop_up 1 more_vert Congratulation‚Ä¶! Kunal Posted a year ago arrow_drop_up 1 more_vert Very informative and great to know stuff Saksham Gupta Posted a year ago arrow_drop_up 1 more_vert great explanation Too many requests error Too many requests",
      "Binary Classification of Insurance Cross Selling  | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Binary Classification of Insurance Cross Selling Playground Series - Season 4, Episode 7 Binary Classification of Insurance Cross Selling Overview Data Code Models Discussion Leaderboard Rules Ujjwal Pandey ¬∑ 2nd in this Competition  ¬∑ Posted a year ago arrow_drop_up 45 more_vert 2nd Place Solution (One model is all you need) - Team Ujjwal Pandey Hi everyone, I would like to extend my gratitude to the Kaggle community and fellow participants for an incredible Playground Series episode. This was a resource-intensive and time-consuming challenge, demanding a great deal of patience to achieve victory. The frequent movement on the public leaderboard, especially in the last 2-3 days, reflected the intense competition. Here's my journey broken into iterations. Iteration 1 In my initial attempt, I trained a quick, lightly tuned set of models (XGB, LGBM, and SnapML ) on my local PC equipped with an RTX 4070. The XGB submission was smooth, achieving a Public LB score of 0.88448 with a CV score of 0.8833 . I faced a problem with LightGBM CUDA flavor because of this bug [CUDA] illegal memory access . Consequently, I had to drop LightGBM from the first iteration. I did not pursue Random Forest or any sklearn libraries, as public discussions and notebooks indicated they were not worth the computational effort SnapML usually takes a lot of time on even light datasets but I tried it anyway as it has GPU support but it also achieved an LB score close to 0.8880 . At the end of this iteration, my score was around 0.88 . To breach the 0.89 mark, I needed further improvements. I had not yet explored CatBoost, as public forums suggested its default settings could yield scores exceeding 0.895 . Iteration 2 A determined effort to tune XGB, LGBM, and SnapML to match CatBoost's performance: I aggressively tuned XGB and SnapML using distributed Optuna on four machines (2 x Kaggle GPU P100, 1 x L4 from Colab, 1 x RTX 4070 from my local setup). To avoid OOM errors during HPO, I didn't load the test set into memory. In this stage I also merged the original dataset and created these four additional interaction features from public notebooks. train_df[ 'Insured_Vehicle_Damage' ]=  train_df[ 'Previously_Insured' ].astype( str ) +  train_df[ 'Vehicle_Damage' ].astype( str )\ntrain_df[ 'Insured_Vehicle_Age' ] =  train_df[ 'Previously_Insured' ].astype( str ) +  train_df[ 'Vehicle_Age' ].astype( str train_df[ 'Insured_License' ] =  train_df[ 'Previously_Insured' ].astype( str ) +  train_df[ 'Driving_License' ].astype( str )\ntrain_df[ 'Insured_License' ] =  train_df[ 'Previously_Insured' ].astype( str ) +  train_df[ 'Driving_License' ].astype( str )\ntrain_df[ 'Insured_Gender' ] =  train_df[ 'Previously_Insured' ].astype( str ) +  train_df[ 'Gender' ].astype( str )\n) content_copy The tuned XGBoost achieved a public LB score of 0.89387 with a CV score of 0.89113 . I tuned LightGBM on CPU using three machines (1 x TPU on Kaggle and 2 x TPUs on Colab). This model achieved a public LB score of 0.89344 with a CV score of 0.89302 . A challenge with using TPU on Kaggle is the automatic shutdown after 3 hours when TPU is not consumed, but distributed tuning allowed me to quickly resume training Next was SnapML though sadly this model couldn't even breach the 0.890 on my CV and hence I filtered this out after this iteration ‚òπÔ∏è. After seeing the performance of neural nets from public discussions and notebooks I gave it a try. There's an awesome library pytorch-tabular which I used to train TABNET and GANDALF and they also achieved CV score of ~0.8910 but they are very expensive to train and didn't offered better performance compared to GBDT. At the end of this stage I had LB score of ~0.8930 so my next stage was to close this gap by trying some different techniques. Iteration 3 I attempted to use neural embeddings from pytorch-tabular CategoryEmbedding model to boost the scores of LightGBM and XGBoost. This was a mistake, as the embeddings turned out to be close to 300 dimensions, requiring high RAM. An initial spike in my CV score to 0.895 was due to a bug caused by a seed mismatch, which I discovered too late after exhausting my Colab credits ‚òπÔ∏è. I tried target encoding to some extent but even that wouldn't push the score of XGB or LGBM ‚òπÔ∏è . At this point, I considered skipping the episode, having exhausted nearly all my compute resources and time with no guarantee of success. With around 30 submissions, I decided to give CatBoost one final attempt. Final Iteration At this stage, I tuned just one catboost model (just one) using again the same strategy distributed optuna on 3 x machines close to 50 HPO rounds, took about 10 hours because unfortunately, catboost GPU doesn't support pruner as compared to XGB and LightGBM. These were my parameters after HPO tuning. 'learning_rate' : 0.11913236771124495 , 'reg_lambda' : 0.5423732686916918 , 'max_depth' : 6 , 'subsample' : 0.9996168133883909 , # I changed this to 1.0 when training full model.\n'leaf_estimation_iterations' : 10 , 'log_max_bin' : 15 content_copy These were my fixed params for HPO \"loss_function\" : \"Logloss\" , \"eval_metric\" : \"AUC\" , \"iterations\" : 3000 , \"random_state\" : 56315 , \"bootstrap_type\" : \"Bernoulli\" , \"grow_policy\" : \"SymmetricTree\" , \"task_type\" : \"GPU\" , \"early_stopping_rounds\" : 100 , \"leaf_estimation_method\" : \"Newton\" , \"use_best_model\" : True , content_copy The above parameters when trained to 5000 iterations  with some tweak would achieve a public LB of 0.89666 but it isn't enough to win it. Through my small experiments on Kaggle notebooks I observed two important things. I can manually reduce the learning rate to 0.085 and increase the iterations to 10000 . Newton-based score_function is superior to Gradient-based. I guess this is what most of the public notebooks missed. As soon as I switched to NewtonCosine or NewtonL2 and used 12 leaf_estimation_iterations my CV itself was close to 0.8960 . Additionally, I removed contrasting duplicates first from the original and then from both train and original after combining as discussed in this discussion: [ https://www.kaggle.com/competitions/playground-series-s4e7/discussion/520253](https://www.kaggle.com/competitions/playground-series-s4e7/discussion/ . I also Binned Aged and Premium features as I found it slightly increases my validation scores. After experimenting with everything I divided the dataset into 4 folds. I wasn't able to train all the folds together and even training a single fold was not possible on Kaggle as catboost required close to (48 gigs) of RAM post-training. I trained the first 3 fold on colab with my remaining computes and the submission this time achieved an LB score of 0.89720 putting me in top 10 but after doing the trick as mentioned by @paddykb it got boosted to 0.89780 . To finally settle the score I trained small-small versions of catboost with same parameter and reduced iterations on Kaggle with different CV-spilt and random states and did average solely based on validation scores (as at this point I didn't had OOF folds to properly blend with) which settled my final score without trick to 0.89728 and with the trick to 0.89788 which was the winning solution with private LB score of 0.89753 . These were my final CV-LB scores of the fold without any trick. Fold CV score LB Score Fold-0 0.8960367441 0.89625 Fold-1 0.8962229192 0.89629 Fold-2 0.8962465823 0.89632 Fold-3 0.8959594369 0.89620 **Fold-4 0.8958430433 - ** Fold-4 is average of all my small models. Summary Key Takeaways Monitor discussions and public notebooks to save experimentation and compute time, allowing calculated decisions with fewer submissions. Be patient with large datasets and try different training strategies. Mistakes are learning opportunities. Distributed optimization is faster, more affordable, and has better error tolerance, especially in competitions like this. Be wary of leakage, even with changing random states, especially in neural embeddings. What Didn‚Äôt Work Target encoding with XGB and LGBM. Neural embeddings didn't boost performance enough compared to the compute required. Changing the default max_ctr_complexity of CatBoost overfit in my case. What I Could Have Done Saving OOF folds and combining would have boosted my score though I couldn't due to their large size. Exploring neural networks more. Trying different CatBoost flavors (maybe lossguide). # Special thanks to @ravi20076 , @arunklenin , @tilii7 , and @optimistix for the healthy competition, and @paddykb , @ivanmitriakhin , @rzatemizel for the trick. 3 Please sign in to reply to this topic. comment 15 Comments Hotness Optimistix Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Congratulations! Wonderful solution, and a timely reminder to look into all the hyperparameters in great detail. neednot_toplay Posted a year ago ¬∑ 368th in this Competition arrow_drop_up 1 more_vert Nice solution. congrats for rank 2 @uryednap Harshit Sharma Posted a year ago ¬∑ 401st in this Competition arrow_drop_up 2 more_vert Congratulations on your impressive 2nd place finish, @uryednap ! Your strategic approach, particularly the use of distributed Optuna for tuning and insights from the Kaggle community, are truly commendable. Ravi Ramakrishnan Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert I appreciate your strategy and your overall approach to the problem. This is a fantastic learning experience for me @uryednap Xuan Truong Bui Posted 6 months ago arrow_drop_up 0 more_vert Thanks so much for sharing your thought process! Could I ask why you needed to create the four additional interaction features? I'm new to this Taras Pedchenko Posted 7 months ago arrow_drop_up 0 more_vert Congratulations! Nice approach MariFel Posted a year ago arrow_drop_up 0 more_vert @uryednap could be shar your notebook? emoji_people AbdalRhamn Hebishy Posted a year ago ¬∑ 1555th in this Competition arrow_drop_up 0 more_vert Great Work . Congratulations,wonderful solution Bhargav Borah Posted a year ago ¬∑ 1043rd in this Competition arrow_drop_up 0 more_vert Congratulations! Got to know about lots of new stuff! Oumar KEITA Posted a year ago ¬∑ 804th in this Competition arrow_drop_up 0 more_vert Amazing job ! Congratulations. I have learnt a lot of things from this solution. Tilii Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert That was some industrious work - congratulations! You could have dusted off all of us big time by combining multiple models, though I understand that resources played a part in making that difficult. Minato Namikaze Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Congratulations! @uryednap amazing work and thank you for sharing the solution write-up. Oscar Aguilar Posted a year ago ¬∑ 9th in this Competition arrow_drop_up 0 more_vert Thanks for sharing your approach, @uryednap . I really like how your approach started improving step by step; this is the data science journey! Chirag Malhan Posted a year ago ¬∑ 364th in this Competition arrow_drop_up 0 more_vert Hey, Thanks for being so honest about your process of taking inspiration from public notebooks. This was my first-ever competition, and I was experiencing massive imposter syndrome because I was mixing public notebook ideas with my own experimentation and still not moving past the top 300 rankers. It's reassuring to know that this was a common practice among the top rankers as well. Your insights were quite inspiring and interesting. Hopefully, I‚Äôll give you a better competition in the next one. Cheers! Ujjwal Pandey Topic Author Posted a year ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Taking inspiration from public notebooks and especially discussions is an excellent way to minimize experimentation time. My approach involves looking at their CV scores and comparing them with my own results. If my method outperforms theirs, I proceed with confidence; otherwise, I incorporate their ideas into my strategy. Always evaluate a notebook based on its CV scores. I always avoid notebooks that blend others' work, even if they have high scores. Relying on copied or incorporated methods limits your growth. Achieving a top rank this way is less fulfilling and doesn't provide the same level of satisfaction. Too many requests error Too many requests",
      "Binary Classification of Insurance Cross Selling  | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Binary Classification of Insurance Cross Selling Playground Series - Season 4, Episode 7 Binary Classification of Insurance Cross Selling Overview Data Code Models Discussion Leaderboard Rules Tilii ¬∑ 3rd in this Competition  ¬∑ Posted a year ago arrow_drop_up 27 more_vert #3 solution | Many individual models and many ensembles First, I want to congratulate everyone who managed to handle this huge dataset and get on the board. No small feat. Next, I want to congratulate in particular to everyone in top 10, who stuck with this competition to the last day. There was quite a bit of LB shuffling in the last 5 days. Finally, I want to alert someone who placed below me that they will be getting a Kaggle t-shirt, as I am not eligible. I was convinced from the beginning that all features should be treated as categoricals. I though initially that Annual_Premium had too many unique values (>53,000) to be used as such, and spent 4-5 days on getting that number down to 5,000-8,000. Obviously that didn't make it into day 1 solutions, and later @paddykb published a notebook showing there is no need to reduce the number of unique features in any variable. Most of my models dealt with data modified by OrdinalEncoder on each variable, resulting in the following tally of unique values per feature: Gender 2 Age 66 Driving_License 2 Region_Code 54 Previously_Insured 2 Vehicle_Age 3 Vehicle_Damage 2 Annual_Premium 55068 Policy_Sales_Channel 156 Vintage 290 content_copy CatBoost worked best by far individually, but oddly enough not that well when ensembling models. For that purpose I used blending by numerical optimization (not really feasible with models > 10), Keras, LAMA NNs and LightGBM. The last two worked the best for ensembling. If you look at my bio you will see that I am not a programmer. During each competition I create hundreds of scripts and run them locally, and none of them are integrated into a big pipeline. I like to divide everything into chunks and run things separately, which is why it is impossible to replicate my whole process here without a major effort. I will try to link the relevant notebooks already on Kaggle, and if I get a breather over the weekend I will try to publish at least one neural network. I will explain what I did, but you have a fair warning to stop reading here if you are interested only in code. Here are my best 5 types of individual models based on CV scores: Model CV Public LB Private LB CatBoost Optuna 0.896733 0.89728 0.89699 Keras FM 0.894276 0.89527 0.89498 Keras embedding 0.894192 0.89469 0.89445 xLearn FFM 0.893223 0.89447 0.89414 LAMA ResNet 0.893647 0.89378 0.89359 Of course I had many CatBoost models that were better than some models here, but this is shown only for variety. I also had 3 other types of LAMA NNs that I didn't use, also an xLearn FM model which I will explain below. Didn't try XGBoost at all. LightGBM was too slow with categorical variables and wasn't producing good models in my hands, but worked like a champion during the ensembling. Others have already talked about CatBoost and LAMA, so I will focus on the middle 3 models. Keras FM refers to a neural network implementation of factorization machines. Rather than creating features by multiplying, dividing or somehow else combining features, we let  the NN do that for us. It is easy to Google factorization machines, so here is the gist. Those models convert all the features, numerical or otherwise, into factors/categories and explicitly model their interactions. They work really well for large datasets, especially in recommender systems, because of their speed. Since I converted all my features into categories, it was custom made for this type of analysis. Here is how the NN model looks like: This will look tiny on the screen, and I suggest you right-hand click on the image and open it in a new tab, where you will be able to use a magnifier. It is a bunch of embedding nodes, one for each feature, that are crossed in dot-product fashion with each other. What that does is take two numerical vectors from embedding layers and squishes them together into a single number. Then there are linear representations of the same features, and finally everything is concatenated and passed through several dense layers. I also used dropout layers but they are not shown in that image. There is a link to an old script below describing Keras FM, and my implementation is similar except for the added dense layers. https://www.kaggle.com/code/qqgeogor/keras-based-fm/script Keras embedding was done similarly to this notebook and I am grateful to @paddykb for the idea to convert all features into categoricals straight-up, without trying to reduce the number of bins. In my implementation Keras embedding didn't work great when all the features were categorical, but it added diversity. It worked better when autoencoder features were added as a separate layer to categoricals - see below. xLearn FFM deals with field-aware factorization machines, which are similar in spirit to FMs but with a different type of data encoding. For an illustration how that looks like: https://www.kaggle.com/code/ogrellier/libffm-model I recommend that you try the xLearn package: https://github.com/aksnzhy/xlearn It works great with factorization models (both FMs and FFMs) and it is multithreaded - very fast. The latest version can't be installed by pip and isn't available on Kaggle, but I think the older versions should work fine. Most importantly, these models were extremely diverse with regard to everything else and contributed nicely to the ensemble, even though they were not great on their own. Below is an image showing cumulative distribution functions of the best CatBoost and xLearn FFM models. FFMs are much better at predicting 0s while CatBoost is much better at predicting 1s, and these two models complemented each other far better than any other two models I tested. Finally, I did some feature engineering by running a denoising autoencoder (DAE) and extracting its latent factors. I chose 3 and 8 factors at the bottleneck, and the latter worked better. Basically, we take the categorical data and convert them into a string of 0s and 1s using pd.get_dummies . Here I used numerical representations of ['Age', 'Annual_Premium', 'Vintage'] scaled to 0-1 range rather than their categoricals, as that would make a dataset very wide. These 8 features found by DAEs had decent predictive abilities on their own, as you can see in a t-SNE plot below. Still, DAE factors were not great alone, and worked the best when added to other features. That eliminated FMs because there were millions of unique values and couldn't be converted to categories, but individual CatBoost and Keras embedding models could handle these extra features and their scores jumped up by ~0.0002. The final solution was a stack of 38 models made by LAMA DenseLight NN, but LightGBM had a near-identical solution. See here for more details. There were at least 8 CatBoost models, some with and some without DAE features. Some models used the features from here . Also had 6-8 each of xLearn FM and FFM models, Keras FMs and Keras embedding models. I added 3 LAMA NN models at the very end and wish I had more of them, as they surely would have given a boost based on diversity with regard to other models. A couple of AutoGluon models were included as well. All of them were stacked together either using an Optuna-driven LightGBM, or as 10-fold Keras and LAMA NNs. I want to thank everyone for excellent discussions, and for sharing your knowledge with patience that I sometimes lack. Special thanks to @paddykb and @ivanmitriakhin for publicizing the reversal of labels that gave most of us a nice LB boost. 1 Please sign in to reply to this topic. comment 11 Comments Hotness Sagnik Mukherjee Posted a year ago ¬∑ 1262nd in this Competition arrow_drop_up 1 more_vert Thanks a lot @tilii7 for providing such a wonderful explanation. Learnt a lot from it. Mart Preusse Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert Congratulations on the third place! The performance of your NNs is astonishing. How did you ensemble your models and did you calibrate them before ensembling? I noticed that using XGBoost for stacking gave me a boost and allowed me to combine oof predictions from different cv and include the subsampling models, which were to different from the rest of the models for regression. Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thanks! Part of the reason I invested heavily into NNs was because CatBoost was working so well with categorical features, while LightGBM was not working in my hands. I knew that FMs and embedding would be competitive. I simply rank the predictions before ensembling, and that brings them on the same scale. Since AUC is a ranking measure, after ranking the predictions one can use even Ridge and Lasso regression for ensembling. Bhargav Borah Posted a year ago ¬∑ 1043rd in this Competition arrow_drop_up 1 more_vert Congratulations! And thanks for this write-up, lots to learn! Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thanks! Glad it was helpful. Optimistix Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Congratulations! Lots to learn from your approach. Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Thanks! There were some attempts with libFFM executable, which is not even a python package. It worked even faster than xLearn, but didn't seem to like AUC as metric. sc0v0ne Posted a year ago arrow_drop_up 0 more_vert Congrats!!!!!  üòÄ Aadit Shukla Posted a year ago arrow_drop_up 0 more_vert Great insight @tilii7 . We learn from your insight Tilii Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Glad to hear it! This comment has been deleted. Too many requests error Too many requests",
      "Binary Classification of Insurance Cross Selling  | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Binary Classification of Insurance Cross Selling Playground Series - Season 4, Episode 7 Binary Classification of Insurance Cross Selling Overview Data Code Models Discussion Leaderboard Rules Optimistix ¬∑ 4th in this Competition  ¬∑ Posted a year ago arrow_drop_up 17 more_vert 4th place solution: Competing Without Compute Well, this was a pretty frustrating as well as rewarding competition. I'm about to launch into a long tale, but for those in a hurry, here's the TLDR: Gathered a veritable zoo of models, ensembled them, kept an eye on CV & model diversity,  & kept ensembling while score increased Kept running out of GPU on Kaggle Kept facing crashes and hangs on my MacBook Kept experimenting with everything: new models, hyperparameters, ensembling approaches, etc. Ended up with nearly 80 models, the top 4 solutions scored identically with 58-64 models Now for the longer version: Prologue: After 7 years of dormancy on Kaggle, I started competing in the Playground Series in late February, 2024. Joining PSS4E2 with only 3 days to go, I found some good public notebooks, played around with them a bit, and finished in the top 9%. This was encouraging, and over the next 3 months, I managed 3 consecutive top 10 finishes, but never wrote about them, since I figured I was doing what everyone else was. Then came PSS4E6 - I made my way to no. 1 by mid-June, & got addicted to seeing my name at the top of the leaderboard, even as I realized that I was overfitting to remain there (until the very end). I figured I'd choose one submission that had a low public LB score but good CV-LB correlation, but I forgot to do it in time (I had two that might have placed #3 or #9), and duly fell to #113 üòÄ End Prologue Lesson learned, I was ready for PSS4E7 - or so I thought. I could never have guessed that we'd be dealing with 11 million rows - it's been quite the experience! As someone with no GPU access outside of Kaggle, I repeatedly faced a resource crunch. Working on my MacBook was alright as long as long as I had say 10-15 models - but as the list of models grew, so did the OOF and test matrices, and my MacBook was soon groaning under the strain, as either Jupyter Notebook or even the machine itself gave up midway. But I plugged away, experimenting in different directions, and relying heavily on the on the generosity of those who shared their insights, findings and code, including but not limited to @rzatemizel , @ravaghi , @alexryzhkov , thiagomantuani, @paddykb , @vopani , @martinapreusse , @siukeitin , @ambrosm , @oscarm524 , @ivanmitriakhin , @ravi20076 , @gauravduttakiit , @innixma , @roberthatch , @omidbaghchehsaraei The models I used and their performance range was as follows: Models CV score CatBoost 0.8940-0.8950 LGBM 0.8920-0.8930 Neural Networks 0.8910-0.8930 XGBoost 0.8910-0.8920 Others 0.8750-0.89 I started off by building on the work of top teams in the AutoML Grand Prix, with @Vopani and @innixma (& team) providing high-scoring single models in CatBoost & XGBoost, and @alexryzhkov & team being particularly generous in providing over a dozen models including several NNs, along with their OOF scores. This was particularly promising, as I knew GBDTs & NNs were likely to blend well. My own LGBM was only around 0.885 at that time, but soon we all realized that a crazy high max_bin value would bring it closer to 0.892. I started with a Ridge Classifier for ensembling. I also tried hill climbing and model selection with recursive feature elimination, but both became untenably slow once the number of models grew into the dozens. Since Ridge was much faster & producing good results, I continued with Ridge for most of the remainder of the competition. Apart from adding many different models, I tried to create more diversity by playing around with hyperparameters, number of folds, and even random seeds - it all worked to some extent. As long as adding a new model improved the CV and LB score, I kept it. I also tried many new models, including several flavors of neural networks, and TabNet. I tried various AutoML tools to make this easier - LAML started to become my favorite AutoML package, as OOFs seem to built into the design philosophy. Thanks to @gauravduttakiit 's public notebooks, it was easy to try out various NNs - I discovered this over a weekend, & proceeded to max out my GPU allowance by Tuesday üòÄ Whenever I found a high performing public notebook that promised to increase model diversity or overall performance, I modified it to produce and save OOFs, and back we went to the trusty ensembler. On the rare occasion that someone provided the OOFs in their output, I grabbed it with extra gratefulness. All along, I kept crashing into resource limitations, the most painful varieties being lost GPU hours (program had errors, or worse - timed out after 12 hours, with 40% GPU blown and nothing to show), and a notebook crashing right before submission - the worst manifestation of this was on the second last day, when I could only make one submission. All the hustle and bustle had got me to 0.89694, when @paddykb shared the cheap trick , and @rzatemizel soon shared it in code. Lo and behold, my score jumped to 0.89751 - a jump of 0.00057 being a gift from the heavens in this crazy Kaggle World. Adding some more models got me over 0.89760, but I was starting to hit a wall of sorts by now. By this time, my collection had ballooned to 70 models, and I was really struggling to keep the cycle going without hitches & glitches. I decided to prune the collection - since RFECV was out of the question, I relied on the weights produced by the Ridge Classifier, and threw out everything with a \"very low\" weight. The CV remained essentially identical, so I left them out. I then tried TabNet and a few other things, and moved closer to 0.8977. We were nearing the end of the month (and we're also nearing the end of this post, I assure you), so I decided to try something other than more models. I considered pseudolabeling, but couldn't get around to it. I decided to try out GBDTs for ensembling - using untuned XGB, CB and LGB along with @rzatemizel 's tuned XGB, I blended the GBDT's OOF and test predictions, and found that I could match the Ridge Classifier's results at 0.89771. Averaging these two led to 0.89776, a quite satisfying result. By now, we were almost out of time, so I resorted to \"blind blending\" - mixing my solution scoring 0.89777 with @rzatemizel 's 0.89727 submission led to a small bump, of 0.89780, with 60 models and a blend of ensemblers as just described. After this, I tried tuning the GBDTs for ensembling, and even though they showed improved scores individually, their ensemble didn't really improve, and I ended up reproducing the same score 4 times with slightly different collections of models (along with 4 missed submissions on the penultimate day). Perhaps I should have tried tuning the Ridge Classifier? I also meant to try out Elastic Net etc., but ran out of time. To sum up, my best score was achieved by a combination of the following: Build, borrow and steal collect 60ish models Ensemble them using a Ridge Classifier Ensemble them using GBDTs, and blend these ensembles. Blend the above two ensemble Blend with the top scoring public notebook A top 3 finish felt well within grasp, but then slipped away. A big congratulations to @ravi20076 , @arunklenin , @uryednap , @tilii7 & everyone else who finished on the leaderboard, and thanks once again to everyone who helped me so much with their insights and code. Epilogue: Even though a top 3 finish eluded me this time, I knew that Kaggle swag was probably coming my way, since @tilii7 had already had a top 3 finish, and probably @ravi20076 as well - and so it is. One day I'll finish in the top 3, and earn the swag that I won't get - but until then, I'll tell myself I was first among those without GPU access beyond Kaggle, and walk around in a t-shirt with a goose on it, looking for people who recognize it as more than just another goose üòÄ 1 Please sign in to reply to this topic. comment 13 Comments Hotness automatylicza Posted 3 months ago arrow_drop_up 1 more_vert Inspiring! Congratulationsüéâ rƒ±za temizel Posted a year ago ¬∑ 56th in this Competition arrow_drop_up 1 more_vert I really enjoyed reading this. Congratulations! Optimistix Topic Author Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Thanks so much! Your notebooks and posts have been a great help these past few months. rƒ±za temizel Posted a year ago ¬∑ 56th in this Competition arrow_drop_up 2 more_vert I spilled coffee on my laptop and I barely managed to update last version of the notebook with the on-screen keyboard. I am really happy that this effort helped someone :) Oscar Aguilar Posted a year ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert Congratulations on achieving 4th place, @optimistix ! Thank you for sharing your journey to reaching such a high rank. By the way, I appreciate the mention.üëç Optimistix Topic Author Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Thanks so much, and you're welcome. I look forward to your code this month! Mart Preusse Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert Congratulations on the 4th place, well done and of course you earn this shirt! My journey and problems in this competition were similar to yours, thus most of the time I tried to reduce the size of the training data. I also heavily relied on the oofs of the public notebooks and had only 3 own models (basically the ones I published). I did not really gave up in this competition, as it was interrupted by holidays, but it was really frustrating in the end due to the crashes. Optimistix Topic Author Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Thanks so much! Hope you enjoyed your holidays. I thought your work on undersampling was great. Looks like this month might be more manageable for us, with \"only\" 3 million rows üòÄ Mart Preusse Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert Thanks, I appreciate this! Holidays were nice, I had time to read :-). Unfortunately, I do not have much time this month, so I will skip this playground and practice NNs instead if it is possible. Good luck for you! Optimistix Topic Author Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Thanks! Good to hear you had a nice break. I'm on short on time as well this month, won't skip but shall try to optimize my efforts. Tilii Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Nicely done! Glad to hear you'll be the one getting the t-shirt. I think you have done everything right, and that should matter the most. I feel the same about my own effort in July. In most competitions margins are tiny at the top, and I am not upset even when others beat me by 0.0001 or less as long as I selected correct submissions. As to the allure of being #1, it is best not to get too comfortable. I have probably reached that spot in half a dozen competitions, and have never kept it. Optimistix Topic Author Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Thanks! Agreed on both points. Definitely sticking to being guided by the CV in the future. Mahdi Ravaghi Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Congrats! It's impressive to see how fast you are progressing. Optimistix Topic Author Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Thank you so much! Frederic Nicholson Posted a year ago ¬∑ 1270th in this Competition arrow_drop_up 2 more_vert congrats your achievement . quite an achievement. how much time did you spend ? Optimistix Topic Author Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Thanks! Definitely way too much time üòõ Too many requests error Too many requests",
      "Binary Classification of Insurance Cross Selling  | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Binary Classification of Insurance Cross Selling Playground Series - Season 4, Episode 7 Binary Classification of Insurance Cross Selling Overview Data Code Models Discussion Leaderboard Rules Mahdi Ravaghi ¬∑ 6th in this Competition  ¬∑ Posted a year ago arrow_drop_up 23 more_vert 6th place solution Notebook: https://www.kaggle.com/code/ravaghi/insurance-cross-selling-6th-place-solution Data Preprocessing I used the original dataset in addition to the competition dataset to train all my models. I changed the data types to reduce memory usage, converted categorical features to numerical values using simple mappings, and added the following features which I borrowed from this notebook . dataframe[ 'Previously_Insured_Annual_Premium' ] = pd.factorize(dataframe[ 'Previously_Insured' ].astype( str ) + dataframe[ 'Annual_Premium' ].astype( str ))[ 0 ]\ndataframe[ 'Previously_Insured_Vehicle_Age' ] = pd.factorize(dataframe[ 'Previously_Insured' ].astype( str ) + dataframe[ 'Vehicle_Age' ].astype( str ))[ 0 ]\ndataframe[ 'Previously_Insured_Vehicle_Damage' ] = pd.factorize(dataframe[ 'Previously_Insured' ].astype( str ) + dataframe[ 'Vehicle_Damage' ].astype( str ))[ 0 ]\ndataframe[ 'Previously_Insured_Vintage' ] = pd.factorize(dataframe[ 'Previously_Insured' ].astype( str ) + dataframe[ 'Vintage' ].astype( str ))[ 0 ] content_copy I was initially hesitant to use these features due to their leaky nature, but they improved both my CV and public LB scores, so I decided to keep them. Models The following models were used in my ensemble: CatBoost ( notebook ) LightGBM ( notebook ) XGBoost ( notebook ) Neural Network ( notebook ) Logistic Regression I saved the OOF predictions and test predictions of each model and used the resulting files in my ensemble. Note that the provided notebooks don't have the exact set of hyperparameters that I used, but the rest of the code is the same as what I used for my final models. For CatBoost and Logistic Regression, I treated all features as categorical. For the neural network, I one-hot encoded some of the features and target encoded one of the features as suggested here . Ensemble I used a simple StackingClassifier as my ensemble technique. To save time and prevent StackingClassifier from training every model from scratch, I used this trick that I learned in the last competition. I log-transformed the OOF predictions of my base models and fed them through the model. By default, StackingClassifier uses LogisticRegression as its final estimator. I tried tuning LogisticRegression , but it didn't help. I also tried a tuned XGBClassifier and LGBMClassifier , but they didn't help either, so I decided to stick with the default settings. Post Processing I applied the glitch in the insurance matrix trick by @paddykb to my test predictions and improved my score by ~0.0006, which is the same improvement as @paddykb reported. Results Here is the CV scores of each of my models trained on 5 folds. 1 Please sign in to reply to this topic. comment 11 Comments Hotness Alexandre Constantino Leal Posted a year ago arrow_drop_up 1 more_vert Congratulations! Thanks for sharing your solution! Bhargav Borah Posted a year ago ¬∑ 1043rd in this Competition arrow_drop_up 1 more_vert Congratulations @ravaghi ! Also, thanks for your crisp solution! Mahdi Ravaghi Topic Author Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thank you @introverstein ! Tilii Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert @ravaghi Very comprehensive! You can rank the predictions before stacking rather than log-transforming, because ROCAUC is a ranking measure. That way all the predictions will be on the same scale. Mahdi Ravaghi Topic Author Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thanks! I'll keep that in mind for next time. Optimistix Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 2 more_vert Congratulations! Very neat solution, as were your models and notebooks. I must thank you for sharing them, as I used them extensively. Mahdi Ravaghi Topic Author Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Thank you! I'm glad my code was helpful to you. Optimistix Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Very much so - I liked it so much, I even dropped others' models into it! üòÄ Oscar Aguilar Posted a year ago ¬∑ 9th in this Competition arrow_drop_up 2 more_vert Thanks for sharing your approach, @ravaghi !üëç When I saw the 'StackingClassifier' in your notebook, I thought about implementing it for the models I built, but I ran out of time. I'll keep this in mind for next time! Mahdi Ravaghi Topic Author Posted a year ago ¬∑ 6th in this Competition arrow_drop_up 2 more_vert This approach worked really well in the last competition, but unfortunately, I didn't choose it for my final submission. After seeing how well it performed in this dataset as well, I decided to go with it, and it paid off. Rubanza Silva Posted 10 months ago ¬∑ 433rd in this Competition arrow_drop_up 0 more_vert Great write up and solution. Very interesting approach. This comment has been deleted. Too many requests error Too many requests",
      "Binary Classification of Insurance Cross Selling  | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Binary Classification of Insurance Cross Selling Playground Series - Season 4, Episode 7 Binary Classification of Insurance Cross Selling Overview Data Code Models Discussion Leaderboard Rules Yosef Lachman ¬∑ 8th in this Competition  ¬∑ Posted a year ago arrow_drop_up 13 more_vert 8th Place Solution For our final model, we used CatBoostClassifier with the following parameters: model = cb.CatBoostClassifier(\n    iterations= 30000 ,\n    learning_rate= 0.02 ,\n    random_strength= 0.1 ,\n    depth= 8 ,\n    loss_function= 'Logloss' ,\n    eval_metric= 'AUC' ,\n    leaf_estimation_method= 'Newton' ,\n    random_state= 1 ,\n    subsample= 0.9 ,\n    bootstrap_type= 'Bernoulli' ,\n    task_type= 'GPU' ) content_copy We also used the target reversals method on our submissions. GPU Setup: We utilized two GPUs for our training process: NVIDIA GeForce RTX 4060 - My personal computer GPU, was used for the initial model training and testing phases, it was much slower but sometimes had higher scores. NVIDIA A100 - Provided by Bar-Ilan University, this GPU enabled us to efficiently handle the large datasets and extensive iterations required for our model. Cross-Validation and Results We employed a 10-fold StratifiedKFold cross-validation approach, running 30,000 iterations for each fold. This method ensured that our model was robust and generalizable across different subsets of the data. The following are the best iterations and corresponding ROC-AUC scores for each fold: Fold 1: Best Iteration = 19,186, ROC-AUC = 0.895919 Fold 2: Best Iteration = 19,431, ROC-AUC = 0.896413 Fold 3: Best Iteration = 17,659, ROC-AUC = 0.895431 Fold 4: Best Iteration = 15,656, ROC-AUC = 0.896021 Fold 5: Best Iteration = 16,038, ROC-AUC = 0.895603 Fold 6: Best Iteration = 18,125, ROC-AUC = 0.895498 Fold 7: Best Iteration = 14,044, ROC-AUC = 0.895487 Fold 8: Best Iteration = 14,509, ROC-AUC = 0.895475 Fold 9: Best Iteration = 15,585, ROC-AUC = 0.895458 Fold 10: Best Iteration = 15,268, ROC-AUC = 0.895620 The total training time was 15 hours. The mean ROC-AUC score across all folds was 0.895693. We had a great time participating in the competition and look forward to many more. Also a huge congrats to Ravi Ramakrishnan and Minato Namikaze on their first places amazing winning approach . Please sign in to reply to this topic. comment 10 Comments 1 appreciation  comment Hotness Bhargav Borah Posted a year ago ¬∑ 1043rd in this Competition arrow_drop_up 1 more_vert Congratulations! I have some questions for you: What criteria did you use to decide that CatBoostClassifier will be your final model? How did you decide against using multiple models? Yosef Lachman Topic Author Posted a year ago ¬∑ 8th in this Competition arrow_drop_up 0 more_vert Thanks. I used extensive parameter testing models like Randomsearch and optuna to find good parameters. It was a lot of trial and error, hence the 95+ models I submitted. And I did not go with multiple models as I was getting very good scores with just catboost especially as it was running on strong GPUs. Optimistix Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Congratulations! Nice combo of a powerful model with two powerful GPUs. Yosef Lachman Topic Author Posted a year ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Thank you! And congratulations to you too! Oscar Aguilar Posted a year ago ¬∑ 9th in this Competition arrow_drop_up 2 more_vert Thanks for sharing your approach, @yoseflachman . Did you engineer any features or just use the given features? Yosef Lachman Topic Author Posted a year ago ¬∑ 8th in this Competition arrow_drop_up 2 more_vert I used both the original features and some that were posted to the discussion forum. They are: Previously_Insured_Annual_Premium Previously_Insured_Vehicle_Age Previously_Insured_Vehicle_Damage Previously_Insured_Vintage Rubanza Silva Posted 10 months ago ¬∑ 433rd in this Competition arrow_drop_up 0 more_vert A single model solution. Great work ! @yoseflachman . This comment has been deleted. Yosef Lachman Topic Author Posted a year ago ¬∑ 8th in this Competition arrow_drop_up 0 more_vert Thank you for your kind words and congratulations! Regarding your approach, I indeed used a similar strategy. I found that a lower learning rate combined with a higher number of iterations helped the model capture subtle patterns without overfitting. Your idea of using a 0.01 learning rate with 50,000 iterations seems like a good plan, especially if you're aiming for even more fine-grained learning. I'd say it's worth experimenting with your proposed settings. Just beware that the training will take a LONG time! 30k iter with 10 fold took 15 hours and that was with early stopping. So I could only imagine if it ever does manage to reach the 50k. What GPU are you planning on running it on? And best of luck, and feel free to share your results or message me privately. This comment has been deleted. Appreciation (1) Alexandre Constantino Leal Posted a year ago arrow_drop_up 1 more_vert Congratulations! Thanks for sharing! Too many requests error Too many requests",
      "Binary Classification of Insurance Cross Selling  | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Binary Classification of Insurance Cross Selling Playground Series - Season 4, Episode 7 Binary Classification of Insurance Cross Selling Overview Data Code Models Discussion Leaderboard Rules Oscar Aguilar ¬∑ 9th in this Competition  ¬∑ Posted a year ago arrow_drop_up 16 more_vert #9 Solution | 24 Models + Hill Climbing First, I would like to express my gratitude to the Kaggle team for organizing this competition. It was my first time handling such a large dataset, which made the project exciting from the start. In this post, I will provide a brief overview of my approach. Initial Modeling In the initial days of the competition, due to the large size of the data, I chose to model the data using Logistic Regression . This approach provided some initial insights about the data. For example, the performance of the Logistic Regression model improved significantly when I used TargetEncoding . This suggests that certain features, even though stored as numbers, should be considered categorical. You can find more details in this post . Data I used both the competition and original datasets. Data preprocessing In order to reduce memory usage, I considered the following preprocessing steps: def converting_datatypes(df, df_train= False ):\n\n    df = df.copy()\n\n    if df_train== False :\n\n        df[ 'Policy_Sales_Channel' ] = np.where(df[ 'Policy_Sales_Channel' ]== 144. , 145. , df[ 'Policy_Sales_Channel' ])\n        df[ 'Policy_Sales_Channel' ] = np.where(df[ 'Policy_Sales_Channel' ]== 149. , 150. , df[ 'Policy_Sales_Channel' ])\n\n    df[ 'Age' ] = df[ 'Age' ].astype( 'int8' )\n    df[ 'Driving_License' ] = df[ 'Driving_License' ].astype( 'int8' )\n    df[ 'Region_Code' ] = df[ 'Region_Code' ].astype( 'int8' )\n    df[ 'Previously_Insured' ] = df[ 'Previously_Insured' ].astype( 'int8' )\n    df[ 'Annual_Premium' ] = df[ 'Annual_Premium' ].astype( 'int32' )\n    df[ 'Policy_Sales_Channel' ] = df[ 'Policy_Sales_Channel' ].astype( 'int16' )\n    df[ 'Vintage' ] = df[ 'Vintage' ].astype( 'int16' )\n    df[ 'Gender' ] = df[ 'Gender' ].map({ 'Female' : 0 , 'Male' : 1 }).astype( 'int8' )\n\n    df[ 'Vehicle_Age' ] = df[ 'Vehicle_Age' ].map({ '< 1 Year' : 0 , '1-2 Year' : 1 , '> 2 Years' : 2 }).astype( 'int8' )\n\n    df[ 'Vehicle_Damage' ] = df[ 'Vehicle_Damage' ].map({ 'No' : 0 , 'Yes' : 1 }).astype( 'int8' )\n\n    if df_train== True :\n\n        df[ 'Response' ] = df[ 'Response' ].astype( 'int8' )\n\n    return df content_copy Feature Engineering I considered the following features: def fe(df_train, df_test, df_original):\n\n    n = df_train.shape[ 0 ]\n    m = df_test.shape[ 0 ]\n    p = n+m\n    df_tot = pd.concat([df_train, df_test, df_original], axis= 0 ).reset_index(drop= True )\n\n    df_tot[ 'interaction_1' ] = pd.factorize((df_tot[ 'Previously_Insured' ] + df_tot[ 'Vehicle_Age' ]).to_numpy())[ 0 ]\n    df_tot[ 'interaction_2' ] = pd.factorize((df_tot[ 'Previously_Insured' ] + df_tot[ 'Vehicle_Damage' ]).to_numpy())[ 0 ]\n    df_tot[ 'interaction_3' ] = pd.factorize((df_tot[ 'Previously_Insured' ] + df_tot[ 'Vintage' ]).to_numpy())[ 0 ]\n    df_tot[ 'interaction_4' ] = pd.factorize((df_tot[ 'Previously_Insured' ] + df_tot[ 'Annual_Premium' ]).to_numpy())[ 0 ]\n    df_tot[ 'interaction_5' ] = pd.factorize((df_tot[ 'Previously_Insured' ] + df_tot[ 'Gender' ]).to_numpy())[ 0 ]\n    df_tot[ 'interaction_6' ] = pd.factorize((df_tot[ 'Previously_Insured' ] + df_tot[ 'Driving_License' ]).to_numpy())[ 0 ]\n    df_tot[ 'interaction_7' ] = pd.factorize((df_tot[ 'Vehicle_Age' ] + df_tot[ 'Vehicle_Damage' ]).to_numpy())[ 0 ]\n    df_tot[ 'interaction_8' ] = pd.factorize((df_tot[ 'Vehicle_Age' ] + df_tot[ 'Driving_License' ]).to_numpy())[ 0 ]\n\n\n    return [df_tot[:n], df_tot.iloc[n:p].drop(columns=[ 'Response' ], axis= 1 ), df_tot[p:]] content_copy Models I considered the following models: Random Forest (under the LGBM framework because it runs faster). For more details, see this pos . LGBMClassifier XGBClassifer TensorFlow. The first version of the TensorFlow I built reached a 0.882 oof ROC-AUC over 10 folds. Then, I used @paddykb TensorFlow model (see this notebook ) because it outperformed my initial TensorFlow model. Catboost. I think everyone in this competition has used some sort of version of the @rohanrao Catboost model. For more detail see this notebook . The following table show the best performance over 10-folds of each of the considered models. Model Competition Data Competition + Original Data CatBoost 0.895311 0.895819 LGBM 0.892605 0.892753 TensorFlow 0.892083 0.892213 XGBoost 0.890959 0.89105 Random Forest 0.873091 0.875128 Ensemble In the Kaggle community, it is widely acknowledged that the ROC-AUC score can be optimized by the ensemble of different model predictions. To achieve this, I employed the hill-climbing strategy. This strategy, akin to climbing a hill, involves combining model predictions in a linear fashion, always moving upwards as long as the objective metric improves. The final ensemble includes 24 models: Model Competition Data Competition + Original Data CatBoost 4 models 4 models LGBM 3 models 3 models TensorFlow 3 models 3 models XGBosst 1 model 1 model Random Forest 1 model 1 model From the above table, it is important to note that not the same models were trained in the data. For instance, 4 CatBoost models were trained on the competition data; however, different features were used to trained those models. Post Processing Finally, I post-process the predictions of the hill-climbing ensemble using @paddykb suggestion presented in this post . I am looking forward to the next episode. Please sign in to reply to this topic. comment 9 Comments Hotness Optimistix Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Congratulations! Always a pleasure to read your code and approach. Samuel Cortinhas Posted a year ago arrow_drop_up 1 more_vert Nice to see hill climbing is still effective :D Oscar Aguilar Topic Author Posted a year ago ¬∑ 9th in this Competition arrow_drop_up 0 more_vert It is, specially with the ROC-AUC metric. Tilii Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert I already wrote about this in the other thread, but will repeat here: In my hands, hill-climbing scores were ~0.00015 below conventional stacking. That's when testing different weights in 0.01 increments (the dataset size precluded running smaller increments for > 10 models). If you want that expressed in different terms, the best individual CV score was 0.896692 after reversing 8149 labels (private LB 0.89699) and the best hill-climbing ensemble was 0.897212 after reversing 8149 labels (private LB 0.89735). On the same set of individual models, stacking gave 0.897386 after reversing 8149 labels (private LB 0.89748). emoji_people Sheikh Muhammad Abdullah Posted a year ago ¬∑ 515th in this Competition arrow_drop_up 1 more_vert Congratulations! I appreciate your effort. kailai Posted a year ago ¬∑ 55th in this Competition arrow_drop_up 1 more_vert Well done! MariFel Posted a year ago arrow_drop_up 0 more_vert share your hill climber code @oscarm524 Oscar Aguilar Topic Author Posted a year ago ¬∑ 9th in this Competition arrow_drop_up 0 more_vert @marifel , take at this post from last year. In that competition, I also used hill-climbing. I hope this helps. Tilii Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert There is a library that simplifies the hill-climbing approach: https://github.com/Matt-OP/hillclimbers/ You load OOF files and predictions, choose the weight increment and the metric, and off it goes. Optimistix Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Unless you hit it with 61 models and over 11 million rows - then it doesn't go anywhere for a vey long time indeed üòÄ Tilii Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert True dat, at least when it comes to this competition. My point was that it is easy to get it working. Optimistix Posted a year ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Yes, I was kicking myself for not trying it earlier - turns it out it may not have helped me this time, but I'll be sure to use it from now on. I quite like the visuals, as I seem to recall you do, too. Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 4, Episode 7 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Health Insurance Cross Sell Prediction Data dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. This notebook gives more details about the dataset used for this competition. 3 files 1.2 GB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 1.2 GB sample_submission.csv test.csv train.csv 3 files 25 columns  Too many requests",
    "data_description": "Binary Classification of Insurance Cross Selling  | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a year ago Late Submission more_horiz Binary Classification of Insurance Cross Selling Playground Series - Season 4, Episode 7 Binary Classification of Insurance Cross Selling Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: The objective of this competition is to predict which customers respond positively to an automobile insurance offer. Start Jul 1, 2024 Close Aug 1, 2024 Evaluation link keyboard_arrow_up Submissions are evaluated using area under the ROC curve using the predicted probabilities and the ground truth targets. Submission File For each id row in the test set, you must predict the probability of the target, Response . The file should contain a header and have the following format: id ,Response 11504798 , 0 . 5 11504799 , 0 . 5 11504800 , 0 . 5 etc . content_copy Timeline link keyboard_arrow_up Start Date - July 1, 2024 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  July 31, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Binary Classification of Insurance Cross Selling . https://kaggle.com/competitions/playground-series-s4e7, 2024. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 6,545 Entrants 2,425 Participants 2,234 Teams 15,844 Submissions Tags Beginner Tabular Binary Classification Roc Auc Score Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s4e8",
    "discussion_links": [
      "/competitions/playground-series-s4e8/discussion/531823",
      "/competitions/playground-series-s4e8/discussion/523656",
      "/competitions/playground-series-s4e8/discussion/531330",
      "/competitions/playground-series-s4e8/discussion/531374",
      "/competitions/playground-series-s4e8/discussion/531424",
      "/competitions/playground-series-s4e8/discussion/531347"
    ],
    "discussion_texts": [
      "Binary Prediction of Poisonous Mushrooms | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 10 months ago Late Submission more_horiz Binary Prediction of Poisonous Mushrooms Playground Series - Season 4, Episode 8 Binary Prediction of Poisonous Mushrooms Overview Data Code Models Discussion Leaderboard Rules Optimistix ¬∑ 1st in this Competition  ¬∑ Posted 10 months ago arrow_drop_up 51 more_vert 1st Place Solution: 72 OOFs, a whole lotta Autogluon, and 31 scores of 0.98512 or above (on the private LB) Apologies for such a long post - to paraphrase the famous words of Blaise Pascal, I didn't have the time to make it shorter. Well, that was a very satisfying competition indeed! The core of my success was the same as described (along with some back story) in my post from last month ( 4th place solution, PSS4E8 ) - large ensembles, and a whole lotta hustling in the absence of serious resources outside Kaggle. While this month wasn't as frustrating as last month, with \"only\" 3 million samples instead of last month's 11, we did have about twice as many variables. But everything was more manageable in terms of space and storage, though I did face some familiar frustrations in terms of GPU quota and 12 hour run limits on Kaggle. The TLDR is similar: Gathered a veritable zoo of models, ensembled them, kept an eye on CV & model diversity, & kept ensembling while score increased Kept running out of GPU and execution time (at 12 hours) on Kaggle Experimented a lot more with Autogluon Kept experimenting with everything: new models, hyperparameters, ensembling approaches, etc. Ended up with nearly 80 OOF arrays, used 72 in the end Ended up with 31 scores of 0.98512 or above (0.98511 being the second highest on the private LB), the first of which was achieved on August 17th, with two weeks remaining in the competition. Before I go on, let me  acknowledge the generosity of those who shared their insights, findings and code, including but not limited to @ambrosm , @siukeitin , @nischaydnk , @gauravduttakiit , @rzatemizel , @ravaghi , @oscarm524 , @ravi20076 , @tilii7 , @roberthatch , @omidbaghchehsaraei , @trupologhelper , @arunklenin , @carlmcbrideellis It was great to have @ambrosm back this month, as another of  his wonderful EDA notebooks helped many of us get up and running - it (and some blending) helped me get to a score of 0.98516 on day 1 (private: 0.98498). A few things were clear right on day 1, including that Autogluon was doing very well on this dataset, and I noticed in @gauravduttakiit 's notebook with LazyPredict results that Random Forests and Extra Trees were quite competitive this time around, and made a mental note about including them in my ensembles. As soon as I saw @siukeitin 's brilliant post about an exact solution to the original dataset, I added the probability of being poisonous as a new feature, reasoning that it could be a proxy for the \"signal\" from the original still present in the current dataset. It helped boost the score of some models, just as including the original did - and even when they didn't boost the score, they added to the diversity of the ensemble. @carlmcbrideellis was the catalyst for that work by @siukeitin , as he provided a dataset with a million mushrooms, based on the original dataset. He also initiated a competition for perfectly predicting the labels on his dataset in the least time - playing around with that helped me figure out that one can speed up LGBM by setting \"num_threads\" = number of CPU cores. I spent a fair amount of time on Autogluon (AG) this month, as there were so many notebooks using it to achieve great scores on the public LB. @gauravduttakiit also showed the importance of using GPUs and long runs with AG, when just using a GPU made the score jump from 0.98482 to 0.98524, without changing the rest of the code. Meanwhile I had to ensemble a few dozen models to get anywhere near that. I immediately launched a long GPU run of AG, which led to the single-most frustrating moment of the competition, as Kaggle killed the notebook after 12 hours, right in the middle of producing the output files üò° From the beginning, I explored ensembling using various methods, including Hill Climbing, which was more feasible this month than last, when it was a nonstarter beyond 10-15 models. This month I used it till the very end, though it took over 2 hours once I went beyond 60 models or so. One of my breakthrough scores (0.98530 without any blending) came via a combination of Ridge and GBDTs for ensembling. However, Ridge generally gave the best combination of speed and LB score, so most of my submissions used that. It helped me get to the first score where I started to feel confident, as an ensemble of about 50 models achieved 0.98525 without any \"blind blending\". Unbeknownst to me, this also achieved a score of 0.98512 on the private LB (with 2 weeks to go), which might have sufficed to win. So in some sense, I was running up the (teeny) margin after this, though of course I had no way of knowing this. Highs and lows of experiments with Autogluon (AG) With about 10 days remaining, I decide to invest more time in experimenting with AG, which seemed likely to help push my scores further. After perusing several notebooks, I noticed that XGBoost and CatBoost were the weakest models within AG, which was interesting, especially since XGBoost was the best performing outside of AG/AutoML. I reasoned that excluding them might improve AG's score by giving more time to the better performing models - it didn't improve, but it didn't worsen either, and one could achieve the same score in about half the time. I then noticed that the top ensemble was almost always of GBM and XT alone, so I dropped everything else, and the score was only about 0.0001 less. Finally, I decided to run individual models alone via AG, and ensemble them myself, to see whether this would allow each model more time, and thereby lead to an improved ensemble score, and it did, though again only by about 0.0001. Finally, I decided to throw my OOFs into AG - but that's a story for later (about two paragraphs later). Three ways to 0.98535, en route to 0.98537 After the \"debacle\" of June, when I held the no. 1 spot for the second half of the month while overfitting to oblivion, I remained focused on building a robust ensemble. But I wasn't above some \"blind-blending\" when I had submissions remaining for the day with too little time to get new results to add to the ensemble, and indeed that was part of how I first got to number 1 this month. I did try to use two other solutions (Gaurav & @nischaydnk 's) built differently from mine but with about the same score of 0.98525, and ended up with 0.98532. A few days later, two such blends of mine with @arunklenin 's 0.98527 got to me to 0.98534 and into the lead. Finally I used the \"insert confident disagreements\" approach to overwrite my prediction with those of another model/ensemble, if the latter produced a sufficiently high probability (say > 0.99), which produced my first LB score of 0.98535, albeit on rather shaky grounds (the private LB for this turned out to be mere 0.98506). So far, my CV scores had generally been < 0.98510 (with ensembles), and < 0.9850 with solo models (range: 0.97844 - 0.98494). I started using AG OOFs along with the solo models, and this finally helped me get to a CV of 0.985087, and LB of 0.98533 (private: 0.98513) with 66 OOFs. At this point, I was starting to feel good, since this was easily the best score I'd obtained without any blending with others submissions. Meanwhile, AG with CPU was giving me about 0.98524. Finally, I decided to throw in some OOFs into AG - I was wary of too many OOFs exhausting the run time on Kaggle, so I used Hill Climbing to decide which OOFs to use, and added the 8 which were chosen by hill climbing on top of the highest scoring AG OOF. Throwing this mix into AG, I launched the run and frantically kept monitoring the intermediate results, until the run concluded with an AG leaderboard score of 0.985124. Excited, I submitted with anticipation, and bingo! the LB score was 0.98532 (private: 0.98516). With ensembles of 0.98533 and 0.98532, I was feeling better and better, though I was quite aware that any number of brilliant Kagglers could overtake me at any time (some probably hidden by the army of blenders lurking nearby as well). At long last, I decided to throw caution to the winds, and threw all 72 OOFs into AG, and to my delight, even a CPU run produced 0.98535 on the LB (private: 0.98512), an 0.98535 that I was much more confident about than the first one. In the meantime, I'd seen many people stuck on 0.98533 and 0.98534 for days, so it did seem that 0.98535 was potentially close to a winning score. I had no GPU quota left, so I searched outside Kaggle, in vain. Saturn Cloud offered 15 hours per month, but you couldn't run anything for that long at a go without assistance from their team. Lightning.ai offered 22 hours per month, but no more than 4 hours of GPU at a time. Nevertheless, I tried to repeat the AG run with 72 OOFs there, and quickly realized that they lacked several packages I took for granted on Kaggle, as they were set up for Deep Learning. So no LGBM (initially a shock!), and so on. I noticed that they had an option to use 32 CPUs, so I decided to go for a 3 hour run with that, reasoning that it might just be better than 12 hours with 4 CPUs on Kaggle. I was afraid that the results might be underwhelming, but to my great relief, it produced another 0.98535 (private: 0.98513). By this time, there were lots of people right behind me - I was more concerned about known strong performers like @tilii7 at 0.98533 and @oscarm524 at 0.98532, since I knew they were doing solid work and not just blending away into the ether. I indulged in a not so blind blend of my 0.98535s, which led to a public LB score of 0.98537 (private: 0.98513) - I knew it wasn't necessarily going to score any higher on the private LB, but at least it might give pause to some of the pursuers üòÄ I also did some trial and error ensembling of my two solid 0.98535s, but didn't choose any of them among the final two, as they scored 0.98535 or lower. Interestingly, my best private LB scores came from here - a 50-50 blend produced a private score of 0.98517, my highest; several others produced 0.98514. A 90-10 blend of my highest 0.98535 with the 0.98533 from a Ridge ensemble of 66 models produced a public score of 0.98533, but private of 0.98516 (second highest). Moral of the story - trust your CV score, and keep building while keeping CV and LB in good agreement. Avoid blind blending as much as possible, tempting though it may be. I had some great plans for the last few days, but a combination of a  family emergency & running out of steam meant most of it remained unrealized. I couldn't follow @tilii7 's advice of learning xLearn, didn't run models like TabNet which I'd run last time, and didn't do a sufficiently deep dive into optimizing any one model, like pushing XGBoost beyond 0.9850 (CV), or rescuing CatBoost from the CV < 0.9848 range, etc. All along, my public LB scores were about 0.0002 more than my CV scores with Ridge, and 0.0001 than my CV scores with Hill Climbing. So I was expecting the private LB scores to be about the same as my CV scores, and that proved to be the case. Early in the month, many had expressed confidence that there wouldn't be a major shakeup, as we had millions of samples, whereas some, like @oscarm524 , expected a shakeup, since people would deal with the noisy data in various ways that may not generalize to the private data. In the end, the blenders proved that one can indeed overfit even datasets with millions of samples, as there was quite a shakeup. On the other hand, people like @neupane9sujal , @bwandowando , @co000l , @ravaghi , @roberthatch and others made impressive jumps of 50-200 positions on the LB! Congratulations to them and everyone who finished in the Top 10 or 25. Personally, after dropping from 1 to 113 two months ago, this month feels like having grown a bit as a Kaggler. All this month, I kept meaning to turn away from Kaggle & spend more time on the course on LLMs that I'm (supposed to be) also doing, but I was pretty much obsessed. Last month, I came 4th but got a tshirt thanks to @tilii7 having already finished in the Top 3 before. I'd said then that one day, I'll earn the t-shirt that someone else gets - it's immensely gratifying to have that come true already. Now that I've managed to get a t-shirt and the no. 1 spot, I shall step back and participate more judiciously, as I really need to put in time on the LLM course (any pointers to interesting datasets for an LLM project? Thanks in advance!). I'll keep submitting from time to time, but shall keep intense participation for the last week of the month, if then. All the best to everyone! It's been an amazing six months chasing the leaderboard in the playground series! Many thanks to everyone who helped along the way. I want to start spending more time on the rest of Kaggle (and elsewhere) now, but shall continue to participate in the Playground Series, which has been one of the best things about a difficult year for me - many thanks to Kaggle, and to all of you who make this such a fun and engaging experience. Happy Kaggling! 10 19 Please sign in to reply to this topic. comment 51 Comments Hotness Harshit Sharma Posted 10 months ago ¬∑ 441st in this Competition arrow_drop_up 7 more_vert Congratulations @optimistix on an incredible win and detailed breakdown‚Äîyour persistence and ensembling mastery truly paid off! üéâ Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thank you so much! Saim Nasir Posted 9 months ago arrow_drop_up 1 more_vert Thanks alot brother, this is something that everyone needs to acknowledge! Optimistix Topic Author Posted 8 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thank you! BHANUPRAKASH Posted 9 months ago arrow_drop_up 1 more_vert Congratulations for acing the top 1 position..!! Optimistix Topic Author Posted 8 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks so much! KBRASK Posted 9 months ago ¬∑ 2050th in this Competition arrow_drop_up 1 more_vert Thank you! Optimistix Topic Author Posted 9 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert You're most welcome! Mahdieh Mortazavi Posted 10 months ago ¬∑ 172nd in this Competition arrow_drop_up 1 more_vert Thank you for generously sharing your knowledge with us. Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert You're most welcome! I've just done what's expected - and I've personally benefited from many others sharing their knowledge, so it's the least I can do. Sahada Jam's Posted 9 months ago arrow_drop_up 1 more_vert Ok. Finally wy TH. ‡∏¢‡∏±‡∏ö‡πÄ‡∏¢‡∏¥‡∏ô Go To Dubai. üòÜüòÜüòÜ Sahada Jam's Posted 9 months ago arrow_drop_up 1 more_vert Okay Go.üòÑüòÑüòÑüòÑ Catadanna Posted 10 months ago ¬∑ 521st in this Competition arrow_drop_up 1 more_vert Kaggle killed the notebook after 12 hours, right in the middle of producing the output files I know that so well, and as you say : üò° Swandip Singha Posted 10 months ago ¬∑ 59th in this Competition arrow_drop_up 1 more_vert congratulation bro for being top 1 Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks, and congrats for your top 2.5% finish! Kunritty He Posted 10 months ago ¬∑ 684th in this Competition arrow_drop_up 1 more_vert Really informational notebook! As a beginner, I was just wondering how much time do you dedicate to feature engineering-finding what changes and additions work and which don't? Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks so much! I generally don't spend too much time on feature engineering, as the noisy nature of the synthetic data used in the playground series often renders it useless. But it depends on the data in a specific competition. I also regularly read and participate in discussions, and sometimes that leads to useful features being added to my arsenal. Tanishk Patil Posted 10 months ago ¬∑ 740th in this Competition arrow_drop_up 1 more_vert Congratulation! Learned something new today. Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks! Glad you found it useful. Anupam Kumar Paul Posted 10 months ago ¬∑ 1298th in this Competition arrow_drop_up 1 more_vert First off, congrats on winning!!! This was my second competition and i will say my ranking has improved and i now know quite few more things. What is OOF though? And could you give some pointers on where to learn all these different models used by everyone? Also as a newbie, how to process the notebooks faster or like defeat the 12 hour limit on kaggle?? Thanks for your time!! Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks! OOF stands for \"out-of-fold\" to see what that means and how such predictions are used for ensembling, you could check out this link . To learn about various models, you can use your preferred combination of textbooks, videos, online materials and Kaggle notebooks and discussions. In terms of processing faster, a few simple things could be optimizing your own Python code, using libraries that support faster computation (e.g. Polars, cuDF, etc.), tweaking hyperparameters of models, etc. Note that defeating the 12 hour limit may not always be possible, or may involve a tradeoff - e.g. you can specify how long Autogluon should run, and can get useful results in as little as 15-30 minutes, but the longer you let it run, the better the results shall usually be. Sujal Neupane Posted 10 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert @optimistix Congratulations on 1st position and that is nicely detailed write-up. I got some new ideas that I'll definately try out. And again congrats!!!!! Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks, and congrats again on your no. 2 finish, and awesome jump of nearly 200 on the LB! MrSimple Posted 10 months ago ¬∑ 118th in this Competition arrow_drop_up 1 more_vert Amazing work! Congrats! Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks so much for the kind words, @mrsimple07 ! EUISANG LEE Posted 10 months ago ¬∑ 939th in this Competition arrow_drop_up 1 more_vert First of all, really congratulations!!! This is my first time participating in this competition, and although I am not ranked high, I was able to think about many things while participating. And while reading your article, I felt inspired and passionate. Lastly, congratulations again. Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks so much! Glad it was useful to you. Pushpit Kamboj Posted 10 months ago arrow_drop_up 1 more_vert can you share the competition notebook for reference Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert There's no single notebook - the work is scattered across dozens of notebooks. However, you could refer to the excellent notebooks by @ambrosm , @ravaghi & @rzatemizel to see how to use multiple models along with generating and saving OOF matrices, and then using them for ensembling. I did the same, but for a large number of models. You can also refer to @siukeitin 's notebook for an exact solution for the original dataset. Pushpit Kamboj Posted 10 months ago arrow_drop_up 1 more_vert Thanks for the help Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert You're most welcome! Robert Hatch Posted 10 months ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert This was an awesome read - well deserved and congrats! Robert Hatch Posted 10 months ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert This was the second competition where the original dataset had no noise, so it was all injected by the GAN. LGB was amazing in the other one too. And never tried XT last time, but in this one I think it was really really amazing for learning a form of \"which half of this half-poisonous half-edible is more than half?\" The nature of the noise probably playes will with the same algorithmic features that make XT good at outlier detection(?) Just speculation but it makes sense to me. Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks so much! Great point about LGB being amazing when the original dataset was noise-free (I think the flooding dataset was somewhat fact-free as well). Interesting speculation about XT - I should look into them in more detail. Robert Hatch Posted 10 months ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert \"fact-free\" ROFL well said! Jack Lee Posted 10 months ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert Congratulations on winning first place, and thank you for sharing your detailed insights! Your use of GPUs and incorporating OOF into AutoGluon has been truly enlightening for me. Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks so much, and congrats on your 8th place finish! Paulo Gaspar Posted 10 months ago ¬∑ 1305th in this Competition arrow_drop_up 1 more_vert Congratulations on such achieving! Thanks for such  an inspiring and detailed explanation. Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks so much for your kind words, @pauloagaspar ! Tilii Posted 10 months ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert @optimistix Well done, and thank you for summarizing everything. Good to see that your determination and a systematic approach paid off. It seems to me that you refined the whole approach to competitions over the past 4-5 months, and in my book that is worth more than any LB placement. Still, I am happy that you also got a top spot, and convincingly so - at least by Kaggle standards. Separately, I got about 4 more mentions in your writing than I deserved, so thank you for that. Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks! The learning is definitely what matters in the long run, but in the short term I'm not above extracting a little extra satisfaction from tiny differences in the fifth decimal place, and ending up high on the LB. And every time you were mentioned was absolutely deserved üòÄ Michael Tinglof Posted 10 months ago ¬∑ 1485th in this Competition arrow_drop_up 1 more_vert Thank you for such a detailed explanation! This is my first time hearing of Autogluon. I will need to give it a shot for the next competition! Again, congrats on the 1st place. Hopefully you'll revel in this for a few months before coming back to the competitions. Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks! I suspect I'll hang around, but in low-key mode. JuseTiz Posted 10 months ago ¬∑ 107th in this Competition arrow_drop_up 1 more_vert Congratulations on securing first place! I found your detailed solution incredibly insightful and learned a lot from it. I have a question regarding model diversity. You emphasized its importance in your approach, and I‚Äôm curious‚Äîdid you incorporate different feature engineering techniques in the out-of-fold predictions to enhance the performance of the final ensemble? Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks! Glad it was useful to you. I'm not sure what you mean by incorporating different feature engineering techniques in the out-of-fold predictions - do you mean, e.g. running the same model with different feature subsets? Or actually engineering different features for different models? JuseTiz Posted 10 months ago ¬∑ 107th in this Competition arrow_drop_up 1 more_vert Thank you for your response. I apologize if my previous question was unclear. To clarify: For example, when you run LightGBM (or any other models), do you use different feature combinations, such as combination A and combination B, each with varying hyperparameters, to generate multiple OOF and test data predictions? Or, are all the results for your final ensemble based on the same feature combination, meaning that all models use exactly the same set of features? Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks for clarifying. I don't usually drop the original features, but yes, newly derived features will be included with some models but not others. Most of the variety comes from playing around with the hyperparameters, including or excluding the original dataset, varying the number of folds, etc. Mahdi Ravaghi Posted 10 months ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Thank you for a very comprehensive write-up! I was waiting anxiously for this. You have worked very hard and smart on this competition and earned yourself a well deserved 1st place. Big congratulations to you. Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks so much! Was delayed partly due to the GP, and partly the aforementioned emergency. I hope it's not too late to find many readers. And congrats once again on becoming a Master! Mahdi Ravaghi Posted 10 months ago ¬∑ 10th in this Competition arrow_drop_up 2 more_vert Thanks! I assume there are many like myself who want to know how you won and learn from your solution. More readers will surely come. Jesus Jhonny Posted 10 months ago arrow_drop_up 2 more_vert Congratulations on Your Amazing Kaggle Win! Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks for the support! gregory gorbov Posted 10 months ago ¬∑ 517th in this Competition arrow_drop_up 2 more_vert Could u please explain what is blind blending ? Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Sorry if you thought it's an established term - you could also say e.g. ad-hoc blending, it's my way of referring to the case when people just combine predictions without any insight into how they were produced, e.g. taking the output of many public notebooks, and simply using their mode as your submission. This often works well on the public LB (or appears to, because you can see the public LB score), but usually fails to generalize to the private LB. gregory gorbov Posted 10 months ago ¬∑ 517th in this Competition arrow_drop_up 0 more_vert Thx very much for clearity. I want to know what is the other way to get weights, could you name a several if it is not a big discussion. As i understand it is better to use local CV score as weights for ensamble cuz it gives more generalizable score Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert It's a huge topic, and this is the best resource that I know of: https://web.archive.org/web/20160304031055/http://mlwave.com/kaggle-ensembling-guide/ SCRIPTCHEF Posted 10 months ago ¬∑ 287th in this Competition arrow_drop_up 2 more_vert Well done, this was definitely an inspiring story and impressive that you pulled through to first despite life getting in the way. Proves a slow steady systematic approach is better than ad hoc blending. Keep it up! Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thank you! Oscar Aguilar Posted 10 months ago ¬∑ 19th in this Competition arrow_drop_up 2 more_vert @optimistix , congratulations on winning first place! These days, winning a competition (even a tabular competition) is not easy and takes a lot of hard work and consistent dedication. Your approach write-up really shows all the work you put into this competition. By the way, thanks for the mention. Optimistix Topic Author Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Thanks so much, and you're most welcome - I always enjoy reading your clean code and write ups. One of the things I forgot to mention in my write up - I also tried your approach of ensembling by model type, and then ensembling the ensembles, but it didn't improve the overall score (but it was fun to see nice big numbers against each model class/category). Too many requests error Too many requests",
      "Binary Prediction of Poisonous Mushrooms | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 10 months ago Late Submission more_horiz Binary Prediction of Poisonous Mushrooms Playground Series - Season 4, Episode 8 Binary Prediction of Poisonous Mushrooms Overview Data Code Models Discussion Leaderboard Rules Lennart Purucker ¬∑ 3rd in this Competition  ¬∑ Posted a year ago arrow_drop_up 74 more_vert [1st Place Solution AutoML Grand Prix] AutoML Grandmasters: AutoGluon Distributed + Post-Hoc Ensembling Heyho everyone! Another exciting 24 hours of automatically fitting machine learning models! Kudos to all the competitors, and a shoutout to Team AGA, who gave us a run for our money with an early 0.98533 score that we spent the next 12 hours trying to beat. In the end, the winner was decided by a difference in score so small that Kaggle‚Äôs leaderboard doesn‚Äôt even visualize it, so we consider ourselves pretty lucky to have eeked it out. We detail our approach to the fourth Grand Prix in the following. But first, we would like to highlight that the International Conference on Automated Machine Learning 2024 is is taking place in Paris from September 09. to 12. (see https://2024.automl.cc/) ! We are going to be at the conference and would be happy to meet other (Auto)ML enthusiasts from Kaggle at the conference :) Overview Figure Summary We used AutoGluon as our AutoML system, installing via source on the latest mainline with tweaks to enable distributed computation (see below). With it, our approach to this competition can be summarized as follows: Preprocessing We replaced noisy observations in the data. Nothing else improved our offline cross-validation (CV) score. Model Fitting We ran a default version of AutoGluon for one hour and a custom version of AutoGluon for four hours. For the custom version, we used the following settings: We used log loss as an early stopping metric while optimizing/picking based on the target metric MCC. We are unsure if this helped, but it improved our offline CV score. We used 16-fold cross-validation and AutoGluon‚Äôs multi-layer stacking implementation. We trained a customized portfolio of models, meta-learned from TabRepo (i.e., zero-shot HPO) We used 100 iterations (instead of the default 25) for post hoc ensembling . Kaggle Tricks AutoGluon rounds to the first 6 decimals of the score when determining tiebreakers during the final weighted ensemble. It turns out that is too few for the deltas we were looking for, so we upped it to 8 at the very end to eke out an epsilon improvement. We manually created a post hoc ensembling logic for this competition in an effort to go from 0.98531 to 0.98533 . We cached all pred probas from all models on all experiments and then ran the post hoc ensembling for the final solution. This allowed us to just barely leapfrog Team AGA! Compute After the last competition, we felt the need to expand our computing power for these (very) large data competitions. Therefore, in addition to using an individual AWS compute instance, we put much effort into using AutoGluon distributed across compute nodes. We used a prototype of a distributed version of AutoGluon to parallelize AutoGluon across compute resources. We used an in-house SLURM cluster (from the University of Freiburg) together with Ray to distribute AutoGluon‚Äôs model training across 1000 CPUs. Code The supplementary code repository for this write-up can be found here: https://github.com/AutoML-Grandmasters/Fourth-AutoML-Grand-Prix/tree/main Our copy-able settings for the custom run of AutoGluon can be found in this file . Preprocessing We used the following code to clean noisy observations that do not exist in the test predictions by setting them to nan. import numpy as np import pandas as pd\n\ntrain_data = pd.read_csv( \"./train.csv\" )\ntest_data = pd.read_csv( \"./test.csv\" )\n\nweird_columns = [ \"cap-shape\" , \"cap-surface\" , \"cap-color\" , \"gill-attachment\" , \"gill-spacing\" , \"gill-color\" , \"veil-type\" , \"veil-color\" , \"has-ring\" , \"ring-type\" , \"spore-print-color\" , \"habitat\" , \"does-bruise-or-bleed\" , \"stem-root\" , \"stem-surface\" , \"stem-color\" ,\n] for col in weird_columns:\n    allowed_vals = test_data[col].unique()\n    train_data.loc[~train_data[col].isin(allowed_vals), col] = np.nan\n    test_data.loc[~test_data[col].isin(allowed_vals), col] = np.nan content_copy Early Stopping Metric To early stop on log_loss , pass the following to AutoGluon‚Äôs fit call: ag_args_fit={\"stopping_metric\": \"log_loss\"} . This can sometimes help as early stopping on threshold-based metrics such as MCC can be too early. Distributed AutoGluon & Compute Here is the current version of distributed AutoGluon: https://github.com/LennartPurucker/autogluon/tree/distributed_autogluon An example script with more details on how to use it can be found here: https://github.com/AutoML-Grandmasters/Fourth-AutoML-Grand-Prix/blob/main/autogluon_distributed_example.py In our experience with it, it is mostly stable but has a few GPU-related problems that we hope to fix for the prototype. We plan to integrate a more mature version of this into mainline AutoGluon in the future (and are already working on it). We used 1000 CPUs spread across nodes with 20 or 32 Intel(R) Xeon(R) Gold 6242 CPUs @ 2.80GHz, each node with around 150 GB of RAM. The cluster is managed with SLURM , and we used Ray to create a sub-cluster that AutoGluon can (natively) use to fit models. To run AutoGluon distributed, only a Ray cluster is needed, which can be created on local compute, SLURM clusters, or cloud resources. Additionally, we used an AWS m7i.48xlarge EC2 instance with 192 vCPUs to run default AutoGluon. Customized Portfolio To obtain a better portfolio than currently existing in AutoGluon, we re-ran the work of the TabRepo paper (to be presented at the AutoML conference), but use a portfolio of size 200 instead of the 100 size portfolio used by AutoGluon‚Äôs best_quality setting. We also included more model families: Linear models and KNN, although some of those models we ended up disabling those since they weren‚Äôt helping and, at times, took a long time to infer. We additionally added configurations with larger max_bin values for LightGBM, XGBoost, and CatBoost. Moreover, we removed a set of configurations that we found to take too long to predict for large datasets. You can find the final portfolio in this file . We filtered the portfolio to models that were working well (see here ). More Iterations for Post Hoc Ensembling We wanted to find a better final post hoc ensemble by giving the greedy ensemble selection in AutoGluon more iterations. Sadly, so far, there is no easy interface to increase this number. Thus, we simply monkey patched our local install of AutoGluon. To do so, one needs to set the value in this line to 100. Kaggle Tricks To adjust the number of decimals for rounding, set the round_decimals variable in autogluon.core.models.greedy_ensemble.ensemble_selection.EnsembleSelection Line 112 to 8 (see here on GitHub). Our manual post hoc ensembling logic can be found in this file . It assumes that you have a finished run of AutoGluon on disk (in our cases we used the default one-hour run for this) and the following artfiacts from another run of AutoGluon: a) the prediction probabilities on test data, b) the out-of-fold prediction probabilities on training data, and c) predictions on test data. Best regards, Lennart, Nick (@innixma), and Arjun (@neonkraft), on behalf of the \"AutoML Grandmasters\" Prior Grand Prix Competition Write-ups: First AutoML Grand Prix Competition , Second AutoML Grand Prix Competition , Third AutoML Grand Prix Competition 3 11 Please sign in to reply to this topic. comment 46 Comments 1 appreciation  comment Hotness Robert Hatch Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 8 more_vert Congrats, well done on first, and officially locking in a place in the top 5! I'll see your 1000 CPUs and raise you‚Ä¶ Umm, Kaggle? A little help here?! üòÜ I thought I was handicapped with only 30 hours of GPU for a 24 hour competition but I guess I wasn't thinking big enough. ;) Thomas Mei√üner Posted a year ago ¬∑ 1352nd in this Competition arrow_drop_up 6 more_vert For the next iteration Kaggle could set hard bounderies like: one submission only Kaggle Kernel allowed only etc Or use smaller datasets such that the competition is less resource demanding. In a way this competition was unnecessarly big given the small original dataset size. Lennart Purucker Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 4 more_vert Agree! IMO, the dataset needs to be smaller as well to reduce the compute requirements for these competitions with AutoML. But only the training data! The synthetic test data could still be very large for the sake of good evaluation. Then, anyone could still quickly find a good model for the training data and we would not need to worry about random noise or lottery tickets for the test data. Assuming there is sufficient data such that transductive learning is not helpful. Robert Hatch Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 2 more_vert Test data size has a semi-big impact too. Especially lightgbm, neural nets, and‚Ä¶ AutoGluon. But 500k rows would probably be fine. They could have 100k test datasets with less variance than 2 million rows if they just avoid accuracy etc. Nick Erickson Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Probably the sweet spot is enough test data to not overfit using normal techniques, but not so much that public notebook blending doesn't overfit, since public notebook blending (by blindly weighting and probing public leaderboard) is something that should have a downside IMO, otherwise it can lead to overly messy solutions. Probably test data in the 100k - 1M range makes sense. Anirudh Dagar Posted a year ago ¬∑ 23rd in this Competition arrow_drop_up 5 more_vert Congrats team AutoML Grandmasters! Indeed a lot of effort clearly to eek out the last bit of improvement, amazing! Lennart Purucker Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 4 more_vert Thank you! I look forward to your write-up on how you achieved this high performance without the post hoc ensembling tricks! Nick Erickson Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 5 more_vert This one was wild. We got a good score early on, but to get just a small improvement required 10x+ effort and some luck. Robert Hatch Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 8 more_vert I officially hate accuracy and all such binary prediction metrics, lol. So painful to know that your advanced Frankenstein model ensemble can easily tell which mushrooms are 99% safe and which are 99.99% safe‚Ä¶ and the world will never recognize your genius, lol. Unless the number happens to spit out higher because of the 0.01% borderline rows go your way. Nick Erickson Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 5 more_vert Absolutely! ROC_AUC and log_loss are far better. I've read so many papers that use metrics like accuracy and f1 that lead to very confusing / misleading results. There are cases in business where a specific decision threshold matters a lot, but for research and general evaluation, metrics like ROC_AUC and log_loss should be the priority, since doing well in these metrics translates to a superior result in many other metrics while being less noisy. Robert Hatch Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 1 more_vert If anything, I could see doing something like an arctangent wave as a way of doing accuracy without doing accuracy. In other words, rewarding going from 49.9% to 50.1% more than any other incremental reward, but still having incremental reward across the board. But thresholds and predictions don't mix well. Data scientist: I've proven beyond a shadow of a doubt that with this data that we don't know whether this mushroom is poisonous. Accuracy metric: great! Let's score you on THAT one! Call it in the air, edible or poisonous? (Disclaimer: Except unfortunately arc tangent wave doesn't have the properties that auc and logloss have, of not requiring meta adjustments to your true prediction. Arc tan would probably just lead to 1s and 0s anyways if used as a comp metric. It's decent as a gauge only when used in good faith in non competitive arena. So not really recommending it) Robert Hatch Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 3 more_vert I suppose there exists or could exist a weighted version of ROC that works like that arctangent wave though. In other words, rank them as a number from 0 to 1, then apply arctan wave scoring to those numbers based on true labels. With relative values you can't gamify it. Melisa Kara Posted a year ago ¬∑ 566th in this Competition arrow_drop_up 3 more_vert Congratulations üéâ Detailed explanations like this are very helpful beginners like me, thank youüå∏ ShariQ Posted a year ago ¬∑ 976th in this Competition arrow_drop_up 3 more_vert Congratulations  üéâ. A very intuitive approach indeed. Akmal Azzam o'g'li Posted a year ago ¬∑ 422nd in this Competition arrow_drop_up 3 more_vert @lennartpuruckerisg i think preprocessing code needs correction: for col in weird_columns:\n    **allowed_vals** = test_data[col].unique()\n    train_data.loc[~train_data[col].isin(**allowed_vals**), col] = np.nan\n    test_data.loc[~test_data[col].isin(**allowed_vals**), col] = np.nan content_copy > Lennart Purucker Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert It does! Thank you for pointing this out. Fixed it. Copy-paster error at 3 am in the morningüòÖ James King Posted a year ago ¬∑ 421st in this Competition arrow_drop_up 4 more_vert I found counting the number of these human input errors in each column or at least setting a flag if the entry had a human input error was helpful. Although I also restricted it to the actual valid inputs. Lennart Purucker Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 4 more_vert Cool idea as well! turtle Posted a year ago ¬∑ 232nd in this Competition arrow_drop_up 3 more_vert Congratulations! Optimistix Posted a year ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert Congratulations! Thomas Mei√üner Posted a year ago ¬∑ 1352nd in this Competition arrow_drop_up 3 more_vert Congratulations! sethpointaverage Posted a year ago arrow_drop_up 4 more_vert Any chance you could provide your performance on more comprehensive metrics like ROC-AUC and F1 score? Not a part of the competition but working on something with my local reading group and wanted to benchmark Lennart Purucker Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 5 more_vert Here are a few validation scores for our second-to-last submission without the Kaggle tricks, which had a public LB score of 0.98531 : Matthews Correlation Coefficient ( Target Metric ): 0.9850930506045819 ROC AUC: 0.9957310456562743 Log Loss ( Early Stopping Metric ): 0.07358660841538525 Balanced Accuracy: 0.9926470731809423 F1 Score: 0.9932400229147786 The optimal threshold after threshold calibration is 0.5 . Note: these are 16-fold cross-validation scores that are slightly biased due to the weighted ensemble training on validation data and early stopping using the validation data. Also, everything but the target metric is not representative of AutoGluon's true performance. Robert Hatch Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 4 more_vert AutoGluon question: Since perpetually people will publish public notebooks and save models but not out of fold predictions, I was wondering whether this code (manually added on top of this notebook ) is expected to work or not: for m_name in [ None , 'LightGBMXT' , 'NeuralNetFastAI' , 'LightGBMLarge' , 'LightGBM' , 'XGBoost' , 'RandomForestEntr' , 'RandomForestGini' ]: try :\n        oof = predictor.get_oof_pred_proba(model=m_name, as_multiclass= False , train_data=train_data) except : print ( f'failed to get oof for {m_name} ' ) content_copy Is there ANY way to get oof preds except from a live sesson? Nick Erickson Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 3 more_vert Easier to do predictor.predict_proba_multi() which gets all model oofs. You would need a live session unless they cache the oofs or the predictor. Robert Hatch Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 4 more_vert I don't see anything in the rules (other than what happens if teammates submit separately before joining), I suppose teams are allowed to add teammates at any time? Or were you always registered as 3 (or more) people? The team name and registering format made me mentally assume teams were locked but don't see that written‚Ä¶ ü§∑üèª‚Äç‚ôÇÔ∏è Nick Erickson Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 3 more_vert Good question! We registered at the start with Arjun, along with others. I don't think there is anything in the rules that all members need to be visible on the leaderboard team on Kaggle. And also, you can only have 3 people on a Kaggle team but there wasn't a limit to my understanding for sizes of teams in the Grand Prix. I think a solution for future Grand Prix is if team members were public, but maybe there are good reasons for them being private I haven't considered. Robert Hatch Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 3 more_vert So you have been working as a team of more than 3 on every competition, and only having two bother to be connected and submitting live on kaggle? I see Yeah, not publishing team members is just one of the weird little artifacts of this being a second class competition. (I'm coining that term to define any Kaggle prize competition or competition track that doesn't have an automatic standard Kaggle leaderboard. ) I've been thinking through a few areas I'd love to see improvements. Maybe I'll have time to make a post Nick Erickson Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 3 more_vert It has generally been Lennart and myself working on the competitions in a Discord call. The other members had expressed interest, which is why we included them in the team at the time we submitted the team at the start of the Grand Prix to be inclusive to future contributions. They have provided some feedback and ideas throughout, and Arjun helped to create EDA tooling so we can highlight promising directions to focus on in the competitions early on. We also created the team with its members under the assumption that team members would be public ü§∑‚Äç‚ôÇÔ∏è Robert Hatch Posted a year ago ¬∑ 11th in this Competition arrow_drop_up 2 more_vert Thanks for clarifying! It doesn't really matter, but I do find it interesting. And, yeah, I was sure you had been aboveboard on this, but I'll admit seeing a third team member was pretty much the cherry on top of the 1000 CPU cluster, lol. In fact, on the CPU clusters, and back to semi- on topic, I really hope/wish that the TPU 20 hours, or a third weekly quota category of 20-40 hours/week was able to be created on Kaggle for something exactly like a single one of those SLURM clusters. 20 CPUs, 100GB RAM (or slightly more of each). Would be a huge help in many competitions (AND would help prevent this rare but present need to misuse TPU resources just for the RAM. Or for the RAM and the CPUs.) Subashanan Nair Posted 10 months ago ¬∑ 159th in this Competition arrow_drop_up 1 more_vert This is a brilliant approach and I was using a MacBook Pro with 32 GB and this shows the processing power required to achieve the point decimal difference. Mind-blowing Dropdead072 Posted 10 months ago ¬∑ 1074th in this Competition arrow_drop_up 1 more_vert Extremely nice reading! Congratulations on such a high score! Seiji Hirano Posted a year ago ¬∑ 553rd in this Competition arrow_drop_up 2 more_vert Congratulations!! I have a question. I believe that using the test data during the preprocessing phase leads to data leakage. You found unique values in the test set and labeled NaN for any values in the training set that did not exist in the test set. Is this also considered data leakage? Is this something that is observed often in competitoins (to improve the score) but not in the real world? I am still learning data science and I feel like I lack a clear understanding of what data leakage is, so your input will be greatly appreciated! Lennart Purucker Topic Author Posted a year ago ¬∑ 3rd in this Competition arrow_drop_up 3 more_vert I believe that using the test data during the preprocessing phase leads to data leakage. It is data leakage but in a \"good\" intended way, in this case. Here, we are leaking information from the test data to our training. But as we know the test data, we do not need to worry about overfitting (i.e., we do not want to generalize to unseen test data). Thus, this leak is tolerated and often beneficial as we can optimize our model and data for the provided test data. More formally, this can be understood as transducivte learning or test-time feature engineering . This is, AFAIK, not a typical machine learning problem but is very common in Kaggle competitions. In research and real-world applications, we often want to generalize to unseen data points. In Kaggle competitions, we want to generalize to a known test dataset. Seiji Hirano Posted a year ago ¬∑ 553rd in this Competition arrow_drop_up 1 more_vert I see‚Ä¶thank you very much for your detailed explanation! Michael Tinglof Posted 10 months ago ¬∑ 1485th in this Competition arrow_drop_up 0 more_vert Thank you for the results and explanation! Such a simple preprocessing solution. Sometimes the simplest idea is the best. Cheers on your success Davi N. Posted 10 months ago ¬∑ 1729th in this Competition arrow_drop_up 0 more_vert Very good approach! Congratulations üéâ @lennartpuruckerisg Sojib Bhattacharjee Posted 10 months ago ¬∑ 1201st in this Competition arrow_drop_up 0 more_vert Congratulations üéâ. And thanks for this detailed approach. Nihal Thomas Posted 10 months ago ¬∑ 464th in this Competition arrow_drop_up 0 more_vert Congrats Team!üíØ hoon0303 Posted 10 months ago ¬∑ 230th in this Competition arrow_drop_up 0 more_vert Congratulations. Thank you for sharing such an excellent solution. Ada Luo daa Posted 10 months ago ¬∑ 318th in this Competition arrow_drop_up 0 more_vert We used 100 iterations (instead of the default 25) for post hoc ensembling. Why your post-hoc assembly find only the positive coefficients ? scipy.optimize.minimize  find the negative coefficients as well. Example: Coefficients: [ -0.29965439 0.07401918 0.69944183 -0.18976061 0.09105342 -0.04374369 0.1343587 0.24898998 0.16412422 0.21495517] Ensembling score: 0.992 266 7868698358 Model 1- Model 10  OOF scores: 0.992 001 7837979175 0.992 102 8442914457 0.992 189 4675716126 0.989 854 1681037041 0.990 472 4016625254 0.991 721 0602047839 0.991 641 1742908521 0.991 7611 635752315 0.991 702 7730678597 0.991 681 5984882633 Lennart Purucker Topic Author Posted 10 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert We use an algorithm that, by design, only finds positive coefficients. This has many positive effects, but mainly to avoid overfitting. The algorithm is described here , and you can find some pointers as to why only positive coefficients are important in one of my works here . Ada Luo daa Posted 10 months ago ¬∑ 318th in this Competition arrow_drop_up 0 more_vert I like to compare your algorithm with scipy.optimize.minimize. If I share dataset with: models OOF and submissions_OOF Can you calculate your algorithm summission.csv I will calculate scipy.optimize.minimize summission.csv And we compare scipy.optimize.minimize and your algorithm by 2 metrics: 1.LB private score and 2.LB public score. Ok? 3 more replies arrow_drop_down HyeLi Jeon Posted 10 months ago ¬∑ 50th in this Competition arrow_drop_up 0 more_vert Thank you for your amazing insights! Congratulations!!! Muhammad N1sar Posted 10 months ago ¬∑ 483rd in this Competition arrow_drop_up 0 more_vert Congratulations üéâ Mark C. Elliott Posted 10 months ago ¬∑ 1112th in this Competition arrow_drop_up 0 more_vert Hi How many models did you build at the first level of AutoGluon? Count 5 folds as one model. How many models did you build at the second level, where predictions are passed from the first level? Is AutoGluon adds predictions of first-level model and it look like one model prediction, as one column? If you have 50 first-level model you vil have 19 train columns+ 50 prediction columns at second-level model? AutoGluon 0.98523 has 2 levels of model and 3d level is  blending? How AloGluon finds the width of each model in blend, give please github link on blend source code? Lennart Purucker Topic Author Posted 10 months ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert How many models did you build at the first level of AutoGluon? How many models did you build at the second level, where predictions are passed from the first level? In both cases, up to a total of ~200, the entire portfolio we shared in our GitHub, which we linked in the post above. This depends on how many models were trained successfully. I do not have the exact number. Is AutoGluon adds predictions of first-level model and it look like one model prediction, as one column? If AutoGluon uses the out-of-fold prediction probabilities of a first-level model, they will be represented as one column for binary classification tasks. In addition, AutoGluon filters the number of models used for stacking to the best N models (where N can be, for example, 25 but depends on the dataset). This logic can be controlled in AutoGluon via ag_args_ensemble at fit and its max_base_models_per_type and max_base_models parameters. So, we would have 19+N columns if we had 50 first-level models. AutoGluon 0.98523 has 2 levels of model and 3d level is blending? Yes. How AloGluon finds the width of each model in blend, give please github link on blend source code? The blending code is greedy ensemble selection and the code is here . I am unsure what you mean by width. Mark C. Elliott Posted 10 months ago ¬∑ 1112th in this Competition arrow_drop_up 0 more_vert max_base_models_per_type Could you show the different model types that are available in Autogluon? Which model type are on your roadmap for adding them to AutoGluon? Lennart Purucker Topic Author Posted 10 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert You can find the current list here: https://auto.gluon.ai/stable/api/autogluon.tabular.models.html We would add any model (or general method) that performs well in our empirical benchmarks. We usually test new algorithms/models that are published. However, the published results (e.g., in papers) rarely transition to actual performance in our benchmarks. There are multiple reasons for this (e.g., engineering, robustness, maintainability, difference in benchmarking quality, ensembling, ‚Ä¶). Mark C. Elliott Posted 10 months ago ¬∑ 1112th in this Competition arrow_drop_up 0 more_vert Can AutoGluon process video and audio? Mark C. Elliott Posted 10 months ago ¬∑ 1112th in this Competition arrow_drop_up 0 more_vert How to modify the AutoGluon code on GitHub so that it doesn't build models but creates an ensemble of the models I give it? (Where and in what form should I provide the models?) Lennart Purucker Topic Author Posted 10 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert By default, AutoGluon always builds an ensemble. If you want to build a post hoc ensemble with models from AutoGluon, you can use this https://github.com/AutoML-Grandmasters/Fourth-AutoML-Grand-Prix/blob/main/ag_post_hoc_ensembler.py Too many requests error Too many requests",
      "Binary Prediction of Poisonous Mushrooms | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 10 months ago Late Submission more_horiz Binary Prediction of Poisonous Mushrooms Playground Series - Season 4, Episode 8 Binary Prediction of Poisonous Mushrooms Overview Data Code Models Discussion Leaderboard Rules Tilii ¬∑ 6th in this Competition  ¬∑ Posted 10 months ago arrow_drop_up 40 more_vert #6 place - A quick reflection First, huge congratulations to @optimistix who has been flirting with the top spot all summer, and finally got there. Also the only person in at least top 100 who didn't move at all from the public LB. How cool is that? My solutions tend to be boring because they all boil down to the same thing: huge ensembles. This one was no exception but I didn't get to 40-50 models as in previous competitions - only to 25. In some order, they were: Type # of models LAMA TabularNN 8 AutoGluon 6 CatBoost 4 Keras FM 3 xLearn FM 2 LightGBM 1 XGBoost 1 Beyond that, nothing fancy. Picked a model that had best CV and that ended up being my second best model overall. The best solution I had was actually by hill climbing, which picked only 13 of the above-mentioned 25 models. Yet it had quite a bit lower CV score, so there was no reason to pick it. It has the same 5-decimal score as the model I picked. My best individual models were by AutoGluon, but those shouldn't be counted because they are ensembles. From actual single models, four Keras factorization machines were the best (private scores 0.98413-0.98433). They treat all the variables as categoricals and model their interactions. After that the best 8 models were still LAMA TabularNNs, followed by xLearn factorization machines and CatBoost models. It would appear that modeling all (or most) variables as categoricals was a way to go. 2 2 1 1 1 Please sign in to reply to this topic. comment 22 Comments Hotness hoon0303 Posted 10 months ago ¬∑ 230th in this Competition arrow_drop_up 5 more_vert Congratulations. Thank you for sharing a great solution. Optimistix Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 4 more_vert Congratulations, and thanks so much! Tilii Topic Author Posted 10 months ago ¬∑ 6th in this Competition arrow_drop_up 2 more_vert Well done! As you predicted, someone else will get a t-shirt you earned. Optimistix Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thanks again! Glad it came true already :-) Michael Tinglof Posted 10 months ago ¬∑ 1485th in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for sharing your methodology. Hoping to be in your spot in a few months. Tilii Topic Author Posted 10 months ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thank you and good luck! ANSH ANEJA Posted 10 months ago ¬∑ 1928th in this Competition arrow_drop_up 1 more_vert congo. will reach there one day fs. MrSimple Posted 10 months ago ¬∑ 118th in this Competition arrow_drop_up 1 more_vert Congratulations, and thanks for sharing your approach! Tilii Topic Author Posted 10 months ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Cheers to you as well! That's a nice placement. emoji_people KALPESH BHOIR Posted 10 months ago ¬∑ 114th in this Competition arrow_drop_up 1 more_vert Congratulations, and thanks for sharing your approach! Tilii Topic Author Posted 10 months ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Cheers to you as well. Anything around top 5% of all competitors is a great achievement. Jordi Rosell Posted 10 months ago ¬∑ 80th in this Competition arrow_drop_up 1 more_vert Thanks for saying this, but top 6th deserves celebration üëç pinoystat Posted 10 months ago ¬∑ 21st in this Competition arrow_drop_up 1 more_vert Congratulations to you and especially to @optimistix for being in the top spot. Due to time constraints I was not able to try keras FM setup. Keras embedding alone pushed me at 21st place. Tilii Topic Author Posted 10 months ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Keras embedding alone pushed me at 21st place. That's the same general principle as Keras FM. They have also linear representations of features in addition to embedding layers. See here and the links in that thread, and maybe search through Kaggle scripts/notebooks. pinoystat Posted 10 months ago ¬∑ 21st in this Competition arrow_drop_up 1 more_vert Thanks a lot for this. I read your discussion here a week back and draw some inspiration on how to build nns. I was reading a lot of your discussions since I am interested in competing using only one deep learning framework ( without the need for gradient boosted decision trees or a bunch of mixed algorithm model). Your writings helped me increased my public LB score.  In this competition, I was not able to implement the interactions part shown in your discussion as well as strategies as per the deepFM paper due to time constraints. But the good news for me is , I got to learn (sort of, crude) on how to build and optimize a deep learning in a context of tabular dataset that is competitive enough without the overfitting (at least in this particular dataset). Lamrot Ibsa Posted 10 months ago ¬∑ 1941st in this Competition arrow_drop_up 1 more_vert great work Tilii Topic Author Posted 10 months ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thanks much! Didn't get to be as active in this competition as I would have liked, but it turned out OK. Anket Hirulkar2 Posted 10 months ago ¬∑ 778th in this Competition arrow_drop_up 2 more_vert Congratulations and thanks for sharing the notebook. 25 models is crazy Tilii Topic Author Posted 10 months ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thank you! You should have seen my 50+ models solutions in previous competitions. Stelios Tsolak Posted 10 months ago ¬∑ 653rd in this Competition arrow_drop_up 2 more_vert Congratulations, great work! Thomas Mei√üner Posted 10 months ago ¬∑ 1352nd in this Competition arrow_drop_up 2 more_vert Congratulations! Which hill climbing implementation do you use? I think there have been multiple variants in the past‚Ä¶? Tilii Topic Author Posted 10 months ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert I think this is the most elegant approach for hill climbing: https://github.com/Matt-OP/hillclimbers/ Mahdi Ravaghi Posted 10 months ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Congratulations, @tilii7 ! What eval_metric did you use with hill climbing? I tried the same implementation as you (roc_auc as eval_metric), but it took a vey long time to run and I just gave up before it completed. 4 more replies arrow_drop_down Chris Deotte Posted 10 months ago arrow_drop_up 2 more_vert Congratulations @tilii7 well done. This would be solo Gold medal solution in non-playground. It would appear that modeling all (or most) variables as categoricals was a way to go. This is interesting. I did not explore this competitions data. Does this mean that you converted numerical variables into bins, thus converting to categoricals? Tilii Topic Author Posted 10 months ago ¬∑ 6th in this Competition arrow_drop_up 3 more_vert Thanks, Chris. The main two problems in this competition were a large number of missing values and some random-noise categories that existed only in train but not test data, or vice versa . I don't think handling of the missing values was that important, but I bet top guys figured out a clever way to deal with a small number of data points that had random-noise categories. In the past I would have binned the categories to reduce the number of unique categorical values, but I learned in the last playground competition that is not necessary as long as the number of unique categorical values is < 10,000. In fact, binning was slightly detrimental to the final result, even though it promoted faster training. See the link explaining that concept in one of my other replies in this thread. Both for NN embedding and NN factorization machines a large number of unique categorical values increased the training time, but it also resulted in better scores. MaDiha üå∑ Posted 10 months ago arrow_drop_up 0 more_vert Congratulations üéâ Too many requests error Too many requests",
      "Binary Prediction of Poisonous Mushrooms | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 10 months ago Late Submission more_horiz Binary Prediction of Poisonous Mushrooms Playground Series - Season 4, Episode 8 Binary Prediction of Poisonous Mushrooms Overview Data Code Models Discussion Leaderboard Rules Jack Lee ¬∑ 8th in this Competition  ¬∑ Posted 10 months ago arrow_drop_up 10 more_vert 8th Place Solution with Autogluonü§î First and foremost, I want to extend my gratitude to all the organizers, participants, and you, the reader! I must admit, there was a bit of luck involved in achieving 8th place. For my model, I simply opted for AutoGluon, but I still want to share and discuss my thoughts with you. In my first run with AutoGluon, I tried using a GPU, but it didn‚Äôt seem to be utilized‚Äîperhaps AutoGluon determined it would actually slow things down. In that initial run, due to time constraints, 17 base models and 13 stacked models were trained. The cross-validation (CV) score was 0.985, with a leaderboard (LB) score of 0.98522. Surprisingly, the private score turned out to be 0.98506, which caught me a bit off guard. Afterwards, I noticed there was quite a bit of noise in the data, such as numerical values and strange words in categorical features that weren‚Äôt present in the test set. I assumed that only single-character categorical features were not noise and set the others to NaN, letting AutoGluon handle them (considering tree-based models can naturally deal with NaNs, I figured this might be better than manual imputation). After making these adjustments, I increased AutoGluon‚Äôs time limit and ran it for three days on two Gold 5320 CPUs provided by my school (a big thanks to them!). This time, 19 base models and 22 stacked models were trained, with the CV score reaching 0.9581, the LB score improving to 0.98525, and the private score hitting a personal best of 0.98507. Later, I discovered that one XGBoost model took two full days to trainüòÖ, so I decided to exclude XGBoost (which had a CV score of 0.9846) for a full retraining. This final training took five and a half days, producing 98 base models and 64 stacked models (some stacked models were lost due to cluster issues). The CV score remained 0.9581, the LB score slightly increased to 0.98528, but the private score dropped slightly to 0.98507. I suspect that XGBoost might still have some significance, and that more complex models could have introduced some overfitting. Finally, thanks again for reading! P.S. Honestly, given the massive number of samples, I was also surprised by the leaderboard shakeup. What do you all think? Please sign in to reply to this topic. comment 1 Comment Hotness Jack Lee Topic Author Posted 10 months ago ¬∑ 8th in this Competition arrow_drop_up 1 more_vert (Sorry, I accidentally wrote a 6 instead of an 8 in the title.) Too many requests error Too many requests",
      "Binary Prediction of Poisonous Mushrooms | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 10 months ago Late Submission more_horiz Binary Prediction of Poisonous Mushrooms Playground Series - Season 4, Episode 8 Binary Prediction of Poisonous Mushrooms Overview Data Code Models Discussion Leaderboard Rules Mahdi Ravaghi ¬∑ 10th in this Competition  ¬∑ Posted 10 months ago arrow_drop_up 20 more_vert 10th place solution (and a potential 5th) Firstly, congratulations to @optimistix , @neupane9sujal , and AutoML Grandmasters for their impressive results in this competition. Kudos to @optimistix for staying at the top of the leaderboard for most of the competition and securing their first 1st place in the Playground Series. Well deserved, @optimistix ! I can't wait to read about your winning solution. (First) Final Solution I submitted my (first) final solution on Friday. It was an ensemble consisting of 9 of my best-scoring individual models. Although this solution had a low public LB score, I was happy with it, mostly because at that time, I didn‚Äôt have any ideas on how I could improve it further. I was just crossing my fingers for a big shakeup at the end! The table below shows the models in this ensemble along with their 5-fold CV and public LB scores. Model 5-Fold CV Public LB AutoGluon 0.98492 0.98523 XGBoost (dart) 0.98490 0.98499 XGBoost 0.98488 0.98503 LightGBM (dart) 0.98482 0.98507 LightGBM 0.98480 0.98501 HistGB 0.98474 0.98496 XGBoost (rf) 0.98465 0.98473 CatBoost 0.98454 0.98487 Neural Network 0.98434 0.98469 Ensemble 0.98501 0.98521 One Last Experiment Yesterday, just a few hours before the competition ended, I decided to run one last experiment. I collected the OOF predictions from all my experiments throughout the competition and ensembled them. I ended up with 32 models, including the original 9. This increased my CV score to 0.985050 and my public LB score to 0.98528. I discovered that some models had the same score in every fold, even though their OOF predictions weren't exactly the same. After removing these models, my CV score increased to 0.985053 and my public LB score to 0.98531. So, I decided to select this and my original submission from Friday as my final two submissions. Today, I found that my 32-model ensemble, which probably contained some duplicates or very similar models, had the highest score on the private LB and could have secured me 5th place. Ensembling very similar or duplicate models goes against my intuition, which is why I didn't have multiple models of the same type in my original ensemble. I probably would never have chosen this ensemble for my final submissions due to the (likely) duplicate models, nor will I in the future. I think it was just pure luck that this ensemble received a higher score on the private LB compared to my other ensembles. 10th Place Solution Anyway, in my 10th place solution, I used 28 models and ensembled them using logistic regression. The inputs to logistic regression were converted to logits before training. I tried various functions on the inputs, but logits provided the best CV score, so I decided to go with that. Here are the 5-fold CV scores of all models in the ensemble: One interesting thing I noticed today is that tuning the threshold helped increase both the CV and public LB scores, but not the private LB score. I tuned the threshold using Optuna and OOF predictions, but I also tried using TunedThresholdClassifierCV . However, the latter didn‚Äôt improve my scores, neither the CV nor the public LB score. Lastly, I would like to thank @ambrosm , @siukeitin , @omidbaghchehsaraei , @rzatemizel , and @oscarm524 for their code and discussion posts. I have drawn inspiration from and learned a lot from their contributions. 2 1 2 2 Please sign in to reply to this topic. comment 9 Comments Hotness emoji_people abdelbasset ben kerrouche Posted 10 months ago ¬∑ 139th in this Competition arrow_drop_up 1 more_vert Congratulations .thank you  for this explanations  it's help's me as beginners Tilii Posted 10 months ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Congratulations on the great job with your models and this summary! I was gearing up after the first day to spend a bunch of time tuning the threshold, but soon realized it made little difference. The dataset was balanced enough and good models were natively coming up with a 0.5 threshold without any external effort. After the first day I had only 2-3 models that required threshold corrections, and those were small. Mahdi Ravaghi Topic Author Posted 10 months ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Thanks! The improvements from tuning the threshold weren't significant, but I still thought it would give me an edge in the final standing. It's surprising that tuning the threshold increased the CV and public LB scores in most of my experiments, but not the private LB score. Sujal Neupane Posted 10 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert @ravaghi Congratulations on 10th place finish!! Thank you for the mention‚Ä¶ Mahdi Ravaghi Topic Author Posted 10 months ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Thank you! Congratulations to you as well. Oscar Aguilar Posted 10 months ago ¬∑ 19th in this Competition arrow_drop_up 1 more_vert @ravaghi , thanks for the mention, and congrats on the 10th rank. I also experienced the same regarding the threshold and not selecting the correct submission. The above solution would secure a top 10 solution. Mahdi Ravaghi Topic Author Posted 10 months ago ¬∑ 10th in this Competition arrow_drop_up 0 more_vert Thanks! And good luck with this month's competition. I hope we both choose good submissions next time. Dipak Das Posted 10 months ago ¬∑ 2204th in this Competition arrow_drop_up 0 more_vert what does OOF prediction mean ? Nilotpal Maitra Posted 10 months ago arrow_drop_up 0 more_vert Congratulations! Too many requests error Too many requests",
      "Binary Prediction of Poisonous Mushrooms | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 10 months ago Late Submission more_horiz Binary Prediction of Poisonous Mushrooms Playground Series - Season 4, Episode 8 Binary Prediction of Poisonous Mushrooms Overview Data Code Models Discussion Leaderboard Rules Oscar Aguilar ¬∑ 19th in this Competition  ¬∑ Posted 10 months ago arrow_drop_up 18 more_vert #19 Solution | AutoGluon + TF + LightGBM + XGBoost + Calibration This was my first time using AutoGluon , and I must admit that I enjoyed its simplicity and great performance. In this post, I will explain my approach. Data Preprocessing In terms of data processing, I considered three cases. Preprocessing presented in AmbrosM's notebook . Preprocessing presented in Stephen Murphy notebook No preprocess Modeling In terms of models, I built several models. The below table summarizes my results. Model # of Models Worst CV score Best CV score Ensemble CV score AutoGluon 20 0.98512 0.98525 - LightGBM 9 0.98423 0.98477 0.98484 XGBoost 7 0.98417 0.98485 0.98488 TensorFlow 6 0.98383 0.98401 0.98452 Note that the above results are based on a 10-fold cross validation strategy. Ensemble In terms of ensemble, I did the following: 0.62 x AutoGluon (best model) + 0.38 x AutoGluon of (LightGBM Ensemble, XGBoost Ensemble, TensorFlow Ensemble) content_copy The above ensemble obtained 0.98527 CV score over 10-folds. Calibration Finally, I calibrated the predictions with IsotonicRegression , which boosted the 10-fold CV score by 0.00003 What did not work I run a few experiments in which I optimize the threshold to come up the label, but I couldn't find consistent results. Please sign in to reply to this topic. comment 9 Comments Hotness Tilii Posted 10 months ago ¬∑ 6th in this Competition arrow_drop_up 3 more_vert Nicely done. My experience with AutoGluon was similarly eye-opening, except it happened a month earlier than yours. I did manual calibration (AKA Platt calibration) using simulated annealing, but it brought the score down by about the same value as yours went up. Generally speaking, I don't think it mattered much to find the optimal threshold or change the calibration, as the two classes were similar in abundance. Itamar Ordani Posted 10 months ago ¬∑ 1854th in this Competition arrow_drop_up 1 more_vert I see a common theme where large ensembles (and AutoGluon) helped immensely. I might try it for the next one. Can you share how long this took to train? Oscar Aguilar Topic Author Posted 10 months ago ¬∑ 19th in this Competition arrow_drop_up 1 more_vert @itamarordani , in this dataset, 'AutoGluon' performed very well with minimal preprocessing. You can specify the training time for the framework to run. Here's a simple example: train = pd.read_csv( 'train.csv' , index_col =0)\n\nweird_columns = [ \"cap-shape\" , \"cap-surface\" , \"cap-color\" , \"gill-attachment\" , \"gill-spacing\" , \"gill-color\" , \"veil-type\" , \"veil-color\" , \"has-ring\" , \"ring-type\" , \"spore-print-color\" , \"habitat\" , \"does-bruise-or-bleed\" , \"stem-root\" , \"stem-surface\" , \"stem-color\" ,\n] for col in weird_columns:\n    allowed_vals = test[col].unique()\n    train.loc[~train[col].isin(allowed_vals), col] = np.nan\n    test.loc[~test[col].isin(allowed_vals), col] = np.nan from autogluon.tabular import TabularDataset, TabularPredictor from autogluon.core.metrics import make_scorer\n\nmcc = make_scorer( name = 'mcc' , score_func =matthews_corrcoef, optimum =1, greater_is_better = True )\n\ntrain = TabularDataset(train)\ntarget = 'class' metric = 'mcc' time_limit = 2 *3600 predictor = TabularPredictor( label =target, eval_metric =metric).fit(train, time_limit =time_limit, presets = \"best_quality\" ) content_copy Purushottam Posted 10 months ago ¬∑ 390th in this Competition arrow_drop_up 1 more_vert Nice üëç. For me, AutoGluon is totally new. Tanishk Patil Posted 10 months ago ¬∑ 740th in this Competition arrow_drop_up 1 more_vert Great Job! hoon0303 Posted 10 months ago ¬∑ 230th in this Competition arrow_drop_up 1 more_vert Excellent Solution. Congratulations. Thomas Mei√üner Posted 10 months ago ¬∑ 1352nd in this Competition arrow_drop_up 2 more_vert Congratulations! AutoGluon took over this leaderboard by storm Optimistix Posted 10 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Congrats! Solid as ever. Sheikh Muhammad Abdullah Posted 10 months ago arrow_drop_up 2 more_vert Congratulations Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 4, Episode 8 The dataset for this competition (both train and test) was generated from a deep learning model trained on the UCI Mushroom dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note: Unlike many previous Tabular Playground datasets, data artifacts have not been cleaned up. There are categorical values in the dataset that are not found in the original. It is up to the competitors how to handle this. 3 files 297.82 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 297.82 MB sample_submission.csv test.csv train.csv 3 files 45 columns  Too many requests",
    "data_description": "Binary Prediction of Poisonous Mushrooms | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 10 months ago Late Submission more_horiz Binary Prediction of Poisonous Mushrooms Playground Series - Season 4, Episode 8 Binary Prediction of Poisonous Mushrooms Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: The goal of this competition is to predict whether a mushroom is edible or poisonous based on its physical characteristics. Start Aug 1, 2024 Close Sep 1, 2024 Evaluation link keyboard_arrow_up Submissions are evaluated using the Matthews correlation coefficient (MCC) . Submission File For each id row in the test set, you must predict target class , whether the observation is editable ( e ) or poisonous ( p ). The file should contain a header and have the following format: id,class 3116945 ,e 3116946 ,p 3116947 ,e\netc. content_copy Timeline link keyboard_arrow_up Start Date - August 1, 2024 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  August 31, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Binary Prediction of Poisonous Mushrooms. https://kaggle.com/competitions/playground-series-s4e8, 2024. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 8,269 Entrants 2,650 Participants 2,422 Teams 17,899 Submissions Tags Beginner Time Series Analysis Tabular Matthews Corrcoef Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s4e9",
    "discussion_links": [
      "/competitions/playground-series-s4e9/discussion/537052",
      "/competitions/playground-series-s4e9/discussion/537349",
      "/competitions/playground-series-s4e9/discussion/537029",
      "/competitions/playground-series-s4e9/discussion/536973",
      "/competitions/playground-series-s4e9/discussion/537173"
    ],
    "discussion_texts": [
      "Regression of Used Car Prices | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 9 months ago Late Submission more_horiz Regression of Used Car Prices Playground Series - Season 4, Episode 9 Regression of Used Car Prices Overview Data Code Models Discussion Leaderboard Rules Mart Preusse ¬∑ 1st in this Competition  ¬∑ Posted 9 months ago arrow_drop_up 94 more_vert # 1 solution - stacked NN Did I really made it to the top? I am still surprised and excited. Way to the solution: I spent the first two weeks with reading the discussions, playing around with catboost and publishing an ensemble. The plan was to collect diverse models and ensemble them with Ridge, with the same pipeline I used in this notebook . The final ensemble, which I chose as my first final submission, would have landed me on the second place and differed from the notebook in the following points: I used 20 cv folds I included original data in some of the models (even two times in LGBM) I did compute a SVR with a rbf kernel as suggested by broccoli beef in this discussion post instead of the linear SVR. I included all categorical features additionally as target encoded to catboost, but I used the median, not the mean for target encoding. I did this leakfree, meaning that I recomputed the targetencoded columns in each fold. Moreover, I used Catboost as classifier, not as regressor. Catboost predicted the outlier prices (see function bin_price ). The hyperparameters were found by optuna. The oof predictions were not used in the ensemble. They were used as an additional feature in a LGBM (or the NN for my second final submission). def bin_price ( data ):\n    df = data.copy() # Calculate Q1 (25th percentile) and Q3 (75th percentile) Q1 = np.percentile(df[ 'price' ], 25 )\n    Q3 = np.percentile(df[ 'price' ], 75 )\n    IQR = Q3 - Q1 # Define the lower and upper bounds for outliers lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR # Identify outliers outliers = df[(df[ 'price' ] > upper_bound)]\n    df[ 'price_bin' ] = (df[ 'price' ] < upper_bound).astype( int ) return df\n\ncat_params2 = { 'early_stopping_rounds' : 25 , 'use_best_model' : True , \"verbose\" : False , 'cat_features' : cat_cols, 'min_data_in_leaf' : 16 , 'learning_rate' : 0.03355311405703999 , 'random_strength' : 11.663619399375248 , 'l2_leaf_reg' : 17.703146378123996 , 'max_depth' : 10 , 'subsample' : 0.9479174100256215 , 'border_count' : 130 , 'bagging_temperature' : 24.032067560148384 } content_copy I included the catboost oof predictions as an additional feature for LGBM I used a second LGBM (LGBM5), where I label encoded all categorical data (rare categories summarized in a category \"rare\" as done with the NN in the notebook) and raised max_bin. lgb_params = { 'verbose' : - 1 , 'early_stopping_rounds' : 25 , 'loss_function' : \"RMSE\" , 'n_estimators' : 2000 , 'max_bin' : 30000 ,\n} content_copy I included fastai computations from Autogluon (with a nested cv over 20 folds to be 100% leakfree) predictor = TabularPredictor(label= 'price' ,\n                             eval_metric= 'rmse' ,\n                             problem_type= \"regression\" ).fit(X_train,\n                                                       pseudo_data = data_original, \n                                                       num_bag_folds = 10 ,\n                                                       num_bag_sets = 2 ,\n                                                       time_limit= 1800 ,\n                                                       included_model_types = [ 'FASTAI' ], \n                                                       keep_only_best = True ,\n                                                       presets= \"best_quality\" ,\n                                                      ) content_copy I ended up with a crossvalidation score of 72300 and the following models (_st means that the catboost oofs are included): The crossvalidation scores of the individual models were: First place solution : I needed a second final submission and I decided spontanously on the last day to submit a forked notebook from Vladimir Demidov . I noticed that his NN is robust to changes, so I added four numerical features: the SVR oof predictions, the LGBM5 oof predictions, the CatboostClassifier oof predictions and XGB predictions (derived from publicy available hyperparameters, unfortunatly I forgot the source). The NN ensemble had a crossvalidation score of 72468, but in the end was better than the Ridge ensemble. My thanks obviously go to @yekenot , @siukeitin , @noodl35 (LGBM hyperparameters), who directly provided parts of the code I used. I also provited strongly from the discussions, especially the AutoML solution threads and the posts from @tilii7 and @roberthatch where I got the idea for outlier classification and @cdeotte who made the entrance to NNs simple for me. I am also very grateful to all who are participating lively in the discussions so that learning is a fulfilling experience. What did not work: I experimented with a lot of models, but most of them did not help me to get a better cross-validation score. Especially XGB did not work for me and although it is accidently included in my final submission I do not think that it was a crucial part of the ensemble. Feature engineering was at least partly working, but all the amazing features introduced by Chris Deotte in this post did not work for me. The corresponding crossvalidation - leaderboard scores in a scatterplot: Edit: I included the CatBoostClassifier, LGBM1_st and LGBM5_st in my notebook . This notebook has a private score of 62957.83525 and would have reached a place within the top three. Edit 2: Autogluon, when using TabularPredictor.fit() , is not using the original data. To use the original data, one has to call TabularPredictor.fit_pseudolabel() . So my Fastai - AutoGluon predictions were not using the original data. For a nice reference of how to use AutoGluon with original data and with a predefined stratification see this great notebook of Mahdi Ravaghi for the next playground competition. 18 23 2 Please sign in to reply to this topic. comment 83 Comments 4 appreciation  comments Hotness Jyotirmoy Das Posted 6 months ago arrow_drop_up 1 more_vert Congrats Brother Chris Deotte Posted 9 months ago ¬∑ 81st in this Competition arrow_drop_up 9 more_vert Congratulations. Great write up. Good explanations and pictures. Your solution is great. You approached the competition perfectly making a diverse set of models and carefully producing OOF and exploring ensemble methods. Your use of median with target encoding and building a stack over classification predictions was creative and effective. I enjoyed all your contributions during the competition. Great job! Mart Preusse Topic Author Posted 9 months ago ¬∑ 1st in this Competition arrow_drop_up 6 more_vert Thank you Chris for your feedback, this means a lot for me! Shashwat Sharma Posted 9 months ago ¬∑ 193rd in this Competition arrow_drop_up 6 more_vert Wow great insights, any particular reason you were able to infer as to why xgb did not work in this case? Mart Preusse Topic Author Posted 9 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert I have no idea. Vladimir Demidov Posted 9 months ago ¬∑ 229th in this Competition arrow_drop_up 6 more_vert Congrats and thanks for your work. I've waited for someone who can improve a DeepTables performance. Now we know how it's good with oof predictions as a feature added. Mart Preusse Topic Author Posted 9 months ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert I am really grateful that you shared your notebook, as it seems that deep tables is easy to work with and robust. I will definitely explore the possibilities in more details. This is also not the only great notebook you published, I read two more in the last week which are very interesting. Tanaa85 Posted 8 months ago arrow_drop_up 1 more_vert My congratulations! Taimour Nazar Posted 8 months ago ¬∑ 302nd in this Competition arrow_drop_up 1 more_vert Good, congratulations! Optimistix Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 3 more_vert A big congrats to you! Really glad to see you at no. 1 after all your contributions in the playground series this year. Neat solution(s), too! Mart Preusse Topic Author Posted 9 months ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert Thanks, I am really happy, even more because I did not see this coming. I learned a lot this year and I love the community here. Nice pic, by the way. Matches your name. Optimistix Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Danke sch√∂n! Riana Azad Posted 9 months ago arrow_drop_up 4 more_vert Such a great insights! Matheus Bonjour Laviola da Silva Posted 8 months ago arrow_drop_up 1 more_vert congrats for ranking 1st Ibrahim DENIS FOFANAH Posted 8 months ago arrow_drop_up 1 more_vert congratulations for ranking 1st Yarza Wynn Posted 8 months ago arrow_drop_up 2 more_vert congrats and thanks for taking the time to write up your thought process! nathfavour Posted 8 months ago arrow_drop_up 2 more_vert impressed! Shepherd Mahadzva Posted 9 months ago arrow_drop_up 1 more_vert Congratulations for ranking first! BHANUPRAKASH Posted 9 months ago arrow_drop_up 1 more_vert Congratulations ! For ranking 1st position.. Rajawat Devendra Singh Posted 8 months ago arrow_drop_up 2 more_vert Congratulations for ranking first! emoji_people Zeel Posted 9 months ago ¬∑ 1213th in this Competition arrow_drop_up 1 more_vert amazing work! GgggGunGun Posted 9 months ago arrow_drop_up 1 more_vert good job for catboostüòÉ Van Patangan Posted 9 months ago ¬∑ 1735th in this Competition arrow_drop_up 1 more_vert Congratulations!  I learned so much from this write up, incredibly helpful. Satyam Kumar Posted 9 months ago arrow_drop_up 1 more_vert looks interesting Vishud Verma Posted 9 months ago arrow_drop_up 1 more_vert Good Job mate. Kartik Gupta Posted 9 months ago arrow_drop_up 1 more_vert Congratulations Mate MrSimple Posted 9 months ago ¬∑ 46th in this Competition arrow_drop_up 1 more_vert good job man dfdSdv Posted 9 months ago ¬∑ 2837th in this Competition arrow_drop_up 1 more_vert good job man Pranay Deep Korada Posted 9 months ago arrow_drop_up 1 more_vert Nice Work, This contribution is great. Jameer Vali Posted 9 months ago arrow_drop_up 0 more_vert DUDE ,Sea is blue but it is his Red SEA Darwin Berrio Posted 9 months ago arrow_drop_up 1 more_vert This contribution is great, but the graphs have a biased representation because it seems that Ensemble is much lower, when in fact the difference between the other models is minimal Mart Preusse Topic Author Posted 9 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert I am not sure if I understand your comment correctly, but I think you refer to the cutoff of the x-axis. I did this because I wanted to show the differences in scores. We compete for small differences. In this competition there is a difference of less than 40 between the first and the second place solution, so a difference of over 140 is important. Too many requests error Too many requests",
      "Regression of Used Car Prices | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 9 months ago Late Submission more_horiz Regression of Used Car Prices Playground Series - Season 4, Episode 9 Regression of Used Car Prices Overview Data Code Models Discussion Leaderboard Rules Gerlando Re ¬∑ 2nd in this Competition  ¬∑ Posted 9 months ago arrow_drop_up 21 more_vert #2 Position | Just FE and AutoML Hello everyone, I'd like to share my approach to this competition, in case anyone is interested. For feature engineering, I took the following steps: Simplified transmission values by consolidating \"Automatic\" and \"Manual\" into \"A/T\" and \"M/T\", respectively Extracted luxury brands from the data Derived features from the engine column, including horsepower, cylinders, and combinations of these Created feature crosses, such as int_ext_col, brand_model, brand_int_col, brand_ext_col, and brand_mileage Identified and marked infrequent categories for each feature as noise, based on quantiles Used car age and mileage to create a mileage_per_year feature Handled missing values, of course For modeling, I experimented with CatBoost Regressor and LGBM Regressor using Optuna, but my best results came from a Weighted Ensemble fitted with AutoGluon. I was able to further improve my score by incorporating additional data. I have to say, I'm still in shock from the final leaderboard shake-up! I didn't expect to end up in second place, especially considering I had very limited time to try out different approaches. I'm thrilled and grateful for the outcome. I also want to extend a big thank you to @roberthatch and @cdeotte for the insightful discussions and contributions throughout the competition. It was a great experience, and I'm glad I got to be a part of it. Thanks again, and congratulations to all participants! ‚úåÔ∏è 7 Please sign in to reply to this topic. comment 9 Comments Hotness Tilii Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 5 more_vert Congratulations! You must have done things the proper way even if you didn't expect to move to #2. Your solution, at least based on published descriptions, would seem to be the only one that benefited from feature engineering. That goes both against the prevalent trend in this competition, and frankly against my intuition, so I'd like to understand it better. First, I don't know what exactly Handled missing values, of course means. I have no trouble understanding these and their contributions: Identified and marked infrequent categories for each feature as noise, based on quantiles Used car age and mileage to create a mileage_per_year feature Extracted luxury brands from the data I guess it is possible that your improvements came mainly from these three categories. What I am not sure about, given what others have tried, is these categories: Derived features from the engine column, including horsepower, cylinders, and combinations of these Created feature crosses, such as int_ext_col, brand_model, brand_int_col, brand_ext_col, and brand_mileage Either these didn't help at all (but didn't hurt either), or you found a more creative way to combine them than the rest of us. Could you please elaborate? Adding original used car data was definitely helpful, so that was also a good call on your part. Oscar Aguilar Posted 9 months ago ¬∑ 365th in this Competition arrow_drop_up 1 more_vert @gerlandore , congratulations! My best solution , it also relies in some basic feature engineering and AutoGluon . However, I didn't select it. Mart Preusse Posted 9 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Congratulations on making the 2nd place and thanks for sharing your solution! It is interesting that you seemingly profited so much from feature engineering. Denoising the features could have played a major role. Which quantile did you use to discard noisy categories? Optimistix Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Congratulations once again! What additional data did you use? Gerlando Re Topic Author Posted 9 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Thank you again! I added the following dataset: https://www.kaggle.com/datasets/taeefnajib/used-car-price-prediction-dataset Optimistix Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Thanks! So it's the original dataset which was used to generate the competition dataset. Loc Quan Posted 9 months ago ¬∑ 1345th in this Competition arrow_drop_up 0 more_vert Thank you for sharing your approach :D It's intriguing how effective your seemingly simple solution is. Would you mind sharing your code if possible? Thank you again in advance. Frederic Nicholson Posted 9 months ago ¬∑ 2831st in this Competition arrow_drop_up 0 more_vert so autogluon is the state of the art auto machine learning package these day? I experimented with pycaret and was very disappointed with the result: many exceptions, long run time and poor scores. DolorNeto Posted 9 months ago ¬∑ 1220th in this Competition arrow_drop_up 0 more_vert @gerlandore Is it possible to share your notebook? Robert Hatch Posted 9 months ago ¬∑ 10th in this Competition arrow_drop_up 0 more_vert Congrats, well done! Too many requests error Too many requests",
      "Regression of Used Car Prices | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 9 months ago Late Submission more_horiz Regression of Used Car Prices Playground Series - Season 4, Episode 9 Regression of Used Car Prices Overview Data Code Models Discussion Leaderboard Rules Optimistix ¬∑ 3rd in this Competition  ¬∑ Posted 9 months ago arrow_drop_up 23 more_vert 3rd Place Solution: An Open Secret - Gather, Ridge, Repeat As mentioned in my previous writeup , I was going to step back and participate more judiciously this month, and so I did. I'll go into the details shortly, but let me first explain the headline, which shares the open secret of how my approach was essentially the same as before: Gather: Collect OOF predictions. Ridge: Build an ensemble - you don't necessarily have to use Ridge Regression, but it generally does very well, and was a good fit for some wordplay üòÄ Repeat: Literally, that - and also to complete the wordplay, of course. Before going into details, I'd like to acknowledge the generosity of those who shared their insights, findings and code, including but not limited to @alexryzhkov (and the AutoML team), @roberthatch , @omidbaghchehsaraei , @cdeotte , @siukeitin , @ravaghi , @oscarm524 , @ravi20076 , @tilii7 , @ricopue , @serhiikravtsov , @allegich , @backpaker Phase 1 : Lurking I had a multi-week LLM project due on the 17th, and had to also review 3 projects by my peers in that course within a week after that. There was also an ongoing health emergency within the family. So it was quite clear that my participation would be quite limited until the last week or so, if not throughout. I kept an eye on the competition in this phase, checking out discussion posts and interesting public notebooks. I occasionally submitted a model to see how it scored, and gave into the temptation of some \"blind blending\" every now and then. It was great to have @cdeotte participating this month, as he made several insightful posts (inviting insightful responses by @siukeitin and others), and also generously shared notebooks. Based on the competitions over the last few months (and especially last month), I figured that the day 1 solutions by Light AutoML Testers (@alexrhyzkov et al.), @roberthatch and AutoML Masters (@innixma et al.) were going to stand the test of time - since @alexrhyzkov & @roberthatch had generously shared their OOFs, I decided that would be my starting point. I also felt that a public LB score close to 72000 (without any blind blending) would probably fare well. The most consequential thing I did in this phase was to throw the 24 OOFs from @alexrhyzkov into AutoGluon, and confirm that they scored 72046/72057/72063 using GPU-P100/CPU/GPU-T4, respectively. Phase 2: Gradually intensive participation Around the 20th, I finally started putting OOFs together. I started by combining OOFs from @alexrhyzkov , @roberthatch & the eventual winner @martinapreusse - these 30 OOFs along with Ridge regression gave me 71958 on the public LB, and it scored 63001 on the private LB, which would have placed 5th. With my first sub-72000 score, I felt like things were on a firm footing, and from there, I went on adding a few models at a time. Given how little time I had, I only got 41 OOFs with 3 days to go - adding a model a day or so. Towards the end, I started adding new OOFs with a vengeance, playing around with whether to include the original dataset or not, changing hyperparameters, using AG with CPU/P100/T4, etc. Only on the second last day did I realize that the KaggleX dataset (generated using a GAN on the same original dataset, and hence likely quite similar to the competition dataset) was in fact accessible, and I quickly reran several notebooks with the KaggleX included. In retrospect, this helped with the public LB and not the private LB. Every now and then, I'd blend with the top scoring public notebook, and this usually improved the public LB, right from a 99:1 ratio (mine:public) down to 70:30 (but seldom beyond that). In the end, I was up to 77 OOFs, and could have added more, but I could see that the CV score was barely improving, or getting worse occasionally. In the end, I chose my best scoring submission achieved without blending with another notebook's submission (71808, private 62958) and the submission with my best public LB score, achieved by blending the former with the highest scoring public notebook (71770, private 62977). All month long, we'd been debating whether there'd be a big shakeup, and there was indeed a massive shakeup, with everyone in the Top 11 besides me moving up 300 ranks or more! I was of course very happy to survive the shakeup, and get my 6th Top 10 finish in the last 7 playground competitions. A big congratulations to @martinapreusse , @gerlandore , @tilii7 and others in the Top 10 (and Top 1% etc.) & beyond who stuck to their guns, and reaped the rewards! So, all's well that ends well. But there was one little revelation still left ‚Ä¶‚Ä¶ Epilogue: A potential no. 1 solution and a no. 2 solution that I overlooked - was Hill Climbing giving me a hint? Every once in a while, I toss in an OOF prediction produced by an ensemble into my collection - I'd discovered this idea in a writeup by a former winner of a playground series (I don't remember which one right now), and it tends to work (but can also lead to overfitting - so use your own judgement). This time, I used such OOFs from AG based on 30 OOFs, and Hill Climbing (HC) after 33 OOFs - they both seemed to help, albeit only a little. It's Kaggle, I'll take it. But when I threw in the OOF prediction produced by AG based on 41 OOFs (OOF score: 72185, public LB: 71855), a curious thing happened - HC with both positive and negative weights allowed worked as usual, but HC with only positive weights allowed didn't add even a single other OOF to this one. I'd never seen this happen before, and wondered whether this was some sort of sign that this was as good as it gets. However, Ridge did produce a better CV and LB score, so on I continued. After the competition ended, I took another look at my scores - by searching for '6295', I found that I had 5 scores better than my final score (62958), all between 62950 and 62955. Then I searched for '6296', and found that I another five scores between 62959 and 62966. '6294' didn't turn up anything, so I figured I had 11 scores between the scores that place 2 and 4, and had dinner. Afterwards, I realized there might be lower scores, and sure enough there was a 62933, a potential no. 2 score! This one was produced by Ridge using 48 OOFs, including the 41 OOF based AG-OOF. This spurred me to look further, and sure enough, there was a 62892 - and guess what, it was from the same run as the AG run with 41 OOFs mentioned above, except that that one was with a GPU (T4), while this was with the CPU(s) (OOF score: 72092, public LB: 71922). So I missed out on a potential no. 1 score. Can't feel bad though, because I wasn't likely to select that, even if I'd stuck with only my own ensembles. PPS : Ridge produced the best ensemble scores For a while, I used various ensemblers - Ridge regression, AG, HC with and without negative weights (didn't use GBDTs this time).  As an example, these were the results using 30 OOFs with various ensembling approaches: Ensembler CV score Public LB Private LB Ridge Regression 72018 71958 63001 Autogluon, GPU T4 72311 71993 63029 Autogluon, GPU P100 72317 72000 63028 Autogluon, CPUs 72334 72011 63029 Hill Climbing 72297 71973 63007 Hill Climbing, positive weights only 72321 71984 62991 This is fairly representative (except that varying the CPU/GPU choice didn't produce AG results in any fixed order). Ridge almost always produced the best CV and LB scores, and though we saw that it occasionally didn't produce the best private LB score, it's blazing quick, and you can't go far wrong with it. Which brings us back to Gather, Ridge, Repeat. 3 Please sign in to reply to this topic. comment 29 Comments 1 appreciation  comment Hotness JK Enola Posted 8 months ago ¬∑ 195th in this Competition arrow_drop_up 1 more_vert Thanks a lot! For a beginner, learn a lot. Lukas Posted 9 months ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Congrats, @optimistix , the consistency of your results in all the last playground series competitions is very impressive! I saw that you also usually use Autogluon for building up your ensembles. At the moment, I treat Autogluon models as any other model in my ensemble and always train k Autogluon models through k-fold CV (and through that get the oofs and the averaged test predictions using always the best¬†Autogluon model of each fold) instead of running Autogluon once and then accessing the oofs/predictions from its individual models directly. For me, that has the advantage that I can train on additional data (e.g. the original data in the past competition) while still validating only on the actual competition data and still using the same CV folds as for all other models. Since I am quite new to Kaggle (competitions), I am however currently reflecting on my approaches to potentially improve my pipeline. In this context and because you wrote about the threat of overfitting when using oofs from ensembles for the overall ensemble, I wanted to ask how you exactly use the Autogluon models for your ensembles (since the best Autogluon model is actually often an ensemble)? Do you use Autogluon in a similar way than me or do you access the oofs/predictions from the individual models of Autogluon directly (and then also potentially ensemble them yourself)? Thanks in advance and again congrats to your awesome results! Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thanks, and congratulations to you, too! I've actually only started using AG for ensembling since the last 3 months or so. I did what you're describing to begin with, but then switched to using the inbuilt command provided by AG to get the OOF predictions (predictor.get_oof_pred_proba), because Kaggle's 12 hour limit on notebooks meant less time per fold of k-fold CV, compared to just letting AG run, and getting the OOF predictions at the end. It works well - the difference in scores obtained is small, but consequential in the playground series where ranks are often decided by differences in the 4th and 5th places. A third approach is to let AG run, and then parse the output to get the OOFs, as shared by @ravaghi here . Lukas Posted 9 months ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Thank you! Ok, that sounds reasonable and thanks for sharing your approach. I think I might consider the approach as well when I compete the next time :). Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert You're most welcome! A side-note - those with extra resources sometimes let AutGluon run for much longer than 12 hours, and I'd try that if I had the resources. Lukas Posted 9 months ago ¬∑ 12th in this Competition arrow_drop_up 1 more_vert Thanks for adding that! I also sometimes let Autogluon run for quite long times and also saw that it can be helpful. One of my ideas for reducing the need for long run times (at least for some of the runs), however, was to sometimes just run individual model types in Autogluon after identifying the best performing model types in the first experiments (if that does not lead to a significant performance drop). Do you maybe also have experience with this strategy and would you then still apply these very long run times? Tilii Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Only on the second last day did I realize that the KaggleX dataset (generated using a GAN on the same original dataset, and hence likely quite similar to the competition dataset) was in fact accessible, and I quickly reran several notebooks with the KaggleX included. In retrospect, this helped with the public LB and not the private LB. This was good thinking, but I think you lucked out on this one. I was part of KaggleX when this competition took place, and am 99% sure that the dataset for it was made by one of the mentors. That means someone unaffiliated with Kaggle, so there was no guarantee they would use a similar synthetic approach to what Inversion did. Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Ah - good point, and I had no way of knowing that. I did want to compare the datasets to be more confident about their similarity, but there wasn't much time left. I also wondered whether it could be a proxy for the private test data, but it sounds unlikely. Vitor Posted 9 months ago ¬∑ 395th in this Competition arrow_drop_up 1 more_vert Congratulations on your results! You've been achieving very consistent results in this Playground Series! I've noticed that AutoGluon has been playing a significant role in this series. I wonder if you keep every single OOF from the models AG fits, or a few of the top-ranking models, or do you only retain the single best one? Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thanks! I treat it the same as any other OOF - as long as the CV and LB both improve, I keep it; otherwise, I take a call. Sometimes that's just based on thinking things through, at other times, it might be based on using a feature selection algorithm (this is ideal, but not always feasible). You can also use things like the weight given by Ridge Regression to the model/OOF, etc. Tilii Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Congrats again! Great job at making many smart choices along the way. As the saying goes, work smart, not hard. You've had your share of hard-earned victories, and I bet it feels good to have some smart-earned as well. I think we converged on a similar strategy for this competition, as I was also using other people's OOFs directly. Never done that before, but I did have a parallel ensemble that included only my models. I would have had the same LB placement with either ensemble. Kind of feels good to know that my own ensemble was slightly better than a combo of my models and of others, but also makes me realize it is OK to use reliable public models. Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thanks again! I do generally try to run the code for myself rather than just take the OOFs - that way, I can tinker with it & generate more OOFs as well (this often means subsampling and ensuring everything works after modification, before launching a long run, esp. with GPUs on Kaggle - lose a little to gain a lot, as it were). But in a pinch, the CV-LB combo tells us whether to use it or not. It's quite impressive that you'd never used others' OOFs directly until now! Robert Hatch Posted 9 months ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Congrats! On a personal note, a little disappointed (in myself, in clear documentation) in that number of oofs‚Ä¶ I guess my code and datasets were too non-standard to notice that I had 20+ personal oofs easily available? (I don't remember at all the number for this competition, 10-35 or so this time I think. Maybe more). I had a notebook and/or dataset usable for ensembling or stacking with base features plus engg features plus \"preds_[name]\" were all oof columns, so pretty easy to extract. But it looked like you used my final ensemble as your oof? Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Thanks! Yes, I used your final ensemble, which thereby started as the top scoring OOF in the first Hill Climbing run. SCRIPTCHEF Posted 9 months ago ¬∑ 446th in this Competition arrow_drop_up 1 more_vert Hi @optimistix , thanks for the initial write up! I'm curious, did you do any feature engineering? I followed a similar methodology to you - gathering oofs (except using hill climbing to ensemble) but massively overfit to the public lb. The only difference I can think of is I used quite a few engineered features + backpackers Mae features. Tilii Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 3 more_vert I am yet to see than anyone had a major improvement coming from feature engineering. I'd be surprised if anyone near the top did. When I tested autoencoder-generated 8 latent features on my surrogate competition dataset , it actually improved the score by 10-20 points. That dataset is mostly free of outliers, which kind of explains why feature engineering worked on it, but not on the actual competition data. Robert Hatch Posted 9 months ago ¬∑ 10th in this Competition arrow_drop_up 3 more_vert It might bear repeating, as Optimistix very partially used my oof predictions, and my own score was good, but I did use feature engineering. I was careful, only spent a couple hours on it, and rejected the vast majority. But I had a few. I think(?) LightAutoML team had some as well, I forget. The big danger was trying too many and/or not being very robust in verifying whether they helped. And especially trying to verify whether it \"worked\" by checking LB score. That's the advantage us Grand Prix winners had in competitions like this. No opportunity to even make that mistake even if we wanted to. Lol Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Yes, team LightAutoML Testers did share some feature engineering they did on day 1, which decreased the public LB score by 150 for them. I used it in a few notebooks, it seemed to help - not sure whether that held true for the private LB, though. Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert I think the most consequential feature engineering by someone near (at!) the top was @martinapreusse 's use of a CatBoost classifier to add the probability of being an outlier as a feature. Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert I used different features across the collection of models - another way of increasing diversity. I did try out the MAE-MSEDiff features as well, the idea was very cool - but only in a couple of OOFs. The feature engineering shared by Team LightAutoML on day 1, which decreased the public LB score by 150 points, was the one I used the most. Not sure whether it helped or hurt the private LB, though. Tilii Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert I got tempted to use MAE-MSEDiff, but didn't in the end. It seemed like one of those too-good-to-be-true features, and I think in the end it proved to be that way. You were probably OK as in the large ensemble both good and bad models get diluted, as long as they are not crazy better than the rest. Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Yes, I basically just used it with and without the original dataset, so just 2 out of 48 to 70-odd OOFs, depending on which collection we consider. My best submissions on the private leaderboard (which I didn't select) didn't include the MAE-MSEDiff OOFs. spadel Posted 9 months ago ¬∑ 547th in this Competition arrow_drop_up 2 more_vert Hey, thanks very much for the write-up, it's quite insightful! I have a few question regarding the usage of OOFs as I don't quite understand it, would be great if you (or anyone else), could get back to me :) The way I understand it, your are referring to using OOF predictions on the full train set (consisting of k sub-sets for each fold) coming from other participants. It was not intuitively clear to me why this could be that valuable, so I did some research and it seems like you want to use OOF predictions to train a meta model which learns to combine several OOF predictions (through Ridge, Lasso etc.), is this correct? In that case, how are you then going to use that trained meta model using your own models? Let's assume you have 20 OOF predictions (presumably from 20 different models?). You would then train a meta model to combine the 20 predictions in order to reduce the CV scores on the training data and can then use the trained meta model for your own ensemble. What I don't understand is, why can't you just use your own OOF predictions to train the meta model, i.e. what is the additional benefit of using OOF predictions from other participants, especially since you cannot use their models for the submission? Also, if you train a meta model to combine OOF predictions and then simply plug in your own model predictions, why would you expect that the meta model is still able to perform as well, as it may have learned that for example OOF prediction with k=14 should be given more weight than k=11? Tilii Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 4 more_vert What I don't understand is, why can't you just use your own OOF predictions to train the meta model, i.e. what is the additional benefit of using OOF predictions from other participants, especially since you cannot use their models for the submission? I normally use only my own OOF predictions, but in this competition I used some from the others. The benefit is that more models leads to more diversity, which is generally good for ensembling. OOF files don't work on their own - we also need matching submission files. We are allowed to use publicly available OOF + submission files from others, because the act of making a notebook public automatically makes all of its contents public. The final model after ensembling will be different from any individual model even though it will include some parts from each of them. Also, if you train a meta model to combine OOF predictions and then simply plug in your own model predictions, why would you expect that the meta model is still able to perform as well, as it may have learned that for example OOF prediction with k=14 should be given more weight than k=11? This is a tough question. Many people think that we can mix and match OOF files regardless of how many folds, and what exact fold splits, were used to generate them. I am not in that group, which is exactly the reason why until this competition I used only my own OOF files that had the same fold splits. However, with a large enough dataset it seems to work even when OOF files from others were added to the mix, but up to a point. In one of my ensembles with 32 models I had roughly half of my own models and half from out there. It worked fine, but if I tried to add more outside models the CV/LB relationships were confusing. That's what made me stop adding outside models, even though there were at least 50 extra available. I would say it is best not to mix and match own and outside models if possible, but mixing will work if we don't overdo it. Vitor Posted 9 months ago ¬∑ 395th in this Competition arrow_drop_up 1 more_vert Many people think that we can mix and match OOF files regardless of how many folds, and what exact fold splits, were used to generate them. I am not in that group I agree that we cannot consistently assess and compare out-of-fold predictions when they are constructed from different splits, different seeds and with a different number of folds. However, don't you think that incorporating these OOFs is also a matter of adding diversity to the ensemble? After all, they were trained on a different subset of data (as the splits have different numbers of folds and random states),  thus capturing different \"dimensions\" of the data. Tilii Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert However, don't you think that incorporating these OOFs is also a matter of adding diversity to the ensemble? After all, they were trained on a different subset of data (as the splits have different numbers of folds and random states), thus capturing different \"dimensions\" of the data. Yes and no. Using different folds is unpredictable: it could add diversity, but it could also muddle the relationship between CV and LB scores. Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert @spadel - Thanks, glad you found the write-up useful. @tilii7 has already answered most of your points, but I feel you might be missing a key point  or two. Could you clarify why you think one cannot use others' models for submission? And why do you think we need their models when using OOF predictions? spadel Posted 9 months ago ¬∑ 547th in this Competition arrow_drop_up 1 more_vert Sure, I'm happy to clarify my misunderstanding (which still persist to some extend). @tilii7 has already clarified that you wanna be also interested in the OOF predictions on the test set, but I got the impression that apparently also the OOF predictions on the train set may be valuable even if you don't have the corresponding submission file or access to the models that created the OOF predictions. This part is not fully clear to me yet. So let's assume you collect 50 OOF predictions on the full train set from other participants and you don't have the corresponding models or submission files. The way I understand it, you want to use these OOF predictions to train a meta model that learns to combine an ensemble of diverse OOF predictions. Now, if you don't actually have access to the model that produced the 50 OOF submissions, what exactly do you do to create your submission for the test set? The way I understand it, you'd assume that your trained meta model somehow generalises to also sensibly combine OOF predictions that you generate yourself, even if it was trained on OOF submission from others. So what you could to is train 50 models yourself and feed the 50 predictions from these on the test set into the trained meta model. If that is how people use it, it is not clear to me why this should work. The meta model has been trained to combine different OOF predictions in a certain way and I don't see why blindly applying the trained meta model onto your own ensemble should produce sensible results. I hope that clarifies my confusion a bit. Thanks! Robert Hatch Posted 9 months ago ¬∑ 10th in this Competition arrow_drop_up 2 more_vert For train, you need either OOF directly, or the models AND the exact fold split information (for example type of split and reproducible seed. Or just the list of indices.) Anyways, for test you need the models OR the test preds directly. This works because in playground competitions the entire test dataset is already defined. For playground competitions, typically the test preds are already there. In fact they are much more common than train OOFs, since a notebook with just a single model will have the submission file. It's also sometimes just a bit of work to clone a public notebook to save the train OOFs when it didn't originally. Then rerun. Hope that helps! Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thanks for confirming which parts were unclear. Firstly, when one's using OOFs produced by others, it's usually because they were shared via a public notebook, which means you also have access to the models, should you need them. You can of course also tweak the models, generate new OOFs and test preds, and so on. It's relatively rare that someone only shares OOF and test predictions, but sometimes they do, e.g. via a Kaggle dataset. However, if you have only OOFs without test predictions or the means to produce them (ie the models), there's nothing to do. Secondly, the most common scenario is that you have the OOFs and test preds, so you don't necessarily need the models anymore. You train the meta model on the OOFs, and then predict using the test predictions, which now serve as the new test data. Sometimes, these OOFs and test preds are directly provided, at other times, you might have to write some additional code to produce and/or save them. Hope that helps. P.S: I guess my answer is more or less the same as @roberthatch 's, but hopefully it adds to the clarity üòÄ SCRIPTCHEF Posted 9 months ago ¬∑ 446th in this Competition arrow_drop_up 2 more_vert Just realised you blended some models with additional data and some without. Since you only have access to some oofs, do you only hill climb using the intersection and discard the oofs from the additional data? Sorry if this is a noob question. Tilii Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 3 more_vert You may hear something different from @optimistix but there is an easy way to solve this problem: remove the original data from OOF files. I would normally add the original data to the end of train data, so it comes down to cutting off everything after data point with an ID# 188532. Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert It's a valid question - if I'm running the code, the idea is to only use the additional data in the training fold, and limit the validation to the competition training data only, so the OOF length is unchanged - see e.g. this notebook by @paddykb . But if short on time, or if I'm borrowing OOFs already generated by someone else that include predictions for the original data, I simply drop the additional rows, the same as what @tilii7 said. Tilii Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 2 more_vert @optimistix I noticed that your solution is not connected to the LB, and you may want to do it for posterity. It is easy to do via a \"Team\" tab on top of this page - just paste the URL. Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Thanks! I'd done it last month, but didn't remember to do it this time (might not have done it for any earlier writeup besides the last one either) - shall go do so now. Chris Deotte Posted 9 months ago ¬∑ 81st in this Competition arrow_drop_up 2 more_vert Congratulations Optimistix! Fantastic job placing in top of both public LB and private LB. I love the slogan \"Gather, Ridge, Repeat\". This technique works great in all Kaggle competitions including both playground comps and prize comps. Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert Thanks so much! That's encouraging to hear, as I plan to start spending more time on prize comps. Mart Preusse Posted 9 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Congrats! You made it again, even without a lot of time, which shows your skill. Somehow I am always way back on the public leaderboard, but you manage to stay on top of the private and public leaderboard. Amazing. Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert Hey - you made it all the way to the top this time! Congrats once again, and thanks! Mahdi Ravaghi Posted 9 months ago ¬∑ 126th in this Competition arrow_drop_up 2 more_vert Congrats! It's great to see you at the top of the leaderboard again. Big respect for maintaining your position in both the public and private LB two months in a row. Really impressive! Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 3 more_vert Thanks so much! Didn't see many notebooks from you this time - guess you were busy as well. But the one that I did see was great as usual, it was nice to see various ensembling approaches together. In fact, it's now a handy reference for newbies on how to ensemble properly. Mahdi Ravaghi Posted 9 months ago ¬∑ 126th in this Competition arrow_drop_up 1 more_vert Thank you for the feedback! I tried to do something different last month with the little time that I had. Hopefully, I will have more time this month and (maybe) catch up to you. Optimistix Topic Author Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert I'm sure you'll do great! My participation will be limited this month as well, but the small size of the dataset might make it possible to do something meaningful in less time. All the best! Appreciation (1) Riana Azad Posted 9 months ago arrow_drop_up 6 more_vert Thanks for this writeup Too many requests error Too many requests",
      "Regression of Used Car Prices | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 9 months ago Late Submission more_horiz Regression of Used Car Prices Playground Series - Season 4, Episode 9 Regression of Used Car Prices Overview Data Code Models Discussion Leaderboard Rules Tilii ¬∑ 4th in this Competition  ¬∑ Posted 9 months ago arrow_drop_up 38 more_vert #4 solution | Beating a dead horse: blending works, but \"blending\" doesn't There was a discussion here just hours before the deadline asking whether blending will work. I will summarize the gist of my discussion in that thread and give a broad overview of my solution. Proper ensembling, which includes blending, has been used for a long time and on a variety of datasets and problems. It works. That's not me saying it, but rather a known fact. Guessing weights to combine public solutions is not blending in the original sense, so I will call it \"blending.\" That one may or may not work, and it is to a good degree dataset- and luck-dependent. The luck was not with us with this dataset, and I thought that would be the case after about 3 days working with this dataset - see here . This \"blending\" approach will work here and there, but not in general. Maybe that's good enough for some of us, but I learned after about 3 competitions that it is not my cup of tea. I selected for scoring my 1st and 3rd best submissions, so very happy about that. Both of them were hill climbing ensembles of 32 models each. The better of the two ensembles (they differ by 8 RMSE points) was made completely of my models. The second ensemble included about half my models, and the other half came from public notebooks that I thought were done well, and most importantly the notebooks that had out-of-fold predictions. Can't do proper ensembling without those! EDIT: Thanks to @roberthatch for reminding me here that I didn't give proper credit to the groups whose OOF models were part of my second ensemble. I also used LAMA's OOF models from here , @cdeotte from here and @ravaghi from here . Ensembles must be diverse to work, so both of mine included Keras factorization machines (FMs), xLearn FMs, Lasso, CatBoost, LAMA NNs, AutoGluon ensembles, and a huge array of individual tree models that were pulled out from AutoGluon directories (a mix of RandomForest, ExtraTrees, XGBoost and LightGBM models). A couple of FastAI NN models, which I don't even know how to make, were pulled also from the cache of AutoGluon models. The first four approaches modeled all the features as categoricals, and generally worked the best. I also used AugoGluon-encoded datasets (with some NLP-generated features) for separate modeling with factorization machines, and that gave a small boost. Tried feature engineering, which gave slightly better CV scores but not LB scores. Didn't use any of it in the end as I suspected overfitting. One thing I do regret is adding the original dataset too late, as it seemed to benefit the overall score. I had only 2 days to incorporate those models with original used car data, and they pushed my ensembles by about 20 points. I suspect that would have been more productive if I started earlier. 7 Please sign in to reply to this topic. comment 32 Comments 2 appreciation  comments Hotness Chris Deotte Posted 9 months ago ¬∑ 81st in this Competition arrow_drop_up 3 more_vert Congratulations Tilii. Great job performing well on private LB. Your instincts about preventing overfitting were spot on. I enjoyed all your sharing during the competition. You provided lots of good suggestions, you shared fun stuff like TSNE plots and feature importance analysis. And you encouraged many lively discussions. Tilii Topic Author Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 2 more_vert Thanks, Chris. I like how you rekindled interest in non-traditional NNs and simple linear models in this competition. There is always something to be learned from your solutions, no matter the LB placement. Riana Azad Posted 9 months ago arrow_drop_up 4 more_vert Congratulations on your success @tilii7 Tilii Topic Author Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Thanks! @rianaazad If I were you, I'd tell Fardin #1 and Fardin #2 to ease up a little bit. Their only purpose on Kaggle seems to be to upvote each of your posts, even if you are only saying \"Congratulations\" and \"Good job!\" At some point somebody will catch up on that pattern. SCRIPTCHEF Posted 8 months ago ¬∑ 446th in this Competition arrow_drop_up 1 more_vert Hi @tilii7 , late message - I have seen you mention xlearn in most of your competition solution posts. I have tried to use it on kaggle notebooks and locally (Mac OS). Locally, it throws error during building and on kaggle I have found that running any xlearn model seems to crash the notebook instantly. Do you have any advice on this? Thanks! Vitor Posted 8 months ago ¬∑ 395th in this Competition arrow_drop_up 0 more_vert @noodl35 If I'm not mistaken, I saw him somewhere else describing this issue and advising the use of an older version. SCRIPTCHEF Posted 8 months ago ¬∑ 446th in this Competition arrow_drop_up 0 more_vert Yes, remember that too! I've tried the last 4-5 versions and they all seem to throw errors. Admittedly i didn't try the rest ‚Ä¶ üòÅ Tilii Topic Author Posted 8 months ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert I don't run any scripts on Kaggle if I can help it, but I've read somewhere that most recent xLearn might be problematic to install. xLearn models mostly add diversity when dealing with categorical datasets, but they are rarely going to beat either NNs or GBMs on their own. MrSimple Posted 9 months ago ¬∑ 46th in this Competition arrow_drop_up 1 more_vert Congratulations on your success @tilii7 Tilii Topic Author Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Great job placing in top 50! Tanishk Patil Posted 9 months ago ¬∑ 832nd in this Competition arrow_drop_up 1 more_vert Congratulation! and looking forward to learn from you. HARSHA VARDHAN V Posted 9 months ago ¬∑ 274th in this Competition arrow_drop_up 1 more_vert can you please post the complete notebook. Tilii Topic Author Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert I don't package my jobs into a single notebook, so it would require a major effort on my part to bring everything together into a single file. There are literally tens of scripts applying various data modifications, then 100s of scripts creating individual models. On top of that there are tens of ensembling scripts that produce final models. Jordi Rosell Posted 9 months ago ¬∑ 734th in this Competition arrow_drop_up 1 more_vert How did you measure to include or not to include the original dataset or what makes you change your opinion? Tilii Topic Author Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert As usual, the best approach for this is to monitor the effect on cross-validation scores, followed by the effect on public LB scores. Adding the original dataset improved both scores, and could have pushed me up a spot or two if I had more time to include those models. Jordi Rosell Posted 9 months ago ¬∑ 734th in this Competition arrow_drop_up 1 more_vert When combining competition train and original data I guess one can't trust CV scores. I don't know if it's worth trying to spend 5 submissions on this: competition, competition and x1 original, and so on‚Ä¶. And then see how CV correlates with LB? Tilii Topic Author Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert When combining competition train and original data I guess one can't trust CV scores. You are right about that, as combined CV scores tend to be overly optimistic. What I did was create out-of-fold predictions for this combined train dataset, remove the original data, and re-calculate the CV score from OOF files. That worked just fine and CV scores correlated with LB. Octavi Grau Posted 9 months ago ¬∑ 326th in this Competition arrow_drop_up 1 more_vert Congrats @tilii7 ! This is a very interesting approach, will get some ideas for the next competition! Would you ensemble so many models in a binary classification or did this make more sense for regression? Tilii Topic Author Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Thanks! I always use large ensembles when I have time to build the models, and they work just as well for classification problems. In this competition I had ensembles that included ~40 models. Rudra kumar Posted 9 months ago ¬∑ 1171st in this Competition arrow_drop_up 1 more_vert Great work! @tilii7 your notebooks always help me understand different things! Congratulations! pinoystat Posted 9 months ago ¬∑ 462nd in this Competition arrow_drop_up 1 more_vert Congratulations @tilii7 . Thanks for this discussion.  I have learned from the very start that this dataseet seems to be quite different. The empirical distribution of my cv score does not even incorporate in its range the final RMSE score in the private leaderbord üòÖ. Tilii Topic Author Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Thanks. Yes, the score discrepancy was the case for all of us. I wrote a separate discussion on that topic. Sarthak Mangalmurti Posted 9 months ago arrow_drop_up 1 more_vert Congratulations @tilii7 !! And thank you so much for sharing your valuable insights. Gerald Schwartz Posted 9 months ago ¬∑ 22nd in this Competition arrow_drop_up 1 more_vert Great work. In some of your discussions you indicated that you used autogluon weighted ensembles in your blends. How were they used as there is no separate OOF data given in the autogluon results. Tilii Topic Author Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 3 more_vert There is a way to tell AG to save OOF files, which I think in simplest terms boils down to including: save_space = False , presets = 'best_quality' , keep_only_best = False , content_copy This takes quite a bit of extra disk space, but in my book it is worth it. There were discussions in the previous playground competition how to set up AG to make OOF files - look for AG's predict_oof and predict_proba_oof commands. Gerald Schwartz Posted 9 months ago ¬∑ 22nd in this Competition arrow_drop_up 0 more_vert Yes. I remember now. Thank you 5 more replies arrow_drop_down Dinesh Posted 9 months ago ¬∑ 419th in this Competition arrow_drop_up 2 more_vert Congratulations @tilii7 . There is indeed big shakeup in finsl positions. Based on results of past few playground results .looks like ensembling has become go-to-approach for good score. Thanks for sharing your approach. Tilii Topic Author Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert It is always a good idea to ensemble models, as long as things are done properly. This has been known for decades, but it is good to be reminded several times in a span of 3-4 months. BwandoWando Posted 9 months ago ¬∑ 490th in this Competition arrow_drop_up 2 more_vert Congratulations @tilii7 for an amazing final push! And thank you for the insights and knowledge you are imparting us. Tilii Topic Author Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 4 more_vert Thank you for your support. I wish to have been at least somewhat off in my overfitting forecast. Yet we learn the best from personally sobering experiences rather than someone teaching us, so maybe this is how it has to be. Optimistix Posted 9 months ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert Congrats! Solid ensembling as usual. Tilii Topic Author Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 3 more_vert Thanks! Since you asked in the other thread: my best individual model was CatBoost working with everything as categorical features, and it would have been ~80 on private LB. I wonder if anyone managed to do some serious feature engineering without overfitting. Hopefully we will find out soon enough. Ayush Dwivedi Posted 9 months ago arrow_drop_up -1 more_vert Try feature engineering S Charlesworth Posted 9 months ago arrow_drop_up 0 more_vert did you read Appreciation (2) Dna Posted 9 months ago ¬∑ 35th in this Competition arrow_drop_up 1 more_vert Excellent job, thanks for sharing.üòÅ MD. ADNAN HOSSEN Posted 9 months ago arrow_drop_up 1 more_vert great work Too many requests error Too many requests",
      "Regression of Used Car Prices | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 9 months ago Late Submission more_horiz Regression of Used Car Prices Playground Series - Season 4, Episode 9 Regression of Used Car Prices Overview Data Code Models Discussion Leaderboard Rules Nick Erickson ¬∑ 5th in this Competition  ¬∑ Posted 9 months ago arrow_drop_up 11 more_vert #5 Solution | üöÄ AutoGluon submission 1 on day 1 Our 5th place final solution was our very first submission to the competition, around 8 hours into the competition launch on day 1. We chose this submission despite it not being one of our top public LB submissions because we had good reason to believe it would do well on private LB, given we did extensive cross-validation testing. Unfortunately, it didn't get a good public LB score (72221.55, aka rank 626 in public LB), and this ended up leading to us getting 6th in this competition's Grand Prix standings since it is scored on the public LB and not the private LB, which ultimately led us to getting 2nd in the Grand Prix overall by a single point delta üòÇ. Oh well, them's the rules ü§∑‚Äç‚ôÇÔ∏è. We used pre-release AutoGluon with some experimental features toggled on: We dropped the original features in the L2 stackers, using only the stack features to train the models, this was shown to be better in CV: ag_args_ensemble = {\"use_orig_features\": False} We hacked in stratified cross-validation splits by treating the numeric label column as the stratification column. This only worked because there were multiple instances of each label in the data. To do it properly you'd want to bin it, will add this in a future feature to AutoGluon. This helped avoid major distribution shifts between folds due to outliers. We used an experimental 2024 version of AutoGluon's zeroshot-HPO portfolio which is slightly stronger. We did not use any additional data. It didn't seem to help in CV and we felt it was too risky. We disabled AutoGluon's ngram feature generation and text special feature generation, as we were not confident it led to an improvement (it was within noise, but removing them sped up model fitting): _feature_generator_kwargs = { \"enable_text_special_features\" : False , \"enable_text_ngram_features\" : False ,\n} content_copy You can see our full write-up here . P.S: We were driven mad during the Grand Prix portion trying to figure out why our public LB scores were so poor despite how confident we were based on our internal CV, to the point of us even going over our code in-depth for any possible bugs. We are glad that it turns out we were right to trust our CV, and as @tilii7 has been saying in other posts, overfitting on the public LB can often be counterproductive. I'm guessing the main reason for low correlation in public and private is due to the density of outliers in the public vs private test splits. Cheers, Nick and Lennart ( @lennartpuruckerisg ) on behalf of the \"AutoML Grandmasters\" 2 Please sign in to reply to this topic. comment 3 Comments Hotness Tilii Posted 9 months ago ¬∑ 4th in this Competition arrow_drop_up 2 more_vert Nice job explaining this in detail. I made another case in my own post that AG with original used car data, and without any other bells and whistles, had a score for #9 place on private LB. I'm guessing the main reason for low correlation in public and private is due to the density of outliers in the public vs private test splits. Yup, see here . emoji_people KennyT Posted 9 months ago ¬∑ 258th in this Competition arrow_drop_up 0 more_vert Thanks for your solution. Could you explain the idea behind \" treating the numeric label column as the stratification column.\", ie you split \"price\" to bins, and uses this bins to label encoding features? Nick Erickson Topic Author Posted 9 months ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Simply to split the data into train and val: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 4, Episode 9 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Used Car Price Prediction Dataset . Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 47.82 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 47.82 MB sample_submission.csv test.csv train.csv 3 files 27 columns  Too many requests",
    "data_description": "Regression of Used Car Prices | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 9 months ago Late Submission more_horiz Regression of Used Car Prices Playground Series - Season 4, Episode 9 Regression of Used Car Prices Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: The goal of this competition is to predict the price of used cars based on various attributes. Start Sep 1, 2024 Close Oct 1, 2024 Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = ( 1 N N ‚àë i = 1 ( y i ‚àí ÀÜ y i ) 2 ) 1 2 where ÀÜ y i is the predicted value and y i is the original value for each instance i . Submission File For each id in the test set, you must predict the price of the car. The file should contain a header and have the following format: id ,price 188533 , 43878 . 016 188534 , 43878 . 016 188535 , 43878 . 016 etc . content_copy Timeline link keyboard_arrow_up Start Date - September 1, 2024 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  September 30, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Regression of Used Car Prices. https://kaggle.com/competitions/playground-series-s4e9, 2024. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 9,606 Entrants 3,306 Participants 3,066 Teams 28,521 Submissions Tags Beginner Time Series Analysis Tabular Mean Squared Error Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s4e10",
    "discussion_links": [
      "/competitions/playground-series-s4e10/discussion/543725",
      "/competitions/playground-series-s4e10/discussion/543766",
      "/competitions/playground-series-s4e10/discussion/543672",
      "/competitions/playground-series-s4e10/discussion/543772",
      "/competitions/playground-series-s4e10/discussion/543735"
    ],
    "discussion_texts": [
      "Loan Approval Prediction | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 8 months ago Late Submission more_horiz Loan Approval Prediction Playground Series - Season 4, Episode 10 Loan Approval Prediction Overview Data Code Models Discussion Leaderboard Rules Hardy Xu ¬∑ 1st in this Competition  ¬∑ Posted 8 months ago arrow_drop_up 166 more_vert 1st Place Solution - CatBoost All The Way Down Hey Kagglers! I used to be pretty active in these playground competitions, but after the December 2023 competition I took a break from Kaggle. On a whim I decided to start working on this one about 10 days ago, and it's been as much of a thrill as it ever was. Getting 1st place was a surprise, to be sure, but a welcome one! Cross-Validation I'm sure you've heard this before, but setting up a robust cross-validation scheme for evaluating the performance of your predictions is VERY important to doing well in these competitions. I see lots of questions from folks on what kind of feature engineering to do, or how to best ensemble models, impute data, engineer features, etc. For a vast majority of these questions, there's no single answer that is universally true for any dataset. The only way to find out what works for a particular dataset is to try various options and see what performs the best, and that's where cross-validation comes in. In these playground competitions, the data is usually split 60-40 between train and test set, and 20% of the test set is used for the public leaderboard. That means that a CV score measures your performance on 60% of the entire dataset, whereas the public leaderboard measures your performance on only 8% , making cross-validation performance a much more reliable indicator of progress than public leaderboard performance. All of the decisions made below were based on optimizing my cross-validation performance. Data Preprocessing Shoutout to various member of the community for the tip to treat the numerical features as categorical. What I found most effective was to maintain both the numeric feature and a categorical copy of it. I didn't do any other feature engineering, as my experience from past playground competitions has usually been that feature engineering is of little use. I did include the original dataset. Modelling My general approach here is the same as the one I used last competition. For each of XGBoost, LightGBM, and CatBoost, I used Optuna to find 10 different sets of 'optimal hyperparameters' and averaged their predictions to get an overall prediction for each. Shoutout to @omidbaghchehsaraei 's post here for the tip to use large max_bin values. I also added a Neural Network that was heavily inspired from @paddykb 's notebook here . The performance of each of these models is as follows: Model CV Score Public LB Private LB LightGBM .96811 .97005 .96637 XGBoost .96767 .96989 .96540 CatBoost .96972 .97299 .96865 NN .96678 .97088 .96577 What I think might have been my secret sauce was that for each of these model predictions, I trained a CatBoost model using the initial model predictions as a baseline. An example of how to do this can be found here . I'm not sure exactly what inspired me to do this, perhaps it was from seeing how amazingly well CatBoost performed on this data, but to my surprise CatBoost was able to significantly improve the performance of each of these model predictions, even the ones that were originally generated using CatBoost. The performance of these CatBoost-improved models are as follows: Initial Model CV Score Public LB Private LB LightGBM .96856 .97048 .96713 XGBoost .96815 .97024 .96611 CatBoost .96997 .97334 .96903 NN .96732 .97117 .96667 I find it impressive that the CatBoost model that used CatBoost predictions as a baseline would have been enough for 3rd place. CatBoost was the king for this comp! The final step was a Neural Network to stack these 4 predictions together. This squeezed out the extra last bit of performance needed to bring the solution to the top. CV Score Public LB Private LB .97059 .97344 .96938 98 11 20 8 2 Please sign in to reply to this topic. comment 94 Comments 17 appreciation  comments Hotness Optimistix Posted 8 months ago ¬∑ 299th in this Competition arrow_drop_up 19 more_vert Congratulations! Very neat solution. I'd thought of using the 10 best models produced by Optuna and averaging, but only for CatBoost. I'd never seen the option of using a pre-trained model as a baseline for CatBoost  until now - thanks for introducing me to this new and cool idea, shall be sure to give it a try in the future. Thomas Mei√üner Posted 8 months ago ¬∑ 789th in this Competition arrow_drop_up 9 more_vert I have never seen this before as well. That is what makes Kaggle so special. There is always someone who discovers a brilliant new idea or technique. Hardy Xu Topic Author Posted 8 months ago ¬∑ 1st in this Competition arrow_drop_up 4 more_vert Thank you! I must confess when I saw you at the top of the public leaderboard, I checked your profile and saw your placements in the past few playground competitions. This result may not have been your best one, but it's incredible how you've been consistently placing in the top 10 throughout the year! Optimistix Posted 8 months ago ¬∑ 299th in this Competition arrow_drop_up 3 more_vert Thanks! It's been a good run for the most part, and I've learned a lot and had great fun. We hadn't seen each other since you took a break this year & I started after that, but it's great to see you, looking forward to more cool solutions from you. Hanif Imaduddin Posted 6 months ago arrow_drop_up 1 more_vert Wow, great!! Mart Preusse Posted 8 months ago arrow_drop_up 5 more_vert Interesting solution! Congratulations on winning and thanks for sharing the details. I do not fully get how you used the Catboost predictions as baseline. Did you use the oof predictions trained on the full training data and inserted them as baseline or did you make a split as suggested in the link? When you made a split, did you use 50-50? And did you then train the baseline predictions on the 50% split and predict the other 50%? Or did you used predictions based on nested crossvalidation? Hardy Xu Topic Author Posted 8 months ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert Thank you! Yes, for the baseline I did use oof predictions. Gerald Schwartz Posted 8 months ago ¬∑ 284th in this Competition arrow_drop_up 0 more_vert As stated in the link, did you use the predicted values rather than predict_proba values or did you round them to integer? Hardy Xu Topic Author Posted 8 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert The baseline needs to consist of 'raw values', which in the context of binary classification means the logits. For XGBoost, LGBM, and CatBoost I don't think there's a prediction function that natively returns logits, so I converted the predicted probabilities into logits for the baseline. Todd Gardiner Posted 8 months ago ¬∑ 205th in this Competition arrow_drop_up 3 more_vert Congratulations on the win @hardyxu52 ! Classy move sharing the catboost method many of us hadn't seen before. Thomas Mei√üner Posted 8 months ago ¬∑ 789th in this Competition arrow_drop_up 4 more_vert Very cool solution, congrats! Ali_Haider_Ahmad Posted 7 months ago arrow_drop_up 1 more_vert Great work! I'm new here and learning from insightful discussions and solutions from experienced individuals like you. I have a quick question: How do you define the callbacks for your models? Specifically, for a given fold, do you select the best model based on the highest AUC value or the lowest loss value? In practice, we often prioritize minimizing loss for better generalization. However, from your comments, it seems things might be different in competitions. Could you clarify this? Yaƒümur Dedeko√ß Posted 8 months ago ¬∑ 421st in this Competition arrow_drop_up 1 more_vert Congratulations! This first-place finish is truly well-deserved. From your robust cross-validation strategy to the creative use of CatBoost, every step reflects your commitment to winning this competition. Your approach of enhancing other models with CatBoost, and the final touch of combining everything with a neural network, is especially impressive! Your attention to detail and meticulous modeling process proved that winning here takes not only technical skill but also strategic foresight. Reaching the top spot in such a competitive field is an achievement worth celebrating. May this victory be a source of inspiration for your future projects. Congratulations again, and enjoy this well-earned success! @hardyxu52 Ben Greenwalt Posted 8 months ago arrow_drop_up 1 more_vert Congratulations! I appreciate the thorough explanation of your methods. Yousef_Ayman Posted 8 months ago arrow_drop_up 1 more_vert congratulations,you deserve 1est placeüëè Joel B. Posted 8 months ago arrow_drop_up 1 more_vert Congratulations! You inspire me. Tariq Bin Bashar Posted 8 months ago ¬∑ 2027th in this Competition arrow_drop_up 1 more_vert congrats bro Cyril Bourgeois Posted 8 months ago ¬∑ 129th in this Competition arrow_drop_up 1 more_vert Well Done. Beautiful combination of specific model typologies from various data interpretation into a catboost L2 pivot ^^ mahayasa adiputra Posted 8 months ago ¬∑ 2555th in this Competition arrow_drop_up 1 more_vert congratulations, what a great work very inspiring emoji_people abdelbasset ben kerrouche Posted 8 months ago ¬∑ 102nd in this Competition arrow_drop_up 1 more_vert congratulation sir you deserve 1est place Saim Nasir Posted 8 months ago ¬∑ 2983rd in this Competition arrow_drop_up 1 more_vert I think I should start taking these competitions more seriously, like this is such a inspiring post that i might as well try to perform these steps on my own on the same dataset. Hardy Xu Topic Author Posted 8 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert I'm glad to hear that! Reading these sorts of solutions posts are great for getting ideas, but the best way to learn and get better is to try to apply those ideas to your own work. Humayra Khanom Rime Posted 8 months ago arrow_drop_up 1 more_vert Congratulations! @hardyxu52 Prashant Kumar Posted 8 months ago arrow_drop_up 1 more_vert Appreciate the clear and thorough solution, @hardyxu52 , and huge congrats on earning first place! This is really well-executed‚Äîcongratulations on the win! üèÜ Oscar Aguilar Posted 8 months ago ¬∑ 242nd in this Competition arrow_drop_up 1 more_vert Congrats @hardyxu52 ! David Bland√≥n Posted 8 months ago ¬∑ 646th in this Competition arrow_drop_up 1 more_vert I have learned so much from this solution, thanks for sharing @hardyxu52 , and congratulations for the first place üéâ! Yuheng Jia Posted 8 months ago ¬∑ 77th in this Competition arrow_drop_up 1 more_vert This post is very detailed and clear, thank you so much and of course, congratulations! Niladri Sarkar Posted 8 months ago ¬∑ 1936th in this Competition arrow_drop_up 1 more_vert Very elegantly done! Congrats on the win üèÜ Gerald Programmer Posted 8 months ago ¬∑ 35th in this Competition arrow_drop_up 1 more_vert congratulationsü•≥ ‚Ä¶I can't seem to find the notebook have you publicly shared it Hardy Xu Topic Author Posted 8 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert I do all my work locally on my machine, so no notebook unfortunately. emoji_people Fuat Yava≈ü Posted 8 months ago ¬∑ 63rd in this Competition arrow_drop_up 1 more_vert Excellent solution . Congratulations on winning the competition  üëèüéâ üéä Kushvinth Madhavan Posted 8 months ago ¬∑ 1891st in this Competition arrow_drop_up 1 more_vert very nice method Ole-Jakob Posted 8 months ago ¬∑ 496th in this Competition arrow_drop_up 1 more_vert Congrats, well done. Thank you for the explanation and insight into pre-trained CatBoost model. Too many requests error Too many requests",
      "Loan Approval Prediction | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 8 months ago Late Submission more_horiz Loan Approval Prediction Playground Series - Season 4, Episode 10 Loan Approval Prediction Overview Data Code Models Discussion Leaderboard Rules OMID BAGHCHEH SARAEI ¬∑ 2nd in this Competition  ¬∑ Posted 8 months ago arrow_drop_up 30 more_vert 2nd place solution Hi everyone, First, I want to thank siukeitin for his insightful posts and comments. His Grandmaster title is well-deserved, and I‚Äôve learned a lot from his work. Second, I‚Äôd like to acknowledge paddykb for his innovative approach of treating all features as categories(in his notebook), which proved to be a game-changer in this competition. His excellent notebook ( PS s4e10 - No Keras, No Loan (cv 0.963) ) also contributed to the diversity of my ensemble models. To be honest, I'm not an expert data scientist. It's surprising to have achieved second place, considering the many talented participants. By the way, my approach was relatively simple, you can see it in my notebook here . In summary, I did not choose my best LB score, 0.97350, with a CV of 0.96954. My final submissions were as follows: CV(5 folds)=0.97107, LB= 0.97217 (21 models) CV(5 folds)=0.97026, LB= 0.97335 (24 models) To achieve this result, I used original dataset and did a little bit feature engineering in a few models (adding new features) for diversity. The screenshots below are from my notebook. Wish you all the best, OMID BAGHCHEH SARAEI 4 Please sign in to reply to this topic. comment 19 Comments 1 appreciation  comment Hotness KennyT Posted 8 months ago ¬∑ 165th in this Competition arrow_drop_up 4 more_vert Hi, congrats for the 2nd position. Second, I‚Äôd like to acknowledge paddykb for his innovative approach of treating all features as categories(in his notebook), which proved to be a game-changer in this competition About this point, can you share the link of this notebook? behzad gerami Posted 8 months ago ¬∑ 745th in this Competition arrow_drop_up 3 more_vert Hi KennyT I was surprised as well after changing all features to categorical.  By the way, I wrote function for these kinds of projects. please take a look. Having opinions from experts will be valuable for me: https://www.kaggle.com/code/behzadgerami/3-functions-for-different-kind-of-base-models OMID BAGHCHEH SARAEI Topic Author Posted 8 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Hi, see this notebook PG s4e10: Loan ü¶àü¶àü¶à Shap , @cdthinh . KennyT Posted 7 months ago ¬∑ 165th in this Competition arrow_drop_up 0 more_vert Thanks very much. Mahdi Ravaghi Posted 8 months ago ¬∑ 8th in this Competition arrow_drop_up 3 more_vert Good to see you in the top 3 @omidbaghchehsaraei , congratulations! I also tried hill climbing but didn't have any success. Tried both the implementation that you used and another by @adaubas which included cross-validation and early stopping. behzad gerami Posted 8 months ago ¬∑ 745th in this Competition arrow_drop_up 4 more_vert Nice job Omid and congrates! happy to find you. Just one thing, I did not see the features that you added in your note book. can you explain more about your feature engineering? Thanks Behzad OMID BAGHCHEH SARAEI Topic Author Posted 8 months ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Hi @behzadgerami , I did not see the features that you added in your note book. Yes, it is true. The screenshots above are from my private notebook with over 20 models, not my public notebook with only 9 models. But, the approach is the same. can you explain more about your feature engineering? In the middle of the competition, I looked at other notebooks to find new features for my models, especially for my poly-LGBM model (=private notebook). I added a new feature to this model for diversity reasons. You can see it below: train [ 'risk_flag' ] = (np .where ((train [ 'cb_person_default_on_file' ] == 'Y' ) & (train [ 'loan_grade' ] .isin ( [ 'C' , 'D' , 'E' ] )), 1 , 0 ))\ntest [ 'risk_flag' ] = (np .where ((test [ 'cb_person_default_on_file' ] == 'Y' ) & (test [ 'loan_grade' ] .isin ( [ 'C' , 'D' , 'E' ] )), 1 , 0 ))\noriginal [ 'risk_flag' ] = (np .where ((original [ 'cb_person_default_on_file' ] == 'Y' ) & (original [ 'loan_grade' ] .isin ( [ 'C' , 'D' , 'E' ] )), 1 , 0 )) content_copy As far as I remember, I added new features to a few models (2 or 3 models). behzad gerami Posted 8 months ago ¬∑ 745th in this Competition arrow_drop_up 2 more_vert thanks, Omid. I wrote a block to find out the best combination of base models for ensembling. then I made an NN model for ensembling. That NN model performed better than the logistic regression model.  let me try your feature engineering then will share them with you. behzad gerami Posted 8 months ago ¬∑ 745th in this Competition arrow_drop_up 2 more_vert this is the function that I have for finding the best combination of base models. plz take a look if you are interested: https://www.kaggle.com/code/behzadgerami/find-best-combination-for-ensemblign Sumit_08 Posted 8 months ago ¬∑ 625th in this Competition arrow_drop_up 1 more_vert Congratulations Bro. I get amazed to see your code. That how quick and well your code gets executed in less time. Oscar Aguilar Posted 8 months ago ¬∑ 242nd in this Competition arrow_drop_up 1 more_vert Congrats, @omidbaghchehsaraei . I tried hill-climbing by the end of the first week of the competition; however, during the second week, I started achieving better results with Ridge as the stacker, and I forgot about hill-climbing. Niladri Sarkar Posted 8 months ago ¬∑ 1936th in this Competition arrow_drop_up 1 more_vert Congrats! Nice idea to use so many different models and their versions üëç emoji_people Bhargav Borah Posted 8 months ago ¬∑ 102nd in this Competition arrow_drop_up 1 more_vert Congratulations @omidbaghchehsaraei ! peyman armaghan Posted 8 months ago ¬∑ 305th in this Competition arrow_drop_up 1 more_vert congratulation!  nice job omidüëç Khwaish Saxena Posted 8 months ago arrow_drop_up 2 more_vert congratulations buddy Himanshu Kaushik Posted 8 months ago arrow_drop_up 0 more_vert Congratulations on the second place @omidbaghchehsaraei Here is another competition where you can get to compete your feature engineering skills and win rewards. This competition is all about feature engineering infact OMID BAGHCHEH SARAEI Topic Author Posted 8 months ago ¬∑ 2nd in this Competition arrow_drop_up 0 more_vert Hi @jeevahimanshukaushik , Thank you for the kind words and the competition recommendation. It's great to see an interesting medical diagnosis competition on Kaggle! However, I'm currently focused on other projects, and I'll definitely keep an eye out for future competitions. Thanks again for the suggestion. emoji_people Fuat Yava≈ü Posted 8 months ago ¬∑ 63rd in this Competition arrow_drop_up 2 more_vert Congratulation @omidbaghchehsaraei . Nice job üëè Mart Preusse Posted 8 months ago arrow_drop_up 2 more_vert Well done, congrats on 2nd! I admit that I still did not try hillclimbing as I never managed to produce enough models. Great that you had success with this strategy. Tilii Posted 8 months ago arrow_drop_up 2 more_vert Well done and richly deserved! I do know a little bit about not being a data scientist. Yet another proof that hill climbing works in these playground competitions. I think by now there is enough evidence that everyone should be using it. Optimistix Posted 8 months ago ¬∑ 299th in this Competition arrow_drop_up 2 more_vert Congrats once again! I'd been expecting you and @ravaghi to be in the Top 10, so much so that it was part of my rationale for expecting a shakeup. You've been doing all the right things for many months now, it's wonderful to see you in the Top 3 (2!). Appreciation (1) Friendly Local Data Guy Posted 8 months ago arrow_drop_up 1 more_vert Great work! Too many requests error Too many requests",
      "Loan Approval Prediction | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 8 months ago Late Submission more_horiz Loan Approval Prediction Playground Series - Season 4, Episode 10 Loan Approval Prediction Overview Data Code Models Discussion Leaderboard Rules Ravi Ramakrishnan ¬∑ 4th in this Competition  ¬∑ Posted 8 months ago arrow_drop_up 45 more_vert Rank 4 approach - thoughtful model choices and effective ensembles Hello all, Thanks to Kaggle for a good classifier episode in the Playground series! I also wish to extend sincere thanks to all the participants in the competition and the forum contributors for their wonderful contributions though the month. I also extend sincere thanks to the community for receiving my artefacts so well in the challenge! Please find below my approach for the competition and the associated write-up - Approach overview My overall approach for the competition was simpler than the recent past editions of the playground series, as I relied on a lesser number of features and more of mindful blending and stacking in this episode. I was of the opinion that a smaller data size could increase one's chances to overfit and hence, retained a simple approach with boosted tree and NN model options without blind blending and with careful consideration for data leakage and cv-scheme analysis I used a simple stratified 10-fold CV scheme with random state = 42 in this challenge and retained this for all the models uniformly. My overall approach for the chosen submission can be illustrated as below- I managed my time in this competition as below- Consider a single model - say Catboost. Work on the model with multiple features and parameters till I am satisfied with the signal extraction using the model Proceed to the next model, say LGBM. Repeat the process on various feature sets till I am confident with the results/ run out of time Repeat the process with different algorithms (single models) till the final week Consider various blending and stacking options on the best CV results across single models Make a final submission choice based on cv-scores I wish to extend special thanks to the kernel here - this helped me greatly in my final push on the leaderboard. My other submission was my own work entirely excluding the public kernel artefacts. I believed in my contributions in the competition and hoped that my work would prevail and am happy my belief indeed prevailed! CV- LB relations for the 2 submissions are as below- Submission details CV 10-fold Stratified K-fold Public Leaderboard Private Leaderboard My independent work + public work - stacked with torch NN 0.97002 0.97393 0.96902 My independent work without public work 0.969954 0.97353 0.96899 Feature sets and Feature engineering I relied on 5 feature sets in a feature store, akin to my TPS- July 2024 process. I designed a gamut of features, featuring a lot of brute-force driven secondary features and varying model parameters, but nothing prevailed more than a simple catboost model with all string values . I presume this dataset was designed in a manner that perhaps was not amenable to secondary features. I used the original data in all my feature sets . I ended up using 3 feature sets out of 5 sets based on the CV scores and choose diverse models to my best capabilities to stack up with a neural network. I discarded my feature sets with 50+ features as the CV results were not encouraging at all. Upon peeking into the private scores across my single models, I am happy I rejected the said features and relied on simplicity. My chosen feature sets included between 15-25 features, all created with simple operations on the existing features without data leakage . What did not work Autogluon models Xgboost - I used the XgBoosts posited in the public work but my private XgBoost models always failed on the LB Linear models like logistic regression Random forest KNN My modus operandi and key take-aways I presume a lot of beginners will read this post and hence, wish to add a separate section on my take on submitting well in this series- Take your time with your first real and meaningful submission - build a pipeline first and ensure it works well. This is highly important for a smooth experiment process through the month You may consider my public artefacts as a base model - I have highly simplified the process to remove clunky elements and have retained and developed a modular structure for convenience. This helps me manage my models effectively and easily Always save your fitted model, OOF score and test set scores with a proper naming convention. This will help you immensely when you blend and fuse models at a later date CV-scheme is one's open secret to success - ignore anything that does not have a proper cv-backup and justification Don't rely on public materials without testing them for compatibility with your process - this is imperative for you to generalize your models well. I encourage one to use the ideas shared in public kernels in your independent work Team up well and learn together - we are here first to learn and then for results A well organized GitHub repo is a great addition - please consider making a repo for your work and place your code and artefacts there. It will immensely help you across competitions Dwell on your mistakes in one episode and learn from them first before moving on - this is a long run learning and is likely to improve you in future episodes NEVER BLINDLY BLEND PUBLIC KERNELS - this is almost certain to fail Concluding thoughts I wish to extend sincere wishes to one and all for Diwali and hope the festival of lights and joy brings in a lot of happiness for all of you on Kaggle and in other walks of life as well! See you all in the next episode of the series and all the best! Regards, Ravi Ramakrishnan 1 2 Please sign in to reply to this topic. comment 39 Comments 3 appreciation  comments Hotness shiron104 Posted 2 months ago arrow_drop_up 1 more_vert hi, i know it is 6 months after this contest ended but can u inform me about how did you preprocess the data? Ravi Ramakrishnan Topic Author Posted 2 months ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Simple interaction features @shiron104 Waheed Hussain Posted 8 months ago arrow_drop_up 3 more_vert I'm particularly impressed @ravi20076 by your thoughtful approach to feature engineering. The focus on simplicity and avoiding over-engineering has paid off. It's a great reminder that sometimes, less is more. Your decision to stick with a smaller feature set and focus on robust models like CatBoost and NN was spot on. The detailed explanation of the feature selection process and model experimentation is invaluable. Thanks for sharing your insightful thoughts and experience, a lot to learn from. aldparis Posted 8 months ago ¬∑ 10th in this Competition arrow_drop_up 3 more_vert Hi @ravi20076 , Thank you for having shared your solution, and congratulations for your result. It's amazing because you wrote XGB and LogisticRegression didn't work for you but it worked for me. I used LogisticRegression for my ensemble : it runs quickly, i can see stability of coefficients, I can factorize colinear models in meta models. XGB wasn't better than LGBM and CatBoost in this competition, but XGB with depthwise method gave some diversity in my ensemble. Oscar Aguilar Posted 8 months ago ¬∑ 242nd in this Competition arrow_drop_up 3 more_vert Congratulations on placing 4th, @ravi20076 ! My best solution also didn‚Äôt use XGBoost , as I found it wasn‚Äôt competitive unless the max_bin parameter was increased into the thousands. However, considering the size of the data, I was concerned that using a high value for max_bin with XGBoost might lead to overfitting. KennyT Posted 8 months ago ¬∑ 165th in this Competition arrow_drop_up 1 more_vert Hi @ravi20076 Congrats for the 4th position. Reading your post, I understand that CV-scheme is very important factor. CV-scheme is one's open secret to success - ignore anything that does not have a proper cv-backup and justification Do you have any guideline (or link) on how to make a good CV-scheme? Ravi Ramakrishnan Topic Author Posted 8 months ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert These are unfortunately not available as readymade materials, but rather is explorative. You will have to try out several approaches and then decide the best approach based on your model results. This is the only way to design a good model in my opinion @cdthinh KennyT Posted 8 months ago ¬∑ 165th in this Competition arrow_drop_up 1 more_vert Thanks @ravi20076 for your advice. Guilherme P Oliveira Posted 8 months ago arrow_drop_up 1 more_vert Congratulations on your result and a BIG thank you for sharing such rich and detailed knowledge!! Your final tips are extremely useful for anyone. Shedrack Eneojo Okute Posted 8 months ago ¬∑ 438th in this Competition arrow_drop_up 1 more_vert Hi Ravi, Please, what if I don't want to use the original data set? Just using only the train and test sets, how do I handle this on your public artefacts, and I want to change the metric to my choice? Tanishk Patil Posted 8 months ago arrow_drop_up 1 more_vert Congratulation! Vey informative. Jianan Zhang123 Posted 8 months ago arrow_drop_up 1 more_vert nice job! very good points! Himanshu Kaushik Posted 8 months ago arrow_drop_up 1 more_vert Thanks @ravi20076 for the detailed posts. Here is a link to the competition that you may like. It can cause huge impact in healthcare industry Sheikh Muhammad Abdullah Posted 8 months ago ¬∑ 106th in this Competition arrow_drop_up 1 more_vert @ravi20076 Congratulation ! Very amazing Solution! David Bland√≥n Posted 8 months ago ¬∑ 646th in this Competition arrow_drop_up 1 more_vert @ravi20076 congrats and thank you for sharing, I always learn a lot just reading your posts! GARFIELD Posted 8 months ago ¬∑ 452nd in this Competition arrow_drop_up 1 more_vert My independent work + public work - stacked with torch NN> Did you stack all the models by choosing NN as the meta model, fitting it on oof-validation-predictions from other models, and predicting on oof-test-predictions? Or you also did oof stacking fitting and predicting? Ravi Ramakrishnan Topic Author Posted 8 months ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert I stacked a neural network atop out of fold predictions @garfield2021 Muhammed Tausif Posted 8 months ago arrow_drop_up 1 more_vert Clear and useful explanation. Wyrden Posted 8 months ago arrow_drop_up 1 more_vert Congrats! Thanks for sharing some advice for beginners. Sumit_08 Posted 8 months ago ¬∑ 625th in this Competition arrow_drop_up 1 more_vert Congratulations Brother üéä üëè Minato Namikaze Posted 8 months ago ¬∑ 280th in this Competition arrow_drop_up 1 more_vert Amazing! Congratulations @ravi20076 and wish you a Happy Diwali ü™î emoji_people Bhargav Borah Posted 8 months ago ¬∑ 102nd in this Competition arrow_drop_up 1 more_vert Congrats, @ravi20076 , on another outstanding performance! Thanks also for the write-up and for including such a thoughtful section for beginners. Shedrack Eneojo Okute Posted 8 months ago ¬∑ 438th in this Competition arrow_drop_up 1 more_vert What do you mean by signal extraction? Did you get you set of features from only one model, say Catboost then experiment with different models on those set of  features?Please,  if you don't mind, could share the link to your notebook?Thank you Ravi Ramakrishnan Topic Author Posted 8 months ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert My baseline materials are already in public domain @shedracka Vitor Posted 8 months ago ¬∑ 223rd in this Competition arrow_drop_up 1 more_vert Congratulations for your placement! I was also afraid of overfitting since the dataset was considerably small. Could you elaborate on how you define your feature sets? Ravi Ramakrishnan Topic Author Posted 8 months ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Simple brute-force features with very basic operations, all of them were tested for cv score and they either did not degrade the score/ marginally improved it @vitormeurerbesen Shedrack Eneojo Okute Posted 8 months ago ¬∑ 438th in this Competition arrow_drop_up 1 more_vert Please,  what is meaning diverse model when ot co es to stacking? Is that the cv score differences should be high? What are the factors to consider before doing blending and stacking? Ravi Ramakrishnan Topic Author Posted 8 months ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert @shedracka models need to have different parameters and should be as different from each other. In this case, I wan't able to choose 2 very different models due to the nature of the assignment, so I focused on selecting model algos and parameters that were at least a few points spread out wrt their cv scores. Some empirical experience and lots of single model candidates are required to make such a choice. Tilii Posted 8 months ago arrow_drop_up 6 more_vert Diverse models have a non-overlapping expertise. Let's say we want to hire two handy men to finish some stuff around the house before moving in. We would more likely hire one person that is good at fixing electrical installation and another that is good at fixing bathrooms, rather than hiring two people that are good at fixing bathrooms. In this context that means picking two models where one of them is better at predicting class 1 (in relative terms), and the other that is better at predicting class 0. When we average them, they complement each other. This is a shameless self-promotion, but if you want to read more about it: https://www.kaggle.com/competitions/playground-series-s4e6/discussion/512220 https://www.kaggle.com/competitions/playground-series-s4e6/discussion/514624 https://www.kaggle.com/competitions/playground-series-s4e7/discussion/523661 SCRIPTCHEF Posted 8 months ago ¬∑ 20th in this Competition arrow_drop_up 1 more_vert Thanks @ravi20076 this is a clear and concise write up. I have two questions: How do you do feature engineering? When you say: 'satisfied with the signal extraction using the model' what does this mean exactly? Thanks for taking the time to do the write up! Ravi Ramakrishnan Topic Author Posted 8 months ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert I did not use any packages for features. I simply tried out a number of combinations by brute-force and tested them for worthiness using the cv-score. Most of them were useless and I simply added them for diversity @noodl35 OpenFE has almost never worked for me from past experience, so I use it as a last resort. Shedrack Eneojo Okute Posted 8 months ago ¬∑ 438th in this Competition arrow_drop_up 1 more_vert Please, Ravi, how do I do feature engineering without domain knowledge. If you have resources, I would be glad to receive Ravi Ramakrishnan Topic Author Posted 8 months ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert @shedracka you need to use some brute-force and check your features with the CV score for inclusion/ exclusion Shedrack Eneojo Okute Posted 8 months ago ¬∑ 438th in this Competition arrow_drop_up 1 more_vert Ravi, I look forward to learning from you Congratulations üéä üëè Shedrack Eneojo Okute Posted 8 months ago ¬∑ 438th in this Competition arrow_drop_up 1 more_vert Congratulations üéä.  Thank you for sharing your solutions Too many requests error Too many requests",
      "Loan Approval Prediction | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 8 months ago Late Submission more_horiz Loan Approval Prediction Playground Series - Season 4, Episode 10 Loan Approval Prediction Overview Data Code Models Discussion Leaderboard Rules Mahdi Ravaghi ¬∑ 8th in this Competition  ¬∑ Posted 8 months ago arrow_drop_up 21 more_vert 8th place solution Firstly, congratulations to @hardyxu52 , @omidbaghchehsaraei , and @nadavcherry for placing in the top 3! It‚Äôs great to have you, @hardyxu52 , back in the playground competitions. I hope to see more of you in future competitions. Data Preprocessing I created five pipelines to train each of my base models. These pipelines included different types of preprocessing for the categorical features. In three of the pipelines, I used the original dataset, but only during training. Validation was done using only the competition dataset. For CatBoost, I treated all features as categorical, as this showed an improvement in both CV and public LB scores. Modeling I used CatBoost, XGBoost, LightGBM (with three different boosting types: GBDT, DART, GOSS), histogram gradient boosting, gradient boosting, AutoGluon, and neural networks in this competition. Except for the neural networks, I trained all of these models on the five pipelines previously mentioned and saved their OOF predictions. I also modified the CV strategy used by @omidbaghchehsaraei in his notebook to be the same as my other models and used some of his models in my final solutions. The neural networks in my solution were inspired by this notebook by @paddykb . Ensembling In my 8th place solution, I gave all the OOF files I had collected to AutoGluon with mostly default settings and let it handle the ensembling. This model achieved a CV score of 0.970887, a public LB score of 0.97329, and a private LB score of 0.96900. Unfortunately, I do not have a notebook for this on Kaggle, as I had to do this on my own computer. This result was obtained after 24 hours of training, which Kaggle notebooks do not allow. However, I do have a notebook on Kaggle for my other submission that achieved the same private LB score as my AutoGluon solution and would also place in the top 10. This solution consisted of ensembling OOF files using ridge and logistic regression, followed by another ensembling step using a weighted average approach. It‚Äôs worth noting that the previously mentioned AutoGluon solution used OOF files from 52 models, but in this multilevel ensemble approach, I found that reducing the number of OOF files improved CV and public LB scores. Initially, I tried using RFECV to select models, but I achieved better results by identifying the best models for the ensemble using a simple brute-force approach. In the end, I used 19 models in both ridge and logistic regression (though not the same 19 models in each). Results The figure below shows the 10-fold CV score of all my base models as well as my ensembles. Note that the AutoGluon scores are not shown in the figure, but as I mentioned previously, the 10-fold CV score it achieved was 0.970887, which is the highest CV score I have achieved in this competition. 2 Please sign in to reply to this topic. comment 6 Comments 1 appreciation  comment Hotness Optimistix Posted 8 months ago ¬∑ 299th in this Competition arrow_drop_up 5 more_vert Congrats! Solid as ever - you're pretty much a fixture in the Top 10, and after @martinapreusse last month & @omidbaghchehsaraei this time, it's your turn to be in the Top 3 real soon. Tilii Posted 8 months ago arrow_drop_up 3 more_vert +1 to that sentiment. Mahdi Ravaghi Topic Author Posted 8 months ago ¬∑ 8th in this Competition arrow_drop_up 2 more_vert Thank you @optimistix , I'll do my best! I was almost sure that you would keep your spot in the top 10 this month as well given how well you have done in the past few months. Optimistix Posted 8 months ago ¬∑ 299th in this Competition arrow_drop_up 0 more_vert Thanks so much! Mart Preusse Posted 8 months ago arrow_drop_up 2 more_vert Congratulations on place 8. Great that you scored this high again. Wish you all the best. aldparis Posted 8 months ago ¬∑ 10th in this Competition arrow_drop_up 2 more_vert Hi @ravaghi , Congratulations for your .9709 CV score. I had only a . 9705 CV score, therefore I'm behind you on private. Congratulations for your 8th place ! Thank you for having shared your solution and thank you for your feedbacks about Hill Climbing. Appreciation (1) redo1 Posted 5 months ago arrow_drop_up 1 more_vert Great explanation. Too many requests error Too many requests",
      "Loan Approval Prediction | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 8 months ago Late Submission more_horiz Loan Approval Prediction Playground Series - Season 4, Episode 10 Loan Approval Prediction Overview Data Code Models Discussion Leaderboard Rules aldparis ¬∑ 10th in this Competition  ¬∑ Posted 8 months ago arrow_drop_up 19 more_vert 10th place solution : no blind blend Hi, Thank you Kaggle for this competition, congratulations to everyone and thank you to have shared so many usefull insights during this episode. I'm on vacation a few days and write this message with my phone. My solution is a LogisticRegression of 4 meta learners : each meta learner (bold below) is a stack of GBMs : Boxplots are the 4 repetitions with various OOF predictions obtained with various random seeds : I wanted robust results. Before this Logistic, I trained more than 30 GBMs, I tried everything I was able to try with categorical hyperparameters of XGBoost, CatBoost and LightGBM and I learned a lot from these personal experiments. I kept both categorical and numerical features for several columns of train dataset ( person_income especially) . I didn't impute missing values, I didn't made feature engineering and kept the original dataset only for training (not for validation). I used optuna to fit hyperparameters of each GBM. Here is my final submission . Good luck for next competitions and have fun ! 1 Please sign in to reply to this topic. comment 10 Comments Hotness Vishesh75 Posted 7 months ago arrow_drop_up 1 more_vert Congratulations! I'm relatively new to the community, but I got great insights from your solution. Optimistix Posted 8 months ago ¬∑ 299th in this Competition arrow_drop_up 3 more_vert Congrats once again! This approach is reminiscent of @oscarm524 's ensemble-of-ensembles approach, with your emphasis on robustness on top. aldparis Topic Author Posted 8 months ago ¬∑ 10th in this Competition arrow_drop_up 2 more_vert To learn to be robust is my goal by playing with Kaggle competitions. Oscar Aguilar Posted 8 months ago ¬∑ 242nd in this Competition arrow_drop_up 2 more_vert Congratulations, @adaubas . I considered doing something similar to your logistic stacker with four CatBoost models, but I ran out of time. I focused too much on Ridge in this competition. emoji_people Bhargav Borah Posted 8 months ago ¬∑ 102nd in this Competition arrow_drop_up 1 more_vert Congratulations, @adaubas ! That was a neat solution. Khwaish Saxena Posted 8 months ago arrow_drop_up 2 more_vert Congratulations, that was a good solution. Mart Preusse Posted 8 months ago arrow_drop_up 2 more_vert Congratulation on place 10 and thanks for sharing your submission notebook. I will check how you utilized XGB as I sometimes get a relatively poor score with it. aldparis Topic Author Posted 8 months ago ¬∑ 10th in this Competition arrow_drop_up 2 more_vert Hi @martinapreusse , I did sometimes had poor score too with XGBoost. But sometimes it is with LightGBM, and some other times it is with CatBoost‚Ä¶ XGBoost is the oldest one of the three, but it is very fast with GPU and sometines it is the best ! With Kaggle I learned to use the 3. And instead of playing with the grow_policy hyper parameter of each GBM techique, I'm using native grow policy of each GBM technique and blend their results. Hope to see you soon in another competition ! Tilii Posted 8 months ago arrow_drop_up 2 more_vert Making a strong ensemble of one model type is very difficult, even when using 3 different implementations. Props to you @adaubas and good to have you back! aldparis Topic Author Posted 8 months ago ¬∑ 10th in this Competition arrow_drop_up 3 more_vert Yes, I was a little bit lucky to have such CV improvments with only GBMs. It's interesting to see the 3 methods of GBM can be quite diverse. And I wasn't able to adapt the fantastic embedding NN from @paddykb to my ideas (had numerical features‚Ä¶). Unfortunately, the public NN didn't improve my CV score Hanif Imaduddin Posted 6 months ago arrow_drop_up 0 more_vert very Great!! Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 4, Episode 10 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Loan Approval Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 6.25 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 6.25 MB sample_submission.csv test.csv train.csv 3 files 27 columns  Too many requests",
    "data_description": "Loan Approval Prediction | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 8 months ago Late Submission more_horiz Loan Approval Prediction Playground Series - Season 4, Episode 10 Loan Approval Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: The goal for this competition is to predict whether an applicant is approved for a loan. Start Oct 1, 2024 Close Nov 1, 2024 Evaluation link keyboard_arrow_up Submissions are evaluated using area under the ROC curve using the predicted probabilities and the ground truth targets. Submission File For each id row in the test set, you must predict target loan_status . The file should contain a header and have the following format: id ,loan_status 58645 , 0 . 5 58646 , 0 . 5 58647 , 0 . 5 etc . content_copy Timeline link keyboard_arrow_up Start Date - October 1, 2024 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  October 31, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Ashley Chow. Loan Approval Prediction. https://kaggle.com/competitions/playground-series-s4e10, 2024. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 9,586 Entrants 4,080 Participants 3,858 Teams 29,624 Submissions Tags Beginner Tabular Finance Roc Auc Score Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s4e11",
    "discussion_links": [
      "/competitions/playground-series-s4e11/discussion/549160",
      "/competitions/playground-series-s4e11/discussion/549197",
      "/competitions/playground-series-s4e11/discussion/549155",
      "/competitions/playground-series-s4e11/discussion/549194",
      "/competitions/playground-series-s4e11/discussion/549964"
    ],
    "discussion_texts": [
      "Exploring Mental Health Data | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 7 months ago Late Submission more_horiz Exploring Mental Health Data Playground Series - Season 4, Episode 11 Exploring Mental Health Data Overview Data Code Models Discussion Leaderboard Rules Mahdi Ravaghi ¬∑ 1st in this Competition  ¬∑ Posted 7 months ago arrow_drop_up 102 more_vert 1st place solution Well, this is a nice and totally unexpected surprise! Congratulations to everyone who survived the shake-up, and thanks to @optimistix and @tilii7 for their support and encouragement last month, which kept me motivated to win the competition of this month. I kept my promise , @optimistix . Funny enough, my winning solution came from an early experiment I ran during the second week of the competition, which, honestly, I didn‚Äôt think had a chance of winning. Over the month, I trained many different models (69 in total) with various data pipelines and configurations, and tried many different methods to ensemble them. However, as it turned out, fewer models and a simpler ensembling approach worked better this time around. I ended up using 24 models (still a lot, though!). The reason I doubted this solution would work was because the CV score was much higher than I expected (0.94173), and the public LB score was lower than many of my other submissions (0.94284). My other selected submission, which I spent a lot more time on, had a CV score of 0.94150 and a public LB score of 0.94285. Despite this, I couldn‚Äôt find any errors in my code, so I decided to trust the CV score and use this solution as one of my submissions. My approach this month was similar to last month. I didn't do any feature engineering and did not drop any features, despite the temptation to remove the Name column. For the modeling part, I trained XGBoost and three variations of LightGBM on four different data pipelines, with the original dataset being included in two of the pipelines. I also used the OOF files from my public notebook . Additionally, I trained two AutoGluon models, one with and one without the original dataset. These two models had the highest CV scores and were probably the two most important models in my ensemble. The scores for each of my models, and my AutoGluon ensemble can be seen in the figure below. After training all the models and collecting their OOF files, I let AutoGluon handle the ensembling, ensuring to define the CV strategy myself to avoid leakage. It worked really well last month, so I decided to try it again, and it didn‚Äôt disappoint! I also experimented with ensembling the OOF files with hill climbing, Ridge, Logistic Regression, and a combination of Ridge and Logistic Regression, but the CV scores for these methods didn‚Äôt even come close to what I achieved with AutoGluon. 52 36 19 16 1 Please sign in to reply to this topic. comment 76 Comments 9 appreciation  comments Hotness Electro Knight Posted 7 months ago ¬∑ 866th in this Competition arrow_drop_up 7 more_vert But wouldn't not cleaning the data mean machine has learnt false things? Like for example if the form is distributed in a specific locality where depression is high. There are 2 people having same name let us take Rohan. both Rohan have depression. But the data of 1st Rohan is used in training and second is used in testing. The machine will learn that the Rohan is cause of depression but in reality that it is the very specific locality that causes it. Like the poverty in area is the actual factor rather than name. Mahdi Ravaghi Topic Author Posted 7 months ago ¬∑ 1st in this Competition arrow_drop_up 9 more_vert What you're saying makes sense and is a valid concern, but keep in mind that we are working with a synthetic dataset. With real data, I would address the issues you're mentioning, but based on what I've learned about synthetic datasets since I began competing in TPS, it's usually best to leave the inconsistencies in the data as is. Electro Knight Posted 7 months ago ¬∑ 866th in this Competition arrow_drop_up 0 more_vert I see thanks. Optimistix Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 4 more_vert Congrats once again! While I'm not surprised that your winning submission was achieved early on (so were my highest private scores, which I didn't choose), it's great that you chose wisely. You always turn in solid work and get great scores and are high on the LB, so it was only a matter of time before you won, and here we are. Celebrate this! Den_Kuznetz Posted 7 months ago arrow_drop_up 1 more_vert Congratulations! That's a great result! JuseTiz Posted 7 months ago ¬∑ 15th in this Competition arrow_drop_up 1 more_vert Very impressive solution! Thank you for sharing. A question: how do you save the out-of-fold (OOF) results from Autogluon? Did you use the .predict_proba_oof method in Autogluon? Mahdi Ravaghi Topic Author Posted 7 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thank you! Yes, I have an example here . Shedrack Eneojo Okute Posted 7 months ago arrow_drop_up 1 more_vert Congratulation ,Mahdi. What another great opportunity to learn from you again Yannick Maehlmann Posted 7 months ago ¬∑ 1789th in this Competition arrow_drop_up 1 more_vert Great solution, I wondered if the 1st place finish would include the name column in the dataset, as I dropped it instantly, but interestingly it does make for better results. Mohamed Drabo Posted 7 months ago ¬∑ 436th in this Competition arrow_drop_up 1 more_vert I want to know how to select my submissions for my public score. I used to think that I should select the submission with the highest score, but then I realized that some of my submissions have a better public score than my selected submission. Mahdi Ravaghi Topic Author Posted 7 months ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert Trust your CV. Works most of the time for me. Beata Faron Posted 7 months ago arrow_drop_up 1 more_vert Congratulations ! Great solution! T_Py09 Posted 7 months ago ¬∑ 1516th in this Competition arrow_drop_up 1 more_vert Congratulations! The explanation of the solution was excellent. Could you please explain in simple terms what is the use of oof files ? @ravaghi Thanks. Tilii Posted 7 months ago arrow_drop_up 1 more_vert I'll let @ravaghi answer your question since that is who you asked. For the future: any time you have a question, I suggest you start typing keywords into the search field at the top of the \"Discussion\" section. Typing oof into that field will give you the same results as the link below: https://www.kaggle.com/competitions/playground-series-s4e11/discussion?sort=undefined&search=oof Those are all the threads in this competition with a word oof , and somewhere in there might be an answer to your question. Mahdi Ravaghi Topic Author Posted 7 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert The same question is asked and briefly answered in this very thread as well. Przemyslaw Kwiecinski Posted 7 months ago arrow_drop_up 1 more_vert Congratulations! Excellent solution. Chanhee0129 Posted 7 months ago ¬∑ 2464th in this Competition arrow_drop_up 1 more_vert That's good Riyad Mehdi Posted 7 months ago arrow_drop_up 1 more_vert Congratulations, very interesting Mayko Rodrigues Ribeiro da Costa Posted 7 months ago ¬∑ 2176th in this Competition arrow_drop_up 1 more_vert Congratulations on the result and exceptional mastery of the tools. Oleksandr Vyshnevskyi Posted 7 months ago ¬∑ 2476th in this Competition arrow_drop_up 1 more_vert Great solution swaminathan07 Posted 7 months ago arrow_drop_up 1 more_vert great work done Dragos Posted 7 months ago arrow_drop_up 1 more_vert Great solution, great notebook! Ayo-Ajakaiye Adedamola Posted 7 months ago ¬∑ 2489th in this Competition arrow_drop_up 1 more_vert Congratulation, Honestly I have to put more work into my modelling, I got bored waiting for my system to load the dataset which got me demotivated, now I know I have to be more efforts to win thanks. A-SP Posted 7 months ago arrow_drop_up 1 more_vert Congratulations, bravo! Mohammad Arshad Ahmad Posted 7 months ago arrow_drop_up 1 more_vert Congratulations on the well-deserved win! It's impressive how you trusted an early experiment and focused on a simpler ensemble approach, proving that persistence and intuition matter. Your detailed breakdown, especially the use of AutoGluon, is both insightful and inspiring. Looking forward to seeing more of your work @ravaghi rƒ±za temizel Posted 7 months ago arrow_drop_up 1 more_vert I was pretty sure I‚Äôd see this post one day in the near future :) Congrats! Mahdi Ravaghi Topic Author Posted 7 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thank you! I thought the same about you, but you have stopped competing in TPS. I'm sure you have more important things to do, but I do hope to see more of you in the playground competitions. KennyT Posted 7 months ago ¬∑ 719th in this Competition arrow_drop_up 1 more_vert Thanks @ravaghi and congrats for the victory. It is another good example of trust your CV than public LB . ü•≥ 1) So in total you have 4 data pipelines? And what are the differences between these data pipeplines? 2) In this competition, the data is still unbalanced. Why dont you use AUC instead of accuracy ? Tilii Posted 7 months ago arrow_drop_up 2 more_vert 2) In this competition, the data is still unbalanced. Why dont you use AUC instead of accuracy? A correct probability threshold, if using, or a well-calibrated model, is needed regardless of whether one uses AUC or accuracy. I also think this question is answered by the fact that @ravaghi won the competition. KennyT Posted 7 months ago ¬∑ 719th in this Competition arrow_drop_up 0 more_vert It is clear to me now. Thanks @tilii7 William178 Posted 7 months ago ¬∑ 556th in this Competition arrow_drop_up 1 more_vert Congrats! I have a question here, differences in common LB and CV scores may be due to differences in data distribution, have you tried using different CV grouping strategies (like stratified sampling, time series segmentation) to verify the robustness of the model? Yaƒümur Dedeko√ß Posted 7 months ago ¬∑ 441st in this Competition arrow_drop_up 1 more_vert Congratulations! The explanation of the solution was excellent. abdelbasset ben kerrouche Posted 7 months ago ¬∑ 637th in this Competition arrow_drop_up 1 more_vert congrat @ravaghi Md Abdul Mutalib Posted 7 months ago arrow_drop_up 1 more_vert Congratulations, @ravaghi , on your great success. Which other model have you tried to get nearly similar accuracy? Too many requests error Too many requests",
      "Exploring Mental Health Data | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 7 months ago Late Submission more_horiz Exploring Mental Health Data Playground Series - Season 4, Episode 11 Exploring Mental Health Data Overview Data Code Models Discussion Leaderboard Rules Jack Lee ¬∑ 4th in this Competition  ¬∑ Posted 7 months ago arrow_drop_up 25 more_vert 4th place solution - preprocess + automlüöÄ I'm very happy to have achieved fourth place! Although I believe there is an element of luck involved, I still want to share my solution. Preprocessing After conducting some EDA, I noticed that there was a lot of unreasonable noise in the data. Based on the data preprocessing of @adyiemaz 's great notebook , I made some modifications, setting unreasonable data to NaN (since the automl framework can automatically handle NaN values). For example, I considered city names like \"Less than 5 hours\" (which I thought should describe sleep time) as unreasonable. In my experiments, this change resulted in better scores in my CV, public LB, and private LB. AutoML Due to time constraints, I didn't perform very complex model selection. I think AutoML is a great choice to reduce workload while still achieving good results. Ultimately, I used AutoGluon and LightAutoML , and simply performed an equal-weight blending of their results. Overall, when I finished 8th last time, I observed that blindly blending and overfitting to the public LB led to low private LB scores. In this competition, I am still primarily focusing on the CV score. AutoGluon AutoGluon provides many pre-tested hyperparameter combinations and bagging+stacking techniques that perform well. Although its CV can sometimes be overly optimistic, it still performed well in this competition. I used the 2024 set of 200 hyperparameters provided by the AutoGluon team. Thanks to their work! However, in retrospect, its private LB score was slightly lower than the experimental_quality preset score from version 1.2.0, but it performed better on CV and public LB. LightAutoML Compared to AutoGluon, LightAutoML offers more deep learning-based models. I used this setup to call all models: general_params = {\"use_algos\": [['lgb_tuned', 'cb_tuned', 'mlp_tuned', 'dense_tuned', 'denselight_tuned', 'resnet_tuned', 'snn_tuned', 'node_tuned', 'autoint_tuned', 'fttransformer_tuned']]} . It performed slightly better than AutoGluon on public and private LB, but slightly worse on CV, which could be due to the overly optimistic CV scores from AutoGluon mentioned earlier. Finally, I want to thank my college for providing CPU computational support, as well as Kaggle platfrom, all the participants and you, the reader of this article! 7 2 1 Please sign in to reply to this topic. comment 6 Comments Hotness abdelbasset ben kerrouche Posted 7 months ago ¬∑ 637th in this Competition arrow_drop_up 1 more_vert congrat and big thanks for your explanation Jack Lee Topic Author Posted 7 months ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Thank you! Optimistix Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 2 more_vert Congrats! LightAutoML's NNs are generally better than Autogluon's, but I didn't get around to using them this time. But even Autogluon on its own did pretty well on the private LB, though it didn't necessarily seem so on the public LB. Jack Lee Topic Author Posted 7 months ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Thank you! And congratulations for your excellent performance on both the public and private LBüòÄ Optimistix Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert Thanks! The public LB was just some blending which I was careful not to choose among the final 2; the private LB was real work but I wish I'd chosen the ones which ended up with the best private scores. Still, shouldn't complain üòÄ William Guesdon Posted 7 months ago ¬∑ 34th in this Competition arrow_drop_up 1 more_vert I was not aware of this. Thank you for sharing :) Optimistix Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 0 more_vert You're welcome! Nick Sibo Zhu Posted 7 months ago arrow_drop_up 0 more_vert You are my god, I am a big fan of you. Thank you Jack! Too many requests error Too many requests",
      "Exploring Mental Health Data | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 7 months ago Late Submission more_horiz Exploring Mental Health Data Playground Series - Season 4, Episode 11 Exploring Mental Health Data Overview Data Code Models Discussion Leaderboard Rules Optimistix ¬∑ 13th in this Competition  ¬∑ Posted 7 months ago arrow_drop_up 18 more_vert 13th place solution: 10 times the work to finish 13th instead of 4th My main goal this month was to not repeat the mistake of last month, when I fell all the way from 1 on the public LB to 299 on the private LB. On that front, I'm pleased to finish 13th, though a tad disappointed not to place in the Top 10, especially because I had as many as 7 submissions which scored high enough on the private LB to be in the Top 10, but I didn't choose them. Before I go on, I'd like to congratulate @ravaghi on a very well-deserved win, and everyone else who finished on the right side of the shake up. I'd also like to acknowledge those who generously shared their insights and code, including but not limited to @ravaghi , @bjoernholzhauer , @cdeotte , @siukeitin , @peymanarmaghan , @igorvolianiuk , @oscarm524 My approach this month was mostly similar to the last few months: trying out many different models,  trying to generate diversity in my collection of OOFs, keeping an eye on the CV-LB correspondence, and trying to learn new things from the discussions, public notebooks, and elsewhere. It worked reasonably well, but funnily enough, some of the submissions which didn't score that high in terms of CV or public LB (and were thereby not chosen among the final 2 by me) ended up doing surprisingly well. Here are a few things to note: A little blind blending, but not submitted: I couldn't resist a little blending with public notebooks every now and then, which artificially kept me high on the public LB for a while. But I made sure to not select any of those submissions, as I was pretty sure that a huge shake up was coming. The \"top two\" submissions - a small collection and a large one: The top scoring submission was typical of my efforts over various TPS competitions this year - a collection of 79 OOFs, ensembled by Autogluon (CV = 0.94192, LB = 0.94371). This scored 0.94151 on the private LB, and secured me the 13th place. The other one (CV = 0.94090, CV = 0.94360, private LB = 0.94147) was a small collection of just 11 OOFs which I really liked - it included 4 CatBoost, 4 XGBoost, 2 LGBM and one MLP OOF(s). This would have placed 24-26, so it was pretty good for a 11 model collection. However, it turned out that I had as many as 16 submissions that would have scored higher than the ones I chose, and most of them would have been hard to guess. 16 submissions better than the ones I chose: It's not surprising to have a few submissions with a higher private score than the ones you chose - but this month was surprising, both because of the sheer number of such submissions (16), and how varied they were. The number of OOFs varied from 7 to 130(!), and the ensembling methods also varied, including Ridge, Lasso, Ridge + Logistic Regression (directly borrowed from @ravaghi ), Hill Climbing (with and without negative weights), Autogluon, LightAutoML. And most of these 16 submissions were from the first two weeks, meaning that in the second half, I was working hard to drive the CV up while maintaining a good CV-LB correspondence, only to end up with modest scores on the private LB. My highest private score (CV = 0.941087, public: 0.94317, private: 0.94180), which would have placed 4th, used 42 OOFs and was achieved with 22 days to go. And somewhat ridiculously, my next best (CV = 0.94156, public: 0.94163, private: 0.94177) was simply Autogluon with the original dataset included, and a few features added. I'm still finding it hard to believe that the private score is higher than both the CV as well as LB - never seen that before, feels like some sort of lottery ticket situation. So why were so many private scores surprising, even with a good CV-LB correspondence? My very first submission this month was simply the sample submission file - since it contained all zeros and the metric was accuracy, the score directly told me the fraction of zeros in the 20% of test data used for the public LB. At 0.82073, this implied the fraction of 1s was 0.17297, which was somewhat lower than the 0.18171 seen in the training data. This made me a bit wary of following @cdeotte 's advice of predicting exactly 17044 1s (given in his fabulous notebook on Hill Climbing) - what if the remaining 80% of test data also had about 17.297% 1s? The private score of my first submission tells us that there are 81.687% 0s, or 18.313% 1s - so maybe I should have followed the advice. Something to go back and check, and also bear in mind for the future. Anyway, all this implies that the unseen 80% had a higher proportion of 1s, which might be one reason why the scores are somewhat surprising. Also, since students were harder to predict than professionals, there might have been a higher proportion of students. What worked, and what didn't: Building a diverse collection of OOFs worked, as usual. Focusing on CatBoost worked in some ways, but not in others. Specifically, I experimented a lot with various hyperparameters of CatBoost, which yielded CV up to 0.94184 (!), and public LB up to 0.94355, but the best of these scored 0.94140 on the private LB, which would have placed around 35 - not bad for a solo model, I guess, but I was hoping for more. On the other hand, many of the CatBoost models with a variety of hyperparameters I usually neglect (e.g. Bayesian, Bernoulli and MVS boosting, Newton Cosine/L2 scoring function, etc.) contributed to the diversity of ensembles. NNs scored a bit lower, but combined well with GBDTs, as usual. In the end, this was not an episode where CatBoost was King - it was one of the top models for sure, but XGBoost, LGBM and even Gradient Boosting were competitive with it. With that, I'll end my write up of this month's efforts - it was an interesting journey, and while I'm a wee bit disappointed to miss out on a Top 10 or even Top 4/5 finish, I'm glad to have survived the shakeup and finished at 13. All the best to everyone for the last TPS competition of the month! Please sign in to reply to this topic. comment 10 Comments Hotness JuseTiz Posted 7 months ago ¬∑ 15th in this Competition arrow_drop_up 1 more_vert Congratulations on making it through the competition and thanks for your detailed solution! The major shakeup in the rankings was expected, but the relationship between the CV and LB scores still came as a surprise. Among my submissions, the one with the best private score had both a lower public LB score and a lower CV score compared to the submission I ultimately chose. But at least in this competition, relying on the CV score didn't lead to a drop in the rankings. üòÇ Optimistix Topic Author Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert Thanks, and congratulations on your 15th place finish! Mart Preusse Posted 7 months ago arrow_drop_up 1 more_vert Thanks for your write up and congratulations on your good performance. I am currently working in a competition with real data which has a lot of challenges in terms of feature engineering and selection. I am also tempted to use a blending solution for the first time, but I think I will resist to max my chances to get the best place possible. Optimistix Topic Author Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert Thanks! Did you mean another competition on Kaggle, or elsewhere? All the best for a strong finish! Mart Preusse Posted 7 months ago arrow_drop_up 1 more_vert It is the game competition on kaggle and I have zero cv-lb correlation. It will not be a strong finish but I hope for top 100. Optimistix Topic Author Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert Sounds like one that I'm also supposedly participating in, but barely so. Well, I can focus on it for two days now, I suppose :-) Chris Deotte Posted 7 months ago ¬∑ 25th in this Competition arrow_drop_up 1 more_vert Congratulations Opimistix finishing in top spot again. I'm impressed that you have consistently achieved top spot in all playground competitions! My approach this month was mostly similar to the last few months: trying out many different models, trying to generate diversity in my collection of OOFs, keeping an eye on the CV-LB correspondence, and trying to learn new things from the discussions, public notebooks, and elsewhere. It worked reasonably well Great strategy! I couldn't resist a little blending with public notebooks every now and then, which artificially kept me high on the public LB for a while. But I made sure to not select any of those submissions This is fun. I like to do this too. (However I didn't have time in this competition) Building a diverse collection of OOFs worked, as usual. Focusing on CatBoost worked in some ways, but not in others. Specifically, I experimented a lot with various hyperparameters of CatBoost, which yielded CV up to 0.94184 (!), and public LB up to 0.94355, but the best of these scored 0.94140 on the private LB, which would have placed around 35 - not bad for a solo model Great single model Optimistix Topic Author Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert Thanks so much! I did have nice big falls to 113 in June and 299 in November, though. Neverthless, it's been a great year overall. Need to work on tearing myself away from spending too much time in these competitions and get more into the other competitions before I become a full-blown expert in modeling synthetic tabular data üòÄ Hardy Xu Posted 7 months ago arrow_drop_up 1 more_vert Great work as always! Regarding your note on many submissions having surprising private leaderboard scores, another factor could be that due to the inherently noisy nature of accuracy as a metric, trying to optimize for it in CV testing might also result in more noisy signals for what kind of changes constitute actual improvement. If you happen to have CV scores saved with a smoother metric like AUC or LogLoss, it could be worth checking if those correspond better with the private leaderboard scores. Optimistix Topic Author Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert Thanks! There was some discussion around this, and I did use AUC and MCC every now and then - MCC in particular seemed to help with CV, esp. with CatBoost and with Hill Climbing, but not necessarily with the LB(s). Looks like you skipped this month's TPS episode? Hardy Xu Posted 7 months ago arrow_drop_up 1 more_vert Yes, this past month I was occupied with the BrisT1D competition . I'll see if I can spend some time on the December edition, although I also have my eye on the Rohlik Sales Forecasting Challenge . Optimistix Topic Author Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert All the best for all of those! Mahdi Ravaghi Posted 7 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Congratulations on being a top performer this month as well! Your CV scores are a bit surprising to me. My CV scores didn‚Äôt come anywhere close to what you achieved. That makes me think some level of luck was involved in my 1st place win. But then again, with hard metrics like accuracy, luck being an element may not be a surprise at all. Optimistix Topic Author Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert Thanks! Well, mostly it was hard to cross 0.9410 with Lasso or Ridge, combining Logistic Regression & Ridge pushed it a bit further, up to 0.9412 or so a few times (but mostly not). 0.9415 and beyond was only with Hill Climbing or Autogluon (AG). Too many requests error Too many requests",
      "Exploring Mental Health Data | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 7 months ago Late Submission more_horiz Exploring Mental Health Data Playground Series - Season 4, Episode 11 Exploring Mental Health Data Overview Data Code Models Discussion Leaderboard Rules Chris Deotte ¬∑ 25th in this Competition  ¬∑ Posted 7 months ago arrow_drop_up 37 more_vert 25th Place - GBDT plus NN - Trust CV Hi everyone. Thank you for the lively discussions and generous sharing. I enjoyed participating in this competition with everyone and I plan to participate in December's competition here ! üéâ This is my second playground competition. In my previous playground competition, I built 100+ models and used hill climbing here (discussion writeup here ). However from my first experience, I learned that more simple feature engineering and simple solutions work well in playground competitions, so in this competition my final solution is an ensemble of only 3 models (with simple feature engineering). Namely CATboost plus XGBoost plus NN. This is a powerful diverse ensemble combination! üî• Use AUC as proxy for ACC (accuracy) metric The metric in this competition was ACC. This metric is not smooth and has lots of random variance when trying to use it to optimize models and decisions. Therefore I used the more reliable metric AUC to locally find the best CV score. Then I chose my best CV AUC ensemble/model as my final submission. 33% CatBoost - CV ACC=0.9401 (AUC=0.9751), LB Public=0.9433, Private=0.9405 My CatBoost model was based on top scoring single model CatBoost public notebook here by @abdmental01 33% XGBoost - CV ACC=0.9400 (AUC=0.9755), LB Public=0.9439, Private=0.9400 My XGBoost model was based on top scoring single model XGBoost public notebook here by @adyiemaz 33% NN (MLP) - CV ACC=0.9399 (AUC=0.9756), LB Public=0.9427, Private=0.9413 My NN by itself would achieve 68th rank on private LB! It's a strong model! I encoded all columns the same way as my public notebook here . Namely I converted every column into categorical strings (and transformed rare values to value = \"RARE\", and nan to value = \"NAN\"). Then I used my NN code from September's playground competition here . All hyperparameters, learning schedule, and architecture were the same. Ensemble - CV ACC=0.9406 (AUC=0.9762), Public LB=0.9438, Private LB=0.9415 I tried adding some other models, but the three above achieved the best ensemble CV. So my final ensemble is only the three models described above and achieved 25th place in Kaggle's \"Exploring Mental Health Data\" competition! üí™ Please sign in to reply to this topic. comment 16 Comments 1 appreciation  comment Hotness Mart Preusse Posted 7 months ago arrow_drop_up 3 more_vert Thanks for sharing your approach and congratulations for finishing at a good place this time! 25th is great for just three models. Interstingly, the public score of the ensemble was worse than the single XGB score and you picked it nevertheless. Good luck for the next competition. giim Posted 7 months ago ¬∑ 1212th in this Competition arrow_drop_up 1 more_vert @cdeotte Thanks for sharing the insights. Have you had an experience or trial at multi-objective optimization using both AUC and ACC? Chris Deotte Topic Author Posted 7 months ago ¬∑ 25th in this Competition arrow_drop_up 1 more_vert Hi. Yes I did look at both. I would mainly make decisions that would boost both. Kyle Bautista Posted 7 months ago arrow_drop_up 1 more_vert Congrats! Great work! Ole-Jakob Posted 7 months ago ¬∑ 49th in this Competition arrow_drop_up 1 more_vert Congrats and thanks for sharing and for the great post about metrics during the competition. Initially, I considered using MCC instead of ACC, but after reading your post I switched to AUC which made it much easier to compare models. Also, your GPU hill climbing implementation is awesome, and the trick to compute ACC based on the distribution was great. Thank you! Hamed Abedi Posted 7 months ago ¬∑ 1826th in this Competition arrow_drop_up 1 more_vert Hi Chris, I always enjoy your insights, and they‚Äôve been incredibly helpful. As a relatively new user on this platform, I‚Äôve always wondered‚Äîdo you typically use the Kaggle runtime for building and training your models (and if so, do you use accelerators like TPUs, etc.), or do you rely on a local server for this? Chris Deotte Topic Author Posted 7 months ago ¬∑ 25th in this Competition arrow_drop_up 3 more_vert Hi. I usually train models locally (on either GPU V100 32GB or GPU A100 80GB). Then in code competitions I upload the saved model weights and in CSV competitions like this one, I upload the submission.csv file. Then in CSV competitions, I use the \"submit\" button (to submit submission.csv) and in code competitions, I create a Kaggle inference notebook that uses the uploaded saved model weights to infer test data. Hamed Abedi Posted 7 months ago ¬∑ 1826th in this Competition arrow_drop_up 1 more_vert Thank you, for sharing your workflow‚Äîvery insightful and much appreciated! Optimistix Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 2 more_vert Congratulations! Very cool for a 3 model ensemble to do so well, and it's especially impressive that your NN outperformed your GBDTs on the private LB. Maybe I'll just plug it into every month's data üòõ Looking forward to your contributions this month - all the best! Mahdi Ravaghi Posted 7 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Congrats @cdeotte , and thank you for your contributions to PS competitions. It's great to have a seasoned Kaggler like yourself in these types of competitions; we have a lot to learn from you. Regarding what you wrote about feature engineering, how about dropping it altogether for this month's competition and see how it goes? I know it's generally bad advice and goes against our intuitions, but in my experience, it's best to leave out feature engineering in the playground competitions. Chris Deotte Topic Author Posted 7 months ago ¬∑ 25th in this Competition arrow_drop_up 5 more_vert Congratulations @ravaghi winning 1st place! Great job! I agree that feature engineering doesn't seem to help much in playground competitions. This might be a result of the synthetic data generation process. Feature engineering is usually about digging deep to find more subtle patterns. Since the original datasets are small, there doesn't seem to be much subtle patterns (because we lack original data size to provide statistically significant patterns besides very basic patterns). Mahdi Ravaghi Posted 7 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Thank you! Your explanation makes a lot of sense. In addition to the size of the original dataset, I think the noise introduced by the generator also plays a role here. Engineering new features just propagates this noise, and makes our models perform worse. Optimistix Posted 7 months ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert While feature engineering is indeed often futile in TPS competitions, the 3rd and 7th place solutions this month are both solo CatBoosts with meaningful feature engineering. Den_Kuznetz Posted 7 months ago arrow_drop_up 0 more_vert Very informative. Great job, you're a cool! Tanishk Patil Posted 7 months ago arrow_drop_up 0 more_vert Congratulation! looking forward to learn from you. This comment has been deleted. Chris Deotte Topic Author Posted 7 months ago ¬∑ 25th in this Competition arrow_drop_up 3 more_vert Our models make probability predictions which are float numbers between 0 and 1. When we compute ACC metric, we discard information first by converting all prob predictions into integers 0 and 1 with p = (pred>0.5).astype(\"int\") . When we compute AUC, we do not discard information. Any metric computed from less information is less informative. The metric AUC uses the full information from the full float probability prediction (and is therefore more informative for making decisions). Regarding ACC is \"not smooth\" means that hill climbing ends too early using ACC as the hill climbing metric and the resultant LB score is worse. When using AUC, the hill climbing progresses longer and achieves a better LB score. The ACC metric seems to jump around more whereas the AUC metric behaves more consistently. Discussions with more information are here and here This comment has been deleted. Chris Deotte Topic Author Posted 7 months ago ¬∑ 25th in this Competition arrow_drop_up 0 more_vert Good summary. giim Posted 7 months ago ¬∑ 1212th in this Competition arrow_drop_up 2 more_vert If I can give an additional argument: accuracy is a low resolution metric, because its is invariant under a lot more transformations than AUC. If you were to move class 1 scores from the 0.8 point to 0.9 the ACC value wold be the same but the AUC would most probably show an improvement and you would want to know about that. ACC sees two score bands 0 to 0.5 and 0.5 to 1 and it only tracks point moving between the two bands, = crossing the threshold. Scores can move a lot without crossing the threshold and you may want to be aware of that when doing model selection. That said I only used ACC and I see my mistake now :/ Appreciation (1) tasmiyashirin Posted 7 months ago ¬∑ 369th in this Competition arrow_drop_up 1 more_vert Thank you for sharing! Too many requests error Too many requests",
      "Exploring Mental Health Data | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 7 months ago Late Submission more_horiz Exploring Mental Health Data Playground Series - Season 4, Episode 11 Exploring Mental Health Data Overview Data Code Models Discussion Leaderboard Rules Baseer Shah ¬∑ 35th in this Competition  ¬∑ Posted 7 months ago arrow_drop_up 2 more_vert 35 place solution (baseline submission 588 places up) This is the only competition i submitted the least submissions with only two baseine notebook submissions that were supposed to be a starting point. I only used a single catboost model with very little preprocessing done on the data. Considering the dataset features, i chose to convert all of the features to category dtype as mostly already were object dtype and it proved to work with this type of data. Here is the link to my notebook : https://www.kaggle.com/code/baseershah/baseline-pss4e11 I did expect overfitting but unfortunately couldn't find time for this competition and i believe with a little more preprocessing and some tuning without overcomplicating the model could've resulted in a more robust model with better generalization capabilities. Please sign in to reply to this topic. comment 0 Comments Hotness Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 4, Episode 11 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Depression Survey/Dataset for Analysis dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Notes: 3 files 28.07 MB csv CC0: Public Domain Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 28.07 MB sample_submission.csv test.csv train.csv 3 files 41 columns  Too many requests",
    "data_description": "Exploring Mental Health Data | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 7 months ago Late Submission more_horiz Exploring Mental Health Data Playground Series - Season 4, Episode 11 Exploring Mental Health Data Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: Your goal is to use data from a mental health survey to explore factors that may cause individuals to experience depression. Start Nov 1, 2024 Close Dec 1, 2024 Evaluation link keyboard_arrow_up Submissions are evaluated using Accuracy Score . Submission File For each id row in the test set, you must predict the target Depression . The file should contain a header and have the following format: id ,Depression 140700 , 0 140701 , 0 140702 , 1 etc . content_copy Timeline link keyboard_arrow_up Start Date - November 1, 2024 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  November 30, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Elizabeth Park. Exploring Mental Health Data. https://kaggle.com/competitions/playground-series-s4e11, 2024. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 7,954 Entrants 2,891 Participants 2,685 Teams 23,174 Submissions Tags Beginner Time Series Analysis Tabular Accuracy Score Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s4e12",
    "discussion_links": [
      "/competitions/playground-series-s4e12/discussion/554328",
      "/competitions/playground-series-s4e12/discussion/554505",
      "/competitions/playground-series-s4e12/discussion/554377"
    ],
    "discussion_texts": [
      "Regression with an Insurance Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 6 months ago Late Submission more_horiz Regression with an Insurance Dataset Playground Series - Season 4, Episode 12 Regression with an Insurance Dataset Overview Data Code Models Discussion Leaderboard Rules Chris Deotte ¬∑ 1st in this Competition  ¬∑ Posted 6 months ago arrow_drop_up 214 more_vert 1st Place - Single Model - Feature Engineering This was a fun competition. In my first two playground competitions (Sept 2024, Nov 2024), feature engineering didn't improve CV nor LB too much, so in those competitions, I spent my time building a large ensemble of diverse models (GBDT, NN, SVM, etc). In this December Kaggle Insurance playground competition, feature engineering helped improve CV score and LB score, so in this competition, I was able to spend time building a single model and engineering features. It was very enjoyable to build a strong single model. My final submission is a single XGBoost model with 611 features! Thank you Kaggle for providing a fun competition that allowed Kagglers to practice categorical feature engineering! Solution Code Notebook I published a simple version of my final submission here which achieves CV = 1.019 . The full version model achieves CV = 1.016 and takes 6 hours on 1xA100 GPU to feature engineer and train. The simple version takes 2 hours on 1xT4 GPU. The simple version uses only 229 out of 611 features . It uses learning_rate = 0.01 versus 0.001 , and n_estimators = 2_000 versus 20_000 , and target_encode(kfold=5) versus kfold=10 . (These simplifications shorten training time but decrease CV score and LB score). Feature Engineering Categorical Columns A common way to improve the CV score and LB score of GBDT (gradient boosted decision trees like XGB, CAT, LGBM) is to provide various encodings for the categorical features. Given a categorical column, the basic encoding is label encoding . More advanced is target encoding mean (i.e. TE ) and count encoding (i.e. CE ). We can even TE median , TE min , TE max , TE nunique . We give the model the original column plus 6 different representations of the original. All 7 of these different encodings are input to the model and give GBDT multiple ways to understand the categorical column and improve CV score and LB score. And of course we can even invent more encodings. Create New Categorical Columns Since encoding categorical columns improves our CV score and LB score, we can create more categorical columns by combining existing categorical columns. Then we can engineer more encodings from the new columns and improve CV score and LB score even more. In my published code here , we create 20 new columns (by combining existing columns). We shared this idea in September's playground competition here . For example we can combine 2 columns together. If we have column Occupation which is categorical with 3 values ['Self-Employed', 'Employed', 'Unemployed'] and we have column Gender which is categorical with 2 values ['Female', 'Male'] . Then we can create a new column by combining these two with train['new'] = train.Occupation +\"_\" + train.Gender . Then the new column is categorical with 6 values ['Self-Employed_Female', 'Self-Employed_Male', 'Employed_Female', 'Employed_Male', 'Unemployed_Female', 'Unemployed_Male'] . We can also combine 3,4,5,6,etc columns together. Treat Numerical Columns as Categorical Another technique that often works is to treat numerical columns as if they are categorical. Then we can encode them with TE mean , TE median , TE min , TE max , TE nunique , and CE . And we can combine numerical columns with other numerical and/or categorical columns and apply TE and CE to the resultant new column. Search Using GPU RAPIDS cuDF-Pandas! In this competition after decomposing the Policy Start Date (into year, month, day, hour, etc), we have 23 original columns. If we create all combinations of 2,3,4,5 and 6 columns we will have 145_000 new columns! This is too many, therefore we need to find and use the best ones. For each combination, we need to create the new column, compute TE and CE which requires nested fold groupby aggreations, then train an XGBoost model. Evaluating a single combination takes time. Using GPU cuDF-Pandas , we can search 10x to 100x more combinations than using CPU, wow! During this competition, I left my computer running day and night in a for-loop running GPU cuDF-Pandas evaluating thousands of random combinations. Whenever feature engineering a combination with TE and CE improves CV score, the code saves it to a list. After running for multiple days, the code found 170 powerful combinations. I publish the best 20 combinations in my solution code here . Learn More About Categorical Encodings If you want to learn more about feature engineering categorical columns, NVIDIA KGMON will present a workshop at NVIDIA's 2025 GTC (i.e. GPU Technology Conference) in San Jose, California on March 17 thru 25th 2025 here . Specifically we will show how to perform TE (target encoding) and CE (count encoding) and explain why it works. And we will show how to incorporate  these features into models to  improve CV score and LB score! 44 21 30 2 1 Please sign in to reply to this topic. comment 94 Comments 21 appreciation  comments Hotness Christopher Akiki Posted 4 months ago ¬∑ 1486th in this Competition arrow_drop_up 1 more_vert If you want to learn more about feature engineering categorical columns, NVIDIA KGMON will present a workshop at NVIDIA's 2025 GTC (i.e. GPU Technology Conference) in San Jose, California on March 17 thru 25th 2025 here. Specifically we will show how to perform TE (target encoding) and CE (count encoding) and explain why it works. And we will show how to incorporate these features into models to improve CV score and LB score! Would love to attend this, when exactly is this taking place @cdeotte ? Edosuyi Enosakhare Posted 4 months ago ¬∑ 1968th in this Competition arrow_drop_up 1 more_vert This is cool I2nfinit3y Posted 5 months ago ¬∑ 306th in this Competition arrow_drop_up 1 more_vert Hey Chris, this article help me a lot about feature engineering! I also wanna know that in the KFold cross-validation,  te( target encode) should be used before dataset split or after that? Is there data leakage if we use target encode for all the train data and then we split it into training dataset and validation dataset ? Mart Preusse Posted 5 months ago arrow_drop_up 3 more_vert Congratulation on winning this playground. I was so occupied with the Santa competition that I realized this just now. I am not surprised. However, I am surprised that you made it with feature engineering and just one model (ok, a very complex one). Thanks for sharing your approach in detail. Chris Deotte Topic Author Posted 5 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Thanks @martinapreusse ! Congratulations winning 1st place in Used Car Playground comp! Sani Kamal Posted 5 months ago arrow_drop_up 1 more_vert Congratulations on winning this playground. diiidar Posted 5 months ago ¬∑ 2290th in this Competition arrow_drop_up 2 more_vert Hello Chris Deotte, your insights and approach in this competition are truly inspiring! I‚Äôm aspiring to become a data scientist and would love to know which online courses you would recommend for mastering this profession. Any guidance would be greatly appreciated. Thanks! Akash Kumar Posted 5 months ago arrow_drop_up 1 more_vert Hi can anyone tell me which encoding method is best for categorical features for this datasets? Chris Deotte Topic Author Posted 5 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert The two most common and most often the best are TE_mean and CE. After that we can add more TE like TE_median, TE_min, TE_max, TE_nunique etc laiflonglearner Posted 6 months ago arrow_drop_up 3 more_vert Awesome work, really appreciate you sharing this! üëè I'm new to Kaggle and still a beginner in data science. Most notebooks I found for learning purposes are usually divided into different sections such as data cleaning, EDA, bivariate analysis, multivariate analysis, data modeling, etc. Is there any reason as to why in Kaggle competitions, we don't need to have our notebook/solution using that commonly used structure? Chris Deotte Topic Author Posted 6 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert A Kaggle notebook has 2 purposes. One purpose is to submit predictions to the leaderboard and the second reason is to make it readable to other Kagglers reading it (which includes adding some EDA and explanations). So some Kagglers just do their EDA and analysis in one notebook (and/or on their local computer), and make their submission is a second notebook. And Kagglers do different things regarding making their notebooks readable to other Kagglers and/or sharing or hiding their EDA. And Kagglers do different things regarding what part of their work they share with other Kagglers (i.e. maybe they just share EDA, or maybe just share final submission model, etc). laiflonglearner Posted 6 months ago arrow_drop_up 0 more_vert That makes a lot of sense, thank you so much! Student Posted 6 months ago ¬∑ 1651st in this Competition arrow_drop_up 3 more_vert Great work! Thank you for sharing. Just one question: how do you prevent over fitting when you test so many combinations of factors (over 145k)? Wouldn't many of the factors improve CV score just by chance? Chris Deotte Topic Author Posted 6 months ago ¬∑ 1st in this Competition arrow_drop_up 6 more_vert The train data has 1.2 million rows. Therefore it is hard to improve CV score by chance (i.e. it is hard to get lucky with 1.2 million OOF predictions all being better by chance). If we have less train data, then we need to be more careful and perhaps only consider combinations of 2 and 3. And not combinations of 2,3,4,5,6. Student Posted 6 months ago ¬∑ 1651st in this Competition arrow_drop_up 4 more_vert In the published code, the function target_encode() implements target/count encoding. In the kfold for-loop, why is mn calculated using train df and not df_tmp? Wouldn't there be a risk of target leakage when the nan rows are filled with mn or when mn is used for smoothing? Chris Deotte Topic Author Posted 6 months ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert Yes, you are correct that using train during each fold iteration is \"leaky\" (but not too much) compared with using the proper df_tmp . The original code from the TE tutorial (where I got this code) uses df_tmp . But the code in RAPIDS cuML Target Encoder uses train . So I tried both (even though the later is a little \"leaky\") Using train and accepting a little leakage improved CV and LB score, so I went with that. I think it is because all the NAN get replaced with the same mean. If we use a different mean per fold, then when we train our XGBoost using this output, than NAN has been transformed into 5 different values, so the GBDT must work harder to make the same prediction for all NAN. But when we use mean from train , then all NAN will get mapped to the same \"leaky\" mean and then GBDT can easily process all NAN similarly. None-the-less, you make a great observation and this is something to explore the ramifications of. Student Posted 6 months ago ¬∑ 1651st in this Competition arrow_drop_up 2 more_vert FWIW, I saw that sklearn has an implementation of target encoder too, and based on this simple example that I have tested, they use df_tmp instead of train. Chris Deotte Topic Author Posted 6 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thank you that is good to know. Masaya Kawamata Posted 6 months ago ¬∑ 6th in this Competition arrow_drop_up 3 more_vert Congratulations on your well-deserved first place! Your discussion posts have been incredibly insightful and valuable, especially for someone like me who is still learning and gaining experience. The clarity and generosity in sharing your approach have made a significant impact on my understanding and growth! Mar√≠lia Prata Posted 6 months ago arrow_drop_up 3 more_vert Congratulations and many top places/solutions on 2025 ChrisD! adityaghai07 Posted 6 months ago ¬∑ 1028th in this Competition arrow_drop_up 1 more_vert Congratulations , the solution looks great and shows your hardwork! HaXL Posted 6 months ago arrow_drop_up 1 more_vert Congrats friend your a inspiration to every kaggler üòÅ Rounak Gera Posted 6 months ago arrow_drop_up 1 more_vert congrats and can you please mention some good resource to learn the ensembling and main techniques required to perform well in kaggle competitions. Thanks in advance Mahmood Yousaf Posted 6 months ago arrow_drop_up 1 more_vert Congratulations and thanks. for sharing! Oluwabori Ige Posted 6 months ago ¬∑ 1009th in this Competition arrow_drop_up 1 more_vert Thank you for sharing @cdeotte . Generally, more research lights are shed on expanding the GPUs which is an intriguing topic. can the possibilities of using LLMs for this type of project be further examined? Maybe not for predictions purposes but for properly aligning the dataset and fitting missing values Metin Amedi Posted 6 months ago arrow_drop_up 1 more_vert Congratulations and thanks. for sharing! Akshat_Sharma_work Posted 6 months ago arrow_drop_up 1 more_vert Woah man!! A really great summary. I just want to know how do I turn it into a fully fledged web application. I heard people use streamlit or flask. I want to know which one is better polarhippo Posted 6 months ago arrow_drop_up 1 more_vert Congratulations and thanks. for sharing! Lucas Morin Posted 6 months ago arrow_drop_up 1 more_vert Hi @cdeotte . It seems you were right about the tips you shared. I love when Kaggle works smoothly like that. Maybe I'll learn to use GPUs for tabular data. Thanks for sharing your work. It's an amazing place to start with. One question: for explainability reasons I am generally stuck with simple (linear / decision tree model). Do you also think GPUs are the way forward ? It will still help with FE. But does it help with mass tuning simple models ? Chris Deotte Topic Author Posted 6 months ago ¬∑ 1st in this Competition arrow_drop_up 5 more_vert GPUs are always the way to go because they are faster. And the key to building the most accurate model is to perform many experiments and try many ideas to improve CV score and LB score. Using GPU let's us perform 10x to 100x more experiments in the same amount of time as CPU. RAPIDS has fast GPU dataframe operations (like Pandas) using cuDF as shown in this notebook (i.e. the notebook you are reading now) and RAPIDS has fast GPU ML models (just like scikit-learn) using cuML. I publish an example of using a linear model cuML here (in previous playground comp) and add non-linear features manually with feature engineering. Sachin Singh Posted 6 months ago arrow_drop_up 1 more_vert i think we should work on this MoriData Posted 6 months ago ¬∑ 611th in this Competition arrow_drop_up 1 more_vert Thanks a lot. it was helpful and I learnt Rashid rk Posted 6 months ago arrow_drop_up 1 more_vert Congratulations, Thnx for sharing üëèüëè Mann Acharya Posted 6 months ago arrow_drop_up 1 more_vert Amazing Win! Thank you for this informative write up! You just opened me up to a new way of learning on kaggle! Too many requests error Too many requests",
      "Regression with an Insurance Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 6 months ago Late Submission more_horiz Regression with an Insurance Dataset Playground Series - Season 4, Episode 12 Regression with an Insurance Dataset Overview Data Code Models Discussion Leaderboard Rules SCRIPTCHEF ¬∑ 2nd in this Competition  ¬∑ Posted 6 months ago arrow_drop_up 25 more_vert Rank 2 approach - brute force ensembling (118 oofs) After participating in these playground series for the last few months, I am glad to have finally snagged a top 3 finish. These few months have been a great learning experience and I am grateful to all members of the community here that have contributed to our shared knowledge or introduced new insightful techniques. I will say that this competition felt a bit more straight-foward than usual as the public lb scores and the cv scores lined up almost perfectly (an x decrease in cv score almost always resulted in around x decrease in public lb score) which meant that it required a bit less finesse in choosing between submissions or identifying where your models may have overfit. This was also reflected in the results; there was almost no shakeup at least in the top 10 compared to some of the past competitions. My overall approach was not novel in any way - it followed the mantra of gathering oof predictions, ensembling, and repeat, which was very similar to here . Additionally, after seeing @cdeotte 's solution, I feel that my solution was very much a 'brute force' approach üòÄ. Nevertheless, there were a few techniques or tools I used that may of use to others which I will explain below. Models and frameworks Automl frameworks are the easiest way to generate many out of fold predictions and I would recommend anyone who hasn't explored the usage of these to try them out in the next competitions. I used quite a few automl frameworks to generate my out of fold predictions, namely autogluon, h2o, FLAML and LAMA. Some notes on these: I found that autogluon generally performed well but was very sensitive to overfitting (a phenomenon called stacked information leakage which is mentioned here by @innixma . This did not occur when using the base features, but when I added a categorical version of healthscore (which was a high cardinality feature) the higher level models experienced this overfitting quite badly. H2o and LAMA performed weaker this time, and the performance didn't reach that of my personally created models even when provided with the same set of features. FLAML pleasantly surprised me with the performance, but this did require longer run times. In the end, to provide FLAML with enough fitting time whilst still only utilising the free kaggle resources, I ended up fitting FLAML five times for each of the five folds concurrently in separate notebooks and combining the predictions afterwards. Pytorch Tabnet, which was not great for predicting, but was very useful for ensembling the oofs at the end Tensorflow Deeptables, the performance was quite weak for this competition. Personal models, nothing special, just implementing various models from different libraries (XGBoost, LGBM, Catboost) to conform to a basic interface which can be found here if interested. Feature engineering Thanks to @backpaker for contributing his set of features. I ran some feature importance and identified a subset of useful features which I experimented using with various models. Also thanks @swagician for breaking down the various feature engineering techniques of backpacker and identifying that the categorisation of health score was crucial and one the main score improvements for me. @backpaker also introduced nonlog catboost oof features (stacking) which provided a slight performance boost, and I tried to further this technique by experimenting by creating another catboost oof which was trained to predict multiclass labels, where the labels correspond to the target feature organised into classes based on the order of the target, i.e., 0 - 9 'Premium Amount' assigned to class 0, 10 - 99 class 1, which also yielded a small performance improvement. I used autofe to generate additional features, which resulted in models which achieved similar performance to the the ones trained on backpacker's features. I didn't combine these feature engineering methods, but rather trained the same models on each of these different features to provide some varied predictions which I thought might fare better when it came to ensembling later. Ensembling I used a Ridge regressor as my main ensembling technique to decide the weights to assign to each oof prediction, but hillclimbers achieved an identical score with less models (73) I also used Tabnet to ensemble the predictions which I then threw in as another oof which did improve the overall ensemble score by providing some non-linear combination of the predictions which proved to be important. Finally I'd like to note that early on in the discussion forum it appeared as most people had ruled out the use of the original dataset, but upon some experimentation, I found that the original dataset improved the performance of tuned models greatly, which reduced my best models at that point from around a cv score of 1.031 to 1.027. Please feel free to ask questions where I have been unclear. Thanks for a great year of data science, I look forward to the next - happy new year! Yuwei (SCRIPTCHEF) Please sign in to reply to this topic. comment 7 Comments 1 appreciation  comment Hotness Sheikh Muhammad Abdullah Posted 5 months ago ¬∑ 82nd in this Competition arrow_drop_up 1 more_vert Congratulations @noodl35 good Solution. sun9sun9 Posted 6 months ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Thank you for sharing your solution.  Happy new year! SCRIPTCHEF Topic Author Posted 6 months ago ¬∑ 2nd in this Competition arrow_drop_up 0 more_vert Awesome 4th place finish! Optimistix Posted 6 months ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert Congrats! Thanks for the shoutout, and also for sharing your OpenFE features - they helped my score as well. I experienced the same leaky results with AG. I've been using FLAML for the past few episodes, but lacked the time and inclination this time - sounds like it would have helped. SCRIPTCHEF Topic Author Posted 6 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Thanks @optimistix ! You've had a very impressive run so far, and I've learnt a lot from you. Looking forward to seeing your solutions this year :) Optimistix Posted 6 months ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert Thanks! My goal is actually to devote less time to TPS episodes this year, but we'll see :-) All the best for the new year! Abdullah TALYAN Posted 5 months ago arrow_drop_up 0 more_vert Congratulations! Thanks for this helpful solution. Appreciation (1) Tanishk Patil Posted 6 months ago arrow_drop_up 1 more_vert Congratulations! Thanks for sharing. Too many requests error Too many requests",
      "Regression with an Insurance Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 6 months ago Late Submission more_horiz Regression with an Insurance Dataset Playground Series - Season 4, Episode 12 Regression with an Insurance Dataset Overview Data Code Models Discussion Leaderboard Rules Optimistix ¬∑ 9th in this Competition  ¬∑ Posted 6 months ago arrow_drop_up 28 more_vert 9th place solution: With a little help from my (Kaggle) friends The month of December got off to a terrible start, as my father passed away. He'd been battling metastatic prostate cancer, so it wasn't unexpected, but it still was (and is) terribly hard. One of the many consequences was that I couldn't/didn't participate properly in this month's TPS episode for the first two weeks. Another side-effect was that my Kaggle login streak ended at 285 days - I'd been looking forward to take it to a year, but don't feel particularly motivated to start over. I'll just login regularly, and forget about such trivialities. Anyway, so by the time I started to truly get into the swing of things with regards to this episode, there were already several great notebooks to peruse, notably those of @martynovandrey & @backpaker , which were highlighted by @cdeotte in a post. @martynovandrey 's excellent notebook in particular provided the first fully reproducible results under 1.40 (even 1.30, for that matter) & as far as I can tell, remained the best scoring fully reproducible notebook (that wasn't just a blend of other submissions) until the very end. I edited these notebooks to save the OOFs, and repeated the same with a few other interesting notebooks. I also borrowed features from various sources - @cdeotte 's insightful post about deriving information from NANs produced a slight bump in performance, as did features from @swagician . By now, I had several CV scores in the 1.028-1.03 range, and public LB scores in the 1.027-1.03 range. A few notebooks had surprisingly low CV scores (< 1.025, with one going to nearly 1.02), but these had LB scores > CV, and had me concerned about overfitting. I also blended with public notebooks a few times, but intended to have at most one such submission among my final 2 (ideally none). Another little bump in my score came courtesy of @noodl35 generously sharing new features generated using OpenFE. Using these ~140 features with Autogluon (AG) immediately resulted in the CV score of AG improving from ~1.045 to ~1.040. Naturally, I added that to my ensemble, and also used these features with several other models. With the last week approaching, I had many ideas for experimentation, but was running low on motivation, and also had a few responsibilities to take care of. An obvious step I didn't get around to was feature selection from the ~170 features. I also couldn't get around to trying out several models I had in mind, or to try a classifier in hopes of the resulting probabilities helping in identifying/predicting outliers, etc. I did, however, play around with various scoring functions and bootstrapping methods in CatBoost, which added to the diversity of the ensembles. I also tried a few different flavors of neural networks, at least some of which helped as well. In the end, my best solo model was a CatBoost with all the features & a 30-fold run (CV: 1.03152, public LB: 1.02935, private LB: 1.03081). In terms of ensemblers, the order of best results had been AG > Ridge > Lasso all along, but at the back of my mind, I knew that Lasso had often produced better scores on the private LB even when Ridge was better on the public LB, so I was wondering which one to choose among my final 2, in case I ended up with a better submission with them than with AG. The reason to anticipate that was the time each method took - Ridge and Lasso took a few minutes, while AG took about an hour, and Hill Climbing was taking several hours once I got beyond a few dozen OOFs. Also my poor MacBook was moaning and groaning under the strain on the last day, and I did end up being an iteration behind with AG. As luck would have it, the final iteration with 81 OOFs saw Lasso outscore Ridge even on the public LB, so my top scoring submission was with Lasso (CV: 1.02729, public LB: 1.02601, private LB: 1.02765), and kept me in the 9th place, where I'd been for a few days. The AG run with the same 81 OOFs finished slightly later (too late to be considered), and scored better (1.02536, , 1.02398, 1.02582). It would have been satisfying to finish with that score, though the rank would have remained the same. Still, I do have the satisfaction of choosing well this time. Many congratulations to @cdeotte for his well-deserved win with an amazing solution, and to everyone who finished well! And thanks to everyone who shared their code and insights, including but not limited to @cdeotte , @martynovandrey , @backpaker , @swagician , @noodl35 , @oscarm524 , @ravaghi , @siukeitin As I look back upon my year on Kaggle, it's been an amazing ride. I started at the very end of February 2024, and then participated more or less fully in the next 10 episodes, with 8 good finishes: Month Rank/Number of participants March 2024 6/2199 April 2024 9/2606 May 2024 7/2788 July 2024 4/2234 August 2024 1/2422 September 2024 3/3066 November 2024 13/2685 December 2024 9/2390 The two remaining months (May and October) were ones where I held the no. 1 for a big chunk of the month (based on the performance on the public Leaderboard, which uses 20% of the test data), but had overfit to such an extent that I fell out of the topmost ranks when the rest of the test data was used for evaluation, though I was still in the top 4% (111/2684) and 8% (299/3858), respectively. Along the way, I also got to pick up some AutoML via the Kaggle AutoML Grand Prix, a competition held on the first of each month from May to September 2024. It's been eye-opening to see the advances in this fast-growing field. All this has also led to some Kaggle swag, and whetted my appetite for bigger challenges. As we start a new year, my goal is to continue participating in TPS, but devote less time, and spend a little more on learning new things, and participating in more prize competitions. I wish you all an amazing 2025, full of new learning and adventures, and may most of your wishes come true (let's leave a few for the years to come üòÄ). Happy New Year, and Happy Kaggling! 1 Please sign in to reply to this topic. comment 9 Comments Hotness Mart Preusse Posted 5 months ago arrow_drop_up 1 more_vert Oh dear. All the best for you. And congratulations on your incredible success in these prior playgrounds. Hope to see you somewhere in a different competition. PawelStepanov Posted 6 months ago ¬∑ 359th in this Competition arrow_drop_up 3 more_vert Congratulations. Sorry for your loss. Happy New Year and all the best. Optimistix Topic Author Posted 6 months ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert Thanks so much - Happy New Year and all the best to you, too! Minyang Han Posted 6 months ago ¬∑ 48th in this Competition arrow_drop_up 1 more_vert CongratulationsÔºÅAll the Best! Optimistix Topic Author Posted 6 months ago ¬∑ 9th in this Competition arrow_drop_up 0 more_vert Thanks, and all the best to you, too! Tanishk Patil Posted 6 months ago arrow_drop_up 1 more_vert Congratulations @optimistix , looking forward to learn from you. Optimistix Topic Author Posted 6 months ago ¬∑ 9th in this Competition arrow_drop_up 0 more_vert Thanks! I might learn from you as well. √ñmer Tanƒ±r Posted 6 months ago ¬∑ 38th in this Competition arrow_drop_up 2 more_vert Congratulations. My goal was to be in the top 10, but I finished 38th :(. Optimistix Topic Author Posted 6 months ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert Thanks! All the best for making the Top 10 soon. Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 4, Episode 12 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Insurance Premium Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 332.74 MB csv Apache 2.0 Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 332.74 MB sample_submission.csv test.csv train.csv 3 files 43 columns  Too many requests",
    "data_description": "Regression with an Insurance Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 6 months ago Late Submission more_horiz Regression with an Insurance Dataset Playground Series - Season 4, Episode 12 Regression with an Insurance Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2024 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: The objectives of this challenge is to predict insurance premiums based on various factors. Start Dec 1, 2024 Close Jan 1, 2025 Evaluation link keyboard_arrow_up Submissions are evaluated using the Root Mean Squared Logarithmic Error (RMSLE) . Submission File For each id row in the test set, you must predict the continuous target Premium Amount . The file should contain a header and have the following format: id ,Premium Amount 1200000 , 1102 . 545 1200001 , 1102 . 545 1200002 , 1102 . 545 etc . content_copy Timeline link keyboard_arrow_up Start Date - December 1, 2024 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  December 31, 2024 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Elizabeth Park. Regression with an Insurance Dataset. https://kaggle.com/competitions/playground-series-s4e12, 2024. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 7,737 Entrants 2,512 Participants 2,390 Teams 17,851 Submissions Tags Beginner Tabular Insurance Regression Root Mean Squared Logarithmic Error Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s5e1",
    "discussion_links": [
      "/competitions/playground-series-s5e1/discussion/560629",
      "/competitions/playground-series-s5e1/discussion/560549",
      "/competitions/playground-series-s5e1/discussion/560535",
      "/competitions/playground-series-s5e1/discussion/560692",
      "/competitions/playground-series-s5e1/discussion/560554",
      "/competitions/playground-series-s5e1/discussion/560653",
      "/competitions/playground-series-s5e1/discussion/560569"
    ],
    "discussion_texts": [
      "Forecasting Sticker Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 months ago Late Submission more_horiz Forecasting Sticker Sales Playground Series - Season 5, Episode 1 Forecasting Sticker Sales Overview Data Code Models Discussion Leaderboard Rules George Koussa ¬∑ 1st in this Competition  ¬∑ Posted 5 months ago arrow_drop_up 27 more_vert 1st place solution Big thanks to @kdmitrie for his many insightful contributions! My solution is largely based on his starter model, which you can check out here . I also took some inspiration for handling holidays from this solution to a previous competition. You can find my first-place solution here . There may be some subtleties to it that I don't fully understand myself, so if you spot anything interesting, I‚Äôd love to hear your thoughts! 8 Please sign in to reply to this topic. comment 15 Comments 1 appreciation  comment Hotness Dang Nguyen Le Posted 3 months ago arrow_drop_up 2 more_vert Great execution! Every element is on point. Piyush007Panchariya Posted 5 months ago ¬∑ 570th in this Competition arrow_drop_up 1 more_vert Congratulations, Great Work Mohamed Drabo Posted 5 months ago ¬∑ 380th in this Competition arrow_drop_up 1 more_vert Hello thanks to share your code, I've learned lot of things. Please could check my code for me : https://www.kaggle.com/code/mohameddrabo/forecasting-sticker-sales-cat-lagmb/notebook yunsuxiaozi Posted 5 months ago ¬∑ 295th in this Competition arrow_drop_up 1 more_vert I have carefully read your notebook and have the following questions: 1.Should time series have offline CV? What I mean is to keep the last 3 years of the training set as the validation set, although the final mape is also calculated, I don't think this is the true CV. 2.I see that you are decomposing each variable and building a separate model. If the order of these decompositions is changed, will the result be different? How to choose the order of decomposition? 3.If the products of each store in a dataset are different, for example, store A has products a and c, and store B has products b and c, how can this solution be extended? 4.Improvement suggestion: The number of holiday days for festivals may vary, and weighting can be considered for festivals, such as a 2-day festival being 1 and  1.5  for a 3-day festival. George Koussa Topic Author Posted 5 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert 1 You're right‚Äîvalidating results only using the data that was used to build the model is not ideal. Proper CV is definitely best practice, though I never got around to adding it in my notebook. If I were to add CV, I would use a 5-fold scheme where the model is trained on the first 2, 3, 4, 5, and 6 years and evaluated on the 3rd, 4th, 5th, 6th, and 7th year, respectively. 2 Actually, @kdmitrie revealed in his discussion post here that it is better to evaluate the factors simultaneously rather than sequentially. Doing so would also mean you wouldn't have to worry about ordering! That said, I do believe that the order matters more for some factors than others. For example, using relative sales to calculate store factor and product factor helps normalize variations caused by other factors. Since external influences affect all products/stores similarly, shifting the order of these two factors likely wouldn‚Äôt change the results much. On the other hand, it is not as easy to isolate the holiday & New Year's factors, particularly from variations in sales over time like the trend factor I mention in my solution. That's why I chose to evaluate those two factors last, after accounting for the effects of the other factors. 3 If each store carries a different subset of products (e.g., store A has products {a, c} while store B has {b, c}), it becomes more difficult to isolate the product effect from the store effect. Some stores might sell unique products, while others may overlap, creating dependencies between the two factors. To account for this, I would calculate the product factor separately for each store. This way, the model captures how each product behaves within a specific store rather than assuming a uniform effect across all stores. 4 That's an interesting suggestion. I'm not too familiar with the Python holiday package, but in the case that it doesn't treat the separate days of a festival as holidays themselves, that could improve performance. I know from a few other discussions that the holiday library misses some holidays that do influence sales. @cabaxiom , for instance, points out here that certain Kenyan holidays were absent from the package. My current treatment of holidays was quite basic, and there‚Äôs certainly room for improvement. A more refined approach would involve identifying and removing holidays that do not significantly impact sales while also adding missing holidays that do. yunsuxiaozi Posted 5 months ago ¬∑ 295th in this Competition arrow_drop_up 0 more_vert thanks for your reply and congratulations. Less submissions Posted 5 months ago ¬∑ 158th in this Competition arrow_drop_up 2 more_vert Congratulations winning 1st place. What a great jobÔºÅ Konstantin Dmitriev Posted 5 months ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert My congratulations for winning the first place! I'm very happy that my work was useful! neednot_toplay Posted 5 months ago ¬∑ 387th in this Competition arrow_drop_up 0 more_vert you have been first throughout the competition @kdmitrie congrats. Chris Deotte Posted 5 months ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Amazing job George. Congratulations winning 1st place solo! neednot_toplay Posted 5 months ago ¬∑ 387th in this Competition arrow_drop_up 2 more_vert looks like a lot of effort for this @georgekoussa I have seen your notebook impressive work!! Ashish Khare Posted 4 months ago arrow_drop_up 0 more_vert What are these wav features? Can you throw some light on it? üôè Joshua Immanuel Posted 5 months ago ¬∑ 1104th in this Competition arrow_drop_up 0 more_vert Congratulations, Great Work dijvivek Posted 5 months ago ¬∑ 876th in this Competition arrow_drop_up 0 more_vert i Need help in 1 competition , can anyone help me in this , i'm new on kaggle neednot_toplay Posted 4 months ago ¬∑ 387th in this Competition arrow_drop_up 0 more_vert sure I will help you. Appreciation (1) Minhaz07 Posted 2 months ago arrow_drop_up 0 more_vert Great Work Too many requests error Too many requests",
      "Forecasting Sticker Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 months ago Late Submission more_horiz Forecasting Sticker Sales Playground Series - Season 5, Episode 1 Forecasting Sticker Sales Overview Data Code Models Discussion Leaderboard Rules Chris Deotte ¬∑ 2nd in this Competition  ¬∑ Posted 5 months ago arrow_drop_up 80 more_vert 2nd Place - Stacking Transformer and Linear Regression Forecasting Challenge Wow, i'm excited to win 2nd place and I feel lucky. Forecasting competitions are hard because choosing the right multipliers for the future years (2017, 2018, 2019) has a greater effect on your final LB score and rank then having a model which predicts days accurately (i.e. what happens on Monday versus Sunday, or holiday versus non-holiday). In this comp, the training data can help us predict days , but what occurs in future years requires guessing and/or assumptions. In this competition we are given sales data for years 2010, 2011, 2012, 2013, 2014, 2015, 2016 and we must predict 2017, 2018, 2019. In each future year, we must predict 90 numbers per day for each country, store, product combination. After we build a linear regression model (with sinusoidal engineered features) and include the effects of: GDP, store, product, country, day of week, day of year, there is still an unaccounted for trend (how data changes per year) in the data (pictured below). Below is an image of prediction error from top public notebooks. The y axis is the multiplier we need to multiply our predictions (i.e. percentage error) to match ground truth. We see this error ranges between plus and minus 6%. So what do we do about the future? We can multiple all future predictions by the last known percentage error which is m=1.06 (like popular public notebooks). We can use no multiplier, m=1.00 . We can use linear multiplier that increases or decreases as we move forward in time m = 1.06 + slope * (year - 2017) for some slope. These different options are pictured above with dotted lines. There are many choices for future multipliers. This is why forecasting is so difficult. We cannot say with any certainty which future trend is correct (without having more information from outside the train data). So we just need to guess. For my final two submissions, I chose constant 1.06 (green dotted line) and mild linear up (orange dotted line) ü§û Transformer Only - Public LB = 0.04867, Private LB = 0.04967 (59th Place) Using only my public starter here , we can achieve public LB = 0.04867 , private LB = 0.04967 and 59th place private with the following changes: train 1 model on all 5 products (for 15 epochs cosine schedule) add 30 boolean features for 30 holidays use first predictions (of 2017,2018) as pseudo label to train second predictions use second predictions (of 2017,2018,2019) as pseudo label to train third predictions use the median of 5 models trained with different seeds (for 1st, 2nd, 3rd predictions) submit the 3rd predictions use no multiplier. Transformer determines what to do about future Note that we use 2 rounds of pseudo labeling (which is in addition to autoregression). For more details about pseudo labeling reading comment below here . We also ensemble 5 copies of the model with itself trained on different seeds. Both these techniques improve accuracy and help us get good predictions far into the future at year 2019 private test data. Linear Regression Only - Public LB = 0.04733, Private LB = 0.04650 (6th Place) Starting with Konstantin's great public notebook (model 1) here , we can achieve public LB = 0.04733 , private LB = 0.04650 , and 6th place private here by adding the effect of holidays. (See all holidays here ). add country holidays keep multiplier m=1.06 Below is an example of how to locate and add holidays for a specific country. Holidays can change day each year, so we perform EDA and put black vertical lines before and after a holiday (for a specific country). Then we look at each year 2010 thru 2016 in a specific country and see if the sales consistently are raised or lowered. If so, we add this holiday to our model. For Singapore we observe that sales are raised for the following 7 holidays each year: Chinese New Year, Easter, Vesak Day, National Day, Eid al-Fitr, Deepavali, Eid al-Adha. These 7 holidays are shown for years 2014, 2015, and 2016 below. For more examples, see my notebook here . For linear regression model, we boost these windows of time. For transformer model, we add boolean features so the model can find and predict these holidays. Stacking - Public LB = 0.04526, Private LB = 0.04498  (2nd Place) By stacking my transformer over linear regression to predict the residuals (error), we can achieve public LB = 0.04526 , private LB = 0.04498 , and 2nd place private ! üéâ (Stacking versus Ensemble is explained in detail here ) The transformer learns patterns that the linear regression model does not learn. The most efficient way to use both is to train the transformer on the prediction error of the linear regression model. So, first we use the linear regression model to predict the train data Jan 2010 thru Dec 2016. We then subtract the predictions from the ground truth to get the prediction error. Next we train the transformer to learn and predict this error (i.e we train transformer with target = truth minus prediction for Jan 2010 thru Dec 2016). Finally we submit the sum of the two models' predictions. 6 12 5 Please sign in to reply to this topic. comment 37 Comments 7 appreciation  comments Hotness Dang Nguyen Le Posted 3 months ago arrow_drop_up 2 more_vert The concept is great and well-presented. narsil (jobs-in-data.com) Posted 5 months ago ¬∑ 965th in this Competition arrow_drop_up 3 more_vert Thanks Chris! I always go through your writeups with my students. A great learning resource! Johannes Heller Posted 5 months ago ¬∑ 4th in this Competition arrow_drop_up 3 more_vert @cdeotte Congratulations on the awesome performance! I was stuck at around 0.044 public LB with a good linear model for day-in-year-ratios and no ideas left but probing public lb with things like country factors (hoping they would work for 2018, 2019, too) in the final weeks. Your approach to predict residuals with a totally different architecture (that is able to find totally different patterns) is fantastic! I didn't even try NN this time, not expecting it could work with this kind of data. Chris Deotte Topic Author Posted 5 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Thanks Johannes. Congratulations achieving solo 4th place! Optimistix Posted 5 months ago ¬∑ 232nd in this Competition arrow_drop_up 3 more_vert Congrats on another great performance, and thanks for the informative notebooks and posts! Hope you continue to participate in the playground episodes, it's great to have you around. Chris Deotte Topic Author Posted 5 months ago ¬∑ 2nd in this Competition arrow_drop_up 4 more_vert Thanks Optimistix! I was scared of a big shakeup here. With time series forecasting we never now what happens in future years. We make guesses and assumptions and hope it works, but sales could have been anything in years 2018 and 2019 (private LB). We're lucky that they were the same multiplier ( m=1.06 ) as 2017 (public LB). Konstantin Dmitriev Posted 5 months ago ¬∑ 3rd in this Competition arrow_drop_up 4 more_vert @cdeotte , I was very impressed with your transformer-based solution and the speed of your LB score decreasing in the competition! That's the power of a true Grandmaster! Rahul Posted 5 months ago arrow_drop_up 1 more_vert Very good üëçüèª Jayant Khandelwal Posted 5 months ago arrow_drop_up 1 more_vert Congratulations, I have learned a lot from your notebooks. Less submissions Posted 5 months ago ¬∑ 158th in this Competition arrow_drop_up 1 more_vert Congratulations, I have learned a lot from your notebooks. Chloris Posted 5 months ago arrow_drop_up 1 more_vert very helpful and nice Olabode James Posted 5 months ago arrow_drop_up 1 more_vert Excellent work @cdeotte , as the idea of training the transformer on the prediction errors of the linear regression, is quite ingenious and I  believe this can make for a new class of machine learning models for temporal data forecasting problems. AkshatBhatnagar29 Posted 5 months ago arrow_drop_up 1 more_vert Very NIce. Goopd Job. Shreyans Solanki Posted 5 months ago ¬∑ 1477th in this Competition arrow_drop_up 2 more_vert The best place to learn new things, here I was just handling the date column and using DL models, i mostly cant understand a thing from this discussion forum but i will try to look it up and grasp all this! Chagin Posted 5 months ago ¬∑ 738th in this Competition arrow_drop_up 1 more_vert That's a very insightful explanation! Thank you for sharing such amazing learning resources! And congratulations üéâ @cdeotte HyperNeonByte Posted 5 months ago ¬∑ 149th in this Competition arrow_drop_up 1 more_vert Hi Chris, thank you for sharing your work and all the other helpful notebooks, I learned a lot following your notebooks. Could you explain more about the following part? Is this different from 'auto regression' that you used in the Transfromer notebook? Thanks! use first predictions (of 2017,2018) as pseudo label to train second predictions use second predictions (of 2017,2018,2019) as pseudo label to train third predictions use the median of 5 models trained with different seeds (for 1st, 2nd, 3rd predictions) Chris Deotte Topic Author Posted 5 months ago ¬∑ 2nd in this Competition arrow_drop_up 3 more_vert Yes, this is pseudo labeling (which is an advanced technique and different than autoregression). Let me be more specific: We train 1 model on all train data Jan 1 2010 thru Dec 31 2016 (all 5 products) We use autoregression and predict 2017, 2018, 2019. We do this 5 times and take the median of the 5 predictions. (note if we use the public notebook we would run it 25 times. We would run PROD=0 for 5 times and take median. Then run PROD=1 for 5 times and take median, etc etc) We now concatenate train.csv with the (median) predictions for 2017 and 2018 (from above) We train 1 model on Jan 1 2010 thru Dec 31 2018 data (the last 2 years are pseudo label) We use autoregression and predict 2017, 2018, 2019. We do this 5 times and take the median of the 5 predictions. We now concatenate train.csv with the predictions for 2017, 2018, 2019 (above) We train 1 model on Jan 1 2010 thru Dec 31 2019 data (the last 3 years are pseudo label) We use autoregression and predict 2017, 2018, 2019. We do this 5 times and take the median of the 5 predictions. So, we did 2 rounds of pseudo labeling . We do all of the above to make the transformer more accurate. This is very important in this competition because we are using autoregression 35 times into the future. With autoregression, the error gets worse each time we move forward in time. My public notebook does not use pseudo label and does not use median of 5 submission.csv . If you do EDA on my public notebook predictions, we see that the predictions for 2019 look worse than 2018 which look worse than 2017. And my public notebook has public LB = 0.0526 and private LB = 0.0603 . Note there is a 0.00800 gap between public and private. When using the procedure above, when we do EDA on the predictions. The predictions for 2019 look just as good as 2018 which look just as good as 2017. The resultant predictions have public LB = 0.0486 and private 0.0496 . Note there is only a 0.00100 gap between public and private! HyperNeonByte Posted 5 months ago ¬∑ 149th in this Competition arrow_drop_up 0 more_vert Thank you Chris for the detailed walkthrough and for sharing your observation about the gap between public LB and private, it is very interesting and insightful! Out of curiosity, what's your experience with the effectiveness of pseudo labeling across different tasks? Is the noise-to-information ratio of the task itself an influential factor in its performance? Are there other factors as well? My understanding of how this technique works is that it improves predictions when the pseudo labels are close enough to the true labels, serving as a good proxy for them during training. By adding several rounds of pseudo labeling, we effectively augment the training dataset with additional data covering 2017‚Äì2019 that comes with reasonably good labels. I noticed that in your original transformer notebook, the correlation between the true target and the CV predictions is over 0.9, which is quite high! But when dealing with a very noisy dataset, say where the correlation between the target and the pseudo labels is below 0.3, would you expect pseudo labeling to be less effective? And are there other reasons why pseudo labeling works well here? I look forward to hearing your thoughts, and many thanks again! ƒ∞smail Uƒüuz Posted 5 months ago ¬∑ 2406th in this Competition arrow_drop_up 1 more_vert It is an very impressive work, Thanks for sharing. Konstantin's notebook is an great source of study √ñzg√ºr Deniz √áelik Posted 5 months ago ¬∑ 307th in this Competition arrow_drop_up 1 more_vert Thanks for the solution! I liked your WaveNet notebook. it‚Äôs interesting, but I found it a bit overwhelming to read. Could you create a simpler version of it? b≈Ça≈ºej makowski Posted 5 months ago ¬∑ 1266th in this Competition arrow_drop_up 1 more_vert Amazing learning resource! Denis Kurovskii Posted 5 months ago ¬∑ 855th in this Competition arrow_drop_up 1 more_vert Good Work! Congratulations! RAHUL ARYA Posted 5 months ago arrow_drop_up 1 more_vert why not use GDP data with holidays ? yihao zhu Posted 5 months ago ¬∑ 182nd in this Competition arrow_drop_up 1 more_vert Your work is excellent and thank you for your detailed sharing. I can learn a lot of knowledge from it, which is of great help to me. ShivaneK Posted 5 months ago arrow_drop_up 2 more_vert very helpful and nice delai50 Posted 5 months ago arrow_drop_up 2 more_vert Congratulations and thanks for sharing as always. A couple of questions here: How did you come up with those specific pseudolabeling phases? I mean, for example you predicted 2017 and 2018 at first and secondly you predicted 2017, 2018, 2019. Why not for example predict 2017 first, then 2018, then 2019? This is a Grandmaster superpower I guess. Regarding the stacking, you trained the LR model and predict the entire train period to calculate the error. Then you used those errors to train the Transformer so you did the pseudolabeling over the errors. Is that correct? The time series of the multiplier was calculated at day level? OFF TOPIC: I had a similar problem at work: we had to predict sales of combinations of products and warehouses. We developed several types of models (e.g. one model for all wares, 1 model per product-warehouse, etc). For each combination of product-warehouse we ended up selecting the best model according to the test set and I am pretty sure that doing such kind of decision overfits a lot to the test set. What is your view on this? Am I correct? (Actually more than 2 questions) Chris Deotte Topic Author Posted 5 months ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Hi. My plan was to do one round of pseudo label using all predictions of 2017, 2018, 2019. However when i looked at EDA, the transformer predictions did not look perfect for 2019. So I used 2017 and 2018 for pseudo. Using pseudo is a big benefit because the test data gives us 40% more train data and NN love lots of data. After doing one round, the new predictions for 2019 looked great. So then i decided to do a second round using all 2017 2018 2019 Yes. First we train transformer on residuals of LR. Then our predictions for 2017 and 2018 are residuals. Then we concatenate residuals 2010 thru 2016 with prediction residuals 2017 and 2018. Then we train on all that for round 1. Yes. The plot in my discussion post is the multiplier per day. Multiplier = prediction_per_day / ground_truth_per_day . I was pretty sure that do this overfits a lot to the test set, but I had a tough time trying to explain my colleagues why. What is your view on this? Am I correct? (4) Maybe. What you describe does \"overfit\" to the test dataset. However if the test dataset is highly predictive of the future, then it will also \"overfit\" to the future and will work well. The problem in forecasting is that validation and test sets are often not very predictive of future performance. Therefore we need to be extremely cautious and not make too many decisions using validation/test data. (Or use it selective for certainly decisions). (For example, we can determine what properties of validation/test data will transfer to future using common sense or rigorous tests. Like daily sales of sunday being more than monday will mostly likely transfer and we can use decisions based on this in valid/test data. But other trends and behaviors do not transfer so we cannot trust valid/test data). Sheikh Muhammad Abdullah Posted 5 months ago ¬∑ 236th in this Competition arrow_drop_up 2 more_vert @cdeotte Good Work. Congratulations. üòç Any tips how to find \"we are overfitting or not\". I didn't spend much time on this because of less Knowledge in Time series problem, but my base models achieve a good score in Public LB but score is worse in private. On the other hand,  one Of my Ensemble Model Score is 0.04848 which is around 40 to 45 on Private LB. But this Model Score on Public is Not Much Good. Chris Deotte Topic Author Posted 5 months ago ¬∑ 2nd in this Competition arrow_drop_up 5 more_vert It is very hard to prevent overfitting in time series. To be honest it usually comes down to luck regarding what we guess or assume will happen in future years. We can use local validation to evaluate if our model predicts day of week (Monday versus Sunday) and holidays well. And other patterns within a single year. But we can't really evaluate whether our model will be able to predict the total sales for a given year. The sales in years 2018 and 2019 (private LB) could have been 5% more or 5% less. And then final scores would go up and down 0.05000 . And then the Kaggler who predicted the correct multiplier for 2018 and 2019 would win. In this comp it was lucky that the same multiplier m=1.06 which worked well in 2017 also worked best for 2018 and 2019. Samith Chimminiyan Posted 5 months ago ¬∑ 332nd in this Competition arrow_drop_up 2 more_vert @cdeotte Excellent Work !!! Lots of knowledge shared. Thank you !!! This comment has been deleted. Too many requests error Too many requests",
      "Forecasting Sticker Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 months ago Late Submission more_horiz Forecasting Sticker Sales Playground Series - Season 5, Episode 1 Forecasting Sticker Sales Overview Data Code Models Discussion Leaderboard Rules Konstantin Dmitriev ¬∑ 3rd in this Competition  ¬∑ Posted 5 months ago arrow_drop_up 31 more_vert 3rd place solution First of all, let me thank the organizers of the competition and all the participants for interesting discussions! In this competition, my solution was based on consecutive building of a multiplicative model, its fine-tuning and cross-validation. I was lucky to get the 1st LB position very soon, and my efforts were focused on how to prevent overfitting. You can check my final solution here 1. The basic model I have made the basic model public , and it reached the public score of about 0.05, that outperformed most of public notebooks without ensembling. The model is fully described in the notebook. In short, it incorporates the following factors: GDP per capita; Store ratio; Country ratio; Periodic product factor; Day-of-week factor; Periodic day of year factor; Periodic date factor; Country-dependent day-of-year factor. 2. Holidays The basic model takes the holidays into account by averaging the sales across the years. This is not correct since the exact date of many holidays may differ year to year. To overcome this difficulty, holidays library was used. However, although this library is cool, it is not perfect. It doesn‚Äôt include several holidays in Kenya (Festival of breaking the fast, Moi Day, Feast of the Sacrifice); uses different names for the same holiday (Kenyatta Day and Mashujaa Day); and needs a care when two holidays are in the same day. Moreover, it outputs ‚Äònormal‚Äô as well as ‚Äòobserved‚Äô holidays. My experiments showed that it is better to consider these two types of holidays as separate holidays. As a result of using this library, each holiday in each country is represented a separate column in the dataframe, where ones are put in the holiday dates and zeros are put elsewhere. It was noticed , that the ‚Äòholiday effect‚Äô takes place a few days later than the actual holiday. My experiments showed, that it could be described with a simple Gaussian curve: H ( t ) = exp ( ‚àí ( d ‚àí d h ‚àí d 0 ) 2 2 œÉ 2 0 ) , where d is the current date; d h is the date of the holiday; d 0 = 4.5 is a response shift, and œÉ 0 = 2 is the width of the Gaussian curve. The whole response is calculated as a convolution of H ( t ) with the data from the corresponding holiday column and its multiplication by amplitude factor, that needs to be determined. 3. Strange 1.06 multiplicator The submission score gets sufficiently better being multiplied by a factor of about 1.06, credit to @cabaxiom for discovering this. It could be improved even further if the predictions for Kenya are also multiplied by about 1.03. This was very strange for me since it was hard to describe this factor, and using it sufficiently decreased the score during CV. I think this factor and its understanding is the key to the competition. Finally, after accounting for all the factors I discovered that the sales depend on time in a complex way, and we need to make a prediction (see the figure below). My efforts to explain it by some kind of economic indicators or by features already existent in the dataset failed. The pattern is uncertain, because it is not linear due to a wave in 2010-2012. Although the linear continuation over 2017 explains the 1.06 and 1.03 factors quite precise, we can‚Äôt be sure that this linear growth exists in 2018-2019. So, I decided to make my first final submission under the hypothesis of linear trend, while the second assumes it to be constant after 2018-01-01. Looking at the private score, it was a good decision: the second submission achieves much better score! To describe the mentioned trend, I used a ReLU function: trend ( d ) = 1 + s ‚ãÖ ReLU ( d ‚àí d 1 ) , where s and d 1 are the slope and shift parameters, correspondingly. To deal with the differences between Kenya and other countries, I trained an additional model on the Kenya‚Äôs data only. 4. Parameters optimization In the basic model, the parameters were optimized using sequential regressions. This seems to be not optimal, and it is better to optimize them simultaneously. Moreover, the MAPE metric used in the competition differs from the MSE. So, instead of this, I‚Äôve built a predict function: predict ( ÀÜ X , ‚Üí Œ± ) = Œ± 0 N ‚àè n = 1 ( ÀÜ 1 + ÀÜ X n ‚Üí Œ± n ) , where the whole dataframe ÀÜ X is divided into N sets of columns ÀÜ X n : ÀÜ X = [ ÀÜ X 1 , ÀÜ X 2 , ‚Ä¶ , ÀÜ X N ] , and ÀÜ 1 is a column-vector of ones. At the beginning, all the parameters are initialized with zeros or something reasonable. Then the training is performed using one or another set of columns. Finally, the training is performed using all columns. This is done for all countries and for Kenya only. I used simple but powerful minimize from scipy.optimize to perform the optimization. This allowed me to quickly experiment with many factors and functional dependencies without the need to create the entire infrastructure necessary for DL frameworks. The 6-fold CV was performed on the per-year basis, and the final model was trained on the whole data. 5. Drop the data I have discovered that if we drop the 2010-2012 data for Kenya, both local CV and public LB score improve significantly. So I did this in my final model. Unfortunately, it was a good idea for improving public score, but it was bad for the private score . The notebook without data drop achieves the private score of 0.04369, that corresponds to the 1st place. 6. What didn‚Äôt work for me in this competition AutoML; DL (I can‚Äôt wait to see Chris‚Äô solution!); More complex holiday response functions as well as efforts to change the d 0 and œÉ 0 ; Additional economic factors except for GDP per capita; ARIMA-like models. 7. Human or AI? Currently, I don't know, what did @georgekoussa used in the competition. However, @cdeotte was so kind to share his DL notebooks. For me, it was particularly interesting, which solution would be better in the competition of such a type, that a human can perform the feature extraction 'by hand'. Okay, we can conclude, that both solutions are approximately at the same level. However, I believe, DL approach would be much better in more complicated tasks. You can check my final solution here 5 Please sign in to reply to this topic. comment 10 Comments Hotness Dang Nguyen Le Posted 3 months ago arrow_drop_up 3 more_vert Such great enthusiasm‚ÄîI can feel the passion! Ravi Ramakrishnan Posted 5 months ago ¬∑ 175th in this Competition arrow_drop_up 3 more_vert @kdmitrie thanks for the writeup! I think you scored well in the previous version of the challenge as well (I think you were in the top 5 I suppose) with a linear model and Fourier analysis. I agree that deep learning is not at all needed here, perhaps that was a reinforcement for me in this challenge! Konstantin Dmitriev Topic Author Posted 5 months ago ¬∑ 3rd in this Competition arrow_drop_up 2 more_vert Yes, I was 5th there. I used multipliers to get higher LB position, and this strategy resulted in the worse private score. So, in the current competition, I decided to exclude any kind of multipliers. However, my submissions with multipliers have much better score! Optimistix Posted 5 months ago ¬∑ 232nd in this Competition arrow_drop_up 3 more_vert Congratulations! You made excellent contributions throughout the month, and more than deserve this high rank. I learned a lot from your notebooks and posts in my first time-series competition. I considered simply submitting one of your results, which would have landed me in the 15-20 range, but decided not to in the end. All the best for the next competition(s)! prosto kostiks<3 Posted 5 months ago ¬∑ 2055th in this Competition arrow_drop_up 1 more_vert –ü–æ–∑–¥—Ä–∞–≤–ª—è—é –≤–∞—Å, —É–¥–∞—á–∏! Shailja7901B Posted 5 months ago ¬∑ 1907th in this Competition arrow_drop_up 1 more_vert Thank you so much for sharing! This is my first competition as a novice data scientist this was very helpful, it gave me an insight about how one must approach a problem. Chris Deotte Posted 5 months ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Congratulations @kdmitrie achieving solo 3rd place! It is a huge accomplishment to both share so much during the entire competition and to still finish in the top 3. That's great! I read and appreciate everything you shared. The notebook without data drop achieves the private score of 0.04369, that corresponds to the 1st place. That's awesome, your mathematical models are very accurate. I'm impressed. I enjoy the systematic approach of your multiplicative model (that you shared) where we continually observe error and create another factor to explain the error. I used this as the basis of my final solution. It is very user friendly to work with and add to. In the basic model, the parameters were optimized using sequential regressions. This seems to be not optimal, and it is better to optimize them simultaneously. That's a great insight and I will explore this in the future. Konstantin Dmitriev Topic Author Posted 5 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thank you very much for your kind words, @cdeotte ! They give me inspiration to work more and do my best. At first, I built my model by sequentially trying to explain its error, indeed. However, some adaptations were needed at every step: dropping the periods that correspond to holidays, considering specific products or stores, etc. Without these adaptations, the currently studied factor can be influenced by the factors that are not included in the model yet. For example, holiday peaks may shift the actual day-of-week ratio. So, I decided to prepare a model in the form of a specific function with parameters. Surprisingly for me, minimize from scipy worked fast and was able to find impressively good solutions without the need to connect GPUs. Johannes Heller Posted 5 months ago ¬∑ 4th in this Competition arrow_drop_up 2 more_vert Congratulations and thanks for your public notebooks! Konstantin Dmitriev Topic Author Posted 5 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thank you! When we were 'fighting for the first place', it was really exciting for me! Everyday I thought, what to invent more to improve my score. You never gave me a single day to rest! Too many requests error Too many requests",
      "Forecasting Sticker Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 months ago Late Submission more_horiz Forecasting Sticker Sales Playground Series - Season 5, Episode 1 Forecasting Sticker Sales Overview Data Code Models Discussion Leaderboard Rules Johannes Heller ¬∑ 4th in this Competition  ¬∑ Posted 5 months ago arrow_drop_up 10 more_vert 4th place solution Congratulations @georgekoussa for his winning notebook! And congratulations to @kdmitrie for a great 3rd place. Whenever I was in first place for a short time, he quickly put me in second place a couple of hours later üòâ This forecasting competition with an artificial dataset was kind of like a puzzle with it's various ratios to be computed. The total sales per year, however, remained a mistery to me. So I'm glad I ended on fourth place, just like on public LB. I expected a greater shakeup. In a nutshell, this is my solution: See the yearly totals as given (I used a mean from earlier predictions and some public notebooks). All the following steps refer to ratios. Use World Bank GDP/capita figures per year for country ratios. Since there were some major discrepancies to the country ratios in the training data, I used a simple scipy linear regresssion to make the ratios fit better, especially for Kenya. Use constant store ratios. I could not identify any seasonal or whatsoever patterns. For each country, separately: Compute the mean product ratios per day, separately for even and odd years (looking at the sincos curves there's an obvious two-year-pattern), then apply some FFT smoothing (thanks @kdmitrie who discussed it somewhere) For the day-of-year-ratios I did linear regression with Sklearn's Ridge and HuberRegressor (not much of a difference). I did some extensive feature engineering. Besides sinus-cosinus features and day-of-week, I tried lots of country-specific holidays, both movable and immovable. The peaks in sale figures usually occurred some days after the holidays. Since Ridge runs pretty fast, I tried to identify the very days that worked per country. Interestingly, some country/holiday combinations had an effect even though the holidays library didn't have them. Finally, compute the absolute figures from the ratios and yearly totals. Since the country ratios were somewhat flawed, I probed some factors (like Kenya * 1.012) against the public LB once I had no good ideas left to submit. But it did not make much of a difference and posed a risk with relation to private LB. My best notebook reached 0.04385 on public lb and 0.4560 on private lb, both earning me fourth place. Please sign in to reply to this topic. comment 3 Comments Hotness Dang Nguyen Le Posted 3 months ago arrow_drop_up 3 more_vert Your commitment is great and truly admirable. Rakhi Thakur Posted 5 months ago arrow_drop_up 0 more_vert keep it up‚Ä¶ This comment has been deleted. Too many requests error Too many requests",
      "Forecasting Sticker Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 months ago Late Submission more_horiz Forecasting Sticker Sales Playground Series - Season 5, Episode 1 Forecasting Sticker Sales Overview Data Code Models Discussion Leaderboard Rules Cabaxiom ¬∑ 5th in this Competition  ¬∑ Posted 5 months ago arrow_drop_up 13 more_vert 5th place solution My solution focused on time series decomposition and was very similar to @kdmitrie 's excellent approach . 1. Decomposition Decompose the time series to remove the effect of: Day-of-week Country (GDP) Store Product Day-of-year 2. Forecasting After decomposition, I attempted to forecast the remaining curve. This led to a key decision: should the forecast follow the upward trend or remain constant. To test both possibilities, I submitted one version allowing the trend to continue and another assuming a constant trajectory, similar to 2016, with the constant trajectory performing much better. I think this decision probably explains some of the LB shakeup. 3. Holidays Incorporating holiday effects had a substantial improvement on leaderboard scores. I used the median of the previous year‚Äôs normalised holiday values to estimate the adjustments. I accounted for: Holiday delays Holidays that do not occur on the same day every year Three additional holidays in Kenya that were missing from the holiday package There's probably still some room for improvement here! Please sign in to reply to this topic. comment 7 Comments 1 appreciation  comment Hotness Dang Nguyen Le Posted 3 months ago arrow_drop_up 2 more_vert Simply great‚Äîthis is exactly what I was looking for! Konstantin Dmitriev Posted 5 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert @cabaxiom , my thanks and congratulations to you! My work is inspired by your solution to a 2022-year competition . That's why it is similar. Thank you once more, for sharing these ideas! Also, I want to emphasize the importance of 'key decision' about main trend continuation, that you have mentioned. I have faced the same problem and decided to submit both linear and constant forecasts. ShivaneK Posted 5 months ago arrow_drop_up 2 more_vert Nice observation Chris Deotte Posted 5 months ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Congratulations Cabaxiom! Great job winning 5th place solo. I enjoyed all the content that you shared. You made the competition more enjoyable for all ! Optimistix Posted 5 months ago ¬∑ 232nd in this Competition arrow_drop_up 2 more_vert Congratulations! Your two excellent notebooks set the pace early on, and it's good to see you in the Top 5 at the end of the month. Sumit_08 Posted 3 months ago ¬∑ 1014th in this Competition arrow_drop_up 0 more_vert Congratulations for this achievement Appreciation (1) Samith Chimminiyan Posted 5 months ago ¬∑ 332nd in this Competition arrow_drop_up 1 more_vert Thank you!!! Nice learning experience. Too many requests error Too many requests",
      "Forecasting Sticker Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 months ago Late Submission more_horiz Forecasting Sticker Sales Playground Series - Season 5, Episode 1 Forecasting Sticker Sales Overview Data Code Models Discussion Leaderboard Rules Pascal Terpstra ¬∑ 6th in this Competition  ¬∑ Posted 5 months ago arrow_drop_up 8 more_vert 6th Place Solution First of all, thanks to everyone who shared interesting discussions‚ÄîI learned a lot from them! My final solution consisted of an ensemble of 3 different models. The first model I used was an adaptation of @kdmitrie 's published notebook. The second model used a multiplicative linear regression model, and the third model was a transformer published by @cdeotte . Model 1 (Public LB = 0.04792, Private LB = 0.04678) Multiplicative model Accounted for leap years (2012 and 2016). In practice, this just meant that I changed the dayofyear column to subtract 1 from all dates after the 29th of February for leap years. Used different country weights: I realized that the predictions for Kenya greatly influence the final score due to the nature of the MAPE metric (Try adding just 1 to all predictions for Kenya and see what happens!). I used a multiplier of 1.02 for Kenya on top of the multiplier of 1.06 for all predictions. I also experimented with slightly lower multipliers for other countries and used observation-specific multipliers where each multiplier was based on the confidence of the prediction (based on the variability of the same prediction in previous years). Model 2 (Public LB = 0.04874, Private LB = 0.04800) A linear regression model is trained which predicts the fraction of yearly summed num_sold for each day in the year. The store fraction was obtained by just taking the mean fraction of num_sold within each product on a given day and country. Product fractions (this means the product fraction of the total num_sold within a country in a day) were obtained using linear regression. Simple cos and sin transformations were used to make strong predictions here. I found it useful to first leave out the year 2016 for validation to investigate any patterns in the error. Model 3 (Public LB = 0.0526, Private LB = 0.06037) The third model was simply the output of the Transformer published by @cdeotte . Ensemble of models (Public LB = 0.04706, Private LB = 0.04722) This was the ensemble of the three models. In hindsight, just submitting the predictions from Model 1 would have been better (easy to say now)! Things I tried that didn't work Using boosting methods (e.g., Catboost, XGBoost) and RNNs. Accounting for holidays in linear regression models (e.g., accounting for the effect after Easter). 1 2 1 1 Please sign in to reply to this topic. comment 3 Comments Hotness Chris Deotte Posted 5 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Congratulations Pascal winning 6th place solo. I think your decision to ensemble 3 models was a good one. Ensembles generally work better and generalize better than single models and help prevent shakeup. ================= I think the problem with the ensemble in this case may be that my public Transformer starter doesn't make as good predictions for 2019 (second half of private LB) as it could. Autoregressive models will do better predicting 2017 (public LB) than 2018,2019 (private LB) because as we repeatedly use predictions to make more predictions the error gets worse. So my starter notebook transformer helped with public LB but didn't seen to help with private LB. My starter notebook can be improved to make better 2019 private LB predictions by ensembling 5 copies of the Transformer with itself. Furthermore we can do 2 rounds of time progressive pseudo labeling to help extend the accuracy into the future. I published my best Transformer only here (to a Kaggle dataset). The file name is submission_v82_5.csv . This Transformer was trained with all data at once (not 5 separate models) and it uses 5 seeds with itself and it uses 2 rounds of pseudo labels. It achieves public LB = 0.04733 and private = 0.04650 . We observe that the gap between public LB and private LB is better than my public starter notebook. I suspect if you ensemble your multiplicative model and your linear model and this better transformer, then the ensemble will do better than any single model alone. Let me know if you try submitting a new ensemble and the result. This comment has been deleted. Chris Deotte Posted 5 months ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert It was interesting to read that you ensembled 5 copies of the Transformer. Do you think using more copies would further improve the score? Yes. Most model submissions are KFold. When we submit GBDT or NN normally, we train KFold and submit an ensemble of K fold models. My public notebook only trains one model on all the data. So instead we should train 5 models (or more like you suggest) using all data and ensemble them. (Perhaps my notebook confused people because it already talked about running it 5 times. But those are not folds. Running it 5 times only generates the equivalent of one full model's predictions. Each time we run it it makes 20% of submission.csv. To replicate folds, we need to run my public notebook 25 times. We would run it 5 times for each PROD=0,1,2,3,4 value. ) It is a very important concept in data science that NN should be ensembled multiple times with itself. Other models like GBDT or ML models it is not important. But NN are very random in their training process. Training multiple NN generates diverse predictions because batches are different each time and the NN initializes to different starting weights. If you plot the CV score of 1 NN versus multiple NN ensembled with itself you will see the score improve 10%, 20% or more. It is very important for NN to remove the extreme variation that NN back propagation has. (And in this comp we ensemble with median because metric is based on MAE). (Somewhere I made a plot with x axis being number of NN copies and y axis being CV score. I forget but the score kept getting better and then slowed down around 30 copies or so. So 5 much much better than 1. And 10 better than 5. And 20 better than 10. And then the gains start to slow down). Chagin Posted 5 months ago ¬∑ 738th in this Competition arrow_drop_up 0 more_vert Congratulations on getting 6th place üéâ Thanks for sharing your methods üòÅ Too many requests error Too many requests",
      "Forecasting Sticker Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 months ago Late Submission more_horiz Forecasting Sticker Sales Playground Series - Season 5, Episode 1 Forecasting Sticker Sales Overview Data Code Models Discussion Leaderboard Rules bogoconic1 ¬∑ 11th in this Competition  ¬∑ Posted 5 months ago arrow_drop_up 9 more_vert 9 Public, 11 Private (Single Model) - Solution Writeup (Public LB 0.04605, Private LB 0.04766) First of all, thanks to Kaggle for organizing a playground forecasting competition after 16 months! (finally). This was the first time I got a top 1% in a competition solo , and done so without copying or ensembling public notebooks, but using my own intuition of what should work for this competition. I will briefly outline my solution here while elaborating on parts not seen on the public notebooks/discussions. The solution code is here - will be adding some comments to it later today. (A) Baseline As @siukeitin shared in this discussion , the \"distribution\" of sales follow a certain pattern and can be decomposed by country (GDP Ratio), product and store Store No visible trend across years was observed -> a constant value was assigned across all years for each store Product Seasonality was observed with frequency either 1 or 2 years depending on product. Thus a Fourier series with frequency = 2 years was fitted for each product Country Yearly values of country GDP / GDP sum for each year is used Day of week Sunday > Saturday > Friday > any other days in sales volume Note: Kenya and Canada sales should be excluded when computing these ratios as their NULLs may introduce bias into them. (B) Holidays Through EDA, we can find that the effect of a holiday lasts beyond the actual day of the holiday itself, and mostly up till 7-9 days after the holiday. The holidays are first sorted in order of date so that the most recent holiday is prioritised (in the case of overlapping holidays effect) Group by the country and the holiday names For each day from 1-9 compute an individual multiplier 3a. Assume day T is one of these days impacted by the holiday, compute sales for day T / sales for day T-7 , if day T-7 is a holiday, propagate the ratio back until a non-holiday is found 3b. clip the minimum to 1 (C) Estimating the total sales and multiplier Now that we have a rough idea of the \"breakdown\" of the sales, it leaves us to estimate the total sales for each calendar year from 2010 to 2019. The simplest way would be to fit a linear least squares to estimate yearly sales from the total GDP (due to the high R^2 of 0.997) - but this comes with some challenges. As per the discussion in (A) - it was shown that Kenya's GDP from worldbank is too high. I subtracted a constant value of 200 from Kenya's GDP as a result The linear model isn't a good fit for sales vs GDP as it violates the assumption where the errors are independent While the line of least squares estimates the slope (or Total Sales / GDP) as 84.54, we notice that the residuals follow a periodic trend! Denoting R(T) as the ratio of Total Sales/GDP for the year T: Therefore, a multiplier is necessary and it should be expected that the multiplier for 2017 > 2016 (1.04) 2018 < 2017 2019 > 2018 Through LB Probing it was found that the optimal multiplier for 2017 is 1.08. Unfortunately, I did not find any heuristics to accurately estimate the multiplier and had to guess it. If anyone has found one feel free to leave it in the comments For one submission I chose 2017: 1.08, 2018: 1.07, 2019: 1.11 For the other submission I chose 1.08 for all three years The second submission scored better on the private LB, which showed that the estimation of 1.11 as the multiplier for 2019 is not accurate. Notes I want to express my sincere gratitude to @cdeotte for sharing beginner-friendly tutorials over the years! Back then when I was competing for the first time in LLM Science Exam, your starter notebooks were incredibly helpful in guiding us toward improving our solution, especially during the times when I, and likely many others, were struggling. Your generosity in sharing knowledge has been invaluable, and I truly appreciate the impact it has had on my growth. Thank you! 1 2 1 Please sign in to reply to this topic. comment 0 Comments Hotness Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 5, Episode 1 For this challenge, you will be predicting multiple years worth of sales for various Kaggle-branded stickers from different fictitious stores in different (real!) countries. This dataset is completely synthetic, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. Good luck! 3 files 21.24 MB csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 21.24 MB sample_submission.csv test.csv train.csv 3 files 13 columns  Too many requests",
    "data_description": "Forecasting Sticker Sales | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 months ago Late Submission more_horiz Forecasting Sticker Sales Playground Series - Season 5, Episode 1 Forecasting Sticker Sales Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2025 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: The objective of this challenge is to forecast sticker sales in different countries. \"At Kaggle, we take stickers seriously!\"‚Ñ¢Ô∏è Start Jan 1, 2025 Close Feb 1, 2025 Evaluation link keyboard_arrow_up Submissions are evaluated using the Mean Absolute Percentage Error (MAPE) . Submission File For each id row in the test set, you must predict the target num_sold . The file should contain a header and have the following format: id ,num_sold 230130 , 100 230131 , 100 230132 , 100 etc . content_copy Timeline link keyboard_arrow_up Start Date - January 1, 2025 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  January 31, 2025 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Elizabeth Park. Forecasting Sticker Sales. https://kaggle.com/competitions/playground-series-s5e1, 2025. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 7,622 Entrants 2,811 Participants 2,722 Teams 22,758 Submissions Tags Beginner Tabular Time Series Analysis Mean Absolute Percentage Error Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s5e2",
    "discussion_links": [
      "/competitions/playground-series-s5e2/discussion/565539",
      "/competitions/playground-series-s5e2/discussion/565542",
      "/competitions/playground-series-s5e2/discussion/565653",
      "/competitions/playground-series-s5e2/discussion/565583"
    ],
    "discussion_texts": [
      "Backpack Prediction Challenge | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 4 months ago Late Submission more_horiz Backpack Prediction Challenge Playground Series - Season 5, Episode 2 Backpack Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Chris Deotte ¬∑ 1st in this Competition  ¬∑ Posted 4 months ago arrow_drop_up 158 more_vert 1st Place - Single Model - Feature Engineering Single Model Wins! My favorite solution in a Kaggle competition is a powerful single model versus a large ensemble. I'm excited that a single model with creative feature engineering wins this competition! Although this was a weird competition with weird data, this was one of my favorite competitions because this was a tricky puzzle to solve that required lots of creative features! Weird Competition Data This competition's data was weird and unnatural as explained here . The techniques that were successful in this competition are not what we would need if we were predicting real backpack prices. However it is important to note that every technique used here is used in other real world models. So it is beneficial to learn these techniques. Final Solution My final solution is a single model trained with 500 features using 1xA100 GPU 80GB. However a single model with only 138 features trained with Kaggle's 1xT4 GPU 16GB also wins first place. I publish this simple Kaggle T4 GPU solution here . Feature Engineering The key to success in this competition was running as many experiments as possible trying as many different feature engineering ideas as possible. To perform experiments as fast as possible, I used RAPIDS cuDF-Pandas as shown in my starter notebook here . In one month, I trained over 300 XGBoost models and tried thousands of different feature engineering ideas! My final solution keeps the best ideas. Below I list some of my favorite ideas from my final solution. Groupby(COL1)[COL2].agg(STAT) Basic groupby stats are explained in my starter discussion here . We pick a column COL1 , then pick a column COL2 , then pick a STAT like \"mean\" , \"std\" , \"count\" , \"min\" , \"max\" , \"nunique\" , \"skew\" etc etc. (If COL2 is a target column, we use nested folds to prevent leakage). Below are more advanced features! Groupby(COL1)['Price'].agg(HISTOGRAM BINS) I had fun inventing this technique. I have never seen it being used before. When we groupby(COL1)['Price'] we have a set of number for each group. Below we display histogram for the group Weight Capacity = 21.067673 . We can count the number of elements in each (equally spaced) bucket and create a new engineered feature with this bucket count to return to the groupby operation! Below we display 7 buckets, but we can treat the number of buckets as a hyperparameter. result = X_train2.groupby( \"Weight Capacity (kg)\" )[ \"Price\" ].apply(make_histogram) X_valid2 = X_valid2.merge(result, on = \"Weight Capacity (kg)\" , how= \"left\" ) content_copy Groupby(COL1)['Price'].agg(QUANTILES) After groupby , we can compute the quantiles for QUANTILES = [5,10,40,45,55,60,90,95] and return the 8 values to create 8 new columns. for k in QUANTILES : result = X_train2.groupby ( 'Weight Capacity (kg)' ).\\ agg ({ 'Price' : lambda x : x.quantile ( k / 100 )}) content_copy All NANs as Single Base-2 Column We can create a new column from all the NANs over multiple columns. This is a powerful column which we can subsequently use for groupby aggregations or combinations with other columns! train [ \"NaNs\" ] = np .float32 ( 0 ) for i ,c in enumerate (CATS):\n    train [ \"NaNs\" ] += train [c] .isna ()* 2 ** i content_copy Put Numerical Column into Bins The most powerful column in this competition is Weight Capacity . We can create more powerful columns based on this column by binning this column with rounding! for k in range ( 7 , 10 ):\n    n = f \"round{k}\" train [n] = train [ \"Weight Capacity (kg)\" ] .round (k) content_copy Extract Float32 as Digits The most powerful column in this competition is Weight Capacity . We can create more powerful columns based on this column by extracting digits! This technique seems weird but it is often used in real life to extract info from a product ID where individual digits within a product ID convey info about a product such as brand, color, etc. (idea from @jordanbarker here ) for k in range ( 1 , 10 ):\n    train[f 'digit{k}' ] = ((train[ 'Weight Capacity (kg)' ] * 10 **k) % 10 ). fillna (- 1 ). astype ( \"int8\" ) content_copy Combination of Categorical Columns There are 8 categorical columns in this dataset (excluding numerical column Weight Capacity ). We can create 28 more categorical columns by combining all combinations of categorical columns. First we label encode the original categorical column into integers with -1 being NAN. Then we combine the integers: for i ,c1 in enumerate (CATS [:-1] ): for j,c2 in enumerate (CATS [i+1:] ):\n        n = f \"{c1}_{c2}\" m1 = train [c1] .max ()+ 1 m2 = train [c2] .max ()+ 1 train [n] = ((train [c1] + 1 + (train [c2] + 1 )/(m2+ 1 ))*(m2+ 1 )) .astype ( \"int8\" ) content_copy Use Original Dataset which Synthetic Data is Created From The following feature seems weird, but it is based on the idea that a product's price is based on manufacture suggested retail price . We can treat the original dataset that this competition was created from as the manufacture suggested retail. And this competition's data as the individual stores' price. Therefore we can help predictions by giving each row knowledge of the MSRP: tmp = orig.groupby( \"Weight Capacity (kg)\" ).Price.mean() tmp.name = \"orig_price\" train = train.merge(tmp, on = \"Weight Capacity (kg)\" , how= \"left\" ) content_copy Division Features After creating new columns with groupby(COL1)[COL2].agg(STAT) , we can can then combine these new columns to make even more new columns! For example # COUNT PER NUNIQUE X_train [ 'TE1_wc_count_per_nunique' ] = X_train [ 'TE1_wc_count' ]/ X_train [ 'TE1_wc_nunique' ]\n# STD PER COUNT X_train [ 'TE1_wc_std_per_count' ] = X_train [ 'TE1_wc_std' ]/ X_train [ 'TE1_wc_count' ] content_copy Final Submission Code I publish a simplified version of my single model code here ! 24 28 14 8 Please sign in to reply to this topic. comment 96 Comments 6 appreciation  comments Hotness payal156 Posted 3 months ago arrow_drop_up 4 more_vert I learn something new everytime I open and this time it was no different! Thankyou for the innovative approach! Aditya Akuskar Posted 4 months ago ¬∑ 724th in this Competition arrow_drop_up 6 more_vert do you by any chance teach, i would really love to learn this things from you and congrats Optimistix Posted 4 months ago ¬∑ 5th in this Competition arrow_drop_up 5 more_vert Congratulations for another first place finish, and thanks for all the code and insights you shared along the way! Hope you keep making time for this series. Bhargav Chirumamilla Posted 4 months ago ¬∑ 362nd in this Competition arrow_drop_up 2 more_vert This is a fantastic breakdown of a winning Kaggle solution, and it highlights some of the key takeaways from the competition. Here are my thoughts on the approach and why it worked so well: Key Strengths of the Approach Single Model with Feature Engineering Instead of Large Ensembles Many Kaggle competitions are won with massive ensemble models, but a well-engineered single model can often outperform them due to its interpretability and efficiency. Training 300+ XGBoost models allowed for extensive experimentation, leading to a robust single model. Innovative Feature Engineering The success of this solution came from trying thousands of feature engineering techniques and keeping the best ones. The Histogram Binning, Quantile Aggregation, and Numerical Column Binning approaches are particularly creative and powerful. The Base-2 encoding of NaN values is an interesting way to preserve missing data patterns. Handling \"Weird Data\" Recognizing that the competition's data wasn‚Äôt natural but still using techniques applicable to real-world problems was a great insight. Features like Extracting Float Digits seem unconventional but are useful in real-world datasets (e.g., product IDs encoding meaningful information). Groupby Aggregation and Derived Features The groupby-based feature engineering using quantiles, mean prices, and histogram bins added critical information to the model. Using the original dataset‚Äôs mean price as a feature was a brilliant move, mimicking real-world MSRP pricing influence. Combining Categorical Features Label encoding categorical variables and creating combinations of categorical columns expanded the feature space effectively. The approach of combining categorical variables with computed ratios added valuable interaction terms. Derived Features Through Mathematical Transformations Division features, such as count per unique values or standard deviation per count, helped the model capture hidden relationships in the data. Lessons for Future Competitions Feature Engineering Matters More Than Model Complexity Instead of relying on deep ensembles, crafting the right features can lead to simpler yet more powerful models. Experimentation is Key Testing thousands of feature combinations and retaining only the best ones ensures the model generalizes well. Understand the Dataset‚Äôs Origin Using domain knowledge, like incorporating MSRP pricing logic, can give a competitive edge. This approach is a textbook example of how thoughtful feature engineering can outperform brute-force ensembling, making it a valuable lesson for both Kaggle and real-world machine learning applications. üöÄ Aditya Chute Posted 4 months ago arrow_drop_up 3 more_vert Massive respect for this achievement! @cdeotte Your commitment to excellence and innovation is truly inspiring. Keep leading the way! Ahmed Samir Posted 4 months ago arrow_drop_up 3 more_vert Your ideas never fail to impress me @cdeotte Congrats! Jeevan Venkatesh Posted 4 months ago arrow_drop_up 4 more_vert This is great, man. I recently started with Kaggle Competitions, and learning from this quality of work is too insightful Samay Ashar Posted 4 months ago ¬∑ 1156th in this Competition arrow_drop_up 4 more_vert Never thought we can use all the NaN's as a separate column - amazing Chris! thanks so much. Sefa Erkan Posted 4 months ago ¬∑ 623rd in this Competition arrow_drop_up 1 more_vert Congratulations! and Thank you for Sharing. Chanhee0129 Posted 4 months ago arrow_drop_up 1 more_vert WOW.. Thats amazing~ Single Model Impact LosiSt Posted 4 months ago arrow_drop_up 1 more_vert That's amazing.Thank you for sharing your notebook. Shahid Akhtar Khan Posted 4 months ago arrow_drop_up 1 more_vert Thank you @cdeotte for sharing the insights. It's really helpful !!! L. Elaine Dazzio Posted 4 months ago ¬∑ 467th in this Competition arrow_drop_up 1 more_vert Are there any situations where you prefer using an ensemble model vice a single model? Gizachew Alemu Posted 4 months ago arrow_drop_up 1 more_vert Congratulations! and Thank you for Sharing. Thecats_Jfm Posted 4 months ago ¬∑ 432nd in this Competition arrow_drop_up 1 more_vert Thank you for sharing! Your approach is truly inspiring and gave me a lot of new insights. Well deserved win! KARTIK SHETTY Posted 4 months ago ¬∑ 1310th in this Competition arrow_drop_up 2 more_vert Feature engineering is crucial. tharun0274 Posted 4 months ago arrow_drop_up 1 more_vert Congratulations!!! Thank you for Sharing your notebook. Abdullah TALYAN Posted 4 months ago ¬∑ 229th in this Competition arrow_drop_up 1 more_vert As Kaggle begginers, we have learned many different perspectives from you. Congrats :) Palvinder Posted 4 months ago arrow_drop_up 1 more_vert Congratulation! Thank you for sharing your code. FrostMagic Posted 4 months ago arrow_drop_up 1 more_vert Thanks, Chris! Could you explain your feature selection approach after feature engineering? With thousands of features generated (e.g., groupby(COL1)[COL2].agg(STAT)), how do you choose the best ones? Do you sort by gain importance, add them one by one to the baseline, and keep only those improving the metric with t-test/Mann-Whitney? Do you add features one by one, in batches, or generate as many as you can and select top-n by permutation importance? Chris Deotte Topic Author Posted 4 months ago ¬∑ 1st in this Competition arrow_drop_up 4 more_vert I try each idea one at a time. Each idea will create about a dozen features together. Then i add those new dozen features to the model. If CV score improves, I keep that dozen, if not I discard them. Then I brainstorm for another dozen. I continue this process. After doing this for a while and successfully finding and adding hundreds of features. I will then consider feature elimination using RFE or XGB feature importance. I will also consider readjusting my XGBoost hyparameters. Then i continue the search process for new features again. Ayush Posted 4 months ago arrow_drop_up 1 more_vert \"Wow, brilliant work! Your creative feature engineering and single-model approach are truly inspiring!\" Oleksandr Vyshnevskyi Posted 4 months ago arrow_drop_up 1 more_vert Congratulations. Very usefull ideas. Thank you for sharing Jeferson Fernando Tiepo Posted 4 months ago arrow_drop_up 1 more_vert Very good solution, I am new to kaggle and also to data science, and I have learned a lot by reading the notebooks, thanks for sharing. Muhammad Amir Rafiq Posted 4 months ago ¬∑ 606th in this Competition arrow_drop_up 1 more_vert Thanks for sharing my friend @cdeotte üòä Yogesh Maurya Posted 4 months ago arrow_drop_up 1 more_vert Many many Congratulations! So much to learn from Grandmasters like you. Thank You. Too many requests error Too many requests",
      "Backpack Prediction Challenge | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 4 months ago Late Submission more_horiz Backpack Prediction Challenge Playground Series - Season 5, Episode 2 Backpack Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Ravi Ramakrishnan ¬∑ 2nd in this Competition  ¬∑ Posted 4 months ago arrow_drop_up 32 more_vert Rank 2 approach - a century of component feature sets and deep ensemble Hello all, Thanks to Kaggle for the intriguing episode in the Playground series! This was such a different episode from the usual Playground episodes! Thanks to my fellow participants for such a healthy competition throughout the month. My overall approach was a deep blend of boosted trees, neural networks and ridge model with deep feature engineering with the architecture as drawn below- Feature engineering As discussed in several public kernels and posts, this was the most important and crucial component of the competition. I used the column WeightCapacitykg as a float and its string twin as separate features. In my sample dataset, this column is labelled as WeightCapacity I prepared a total of 1600+ features across 9 datasets and stored them in a feature store to retrieve and use across the month. I used OrdinalEncoder for all string/ category variables and then combined them into 1-2-3-4-5-6-7 gram combinations and stored them in separate datasets for easy retrieval I also used the idea of joining the original features from the kernel here and used them with my engineered features in 2 separate datasets. I considered only bigrams and trigrams here as the number of features exploded a lot and resources were not sufficient to handle the volume of data generated. I used Colab TPU to prepare these features as I obtained a virtual machine with more than 200GB RAM on my TPU. I stored all my component features in separate parquet files for easy retrieval and usage and used Polars for subsequent feature retrieval and usage. I used TargetEncoder from CuML for all my encoding purposes and used mean, median, count, nunique as aggregators I have open-sourced a set of features for you to peruse here . CV scheme I used a 20-fold cross validation scheme for all my models, including the classifier to keep consistency across all single and blended models cv = KFold(n_splits = 20, shuffle = True, random_state = 42) Level-1 Model training I trained catboost, xgboost and lightgbm models as my layer-1 models on separate feature sets drawn from the gamut of features described above. Each model comprised of a separate feature set, with important features in common. Certain features like WeightCapacity, WeightCapacitykg, Brand, Brand-Color-Size, Brand-Material-Size, Brand-Color-Material-Size , etc. were almost always present, while other features were model specific. I identified top 50 important features and retained them in all my models, and varied the features otherwise in my component models. I designed a total of 65 boosted tree models with the below CV scores across single model solutions - Model type Number of single models designed CV score range XGB  Regressor 21 38.6463 - 38.75856 LGBM Regressor 20 38.6471  - 38.66303 Catboost Regressor 21 38.6480  - 38.74479 Additionally, I also designed separate boosted tree models with the Autoencoder as below- Model type Number of single models designed CV score XGB  Regressor 1 38.65556 LGBM Regressor 1 38.65727 Catboost Regressor 1 38.658758 Additionally, I also designed a simple dense NN classifier with integer targets for some diversity to the ensemble. This model was a poor choice for a single submission, but it added a needed diversity to the ensemble and created a minor gain upon blending. Model type Number of single models designed CV score Dense NN classifier 1 38.891892 Public artefacts I used the public autoencoder model with a few adjustments and also executed the kernel here and used them in my ensemble Thanks to the authors of these kernels! Level-2 Model training I had to blend these boosted tree models into a meaningful ensemble and thought of using a simple MLP as a stacker model. I used Kaggle and Colab TPU to train these NN models. One often uses CPU and GPU resources but procrastinates the 20-hour of TPUs available per week! Using these resources to good effect was key to a lot of experiments this month! I created a total of 35 stacker NN models with varying model OOF features with the CV range as below- Model type Number of single models designed CV score range NN stacker 35 38.63546 - 38.65008 Level-3 Model training and post-processing This is the last layer in the model process, a simple ridge model that blends the results of the L1- boosted trees + L2 NN models, the public artefacts and the MLP classifier model for the submission My final submission contains a combination of 100 models and has a CV score of 38.62860836 and a Public LB score of 38.82326 and a private LB score of 38.62947 I also rounded the predictions to the nearest integer value - this slightly downgraded the CV but improved the LB score. Since the CV was less optimal, I chose this as an alternative submission. This one scored slightly lower on the private leaderboard with score of 38.63039 What did not work for me Sample weights Appending the original data with the competition data Variance and std-dev aggregators in features while performing target encoding Post-processing - I found a few repeated rows between the train and test sets and between the train and extra training data as well. Copying the targets across these common rows in my submission result did not help me at all Using any model other than Ridge in level-3 My key takeaways from the assignment This was a GPU intensive assignment and I learnt how to manage my resources better with such a lot of GPU oriented training through the month I learnt the art of using TPUs for FE and model training here. TPUs work wonderfully with NNs and this is a fast way to iterate through multiple experiments quickly! I became better at feature encoding with emphasis on Target Encoding - this is a big gain for work-assignments as well! Training GPU stack Stage GPU/ TPU usage Feature creation Colab TPU XGboost A6000 Ada + 128 GB RAM LGBM A6000  + 128 GM RAM Catboost A6000Ada x 2  + 256 GM RAM L2-NN Colab TPU/ Kaggle TPU Ridge Local CPU Model parameter tuning + feature experiments Local GPU (3090) + 128GB RAM Concluding remarks Congrats to my fellow swag prize winners, best wishes to all the participants and happy learning to one and all! Hope for the best in the upcoming Playground episodes and across Kaggle featured competitions as well! Regards, Ravi Ramakrishnan 8 Please sign in to reply to this topic. comment 13 Comments 1 appreciation  comment Hotness Mohammad Ziyari Posted 4 months ago arrow_drop_up 1 more_vert Your feature engineering approach is truly impressive, especially the extensive use of target encoding with multiple aggregators. The level of detail and systematic experimentation you put into this is commendable! Given that target encoding can introduce label leakage, especially in high-cardinality categorical features, how did you mitigate potential overfitting? Did you explore any alternative encoding strategies, such as leave-one-out target encoding or category embeddings, to compare their impact on model generalization? Would love to hear your insights on this. Ravi Ramakrishnan Topic Author Posted 4 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert alternative encoding strategies, such as leave-one-out target encoding or category embeddings Yes, I tried to use Catboost encoder from category_encoder library, but it was far too time-consuming and wasn't too helpful. Given that target encoding can introduce label leakage This is a very valid point, and hence, we use a nested-cv scheme to prevent this problem. You can peruse the cuml.preprocessing.TargetEncoder() documentation to know more. Please note that this is a general implementation of the k-fold strategy while other strategies will need code adjustments using the source code provided in the source files. emoji_people neednot_toplay Posted 4 months ago ¬∑ 278th in this Competition arrow_drop_up 1 more_vert can you share me your notebook here. please @ravi20076 Ravi Ramakrishnan Topic Author Posted 4 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert I have a few kernels in the public forum @ayushparwal Final solution uses the same Example - https://www.kaggle.com/code/ravi20076/playgrounds5e2-public-baseline-v1 emoji_people neednot_toplay Posted 4 months ago ¬∑ 278th in this Competition arrow_drop_up 1 more_vert I see @ravi20076 how much time you have took to learn all this when you were studying this and how do you learn this concepts can you please share? 3 more replies arrow_drop_down Minato Namikaze Posted 4 months ago ¬∑ 474th in this Competition arrow_drop_up 1 more_vert Congratulations @ravi20076 , An amazing win for the year ! Shedrack Eneojo Okute Posted 4 months ago arrow_drop_up 1 more_vert Congratulation. Please, can you share the link of your solution? Thank you Ravi Ramakrishnan Topic Author Posted 4 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert I don't plan to do that üòÉ @shedracka Sarun P M Posted 4 months ago ¬∑ 128th in this Competition arrow_drop_up 2 more_vert üåπ Congrats on the second position  üåπ. Nice explanation. My GPU resource was totally exhausted, and I became helpless at the end. Final submissions were only obtained from CPU-based notebooks. üò™ Ravi Ramakrishnan Topic Author Posted 4 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert This was a big problem in this competition - it was a biggest GPU wins contest leaving the rest behind @sarunpm I can understand the situation, but March playground episode is very different and GPUs don't matter here! PawelStepanov Posted 4 months ago ¬∑ 99th in this Competition arrow_drop_up 2 more_vert Congratulations. A very interesting solution and excellent work with memory. Thanks for sharing. Optimistix Posted 4 months ago ¬∑ 5th in this Competition arrow_drop_up 2 more_vert Congratulations! Looks like you ended up spending more time on this episode than looked likely in the first week üòÄ Ravi Ramakrishnan Topic Author Posted 4 months ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Yes, from almost walking away from the competition to winning a swag prize here, I came a long way @optimistix ! Thanks for the wishes! Optimistix Posted 4 months ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert You're welcome. I've wondered about this - does swag eligibility reset every year, making you eligible again this year; or would you still be bypassed, because you've received swag in the past? 4 more replies arrow_drop_down Appreciation (1) Berker ERYILMAZ Posted 4 months ago ¬∑ 892nd in this Competition arrow_drop_up 1 more_vert Congratulations! Too many requests error Too many requests",
      "Backpack Prediction Challenge | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 4 months ago Late Submission more_horiz Backpack Prediction Challenge Playground Series - Season 5, Episode 2 Backpack Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules automatylicza ¬∑ 3rd in this Competition  ¬∑ Posted 4 months ago arrow_drop_up 27 more_vert 3rd Place Solution in Three Words! Determination ‚Äì Creativity ‚Äì Luck I'll leave it to you to decide what luck is Hello everyone! My name is Sebastian, and first of all, I would like to thank Mr. Pawe≈Ç Godula ‚Äì narsil (jobs-in-data.com) for spreading the Kaggle ideology in Poland and, most importantly, for playing a key role in helping me discover, after many years, what I will pursue in life and in which field I will become one of the best in the world. Chris Deotte ‚Äì Thank you for what you do and how you do it. WooHoo!!! Main Stages of the Solution An essential part of my solution is the code that was shared by other competitors. The main characters are: @cdeotte , @masayakawamata , @mikhailnaumov and @vyacheslavbolotin . Feature Engineering Distance Features ( feh_distance ) For the raw datasets train_raw and test_raw , new variables are computed based on the distances between selected attributes (after mapping them to numerical values). Columns such as _2_1 , _2_2 , ‚Ä¶ _5_1 are created, which represent the square roots of the sums of squared differences of selected (mapped) attributes, e.g., (x1 - x3)¬≤ + (x2 - x4)¬≤, etc. This yields a dozen or so features expressing the ‚Äúsimilarity/difference‚Äù within the backpack. Creation of Combined (COMBO) Features For each original categorical column (e.g., Brand , Style , Color , etc.), a new feature is created by combining it with Weight Capacity (kg) . Example: new_col = Brand * 100 + Weight Capacity (kg) . This helps capture the interactions between the categories and the backpack‚Äôs carrying capacity. Statistics from an External Dataset ( orig_price_* ) Based on an external dataset ( Noisy_Student_Bag_Price_Prediction_Dataset.csv ), the following values are calculated: orig_price_mean , orig_price_std , orig_price_min , orig_price_max , and orig_price_median . The variable orig_price_missing is used to capture cases when a given combination did not appear in the external dataset. Group Aggregations and Target Encoding GPU-accelerated grouping (using cudf ) was employed for faster computation of statistics such as mean, std, min, max, median, count, and skew. Multiple aggregations were performed, including grouping by Weight Capacity (kg) and the COMBO features. Additionally, Target Encoding (implemented via cuml.preprocessing.TargetEncoder ) was applied to the columns in BASE_FEATURES . As a result, each feature is replaced by the (smoothed) average Price within its category. Missing Value Indicators ( _NaN_* ) For each of the 7 main categorical features, a missing indicator was defined (e.g., _NaN_Brand = 1 if Brand equals ‚ÄòMissing‚Äô). Additionally, the column _7_NaNs sums up the number of missing values across all key fields. Autoencoder for Feature Extraction with a Supervised Layer An autoencoder (built with Keras + TensorFlow) was constructed and trained, featuring two main output branches: Reconstruction of the original numerical features (reconstructed = Weight Capacity (kg) ), Prediction of the target value ( Price ) in the supervised branch. Consequently, the hidden layer ( latent ) contains a representation of the numerical features that also ‚Äúknows‚Äù how to assist in predicting the price. The latent vector becomes a valuable feature appended to the final input of the models. Models and Ensembling Four tree-based models were utilized: LightGBM, XGBoost, XGBoost (with a different configuration), and CatBoost. Finally, stacking was applied: the prediction vector from (LGBM, XGB, XGB2, CatBoost) is used as input to a BayesianRidge model, which finalizes the ensemble prediction. The coefficients of the tree-based models and BayesianRidge are tuned to minimize the RMSE. Training and Prediction 10-fold KFold for result stabilization, Each iteration generates Out-Of-Fold (OOF) predictions from 4 models (LGBM, XGB, XGB3, CatBoost), Stacking (using BayesianRidge), The final test prediction is the output of the Bayesian model applied to the stacked predictions. The submission that achieved the best result was carefully blended with several public submissions In Summary: The notebook makes intensive use of feature engineering ‚Äì both classical (group aggregations, target encoding, handling missing values) and more advanced (autoencoder, distance features, external price data). The final ensembling combines several tree-based models and leverages BayesianRidge in the last layer, which further stabilizes the results and reduces RMSE. A Few Loose Thoughts ‚Äì from a Newbie to Newbies I‚Äôve done many things in life, but none of them had anything to do with IT. About three months ago, I started learning Python and SQL, and only two months ago did I discover Kaggle, so everything I write about my competition experiences might contain errors, and I might be mistaken. I‚Äôve played many games, and often the deciding factor was whether a game was challenging enough. Kaggle is the most challenging game I know, and the satisfaction from it is on a completely different level. A Few Loose Thoughts ‚Äì from a Newbie to Newbies , which ran through my mind at the finish of the Backpack Prediction Challenge . Perhaps tomorrow I‚Äôll have different conclusions, so please don‚Äôt attach too much importance to them. The reflections from the end of the competition are mainly non-technical, as I‚Äôll only be ready to tackle those in a few months. Never spend too much time on the fundamental understanding of key aspects of the competition, even those that seem the simplest. If I thought I understood something, I often discovered there was more to it. Even if we don‚Äôt find anything entirely new, delving into topics such as evaluation metrics or exploratory data analysis (EDA) can inspire ideas that seem to come from a completely different area. Chasing the leaderboard score from the very beginning might not be the best strategy. Optimization ‚Äì it allows for rapid testing of ideas, and sometimes it can prove crucial at the end of the competition, when models start growing and merging. The desire to win serves as a fantastic learning method, or at least I hope so. Theory vs. Practice ‚Äì it seems to me that training dozens, or even hundreds, of different models offers more insights than reading a few books. Concepts I repeatedly read about before but couldn‚Äôt grasp now seem obvious. Creativity and unconventional ideas are rewarded, but they should be applied at the right moment ‚Äì for instance, in the middle of the competition when I couldn‚Äôt enhance the signal found by XGB using standard methods. After testing a dozen very strange solutions, I discovered a function that significantly improved the score. This function amplified the deviation from the median prediction‚Äîthe greater the deviation, the higher the value. However, as I refined the model, I noticed that this idea ceased to work as the prediction quality improved. Experimentation ‚Äì I believe that I will once again discover something interesting that no one else has thought of, but now I know it‚Äôs best to search and experiment later, rather than in the middle of the competition. Hardware ‚Äì I performed the vast majority of computations on the free resources provided by Kaggle (my own hardware is much weaker). Although I spent most of the last few days of the competition on an ultimately unsuccessful attempt to optimize my automatic feature selector, I believe this is not a barrier. I‚Äôd bet it was possible to win even on a CPU, plus a great idea. Overall Planning ‚Äì if I were to start this competition over again, I would do everything differently, focusing primarily on refining one model and methods for discovering new, strong features, and perhaps incorporating more models at the end for a better outcome. Selection of Kaggle Materials ‚Äì it‚Äôs crucial to carefully select the materials we use. Gathering three or four excellent ideas from public notebooks, at least at the Playground stage, already provides a lot. Chris Deotte ‚Äì if you come across his posts, just stick around longer. What do you think? What experiences do you have? Good luck to everyone in future competitions. Thank you all for a wonderful competition. See you on the trail and at the top of the LB. Sebastian Kruszek automatylicza@gmail.com Codziennie Silniejsi! 10 3 1 Please sign in to reply to this topic. comment 15 Comments 1 appreciation  comment Hotness Dang Nguyen Le Posted 3 months ago arrow_drop_up 3 more_vert Excellent work; you did a great job on this! narsil (jobs-in-data.com) Posted 4 months ago ¬∑ 442nd in this Competition arrow_drop_up 5 more_vert This is an amazing result. You just started‚Ä¶ it is great to see our group KNAI stronger and stronger on Kaggle leaderboards! Codziennie Silniejsi! (Our motto is btw exactly what all Grandmasters say - Kaggle is about iteration and being better 1% every day). payal156 Posted 3 months ago arrow_drop_up 1 more_vert This is some tough job - all made easy due to your perseverance! Thanks! Jimmer Posted 4 months ago ¬∑ 126th in this Competition arrow_drop_up 3 more_vert Gratulujƒô! Codziennie Silniejsi! JUHUUU! Michal Bogacz Posted 4 months ago arrow_drop_up 1 more_vert @automatylicza Gratulacje! Olabode James Posted 4 months ago ¬∑ 2879th in this Competition arrow_drop_up 1 more_vert Sebastian, this is an amazing effort - and what is remarkable is that you started with serious kaggling 3 months ago - please stay around for long, as I believe there is still a lot of potential where your amazing solution came from, looking forward to more competition entry from you and good luck on future competitions. Sarun P M Posted 4 months ago ¬∑ 128th in this Competition arrow_drop_up 1 more_vert üå∫ Congrats on the third price, and thanks for sharing the details. üå∫ PawelStepanov Posted 4 months ago ¬∑ 99th in this Competition arrow_drop_up 1 more_vert Congratulations. Great idea and very good explanation. I wanted to ask how much the results improved using Distance Features? automatylicza Topic Author Posted 4 months ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Hi I don't know exactly, I don't have it in my notes but from what I remember it was significant. PawelStepanov Posted 4 months ago ¬∑ 99th in this Competition arrow_drop_up 0 more_vert Got it, thank you Brian Ketelboeter Posted 4 months ago ¬∑ 2492nd in this Competition arrow_drop_up 2 more_vert Awesome summary. You obviously have advanced quite quickly, congratulations. I completely agree that the best idea is just to spend your time on building models, working on data etc. over reading. You have amazing skills. Rushi Lunagariya Posted 4 months ago ¬∑ 1854th in this Competition arrow_drop_up 2 more_vert This is good approach @automatylicza . Thanks for sharing it. HRIDAY SAMDANI Posted 3 months ago arrow_drop_up 0 more_vert As you rightly mentioned about learning, I would like to study your notebook if you could please share to gain some insights as I am also new to machine learning, but an ardent learner Justin Tran Posted 4 months ago ¬∑ 3079th in this Competition arrow_drop_up 0 more_vert This summary has helped me understand, as a beginner, what I should be focusing my attention to. Thank you. Chanhee0129 Posted 4 months ago arrow_drop_up 0 more_vert Congratulation! Good Summary! Appreciation (1) Howard Liao Posted 4 months ago ¬∑ 88th in this Competition arrow_drop_up 1 more_vert Thank you for sharing it. Too many requests error Too many requests",
      "Backpack Prediction Challenge | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 4 months ago Late Submission more_horiz Backpack Prediction Challenge Playground Series - Season 5, Episode 2 Backpack Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Optimistix ¬∑ 5th in this Competition  ¬∑ Posted 4 months ago arrow_drop_up 14 more_vert 5th place: Finding the SigNeedle in the NoiseStack When this playground episode started, I was expecting a classification problem, and was a bit surprised to see that we had another regression problem. Moreover, it became clear within a day or two that none of the features seemed related to the target, and the original data didn't seem to be \"real world\" at all. Several Kagglers pointed this out to Kaggle admin, and many of us were hoping for a 3 week competition with a new dataset. Instead, Kaggle responded with 12 times as much training data we had. While this motivated some - notably @cdeotte - to exploit the synthetic data generation process to extract some signal, others including me were significantly demotivated. Perhaps the most consequential thing I did in this month's competition was to ask the creator of the original dataset (@souradippal) how he created the dataset, and when he graciously shared the code along with a description, it became clear that the target was indeed sampled at random. However, the details of the process again created scope for extracting some more signal, which @cdeotte skillfully exploited. For most of this month, I wasn't very interested in working on this competition, and just ran a few scripts and tried some blends. In contrast to most months, I hardly ran any code on my own laptop, just using Kaggle notebooks. I didn't even start building an ensemble until there were just 5 days to go. But the posts and scripts generously provided by @cdeotte & others finally motivated me to try for the last few days, and I finally started building my own ensembles. I should thank the following for sharing code and insights: @cdeotte , @masayakawamata , @mikhailnaumov , @vktsyp , @souradippal , @vyacheslavbolotin , @yunsuxiaozi , @rtenorioramirez , @ravi20076 , @siukeitin , @paddykb In the end, I had an ensemble of 25 models, with CV = 38.59273 (using Autogluon). This was the best CV I achieved, but the LB was 38.85316. The difference of over 0.26 incidicated that I might be overfitting the training data, since most other solutions only had a difference of about 0.2 between CV and LB. So I preferred the ensemble with the best LB score while maintaining LB - CV ~ 0.2, which had CV = 38.63914 and LB = 38.83889 (using Ridge Regression). As a standalone final submission, this would have scored 38.64268, and netted me 18th place. However, I couldn't resist blending my ensembles with @mikhailnaumov 's top scoring public notebook - this yielded a slightly worse public LB score of 38.8224, but I chose it because I knew such blends often do well on the private LB, and indeed, the private LB score was 38.63455. So I ended up at no. 5. I almost feel like I owe an apology to @mikhailnaumov for using his solution to leapfrog him üòÄ Along the way, I did try to build on some of the solutions by @cdeotte and others, and did so to some extent, but also kept running into GPU & memory issues. All in all, it was an odd month, where I wasn't motivated to work on the competition for the most part, yet ended up at no. 5, albeit without the satisfaction of earlier Top 10 finishes where I'd put in the thought and effort on a consistent basis. Let me finish by congratulating @cdeotte , @ravi20076 , @mikhailnaumov , @masayakawamata , @swagician and all others who finished at the top, and wish everyone all the best for March - Happy Kaggling! Please sign in to reply to this topic. comment 7 Comments Hotness Dang Nguyen Le Posted 3 months ago arrow_drop_up 3 more_vert I appreciate your detail‚Äîit makes it even greater. Jimmer Posted 4 months ago ¬∑ 126th in this Competition arrow_drop_up 1 more_vert Nicely done! Thank you for solution and stay optimistic :D Optimistix Topic Author Posted 4 months ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Thanks! I'm always optimistic, but it was overoptimistic to expect Kaggle to change the dataset, or for them to choose an original dataset with proper documentation in March. But the March episode has a very small dataset, so I'm optimistic about not needing to spend much time on it, and learning other things this month üòÄ Jimmer Posted 4 months ago ¬∑ 126th in this Competition arrow_drop_up 1 more_vert It's true, it's much faster to work on the rainy March dataset. Stronger every day! ü•≥ Optimistix Topic Author Posted 4 months ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert All the best! Gerald Schwartz Posted 4 months ago ¬∑ 244th in this Competition arrow_drop_up 1 more_vert I am curious, what was the private score of your 25 model ensemble? Optimistix Topic Author Posted 4 months ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Hi - it scored 38.65644 on the private LB, so it was indeed overfitting. Unusual for Autogluon, but then this was an unusual dataset. PawelStepanov Posted 4 months ago ¬∑ 99th in this Competition arrow_drop_up 1 more_vert Congratulations. The competition was indeed strange, but to some extent it was even interesting. Optimistix Topic Author Posted 4 months ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Thanks, and congrats to you as well for finishing in the Top 3%! Yes, it was interesting in its own way. PawelStepanov Posted 4 months ago ¬∑ 99th in this Competition arrow_drop_up 1 more_vert Thank you, this is my best result at the moment and I am pleased with it. üòÄ Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 5, Episode 2 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Student Bag Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 4 files 337.74 MB csv MIT Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 337.74 MB sample_submission.csv test.csv train.csv training_extra.csv 4 files 34 columns  Too many requests",
    "data_description": "Backpack Prediction Challenge | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 4 months ago Late Submission more_horiz Backpack Prediction Challenge Playground Series - Season 5, Episode 2 Backpack Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2025 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: Predict the price of backpacks given various attributes. Start Feb 1, 2025 Close Mar 1, 2025 Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = ( 1 N N ‚àë i = 1 ( y i ‚àí ÀÜ y i ) 2 ) 1 2 where ÀÜ y i is the predicted value and y i is the original value for each instance i . Submission File For each id in the test set, you must predict the Price of the backpack. The file should contain a header and have the following format: id ,Price 300000 , 81 . 411 300001 , 81 . 411 300002 , 81 . 411 etc . content_copy Timeline link keyboard_arrow_up Start Date - February 1, 2025 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  February 28, 2025 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Elizabeth Park. Backpack Prediction Challenge. https://kaggle.com/competitions/playground-series-s5e2, 2025. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 8,597 Entrants 3,566 Participants 3,393 Teams 23,588 Submissions Tags Beginner Tabular Mean Squared Error Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s5e3",
    "discussion_links": [
      "/competitions/playground-series-s5e3/discussion/571176",
      "/competitions/playground-series-s5e3/discussion/571216",
      "/competitions/playground-series-s5e3/discussion/571021",
      "/competitions/playground-series-s5e3/discussion/571139"
    ],
    "discussion_texts": [
      "Binary Prediction with a Rainfall Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 3 months ago Late Submission more_horiz Binary Prediction with a Rainfall Dataset Playground Series - Season 5, Episode 3 Binary Prediction with a Rainfall Dataset Overview Data Code Models Discussion Leaderboard Rules Chris Deotte ¬∑ 2nd in this Competition  ¬∑ Posted 3 months ago arrow_drop_up 141 more_vert 2nd Place - GBDT + NN + SVR + Original Data Wow, what a shakeup! I was afraid of a shakeup from day one, so I kept my solution simple. Tabular Data In general with tabular data, I like to blend GBDT and NN. Then I try adding a few ML models like SVR, LR, KNN, etc. Furthermore in Kaggle playground competitions, we must decide how to use the original dataset that synthetic data was created from. Feature Engineering When train data is small (few rows) I do little or no feature engineering because it is easy to overfit train data. When data is large (many rows like December and February playground comps), I do lots of feature engineering. In this competition, I chose to do no feature engineering. My solution is just an equal average of multiple models where each model trains on the data \"as is\" without feature engineering. So in this competition, I spent my time training different diverse models (using original data in different ways). And evaluating local ensemble OOF CV scores using Group K Fold. (And each ensemble uses equal weight averaging to avoid ensemble overfit). Group K Fold In this competition, I used Group K Fold with 6 folds. I split the train data into 6 years and put each year in its own fold using Group K Fold. Because the test data is two new years of data train [ 'group' ] = train [ 'id' ] //365 content_copy Original Data as New Rows The train.csv data is 6 years and 2190 rows. The original dataset is 1 year and 366 rows. One way to add the original data is pd.concat() and add new rows .  (It then becomes group=7 for training and is ignored in the validation score calculation). train = pd.concat([train,orig],axis= 1 ) content_copy => XGBoost - CV 0.893, Public LB 0.848, Private LB 0.90317 Single model uses max_depth=3 , colsample_bytree=0.9 , subsample=0.9 like version 1 of my XGB starter notebook here . We use data \"as is\" without feature engineering. (Uses train data with orig as new rows). => TabPFN - CV 0.894, Public LB 0.867, Private LB 0.90193 Single model uses data \"as is\" without feature engineering. (Uses train data with orig as new rows). => [Two Model Blend] - CV 0.897, Public 0.859, Private LB 0.90474 - [11th Place] This equal weight ensemble of two models above achieves 11th Place! Original Data as New Columns The train.csv data has 11 feature columns. The original data as 11 feature columns. One way to add the original data is pd.merge() and add new columns . (We shared this idea last playground comp here ) m = train .rainfall .mean () for c in COLS:\n    n = f \"{c}2\" train [n] = train [c] .map ( orig .groupby (c) .rainfall .mean () )\n    train [n] = train [n] .fillna (m) content_copy => RAPIDS SVC - CV 0.896, Public 0.852, Private 0.90610 - [2nd Place] Single model uses RAPIDS SVC with C=0.1 , kernel='poly' , degree=1 similar to my starter notebook here . We use data \"as is\" without feature engineering.  (Uses train data with orig as new columns). This single model achieves 2nd Place! => [Three Model Blend] - CV 0.898, Public 0.855, Private LB 0.90728 - [1st Place] This equal weight ensemble of three models above achieves 1st Place! My Final 2 Submissions For my final 2 submissions, I trained a few other models (CatBoost, LogisticRegression, XGBoost, SVR) and submitted two different 6 model equal weight blends. The three models above are the strongest. The additional models boosted ensemble CV score to 0.900 and 0.901 but did not boost private LB score beyond 0.906 . My final 2 submission 6 model ensembles were: => [Six Model Blends] - [2nd Place] CV = 0.900, Public LB = 0.857, Private = 0.90604 CV = 0.901, Public LB = 0.867, Private = 0.90599 23 11 14 Please sign in to reply to this topic. comment 72 Comments 10 appreciation  comments Hotness Zeyuuu17 Posted 23 days ago ¬∑ 796th in this Competition arrow_drop_up 1 more_vert Thanks for sharingÔºÅÔºÅIdea is really inspiring and I lean a lot~ Nabin Koirala452 Posted 24 days ago arrow_drop_up 1 more_vert Wow nice work!!! mahmoudelbahy_ Posted 2 months ago arrow_drop_up 1 more_vert The results are great TonSonKrubb Posted 2 months ago arrow_drop_up 1 more_vert Nice work for this task Roman@Ahmed Posted 2 months ago ¬∑ 2133rd in this Competition arrow_drop_up 1 more_vert Great jobs Satwik Reddy Sripathi Posted 2 months ago arrow_drop_up 1 more_vert Great Approach! Thanks a lot for sharing this. Zahra Alipour Posted 2 months ago arrow_drop_up 1 more_vert The results are fantastic:) FLust3R Posted 2 months ago ¬∑ 4219th in this Competition arrow_drop_up 1 more_vert Nice!! good work Dang Nguyen Le Posted 3 months ago ¬∑ 895th in this Competition arrow_drop_up 5 more_vert What a great accomplishment‚Äîtruly inspiring! Yash_Lalit_Sharma Posted 2 months ago ¬∑ 2072nd in this Competition arrow_drop_up 1 more_vert Thank You Sir, I learned some new tips .I will implement it  in next competetion ! Arindam Singh Posted 2 months ago arrow_drop_up 2 more_vert Thanks for sharing your rule of thumb for feature engineering. I had been exploring a few methods, yours seems better, will try on different datasets to check. Btw Really liked your approach of skipping feature engineering for smaller data, just realised it was 366 rows originally, augmented to 2190. Aminul Islam Arnob Posted 3 months ago arrow_drop_up 1 more_vert Great approach ! Debisree Ray Posted 3 months ago ¬∑ 2308th in this Competition arrow_drop_up 5 more_vert @cdeotte Thank you. Regarding the feature engineering point, could you please clarify what you mean by small versus large data? Also, is there a way to subjectively determine when feature engineering might lead to overfitting versus when it is truly beneficial? Chris Deotte Topic Author Posted 3 months ago ¬∑ 2nd in this Competition arrow_drop_up 9 more_vert Hi. Small and large refer to the number of rows in train data. Assessing whether data is small or large is complicated in Kaggle playground competitions. Because in this competition, train data has 2190 rows which usually can support a few features however it isn't a real 2190 rows because it was created from an original dataset with only 366. This train data is more like 366 real rows, then apply data augmentation to get 2190 rows. So it is still only signal from 366 rows which is very small data. (i.e. you cannot pretend that small data is large data by just apply data augmentation to make more data). ======== People have \"rules of thumb\" regarding how many columns we should make per row. It is something like 10, 25 or 50. So basically with 366 rows, we already have 11 columns which is saturated. If our train data has too many columns per row, then the model will find fake patterns and improve CV score fake because of the \"curse of dimensionality\" - a math concept that says with enough columns we can perfectly classify any dataset regardless of whether the columns have real signal or are just random numbers. Mohamed Drabo Posted 3 months ago ¬∑ 176th in this Competition arrow_drop_up 3 more_vert How to perform classification with too many classes? I don‚Äôt know much about the subject, but I'm working on a project where I need to predict a large number of classes‚Äîaround 2000. I want to predict the appropriate drugs for patients, but it seems impossible due to the high memory usage. Is there another solution to handle this? Muhammad Qasim Shabbir Posted 3 months ago ¬∑ 572nd in this Competition arrow_drop_up 2 more_vert Use less data ‚Äì Remove extra patient info to make the model faster. Use a simple model ‚Äì Try XGBoost or LightGBM, which work well with big data. Predict step by step ‚Äì Don‚Äôt pick from 2000 at once‚Äînarrow it down in stages. or you can  use  deep learning model for classificaiton and  keep batch size small. Hamed Abedi Posted 3 months ago ¬∑ 1174th in this Competition arrow_drop_up 3 more_vert Hi Chris, I think everyone loved your decision to participate in play series. This helps us to learn from you. Thanks again ! Just one question: earlier in this competition, you were the first one to quickly reach perfect 1.0 score on public LB. Can we know how you did it that fast ? because it took several days for the others to achieve that ! Chris Deotte Topic Author Posted 3 months ago ¬∑ 2nd in this Competition arrow_drop_up 6 more_vert When public LB is a small number of rows (like 146) and we know which rows are public LB, then we can just download public code submissions. I downloaded a hundred public notebook submission.csv files (download from all versions of each public notebook). Each notebook has an LB score. Then it is a simple optimization problem. Find the public LB true targets such that the downloaded hundred public notebook achieve the LB score that they say they achieve. Demon Sheriff Posted 3 months ago ¬∑ 1580th in this Competition arrow_drop_up 0 more_vert but isn't this leaderboard probing ? üòÖ 3 more replies arrow_drop_down SeshuRaju üßò‚Äç‚ôÇÔ∏è Posted 3 months ago arrow_drop_up 3 more_vert @cdeotte Thanks for your insights! Do you have any LB probing or submission tricks to better understand the private LB and minimize shakeup? is plans to share the source code for learning purposes?, as i want to play with late submissions. Chris Deotte Topic Author Posted 3 months ago ¬∑ 2nd in this Competition arrow_drop_up 3 more_vert Shakeup in Kaggle tabular data playground competitions is about how many rows are in train and test data. When number of rows is small, we need to be careful not to do too much stuff because it is easy to overfit train data. We also need to mostly ignore public LB. In this situation it is best to build different models without feature engineering and make an equal weight ensemble. When data is large, then we don't need to worry about overfitting. We can feature engineer and make more complicated ensembles and/or stacks. Demon Sheriff Posted 3 months ago ¬∑ 1580th in this Competition arrow_drop_up 4 more_vert I was trying to engineer features based on the days column to check for seasonality in rainfall, but it only made my model worse. I noticed that you trained the model \"as is\" without feature engineering and still achieved a top score. Is this a common trend for small datasets or does it depend on specific nuances of the dataset? agmbennett Posted 3 months ago ¬∑ 2001st in this Competition arrow_drop_up 3 more_vert Hey Chris, thanks for this. I always tend to look for more feature enhancements to boost the score. However, in many of your notebooks I see you often keep original features. How do you make this decision, and how do you find iterative improvements by tuning models / model selection? I seem to hit a wall when tuning or adding more models to an ensemble. Chris Deotte Topic Author Posted 3 months ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Hi. The first thing we always do is create a local validation scheme (to evaluate experiments). Next we need to assess how easy or hard it is to overfit the train data (and artificially improve our CV score). Lastly we need to understand what things we can do when there is a risk of overfitting and what things we can do when there is no risk of overfitting. agmbennett Posted 3 months ago ¬∑ 2001st in this Competition arrow_drop_up 1 more_vert Ok, when you validate locally how are you determining how easy / hard it is to overfit the data? CV only seems to give so much info. 8 more replies arrow_drop_down TooNovice Posted 3 months ago arrow_drop_up 4 more_vert @cdeotte thank you!. I have a question regarding the CV strategy. I understand you have 6 folds because you divide your training base into 6 years. My question is, what would each fold look like? It would be something like: Example of the first fold: ‚Ä¢ Training: years 1, 2, 3, 4, 5 ‚Ä¢ Test (validation): year 6 Second fold: ‚Ä¢ Training: years 1, 2, 3, 4, 6 ‚Ä¢ Test: year 5 ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶. Sixth fold: ‚Ä¢ Training: years 2, 3, 4, 5, 6 ‚Ä¢ Test: year 1 If so, I would understand that this strategy doesn't require the training years to be prior to the validation year, meaning it wouldn't maintain temporal consistency, right? Why wouldn't it be necessary to maintain temporal consistency in this problem?. In general, what CV strategy would you recommend in a binary raining prediction? Thank you! Chris Deotte Topic Author Posted 3 months ago ¬∑ 2nd in this Competition arrow_drop_up 3 more_vert Hi. Your example is correct. I do not think this is real time series data. The original dataset is only 1 year. I believe that Kaggle just ran their synthetic data generator 8 times to generate 6 years of train data and 2 years of test data. So afterward, we have 8 years of data but there is no order to this data. We cannot say one year comes before another year. They are just equally created years. Chris Deotte Topic Author Posted 3 months ago ¬∑ 2nd in this Competition arrow_drop_up 3 more_vert But note even if this was real time series data, sometimes people use a Group K Fold yearly scheme like this. It depends if different years have \"cause and effect relationship\" or \"correlation relationship\". If different years are only correlated but not cause-related, then we can use Group K Fold with years as groups and ignore the order of the years. (For example if we train a model with pairs of years, can the model learn to predict which year comes before which year. If no, then using Group K Fold with years is fine). TooNovice Posted 3 months ago arrow_drop_up 1 more_vert thanks, I see. How could one check effect relationship in a binary rain prediction? My impression is that the rainfall of one year may be correlated with that of another, but I don't see how there could be an effect relationship. 5 more replies arrow_drop_down sashAI Posted 3 months ago arrow_drop_up 1 more_vert Nice work indeed!!! ines ner Posted 3 months ago arrow_drop_up 1 more_vert ‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶.. Roman@Ahmed Posted 3 months ago ¬∑ 2133rd in this Competition arrow_drop_up 1 more_vert Topper share knowledge what are apply this competition where we will learn many steps Muhammad Qasim Shabbir Posted 3 months ago ¬∑ 572nd in this Competition arrow_drop_up 3 more_vert https://www.kaggle.com/code/cdeotte/rapids-svc-w-feature-engineering-lb-0-856 I think this notebook in above disscusion starter notebook at the start of discsusion Versempra Posted 3 months ago arrow_drop_up 1 more_vert Excellent work!!  what an interesting approach that I could learn from! Musti Musik Posted 3 months ago arrow_drop_up 1 more_vert nice!! good work Gaspard Obert Posted 3 months ago ¬∑ 1619th in this Competition arrow_drop_up 1 more_vert Amazing work !! Thanks a lot for sharing your approach ! Too many requests error Too many requests",
      "Binary Prediction with a Rainfall Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 3 months ago Late Submission more_horiz Binary Prediction with a Rainfall Dataset Playground Series - Season 5, Episode 3 Binary Prediction with a Rainfall Dataset Overview Data Code Models Discussion Leaderboard Rules kgmuzu ¬∑ 6th in this Competition  ¬∑ Posted 3 months ago arrow_drop_up 15 more_vert Two XGBoost models + rainfall fraction per group I calculated two OOF with two XGB models, one for the training data and one for the original data. Then combined the predictions with .3 weight for the original data. I also added extra features by grouping by each column and calculating mean, sd, skew and rainfall fraction. What was most prominently picked up by the XGB was the rainfall fraction  (the sum of days in a group divided by the rainy days). I used the same inner fold outer fold structure as Chirs D. last month. That didnt result in a fantastic public leaderboard score but surprisingly did very well for the private LB score. model = XGBRegressor(\n        objective=\"reg:logistic\",\n        max_depth=6,  \n        colsample_bytree=0.9, \n        subsample=0.9,  \n        n_estimators=10000,  \n        learning_rate=0.1,  \n        enable_categorical=False,\n        early_stopping_rounds=100,\n        verbosity=2,\n        eval_metric=['auc'],\n    ) 1 Please sign in to reply to this topic. comment 1 Comment Hotness Onur Ko√ß Posted 3 months ago arrow_drop_up 0 more_vert Can your share your notebook? @kgmuzu Too many requests error Too many requests",
      "Binary Prediction with a Rainfall Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 3 months ago Late Submission more_horiz Binary Prediction with a Rainfall Dataset Playground Series - Season 5, Episode 3 Binary Prediction with a Rainfall Dataset Overview Data Code Models Discussion Leaderboard Rules Spiritmilk ¬∑ 18th in this Competition  ¬∑ Posted 3 months ago arrow_drop_up 22 more_vert 18th place solution: single xgboost with custom AUC loss I‚Äôm glad to have maintained a relatively stable ranking in this shake-up competition. In reality, the competition unfolded in two distinct phases for me. Early stage private score 0.90577 At this stage, my best model was submitted on March 4th. It was a blending of a neural network and GBDT, based on the following excellent notebooks. @aryagokh 's NN notebook @mariusborel 's GBDT notebook The improvements I made can be found in their comments sections. And other contributions I made can be found in the forum. Later stage private score 0.90395 In the later stages of the competition, due to my other personal busy tasks and confusion from my overly messy notebooks, I didn't have the time to organize them. So, I decided to abandon my previous efforts, rewrite the notebooks, and spent the last week refining them. Dataset One thing worth noting is that we know about the public dataset, would it be better to incorporate them during training? However, in my experiments simulating the private leaderboard, only 50% of the cases achieved a better score. I always feel that I have bad luck, so I gave up on adding them during training. Feature Features are from me,such as Some cloud values have better predictive accuracy and from @cdeotte ‚Äôs notebook. For me, among various feature selection methods, Recursive Feature Elimination (RFE) consistently delivers better CV results, but its computational cost is prohibitive. Therefore, I adopted the FORWARD FEATURE SELECTION approach outlined in @cdeotte ‚Äôs notebook for feature selection. Kfold I use Kfold(6) (It is equivalent to groupkfold by year) and Kfold(5,shuffle=True) . Model xgboost with AUC custom loss In fact, someone in an earlier competition used AUC loss and achieved a 4% ranking. https://www.kaggle.com/code/michaelbryantds/auc-custom-loss-function-top-4 Why did I choose XGBoost over other GBDT algorithms or the neural networks used by predecessors? On the one hand, using custom AUC loss with neural networks is very time-consuming. On the other hand, XGBoost's custom loss functions are more practical. For example, when customizing BCE, XGBoost can produce exactly the same results, while LGBM seems to do some internal optimization, leading to slight differences. Moreover, XGBoost made me aware of the difference in base_score. Submission seletion I choose the two notebooks with the highest CV minus public LB. Acknowledgements Thanks  to the authors of the notebooks mentioned above for their significant contributions. Additionally, I would like to express my gratitude to those who actively shared insights in notebooks and forums but were not mentioned. Their dedication and contributions are also greatly appreciated. 2 1 Please sign in to reply to this topic. comment 18 Comments Hotness Dang Nguyen Le Posted 3 months ago ¬∑ 895th in this Competition arrow_drop_up 4 more_vert I'm really impressed; this is simply great. Sarun P M Posted 3 months ago ¬∑ 1808th in this Competition arrow_drop_up 3 more_vert I got a good private 0.89 score with XGB using optuna, but I totally ignore as the submission selection because of its lower public score. A lesson learned. Optimistix Posted 3 months ago ¬∑ 363rd in this Competition arrow_drop_up 3 more_vert Congrats! You did most of the early running, and provided the most insights of any user this episode - I was hoping to see you at the very top, but this is still a great result, and a cool solution too, with the custom loss. cpv Posted 3 months ago ¬∑ 1086th in this Competition arrow_drop_up 1 more_vert Great work, congratulations! Thanks for posting. Dhruv Posted 3 months ago ¬∑ 2944th in this Competition arrow_drop_up 2 more_vert great congrats Chanhee0129 Posted 3 months ago arrow_drop_up 1 more_vert I am beginner. I learned new things in your discussion. Thanks!! kgmuzu Posted 3 months ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Great work and thanks for the outline! Regarding: One thing worth noting is that we know about the public dataset, would it be better to incorporate them during training? However, in my experiments simulating the private leaderboard, only 50% of the cases achieved a better score. I always feel that I have bad luck, so I gave up on adding them during training. I dont get what you are trying to say. A bit more specifically, what do mean with public dataset? How did you simulate the private LB? Spiritmilk Topic Author Posted 3 months ago ¬∑ 18th in this Competition arrow_drop_up 1 more_vert Extract two years from the training set as the test set based on  kfold. Pratyush Sharma Posted 3 months ago arrow_drop_up 1 more_vert Awesome post! Could you also explain how you determine the best model to use? As a beginner, I‚Äôm currently working with PyCaret. Spiritmilk Topic Author Posted 3 months ago ¬∑ 18th in this Competition arrow_drop_up 1 more_vert Submission seletion I choose the two notebooks with the highest CV minus public LB. This is the method I use. klogw Posted 3 months ago ¬∑ 1387th in this Competition arrow_drop_up 1 more_vert Congratulations! Do you think the use of the custom AUC loss was the main reason you achieved such a high rank? I‚Äôve never tried combining XGBoost with a custom AUC loss before, but I‚Äôd love to give it a try while reviewing this competition‚Äîor in future ones! Spiritmilk Topic Author Posted 3 months ago ¬∑ 18th in this Competition arrow_drop_up 1 more_vert Due to the small size of the dataset, it is difficult to determine which factor is the primary cause. To judge whether it is the primary cause, we might need ablation experiments. However, I believe it is indeed one of the important factors. Comparing the results with and without its use, there has been an improvement in both CV, private score, and public score. (But note that I did not select the notebook with the highest public score.) Hamed Abedi Posted 3 months ago ¬∑ 1174th in this Competition arrow_drop_up 1 more_vert I really learned a lot from you in this competition! Congrats! One question: I also had an XGBoost model which would grant 130th position if selected, But that had one of my worst performance in CV and public LB and I didn't select it. TL'TR, It seems XGBoost really worked well in the private LB. Did you have similar experience ? or your XGBoost model was consistent between CV and LB ? Spiritmilk Topic Author Posted 3 months ago ¬∑ 18th in this Competition arrow_drop_up 1 more_vert Yes, XGBoost seems to have a better private LB, and it's the only single model of mine that achieved a score above 0.90. And I've always believed that under the same CV conditions, the better the public score, the better the private score tends to be; conversely, the worse the public score, the better the private score tends to be. Similar discussions can be found here . It turned out that this observation was roughly correct. The model with the largest CV-LB gap gave me the highest private LB. I haven't encountered a situation where both the CV and public LB were the worst, yet the private LB was the best. But I suppose you could check whether the gap in your XGBoost model is the largest or relatively large. Hamed Abedi Posted 3 months ago ¬∑ 1174th in this Competition arrow_drop_up 1 more_vert The model with the largest CV-LB gap gave me the highest private LB. Oh wow! interesting observation! I saw that the models performance have seasonality (one work better for specific days and the other work better on the rest of the year as shown in below plot). But I couldn't take benefit of that. I think your approach somehow has this benefit as the first 146 days of the year used for public LB and you selected a model to work better on the rest of the year (other days repeated two times in the private LB, but first 146 days only appears once in it). and you selected a model to have a good CV performance but not good in first 146 days. Good job again, and thanks for sharing! yunsuxiaozi Posted 3 months ago ¬∑ 1356th in this Competition arrow_drop_up 1 more_vert I looked at the feature selection method in Chris's notebook, which is to multiply the original features pairwise to construct 55 features, and then add one feature to the data each time. If the effect is better, it will be retained, and if the effect is not good, it will be deleted. This method cannot traverse all situations and may fall into local optima? Spiritmilk Topic Author Posted 3 months ago ¬∑ 18th in this Competition arrow_drop_up 1 more_vert Yes. Moreover, this method depends on the order. This is a trade-off between time and efficiency. Additionally, we don't need to excessively worry about getting stuck in a local optimum. Because the separate operations of feature engineering and parameter tuning we widely use are already at a local optimum. It would require too many resources to reach the global optimum. Chris Deotte Posted 3 months ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Congratulations Spiritmilk surviving the shakeup and staying top ranked on both public and private LB! That custom AUC loss for XGBoost looks great. I will explore it and use it in future competitions. Thanks! Spiritmilk Topic Author Posted 3 months ago ¬∑ 18th in this Competition arrow_drop_up 1 more_vert Thanks, I've also learned a lot from your sharing! Too many requests error Too many requests",
      "Binary Prediction with a Rainfall Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 3 months ago Late Submission more_horiz Binary Prediction with a Rainfall Dataset Playground Series - Season 5, Episode 3 Binary Prediction with a Rainfall Dataset Overview Data Code Models Discussion Leaderboard Rules Kirderf ¬∑ 37th in this Competition  ¬∑ Posted 3 months ago arrow_drop_up 12 more_vert 37th place solution - TabPFN with only comp. data and basic FE. My approach for the solution: As the dataset was extremely small TabPFN model is a great choice, also skipping the feature engineering, e.g. creating more features, to reduce the risk of overfitting on the small dataset, only handling basic FE. Also skip adding any other dataset as it could affect the distributions between the small train and test dataset as they were generated from the same LLM. Model framework: https://github.com/PriorLabs/TabPFN Result: Using only TabFN model and framework, simple feature engineering and only the synthetic generated data gave the best score on the private score. Other: Next best score I had with XGB, trained with adding the extra original data to the generated. XGB is usually the best pick with binary dataset. In the mirror one maybe should have ensembled them both and had a better score. That's it! Happy Kaggling! Please sign in to reply to this topic. comment 2 Comments Hotness Hamed Abedi Posted 3 months ago ¬∑ 1174th in this Competition arrow_drop_up 0 more_vert Thanks for your valuable insight ! Also, I just saw your profile, it seems that you have been unlucky many times! 57 competition medals, but didn't get any gold yet, I hope you a good luck and reaching your potential ‚úåüèº Vladimir Demidov Posted 3 months ago arrow_drop_up 1 more_vert There are several individuals in a similar situation in the Competitions section at Kaggle, but he is the most prominent example among all, I guess. This shows how difficult it is to compete solo. Hamed Abedi Posted 3 months ago ¬∑ 1174th in this Competition arrow_drop_up 1 more_vert Oh man, you also had this bad luck I think ! Wow ! IMO, Kaggle competition titles should change in a way that these people you mentioned (like you and him) get the Master or GM title in competitions. maybe for example 10 silvers count as 1 gold at least ! But I believe even if that does not happen, you will soon get many golds and GM title ! Good luck ! Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 5, Episode 3 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Rainfall Prediction using Machine Learning dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 183.8 kB csv MIT Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 183.8 kB sample_submission.csv test.csv train.csv 3 files 27 columns  Too many requests",
    "data_description": "Binary Prediction with a Rainfall Dataset | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 3 months ago Late Submission more_horiz Binary Prediction with a Rainfall Dataset Playground Series - Season 5, Episode 3 Binary Prediction with a Rainfall Dataset Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2025 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: Your goal is to predict rainfall for each day of the year. Start Mar 1, 2025 Close Apr 1, 2025 Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each id in the test set, you must predict a probability for the target rainfall . The file should contain a header and have the following format: id ,rainfall 2190 , 0 . 5 2191 , 0 . 1 2192 , 0 . 9 etc . content_copy Timeline link keyboard_arrow_up Start Date - March 1, 2025 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  March 31, 2025 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Elizabeth Park. Binary Prediction with a Rainfall Dataset. https://kaggle.com/competitions/playground-series-s5e3, 2025. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 9,768 Entrants 4,545 Participants 4,381 Teams 38,665 Submissions Tags Beginner Tabular Time Series Analysis Weather and Climate Roc Auc Score Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s5e4",
    "discussion_links": [
      "/competitions/playground-series-s5e4/discussion/575784",
      "/competitions/playground-series-s5e4/discussion/575840",
      "/competitions/playground-series-s5e4/discussion/575862",
      "/competitions/playground-series-s5e4/discussion/575782",
      "/competitions/playground-series-s5e4/discussion/575839",
      "/competitions/playground-series-s5e4/discussion/575783"
    ],
    "discussion_texts": [
      "Predict Podcast Listening Time | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 months ago Late Submission more_horiz Predict Podcast Listening Time Playground Series - Season 5, Episode 4 Predict Podcast Listening Time Overview Data Code Models Discussion Leaderboard Rules Chris Deotte ¬∑ 1st in this Competition  ¬∑ Posted 2 months ago arrow_drop_up 220 more_vert 1st Place - RAPIDS cuML Stack - 3 Levels! Thank you Kaggle for a great playground competition. This month's playground competition has a great dataset with lots of interesting patterns within! (and it felt more like real data and less like synthetic data). And there are lots of strong competitors which made this competition fun and exciting! The target Listening_Time_minutes is approximately equal to the linear relationship 0.72 x Episode_Length_minutes as described in my 3 discussion posts here , here , here . The other 9 features modulate this linear relationship. Based on this insight, I stacked the following approaches: My favorite solution for a Kaggle competition is a single model , my second favorite solution is hill climbing ensemble . Neither of these solutions could win 1st place in Kaggle's April playground competition because the data has too many interactions and too many deep patterns. For Kaggle's April playground competition, we need a large diverse deep RAPIDS cuML stack of 3 levels ! Hill Climbing (linear level 2 model) versus Stacking (non-linear level 2 model) Hill climbing (or ridge) ensemble generally works well. However in this competition, the dataset was so complicated that a deep stack was the best solution. The most important feature is Episode_Length_minutes . It contains 90%+ of the signal. But it is missing for 11.6% of the data! This means there are two scenarios; Predict target Listening_Time_minutes with Episode_Length_minutes Predict Listening_Time_minutes without Episode_Length_minutes Hill climbing (and ridge) cannot do this (because it uses a linear level 2 model). Imagine that we make one model that does great predicting target with ELM and we build a second model that does great predicting target without ELM. Hill climbing will just take a weighted average of all predictions. But a stack (non-linear level 2 model) will use predictions from one model when predicting with ELM and use the predictions from another model when predicting without ELM. In other words, instead of taking all predictions from all models, it will take the best predictions from each model (for different situations)! RAPIDS cuML Stack - 3 Levels of Models! The secret to building a strong stack is diverse models. (And every model trains with the same 5 KFolds and we must remove all leaks from target encoding, pseudo labeling, etc). Diversity comes from different feature engineering and different models (and/or model hyperparameters). For each new model I built, I engineered different sets of features using the speed of RAPIDS cuDF . Each model has different customized features that benefit the new model best. And I trained lots of diverse models using the speed of RAPIDS cuML ! All models below use the speed of GPU ! Diversity x5 To add diversity to our stack we can take each of the 12 model depicted above and train it in at least  5 different ways described below. Additionally, we can change feature engineering and/or hyperparameters and train more ways. My final stack uses 75 models . So I approximately created each of the above 12 models in 6 different ways! Every day during the month of April, I spent a few hours and built new diverse models. Using 3xA100 GPU and the speed of RAPIDS cuDF and cuML I would build about a dozen new models (with new complex feature engineering) each day and keep the few models which improved my stack! (1) Different Sets of Feature Engineering The typical way to predict Listening_Time_minutes is to train a model using KFold and all columns of train.csv . Additionally we can create more columns with feature engineering. We can build multiple GBDT models each using different engineered features. This provides diversity to our stack. Also we can change GBDT hyperparameters. For example, some times we use max_depth=10 and sometimes we use max_depth=0, max_leaves=1024 . These find different interactions and create diverse models. Furthermore, sometimes we can use max_depth=20 to get more interaction and sometimes max_depth=5 for less interaction. Below are 4 other ways to train models in April's playground competition. (2) Remove Episode_Length_minutes from All Rows! Based on my discussion here , the feature/column Episode_Length_minutes is important. We can remove Episode_Length_minutes from all rows and train a model to predict Listening_Time_minutes from all other columns. These models will be strong predicting target when Episode_Length_minutes is missing. And the stack will use these models when appropriate. (3) Predict Ratio of Target divided by Episode_Length_minutes Based on my discussion here , for each model, we can create a new target with train['new_target'] = train.Listening_Time_minutes / train.Episode_Length_minutes . We can train models to predict this new target. We can then multiply this prediction by Episode_Length_minutes or an imputed value of Episode_Length_minutes from below. (4) Predict Episode_Length_minutes (use Train.csv and Test.csv) Based on my discussion here , the feature Episode_Length_minutes is so important, we can train models to predict Episode_Length_minutes . Futhermore, we can use both train.csv and test.csv data to train and predict Episode_Length_minutes . Because both train.csv and test.csv have all the columns necessary. Afterwards, we can use these ELM predictions in at least 3 ways. (1) We can impute missing values with these ELM preds then train a model. (2) We can replace every row's ELM (both missing and non-missing) with these ELM preds, then train a model. (3) We can multiply these ELM preds by the Ratio preds (from above) to predict the target Listening_Time_minutes . All 3 of these ideas will make new diverse models! (5) Pseudo Label (use Train.csv and Test.csv) Based on my discussion here , we see that many columns are important. We can use more information from more columns by using the columns from test.csv .  We can add test.csv data with pseudo labels to the training of all our models. Stacking Models CV Scores Below are the CV scores for level 1, level 2, and level 3 models (without pseudo labeling). The LB scores are basically the same as the CV scores: Level 1 Model Notes CV Score RAPIDS cuML Lasso uses 6000 features! 13.2 RAPIDS cuML SVR uses 6000 features! 13.2 RAPIDS cuML KNN Regressor k=51, weight by distance 12.8 RAPIDS cuML Random Forest max_depth = 32 12.1 NN - MLP Built by ChatGPT 12.0 NN - TabPFN 20x \"SUBSAMPLE_SAMPLES\": 10_000 13.2 GBDT - XGBoost 4x models with 4x feature sets 11.8 GBDT - LGBM diverse from XGBoost 11.8 GBDT - Boost over RAPIDS Lasso predict Lasso residuals 11.9 GBDT - Boost over RAPIDS SVR predict SVR residuals 11.9 GBDT - Boost over NN MLP predict MLP residuals 11.9 AutoML AutoGluon public notebook here 12.4 --- --- --- Level 2 Model Notes CV Score GBDT XGBoost uses 73 level 1 models 11.56 NN - MLP uses 73 level 1 models 11.56 --- --- --- Level 3 Model Notes CV Score Weighted Average 50% / 50% 11.54 . CREDITS: Thank you @pirhosseinlou for XGBoost single model here and @greysky for LGBM single model here which I used as two of my XGBoost \"4x models with 4x feature sets\" (and then made a dozen variations of). And thank you @itasps for your AutoML AutoGluon model here . I incorporated all 3 of these public models into my final stack (by re-running with my stack's KFolds and then making a dozen variations of each)! Final Submission - CV 11.54, Public LB 11.51, Private 11.44, First Place! My final RAPIDS cuML stack has CV 11.54, Public LB 11.50, Private 11.44, First Place! Post Comp Analysis Now that the comp ended, I compare Hill Climb to Stack with my 73x L1 models: Hill Climbing - CV 11.64 - Public LB 11.57 - Private LB 11.503 Stack - CV 11.54 - Public 11.51 - Private 11.448 21 26 33 12 Please sign in to reply to this topic. comment 121 Comments 12 appreciation  comments Hotness Muhammad Khibban I'tishom Posted a month ago arrow_drop_up 1 more_vert Congratulations on coming in first place! I have two questions about how you trained TabPFN: Why did you choose 20 as the number of subsamples? Is there a recommended way to determine the appropriate number of subsamples? As far as I know, TabPFN was optimized for datasets with ‚â§1000 rows. Why did you choose the 10.000 rows in each subsample instead of 1000? ABHAY SINGH Posted a month ago arrow_drop_up 1 more_vert Thankyou for your solution. Miku Tsuchiya Posted a month ago ¬∑ 672nd in this Competition arrow_drop_up 2 more_vert @cdeotte Congratulations on coming in first place. Even now that the competition deadline has passed, I am still studying, referring to the solutions of the top competitors. I have one question. What is the RMSE (loss function) for predicting Episode_Length_minutes? In my testing, the value was quite large, about 30, with almost no feature engineering, so I don't know whether the training is going well or if there is a mistake in the code. If you don't mind, could you please tell me? Chris Deotte Topic Author Posted a month ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert I forget. But the way to tell if your model is learning or not is to compare your model to predicting just the mean. So, use the mean of Episode_Length_minutes as a prediction for all rows and compute the RMSE from that. That is the baseline. Then see if your model does better than that. If it does better than that, then your model is learning. Miku Tsuchiya Posted a month ago ¬∑ 672nd in this Competition arrow_drop_up 0 more_vert @cdeotte Thank you for your answer! When the average value of Episode_Length_minutes was used as the predicted value for all rows, the RMSE was \"32.964394\", while the RMSE of the model I created was \"31.63631\". In other words, it's a slight improvement, but it doesn't seem to be much different from when the average value was used. It doesn't seem to be a program error, so just finding this out was a great learning experience. I'd like to continue studying. Bryan Yahir Hernandez Posted a month ago arrow_drop_up 1 more_vert Congrats on your win! This was an impressive solution. What was your CV setup? Chris Deotte Topic Author Posted a month ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thank you. CV was 5 KFold. Towhidul Islam Posted a month ago arrow_drop_up 0 more_vert Thanks for sharing here. Vladimir Demidov Posted 2 months ago ¬∑ 34th in this Competition arrow_drop_up 10 more_vert What kind of conversation with ChatGPT should be to build such a good NN? Is it crafted anything surprising, a breakthrough? Anyway, that approach deserves a guide to be published. Chris Deotte Topic Author Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 13 more_vert Thanks! üòÄ ChatGPT suggested the MLP architecture and ChatGPT wrote the vectorized code to normalize numerical features. The architecture isn't fancy, we could have made it ourselves. But it saved time with trial and error. I described my features and some properties of the data and it produced the architecture below. The NN is trained 10 epochs with reduce on plateau learning schedule. Batch size 512 with start LR=1e-3 Adam optimizer. The easiest way to make strong NN features is just take all the features from our strongest GBDT. Then we One Hot Encode all the categoricals and normalize (subtract mean divide std) all the numericals. Then finally impute missing to zero. (We could alternatively make embeddings but the following is the easiest way to get another quick NN for stack). I also built other NNs from scratch with new feature engineering from scratch, but below is the simplest conversion from GBDT to NN: Preprocess to convert GBDT Features to NN Features print ( \"Normalizing...\" , end= '' ) norm_cols = [c for c in X_train.columns if c not in CATS ] means = X_train [norm_cols] .mean ()\nstds = X_train [norm_cols] .std ()\nstds = stds .replace ( 0 , 1 )\nX_train [norm_cols] = (X_train [norm_cols] - means) / stds\nX_valid [norm_cols] = (X_valid [norm_cols] - means) / stds\nX_test [norm_cols] = (X_test [norm_cols] - means) / stds print ( \"done\" ) print ( 'Before:' ,X_train.shape, X_valid.shape, X_test.shape) combined = pd .concat ( [X_train,X_valid,X_test] ,axis= 0 )\ncombined = pd .get_dummies (combined, columns =CATS )\nX_train = combined .iloc [:len(X_train)] X_valid = combined .iloc [len(X_train):len(X_train)+len(X_valid)] X_test = combined .iloc [len(X_train)+len(X_valid):] print ( 'After:' ,X_train.shape, X_valid.shape, X_test.shape) del combined print ( \"Impute missing\" ) X_train = X_train .fillna ( 0.0 )   \nX_valid = X_valid .fillna ( 0.0 )   \nX_test = X_test .fillna ( 0.0 ) content_copy NN Architecture from tensorflow .keras .layers import BatchNormalization, Dropout\nfrom tensorflow .keras .layers import Activation\n\ndef build_model (size=len(FEATURES)):\n    x_in = Input (shape=(size,))\n    x = Dense ( 512 )(x_in)\n    x = BatchNormalization ()(x)\n    x = Activation ( 'relu' )(x)\n    x = Dropout ( 0.3 )(x)\n\n    x = Dense ( 256 )(x)\n    x = BatchNormalization ()(x)\n    x = Activation ( 'relu' )(x)\n    x = Dropout ( 0.3 )(x)\n\n    x = Dense ( 128 , activation= 'relu' )(x)\n    x = Dense ( 1 , activation= 'linear' )(x)\n\n    model = Model (inputs=x_in, outputs=x)\n    return model content_copy Roman@Ahmed Posted 2 months ago ¬∑ 1696th in this Competition arrow_drop_up 2 more_vert Your detailed explanations and clear thought process are truly enlightening. Thank you for sharing your knowledge neednot_toplay Posted 2 months ago ¬∑ 468th in this Competition arrow_drop_up 1 more_vert its detailed and well structured. Bilal Awais Posted 2 months ago arrow_drop_up 3 more_vert Thanks for initiating this insightful discussion! I've noticed that feature interactions and categorical encoding play a big role in improving model performance in this competition. Using techniques like target encoding or frequency encoding on the key categorical features gave my model a nice boost. Has anyone tried combining these with stacking or ensembling? I'd love to hear your thoughts and results! David Abert Posted 2 months ago arrow_drop_up 2 more_vert Hello Chris, thanks for sharing your winning solution. I got a basic question that I am sure you already answered tons of times: How do you evaluate a new set of feature? I mean, over which model do you evaluate them? Do you create a basic model and train on that? Do you perform CV on that model to evaluate it? Do you also tune its hyperparameters? Chris Deotte Topic Author Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Hi. I build each model individually. So for linear regression I build one set of features (mainly OHE and products of OHE). For GBDT i build another set of features (lots of group by aggregation stuff), etc etc Then i add a few dozen at a time and evaluate CV score. If CV improves I keep the dozen, otherwise i discard them. David Abert Posted 2 months ago arrow_drop_up 1 more_vert Thanks for your response. When you create features, do you craft them manually or you automatize it with an script? Feel free to not reveal it if its a secretüôÇ‚Äç‚ÜïÔ∏è Hamed Abedi Posted 2 months ago ¬∑ 425th in this Competition arrow_drop_up 5 more_vert Congratulations on your first place again! It‚Äôs impressive that you shared such valuable insights throughout the competition and still managed to win by such a large margin! You are a true legend! I saw your interviews on Youtube videos, and I should admit that you are a great teacher as well! Two questions by the way: How do you determine that spending time, training weaker models (e.g., those with a 13.2 CV score in your case) would add more value to your final ensemble compared to focusing on a few stronger models? My intuition tells me to build several high-performing models first and then ensemble them; In general, how do you identify which weaker models will bring useful diversity? How do you see the future with the progress of LLMs? you mentioned that your ChatGPT-based approach could easily hit a 12.0 CV score (Unfortunately same as my best score! üòÄ), something that wasn‚Äôt possible a few years ago. In your view, will these models soon be able to benefit from their multi-modality and explore different methods and outperform human data scientists? And what uniquely human power, do you think computers will never replicate? Chris Deotte Topic Author Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 5 more_vert Thank you. Great questions. (1) When building an ensemble it is all about diversity and nothing about performance. I have models with CV score 26 RMSE which boost my stack CV score and LB score! (These are models that use all columns except Episode Length Minutes). For me building diverse models is the fun part. Each day I say \"how can i solve this problem in a completely new way unlike anything i have done before?\". In general if it is completely new approach it will boost stack CV score and LB score everytime! And if it doesn't then no big deal because it was fun to try something unconventional. (2) This is a very good question. I worked 1-4 hours every day for 1 month. Each day I built new novel models and each day 1 or 2 models boosted my stack CV score and LB score. I believe that soon LLM will be able to do everything I did over 1 month in just a few hours with enough compute (like 1000 GPU). It's crazy to imagine. Hamed Abedi Posted 2 months ago ¬∑ 425th in this Competition arrow_drop_up 1 more_vert That's very insightful, I will test new methods in my ensemble next time! Also, I hope that the future LLM still needs our mentorship ! üòÄ Chris Deotte Topic Author Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 5 more_vert UPDATE: Now that the comp ended, I compare Hill Climb to Stack with my 73x L1 models: Hill Climbing - CV 11.64 - Public LB 11.57 - Private LB 11.50 Stack - CV 11.54 - Public 11.51 - Private 11.44 rshukla4 Posted 2 months ago ¬∑ 1832nd in this Competition arrow_drop_up 3 more_vert Hello Chris, Thank you so much for sharing this approach. I haven‚Äôt participated in a competition for a while, but it‚Äôs really amazing to see how intricate and thoughtful your method was. Heading into the summer, I look forward to learning more from you and others. I‚Äôm happy to be back on Kaggle after a long long break. See you in the Predict Calories Expenditure! Shahzad Hassan Posted 2 months ago arrow_drop_up 3 more_vert As new in the field, can we get the entire notebook repository for learning please. Shahzad Hassan Posted 2 months ago arrow_drop_up 4 more_vert would be thankful if we can get the detail solution notebook please Zahra_Bashirrr Posted 2 months ago arrow_drop_up 1 more_vert Thank you so much for sharing such an insightful and comprehensive breakdown of your winning approach in the Kaggle Playground competition. Your dedication to experimenting with diverse models, feature engineering, and stacking techniques using RAPIDS cuML is truly inspiring. The depth of your analysis and the strategic methodology you employed showcase not only your expertise but also your passion for data science and machine learning. Your willingness to share these valuable insights and lessons learned with the community is greatly appreciated and exemplifies the spirit of collaboration and learning that makes Kaggle such a vibrant and enriching platform. Congratulations on your well-deserved first-place finish, and thank you once again for your generosity in sharing your knowledge! Glitch Posted 2 months ago ¬∑ 67th in this Competition arrow_drop_up 3 more_vert Thanks for sharing amazing insight again highly intutive Chris Deotte Topic Author Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Thank you Glitch! Congrats on your top 100 finish as rank 67th! Pascal Terpstra Posted 2 months ago ¬∑ 17th in this Competition arrow_drop_up 4 more_vert Very impressive solution, and thank you for your detailed explanations -- this makes Kaggle such a fun place! Do you think scores in previous playground competitions (specifically regression settings) could have improved significantly if more models were included? If not, why is that? Is it because this dataset contains too many deep patterns? And how do you realize that the data contains many deep patterns and could be significantly improved with ensembling (just trial and error)? Chris Deotte Topic Author Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert This is a great question. I don't have an exact answer. In this comp it was just an intuition. Many things provided hints: My first single model with lots of feature engineering was beat by public notebook AutoML AutoGluon. My model had CV/LB 12.5 and AutoML had CV/LB 12.4. This was weird because AutoML has never beat my single models before. (AutoML doesn't feature engineer nor target encode, so it was very surprising to see such good performance here). Next I noticed that GBDT performed better and better with very deep trees. The best single model public notebook used max_depth=19 . That is weird. I have never seen that before. In the first week, other Kagglers were beating me on LB. I was working hard feature engineering and building a strong single model, so I was surprised that I wasn't higher on LB in the first week. I tried to guess what these other Kagglers were doing. Based on their previous solutions to previous comps, I assumed that were stacking or ensembling. Then when doing EDA, I saw there were so many \"if else cases\". Like when Episode Length Minutes is not missing it is most important to predict target. When it is missing other columns are most important. And there is so much interaction like this podcast on this day at this time with this sentiment has different target. So much \"this with this with this with this\". All these clues indicated that one model was not going to be enough to extract all the signal here. We need an assortment of diverse models. Then we need an L2 stacking model to put it all together. (However in retrospect 2nd place did build a single model which is amazing!) And the icing on the cake was went I built my first stack. At one point when i was 3rd or 4th on LB in the first week, I took all of my previous experiments and just trained an XGB on their OOF and Test PRED. (It is good practice to save all OOF and Test PRED to disk for every experiment). To my surprise, the CV score and LB score was great and I jumped into first place LB. From that point on, I continued with stack and diversity approach. narsil (jobs-in-data.com) Posted 2 months ago ¬∑ 423rd in this Competition arrow_drop_up 1 more_vert Awesome work - thanks! A question re hardware acceleration - very much needed for hundreds of models. 1/ What is the speedup for XGB and LGBM for CPU vs GPU? 2/ Do you have proven code samples for XGB and LGBM using GPU acceleration? Chris Deotte Topic Author Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Thanks. It depends on the specific GPU and specific CPU. I think XGB is like 10x faster on GPU. I'm not sure about LGBM. I do know that LGBM has two GPU versions. If you just pip install LGBM and run it on GPU, it doesn't use as much GPU as if you recompile it to use GPU (more info here ) narsil (jobs-in-data.com) Posted 2 months ago ¬∑ 423rd in this Competition arrow_drop_up 1 more_vert Thanks , I was observing something very similar re lack of GPU speedup with lgbm Towhidul Islam Posted 2 months ago arrow_drop_up 1 more_vert I will have some questions to ask. So, I would expect your responses at that time. Such an outstanding output! Parth Rait Posted 2 months ago ¬∑ 100th in this Competition arrow_drop_up 1 more_vert Truly astounding i didnt think half of it like and seeing ensemble used like this was not on my todo list , tho i have a question how to reduce computing power because my laptop is not able to handle most of these codes and i have to use basic or even just bad versions of such codes Robin van Hoorn Posted 2 months ago ¬∑ 485th in this Competition arrow_drop_up 1 more_vert Awesome solution. I learned a lot. Can you explain how the level 2 model is able to distinguish between when an instance might have had a missing episode length minutes? This is important for the aggregation, as you point out, but this information does not seem to be available to the level 2 model? Do you add a flag to your level 2 model? Or does it somehow infer this info? Chris Deotte Topic Author Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Hi. Thanks! Yes, I add a flag. I engineer features from train.csv and add them to my L2 model in addition to the columns of L1 OOF. One features is df['ELM_nan'] = df.Episode_Length_minutes.isna().astype('float') . Pham Le Tu Nhi Posted 2 months ago ¬∑ 1834th in this Competition arrow_drop_up 1 more_vert So cool! I didn't think of all these at all. Thank you! (learning learning learning) Edwin Daniels Posted 2 months ago ¬∑ 866th in this Competition arrow_drop_up 1 more_vert Congrats on the win! The exhaustive effort and detailed write up is impressive! Alif Sathar Posted 2 months ago arrow_drop_up 1 more_vert that's very impressive how much work there. Thank you for sharing! <3 Johannes Heller Posted 2 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Congratulations on an impressive and well-deserved 1st place, once again! Your approach definitely includes a lot more diversity which was probably a key to your consistently strong lead throughout the competition. I had an intuition about using Episode_Length_minutes as a target feature. I tried predicting the ratio but it didn‚Äôt really work out. I've experimented with RAPIDS in Kaggle Notebooks but unfortunately didn't get it to work on my local PC (Windows with RAPIDS running on WSL2). Nyok Mawut Posted 2 months ago ¬∑ 2866th in this Competition arrow_drop_up 1 more_vert \"Congrats on your Kaggle win, Chris! üéâ Brilliant achievement. Would love to learn from your solution‚Äîcould you share your winning model? Thanks and cheers! - Nyok Mawut\" nyokmawut95@gmail.com Thank you!!! Anas Posted 2 months ago arrow_drop_up 1 more_vert its awesome work! applause Too many requests error Too many requests",
      "Predict Podcast Listening Time | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 months ago Late Submission more_horiz Predict Podcast Listening Time Playground Series - Season 5, Episode 4 Predict Podcast Listening Time Overview Data Code Models Discussion Leaderboard Rules Farukcan Saglam ¬∑ 2nd in this Competition  ¬∑ Posted 2 months ago arrow_drop_up 75 more_vert 2nd Place |¬†Single LightGBM and Target Encoding My solution consists of a single LightGBM model and target encoding of features. I've already shared 90% of the work publicly. I've never been great at writing up solutions, so I just shared the data generation notebook . The final processed training dataset contains 1552 features and 794868 rows. Fortunately, with careful data type casting and avoiding unnecessary copy operations, I was able to train the model on Kaggle CPUs. Training takes about 4 hours. I trained on all the data using 5 different seeds. LightGBM Hyperparameters objective = 'l2' metric = 'rmse' n_iter = 12000 max_depth = 15 learning_rate = 0.008 num_leaves = 480 colsample_bytree = 0.25 New Features Mul_Hpp_Elm = Host_Popularity_percentage * round(Episode_Length_minutes) Mul_Gpp_Elm = Guest_Popularity_percentage * round(Episode_Length_minutes) Rounded_Episode_Length_minutes = round(Episode_Length_minutes) // 2 Rounded_Host_Popularity_percentage = round(Host_Popularity_percentage) // 2 Rounded_Guest_Popularity_percentage = round(Guest_Popularity_percentage) // 2 Target Encoded Features | pair_size = [1, 2, 3, 4, 5, 6] Podcast_Name Episode_Length_minutes Episode_Num Episode_Sentiment Host_Popularity_percentage Guest_Popularity_percentage Number_of_Ads Publication_Day Publication_Time Rounded_Episode_Length_minutes Rounded_Host_Popularity_percentage Rounded_Guest_Popularity_percentage Descriptive Statistics on Target Encodings (Column-wise) Mean, standard deviation, min, max (aggregated globally) Mean, standard deviation, min, max (aggregated by pair_size) Mean, standard deviation, min, max (aggregated by source column) 15 Please sign in to reply to this topic. comment 38 Comments 1 appreciation  comment Hotness Chris Deotte Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 9 more_vert Absolutely Amazing @greysky ! I did not think it was possible to build such a high performance single model. I am very impressed. And the fact that you trained on Kaggle jupyter notebooks is even more impressive! Congratulations on your 2nd place solo finish. Thank you for sharing your solution code. I plan to study all your feature engineering techniques. And thank you for all your sharing during the competition. It was very helpful, I reviewed and studied all your code during the competition and incorporated ideas into my final solution. Farukcan Saglam Topic Author Posted 2 months ago ¬∑ 2nd in this Competition arrow_drop_up 3 more_vert Thank you for your kind words, @cdeotte . Thanks to your answers in the discussions, I used cuDF and cuML for the first time for data manipulation. If I hadn't used those libraries, I wouldn't have been able to conduct enough experiments to achieve 2nd place. The rounded features were responsible for my leaderboard jump in the final weeks. Srijan004 Posted 2 months ago ¬∑ 361st in this Competition arrow_drop_up 5 more_vert Congrats! @greysky . I have a doubt that why do LightGBM performs so much better than most of the other algorithms? For example, even a single LGBM with no feature engineering at all scores ~12.57 LB, which is like already in top 25%. Farukcan Saglam Topic Author Posted 2 months ago ¬∑ 2nd in this Competition arrow_drop_up 4 more_vert In most scenarios, I get better scores with LightGBM, but I'm not sure if it's because I'm more experienced with that framework or if it's really that good. Optimistix Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 4 more_vert To add to what @greysky said - in many playground competitions, one of the \"Big 3\" GBDTs (XGB, LGBM, CATB) seems to do better than the others, for reasons that may not always be clear. Nonetheless, all 3 are reliably strong performers on tabular data, and in-depth knowledge of any one will usually help you squeeze more out of it than others. Sylvester d'Almeida Posted 2 months ago ¬∑ 2207th in this Competition arrow_drop_up 1 more_vert So my question is, are these 3 (XGB, LGBM,  and CATB) enough to learn and use? Since there isn't any out there that does better? Optimistix Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Enough for what? LuminousC Posted 2 months ago ¬∑ 56th in this Competition arrow_drop_up 6 more_vert Such a cool solution with a single model plus careful feature engineering work. Congratulations. Hope to see you in further competitions. MG√∂ksu Posted 2 months ago arrow_drop_up 3 more_vert Congrats and thanks for sharing! How do you decide which model hyperparameters to go with? Do you search for new ones every time you add a new feature? I mean, do you have a systematic approach for selecting them? Farukcan Saglam Topic Author Posted 2 months ago ¬∑ 2nd in this Competition arrow_drop_up 7 more_vert Thank you! There are a huge number of features with strong predictive power. Because of that, I set a low colsample_bytree and high max_depth and num_leaves values for this problem. I didn‚Äôt change the hyperparameters much and didn‚Äôt perform hyperparameter tuning due to the long training time. When I add new features, I run the feature engineering pipeline from the beginning and measure the CV score on just one fold using same hyperparameters. Marcel Posted 2 months ago ¬∑ 473rd in this Competition arrow_drop_up 0 more_vert Hey Farukcan, quick follow‚Äëup: When you batch new features and test them on one fold with your usual params, what happens as your feature count approaches ~1.5‚ÄØK‚Äîdo you still run the full ~3‚ÄØ000 boosting rounds on all ~700‚ÄØK rows each time? Doesn‚Äôt that take hours per experiment? At what point do you throttle things down (fewer rounds, smaller sample, etc.) to keep iteration time reasonable, or do you just accept the longer waits as the cost of validating new features? Farukcan Saglam Topic Author Posted 2 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert This is where Kaggle‚Äôs parallel computing quota comes in. I usually run up to 5 different ideas (feature engineering, hyperparameters, etc.) in parallel using Kaggle notebooks. Once they‚Äôre done, I launch another 5, and so on. With around 1.5K features, each experiment takes approximately 1 hour. This approach lets me run dozens of experiments in a short time. I use the same technique when ensembling random seeds for submission too. paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 0 more_vert Does running more notebooks at a time slow how fast each notebook can run? Farukcan Saglam Topic Author Posted 2 months ago ¬∑ 2nd in this Competition arrow_drop_up 3 more_vert I'm sharing the training notebook with seed=42 (each model uses its respective target encoding seed): https://www.kaggle.com/code/greysky/podcast-model-training Ge Cong Posted 2 months ago ¬∑ 44th in this Competition arrow_drop_up 4 more_vert Congrats! Your solution reminds us the possibilities Kaggle CPU instance could offer! Optimistix Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 4 more_vert Congrats! Super cool to place 2nd with a solo model, and using only Kaggle CPUs. Johannes Heller Posted 2 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Congratulations on an amazing 2nd place! My best single models were LGBM, too, with Target Encoding being the most important features, by far. But my best single models are far away from your model! Farukcan Saglam Topic Author Posted 2 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Congratulations to you as well, Johannes. If you had a little more time, you could have passed me in the LB. I didn't think the stacking method would improve the score that much. Johannes Heller Posted 2 months ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert I really like the statistics (mean, std, min, max) you computed on the TE features. I haven't seen that used elsewhere. For such an extremely TE-heavy approach (mine was, too), this might have provided a significant boost. Did you keep track of whether it actually did? Farukcan Saglam Topic Author Posted 2 months ago ¬∑ 2nd in this Competition arrow_drop_up 0 more_vert I implemented those statistics in the middle stage of the competition. Each aggregation level (global, by pair size, and by source column) improved the score by around 0.02 (in total ~0.05). If they still have the same importance in the final version of the dataset, I could argue that I would‚Äôve ended up in 4th place without these features. delai50 Posted 2 months ago ¬∑ 36th in this Competition arrow_drop_up 1 more_vert Congratulations for such amazing result with just one model! üôå You used 12k estimators in your final model but did you keep that value during all the experimentation as well? Or you use early stopping or something? I used early stopping and noticed that the optimal number of rounds highly depends on the feature set I was experimenting with. Thanks for sharing! Farukcan Saglam Topic Author Posted 2 months ago ¬∑ 2nd in this Competition arrow_drop_up 2 more_vert Thank you! I use (n_iter=3000, lr=0.1) for my experiments. If I see a good improvement on the validation set, I switch to (n_iter=12000, lr=0.08) for submission. In general, I don't recommend using early stopping. It tends to underfit or overfit depending on the data. It was really hard to overfit in this competition. For a short period, I used (n_iter=40000, lr=0.06) for my submissions, but later I had to reduce it due to the runtime limit delai50 Posted 2 months ago ¬∑ 36th in this Competition arrow_drop_up 0 more_vert Very Interesting! If I understand correctly, you considered n_iter = 3000 to be a sufficiently high number of iterations to capture the usefulness of a new feature. My main concern was engineering a useful new feature that demonstrates its effectiveness only after a large number of iterations (does this actually happen in practice?). If n_iter is set too low, it might not be detected and for that reason I thought setting a very high n_iter along with early stopping would be a good practice. 3 more replies arrow_drop_down Roman@Ahmed Posted 2 months ago ¬∑ 1696th in this Competition arrow_drop_up 1 more_vert Your solution in the Kaggle competition was truly impressive and very helpful üéâ. I learned a lot from it. Thank you for sharing such a valuable approach..üéâ Shah Nawaj Posted 2 months ago ¬∑ 856th in this Competition arrow_drop_up 1 more_vert Congratulations! Amazing work @greysky paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 1 more_vert I tried doing this, but constantly faced memory issues. How did you optimize memory usage? Farukcan Saglam Topic Author Posted 2 months ago ¬∑ 2nd in this Competition arrow_drop_up 3 more_vert I cast all the float64 types to float32. I use the CPU instead of the GPU (otherwise, it crashes due to memory issues). While splitting X and y from the DataFrame, I use drop with the inplace argument to avoid unnecessary copying. Lastly, I use LightGBM, which doesn't consume much memory. But even after all these steps, it barely fits in memory. Memory usage reaches as high as 29/30 GB during LightGBM initialization and then drops. paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 0 more_vert This is slightly different from what I experienced. I noticed even though you get 30 GB of RAM on the cpu and 15 GB of VRAM on the T4 GPU, if you utilize cuDF the memory is handled more efficiently. For example, my code would crash on the CPU but runs on the GPU with cuDF. Also, for my code, float64 doesn‚Äôt seem to reduce memory usage by a significant amount that much by float32. I think this might be because I am using XGBoost instead of LightGBM, but I am not sure. Hamed Abedi Posted 2 months ago ¬∑ 425th in this Competition arrow_drop_up 1 more_vert Thanks for sharing your insights! It's interesting you could manage to use kaggle kernels with that many features ! I personally failed to go beyond 500 features ! Ishan Raj Posted 2 months ago ¬∑ 52nd in this Competition arrow_drop_up 1 more_vert Amazing work, I tried using LightGBM model, but now I can see where my solution lacked. Thanks for sharing the notebook as well. paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 2 more_vert For these features - Mul_Hpp_Elm = Host_Popularity_percentage * round(Episode_Length_minutes)\nMul_Gpp_Elm = Guest_Popularity_percentage * round(Episode_Length_minutes)\nRounded_Episode_Length_minutes = round(Episode_Length_minutes) // 2\nRounded_Host_Popularity_percentage = round(Host_Popularity_percentage) // 2\nRounded_Guest_Popularity_percentage = round(Guest_Popularity_percentage) // 2 - what is the point of rounding Episode_Length_minutes and rounding the popularity column floor divided by 2? These seem pretty specific. Amin Posted 2 months ago ¬∑ 1384th in this Competition arrow_drop_up 0 more_vert Congratulations for such an intresting solo model trained on kaggle notebook. same question: how did you get to rounding ELM, HPP and GPP features dividing by 2 . comes from exprience knowledge or did they have meaning Farukcan Saglam Topic Author Posted 2 months ago ¬∑ 2nd in this Competition arrow_drop_up 0 more_vert You can think of the rounding operation as binning with bin_width = 1. I started with 1, and when I saw that this method brought significant improvement, I also tried 0.5, 2.0, and 3.0. The most successful one was 2.0. But this was during the last week of the competition. If I had more time, I would have tried quantile binning and floor() as well @paperxd , @aaminim Cyril Bourgeois Posted 2 months ago ¬∑ 20th in this Competition arrow_drop_up 2 more_vert Well done ! The best mono model of this competition ! OldStatMagic Posted a month ago arrow_drop_up 0 more_vert Congratulations @greysky and thanks for sharing your approach! I had a quick question regarding your LightGBM configuration‚Äîhow did you settle on such a high num_leaves (480) and max_depth (15) without overfitting, especially with such a large feature set? Did you rely on cross-validation trends or some other heuristics to guide these choices? Towhidul Islam Posted 2 months ago arrow_drop_up 0 more_vert Amazing! Congratulations. Roman@Ahmed Posted 2 months ago ¬∑ 1696th in this Competition arrow_drop_up 0 more_vert Thanks, @greysky . Your solution was articulated in a way that made it very easy to comprehend. emoji_people palakpaneer Posted 2 months ago ¬∑ 51st in this Competition arrow_drop_up 0 more_vert I'm learning a lot here, thank you for sharing. To improve my performance in the next competition, I‚Äôd appreciate it if I could ask you a few questions. Regarding the  Descriptive Statistics on Target Encodings (Column-wise), I believe the aggregated globally values (mean, standard deviation, min, max) would be the same across all rows. Therefore, they wouldn‚Äôt carry meaningful information for Lightgbm. So, can I ask why you made this feature and use it in your sophisticated model? I guess you use that data to ensure there is no data leakage. Also, I have the same question for the \"aggregated by pair_size\". Thank you for taking your time to read this. Farukcan Saglam Topic Author Posted 2 months ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert I might have confused the terms \"row-wise\" and \"column-wise\" in the write-up. The operation performed on each row separately (axis=1). agg_list = [ 'mean' , 'std' , 'min' , 'max' ] for agg in agg_list:\n    select_columns = encoded_columns\n    df_train[ f'mean_ {agg} ' ] = df_train[select_columns].agg(agg, axis= 1 )\n    df_test[ f'mean_ {agg} ' ] = df_test[select_columns].agg(agg, axis= 1 ) content_copy The same logic applies to the pair_size features. If we had used (axis=0), the result would have been the same for every row, making it useless for the model. agg_list = [ 'mean' , 'std' , 'min' , 'max' ]\nr_list = [ 1 , 2 , 3 , 4 , 5 , 6 ] for r in tqdm(r_list):\n    select_columns = encoded_columns[encoded_columns. str .contains( f'_ {r} ' )] for agg in agg_list:\n        df_train[ f'mean_ {agg} _ {r} ' ] = df_train[select_columns].agg(agg, axis= 1 )\n        df_test[ f'mean_ {agg} _ {r} ' ] = df_test[select_columns].agg(agg, axis= 1 ) content_copy emoji_people palakpaneer Posted 2 months ago ¬∑ 51st in this Competition arrow_drop_up 0 more_vert Thank you for your clarification. This method is the first time I've seen it. So, I didn't have this idea. shakhauat hassain‚ö° Posted 2 months ago arrow_drop_up 0 more_vert Awesome, this is a dream for me. Alim Dauletbek Posted 2 months ago arrow_drop_up 0 more_vert Good work there for the feature selection raydowns Posted 2 months ago arrow_drop_up 0 more_vert it was funny but great! Cem D√º≈üenkalkan Posted 2 months ago arrow_drop_up 0 more_vert congrats, cool solution emoji_people Ali_Haider_Ahmad Posted 2 months ago ¬∑ 19th in this Competition arrow_drop_up 0 more_vert The long training time was one of the most annoying and limiting factors for my ideas! It's amazing how you were able to train the models in such a relatively short time! Congratulations! Too many requests error Too many requests",
      "Predict Podcast Listening Time | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 months ago Late Submission more_horiz Predict Podcast Listening Time Playground Series - Season 5, Episode 4 Predict Podcast Listening Time Overview Data Code Models Discussion Leaderboard Rules Johannes Heller ¬∑ 3rd in this Competition  ¬∑ Posted 2 months ago arrow_drop_up 27 more_vert 3rd Place - Target Encoding and 3 Levels Thanks for a fun and interesting competition - the dataset was large enough for meaningful CV but still small enough to work comfortably on standard local hardware. Congrats to @cdeotte for yet another exceptional performance, and to @greysky for an impressive second place with a single (!!!) LGBM model! My approach relied heavily on Target Encoding and a large ensemble of models with diverse features and hyperparameters. During most of the competition, I used weighted averaging via Nelder-Mead or Hill Climbing. As a last-minute experiment, I tried stacking - something that‚Äôs never worked for me in a Kaggle competition - which got me an immediate boost from 11.66 to 11.62 CV on the very first and only try. I should have explored and optimized it earlier. I suspect stacking worked better here because of the high percentage of missing values in the single most important feature. In the end, I blended my stacking ensemble (80%) with my best scoring Hill Climbing ensemble (20%), effectively turning this into a three-level approach. All models were trained with 5-fold CV (simple KFold). I believe going for 7 or 10 folds could have slightly improved the score, but the tradeoff in runtime didn‚Äôt seem worth it for me. I did some quick hyperparameter tuning with Optuna, but interestingly, the final ensemble benefitted from including some older, non-optimized models. Diversity seems to have worked especially well in this competition. Model Overview Level 1: 10 √ó LGBM (various features and params) 5 √ó XGB 4 √ó CatBoost 2 √ó RandomForest 1 √ó ExtraTrees 4 √ó HistGradientBoostingRegressor Level 2: Hill Climbing LGBM Level 3: Weighted Averaging Interestingly, HistGradientBoostingRegressor consistently received negative weights (-0.08 to -0.21) during Hill Climbing but were never discarded. Does anyone have an explanation for this? Other models (linear models) were rejected in Hill Climbing. Best Single Model CV Scores: LGBM: 11.79 XGB: 11.81 CatBoost: 11.93 ExtraTrees: 11.96 HistGB: 11.99 RandomForest: 12.05 What Worked Well Target Encoding: This was the backbone of most of my feature engineering. I experimented with median, min, max, nunique but mean TE scored best. I concatenated 2- to 7-grams, converting all columns to string before. I dropped TE features with a very high or very low cardinality. To speed things up, I saved TE features as parquets per fold. This forced me to use the same KFold settings the whole competitions to avoid leaks. My top model used about 270 TE features. I used a custom TE implementation (no smoothing, no fillna), though using the Scikit-learn version would likely have yielded similar results. Fixing extreme outliers for Number_of_Ads and Episode_Length_minutes helped, see https://www.kaggle.com/code/stopwhispering/podcast-eda Adding number of decimal digits of string-formatted Episode_Length_minutes as a feature. Thanks @AngelosMar for that interesting finding. Adding as a feature worked slightly better than manually correcting predictions with high number of decimal digits. Some Divide/Minus/Plus/Combine interaction features Original Dataset: For the original dataset I went for the concatenation approach this time, just adding original data to the train dataset. For CV scoring I ignored the original dataset records. I also added a flag 'is_original'. What Didn‚Äôt Work Predicting ratio (Target / Episode_Length_minutes) instead of target with a separate model for Episode_Length_minutes being NaN Any kind of linear model or NN Imputation Binning, Clustering Label encoding of categorical variables Feature Selection For feature selection (that is mostly Target Encoding this time) I used a very greedy variant of Sequential Feature Selection using an LGBM with high learning_reate and low n_estimtors: After scoring all candidates, I selected the best scoring ~10-20 and started again - far from optimal but my local hardware didn't allow for more sophisticated selection. I tried some Recursive Feature Elimination afterwards, but that simply took too long and didn't yield good results. I did, however, remove features with extremely high correlation to other features. Updates: added some details on TE, original dataset, and feature selection. Please sign in to reply to this topic. comment 14 Comments 1 appreciation  comment Hotness Optimistix Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Congrats! Inspired move to try out stacking towards the end - almost like lunging at the tape in a photo-finish üòÄ paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 1 more_vert This is my first time, can you explain what it means to have a multi-level model? Does it mean you use the predictions from the level one models and feed into another model to predict a new set of predictions? Chris Deotte Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Yes. Every model uses the same KFold. Then we train level 2 on the OOF and Test PREDs from level 1. And train level 3 on 2. paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 0 more_vert Why does it matter to keep random_state the same? Chris Deotte Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Congratulations on 3rd place solo finish! I think if you discovered stacking's performance in this comp earlier you would have finished even higher! As a last-minute experiment, I tried stacking - something that‚Äôs never worked for me in a Kaggle competition - which got me an immediate boost from 11.66 to 11.62 CV on the very first and only try. Same here! I basically never stack, it just never seems to work for me. I always just use hill climbing ensemble. But then one day in the first week I tried it (by training XGB on all my experiments' OOF) and it improved my CV score and LB score a lot. I suspect stacking worked better here because of the high percentage of missing values in the single most important feature. I like your explanation about why it worked so well. I think this is the main reason. Johannes Heller Topic Author Posted 2 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thanks @cdeotte . Honestly, I'm still surprised with Stacking giving such a boost here. Stacking, unlike simple averaging, is able to catch more complex patterns - like (1) SVR works best for rows with feature combination A-B-C and (2) NN works best for rows with feature combination E-F-G. But my models are mostly GBDTs. Even with diversity in hyperparams and feature sets I'd expect them to perform pretty similar. Maybe it's really the episode length NaN rows which are predicted better by some models. Chris Deotte Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Yes this dataset is mysterious. There are many things about this dataset that are weird (and I have never seen before). For example why does max_depth=19 work so well? I have never used trees this deep! (i.e. d=19 is the best public single model). This dataset must have incredible feature interaction (maybe due to ELM nan or something else). I think stacks excel at extracting feature interaction. Abish Pius Posted 2 months ago ¬∑ 1022nd in this Competition arrow_drop_up 2 more_vert Just a curiosity question, how did you decide to include the various number of models in level 1? Like why the 10 LGBMs but 5 XGBs and then 4 Catboosts, etc.? emoji_people Ali_Haider_Ahmad Posted 2 months ago ¬∑ 19th in this Competition arrow_drop_up 0 more_vert I believe he used 10 LGBMs because they are generally faster than XGBs, and I've also noticed LGBMs perform better in my tests. Johannes Heller Topic Author Posted 2 months ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Yes, definitely. I would have experimented more with CatBoost but each CatBoost model took ages to train. Johannes Heller Topic Author Posted 2 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert All in all, I trained around 100 models. From the beginning, I saved every single OOF and test preds as parquet files. My Hill Climbing script reads the OOFs and computes roughly optimal blending weights (both positive and negative). Every now and then, I ran the Hill Climber and removed those OOFs and test predictions that weren‚Äôt assigned any weights to speed up hill climbing. Unexpectedly, even some very early OOFs from before I added TE were still assigned some weight. The numbers given in the description above are what remained, i.e. what Hill Climbing still considered relevant. Roman@Ahmed Posted 2 months ago ¬∑ 1696th in this Competition arrow_drop_up 0 more_vert Detailed explanations and clear thought process are truly enlightening. Thank you for sharing your Solutions paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 0 more_vert When doing hill climbing do you use optuna to find the best weights or do you write your own code? Johannes Heller Topic Author Posted 2 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert I wrote my own implementation of Hill Climbing but there's nothing special about it - every implementation would do. I don't know if Optuna's probabilistic approach would be a suitable choice here. A good (and super simple to use) alternative would be Scipy's Nelder-Mead optimization. paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 0 more_vert Oh ok thanks. I will try your recommendation! emoji_people Ali_Haider_Ahmad Posted 2 months ago ¬∑ 19th in this Competition arrow_drop_up 0 more_vert Great, congratulations! I think you had a great chance of progressing if you had used a neural network. Johannes Heller Topic Author Posted 2 months ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Thanks! I think you're right. Diversity was key in this competition. @cdeotte won with a super-diverse ensemble, both in terms of architecture and features/targets. Appreciation (1) Eugen Sedlar Posted 2 months ago arrow_drop_up 0 more_vert CongratulationsÔºÅ Too many requests error Too many requests",
      "Predict Podcast Listening Time | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 months ago Late Submission more_horiz Predict Podcast Listening Time Playground Series - Season 5, Episode 4 Predict Podcast Listening Time Overview Data Code Models Discussion Leaderboard Rules Ravi Ramakrishnan ¬∑ 4th in this Competition  ¬∑ Posted 2 months ago arrow_drop_up 50 more_vert Rank 4 approach - lots of features, lots of simple models and a ridge blend! Hello all, Thanks to Kaggle for a good CV-LB correlated regression tabular assignment! Also thanks to my fellow participants for their contribution to the forums as well! I am happy to present my solution for rank 4 here as below- CV scheme I used a simple 10-fold cv scheme as below- Kfold(10, random_state = 42, shuffle = True) Overall model design The figure below describes my overall model effort for this competition - As indicated in the figure above, I opted for a simple pipeline with brute-force feature engineering and simple models blended with a ridge regression. The below sections illustrate the process in detail- Feature engineering I opted for simple interaction features featuring bigrams, trigrams, 4-grams, 5-grams, 6-grams and 7-grams across the dataset. I converted numerical columns to string datatypes and included them in the interactions. I also used a few random numeric column integrations involving numeric columns in the data. I also created a feature for the number of nulls across a row, featuring the starting columns only I dropped a few features that had extremely low cardinality/ quasi constant features. This step did not have any meaningful CV impact though Model training I trained 382 single models over the month mixing and extracting features from my data store explained above. I opted for a simple blend involving common boosted tree models involving - Xgboost performed well in the initial stage of the competition, with lower features in the feature store as the number of features increased, this model took longer to train and CV scores were sub-optimal to LGBM increasing tree depth proved to be a useful tactic for cv-lb boost learning rate at 0.005 - 0.0075 was the best range for my models throughout early stopping rounds in the range of 550-600 balanced my hardware constraints with suitable models training models on an A6000 GPU was a good experience. I used 128 GB RAM throughout LightGBM the best single model choice - I used both gbdt and goss options and both performed extremely well individually. My 10-best single models are all LGBM gbdt and goss options tree depth of -1 was an odd choice, but a good choice learning rate of 0.01 and below was a good choice tuning more parameters turned out to be a sub-optimal cost-benefit exercise, so I preferred to hand tune parameters and build models I trained these models on an A6000 Ada 128 GB RAM and the results were good Catboost did not perform extremely well on the CV scheme, but provided needed diversity to the ensemble memory issues surfaced with a wider feature set, so I used this model option with smaller number of features (< 350- 400) learning rate of 0.01 and tree depth of 12 proved to be good options for me I preferred an A6000 GPU and 256 GM RAM for these models Autogluon I elicited feature importance using my single tree models and shortlisted 25-100 repeatedly important features across most model options I prepared 8 feature sets using these prime importance features * and trained Autogluon models I used L4 GPU on Colab and 12-18 hour runtimes for the AutoML run Ensemble blend I choose a simple ridge model to blend my single models for a submission A simple StandardScaler ensured that all predictions were scaled before feeding into the ridge model Post-processing I chose to round off all my predictions to the nearest target value. This entailed a very small CV score improvement and the same score on the public leaderboard though! CV-LB details As indicated earlier, I enjoyed a near-perfect CV-LB relation all throughout with the below CV scores across single models and the ensemble- Single model details Model algorithm CV score Public Leaderboard Number of features XgBoost 11.89035 - 12.76437 11.91435 - 12.79546 717 - 10 LightGBM gbdt 11.80357 - 12.87457 11.83467 - 12.91435 925 - 10 LightGBM goss 11.83367 - 12.84623 11.85891 - 12.87594 817 - 10 Catboost 12.00325 - 12.86532 12.00134 - 12.89734 385 - 10 Final Submission details Model algorithm CV score Public Leaderboard Private Leaderboard Number of features / model components Ridge 11.61414226 11.64459 11.54182 382 Ridge with post-processing 11.61414171 11.64460 11.54182 382 Key takeaways Simple models have lots of power and should be used well to elicit a good score Participating in competitions where CV-LB relation is strong feels good and is highly enjoyable! Building reusable code is of great value! I reused my pipeline from Playground S5-E2 almost completely to best effect! A good GitHub repo with relevant code is akin to gold! Data stores are valuable when one wishes to iterate through features and build models quickly and effectively I did not use any public code in my pipeline this time as I thought it would be great to test my own indigenous models and experiment thereby. I think my idea prevailed! References Selected single models - https://www.kaggle.com/datasets/ravi20076/playgrounds5e4modelsv1 Selected model CV scores - https://www.kaggle.com/code/ravi20076/playgrounds5e4-modelscores-v1 Final comments Sincere thanks to Kaggle and my fellow participants for a great experience and best wishes for a successful journey ahead! All the best, happy learning and regards! Ravi Ramakrishnan Tabular Regression Gradient Boosting Ensembling 3 3 Please sign in to reply to this topic. comment 25 Comments 2 appreciation  comments Hotness Chris Deotte Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 12 more_vert Congratulations on another strong finish Ravi! Using 'boosting_type': 'goss' with LGBM is a good diversity idea (and speeds up training). I forgot to try (and include) this in my solution. When you use max_depth = -1 for LGBM, did you set num_leaves or let tree grow as deep as possible? It was very surprising in this comp how deep trees did so well. The best public notebook single model used max_depth=19 which is crazy. I never use such deep trees in previous comps. Ravi Ramakrishnan Topic Author Posted 2 months ago ¬∑ 4th in this Competition arrow_drop_up 10 more_vert Thanks a lot @cdeotte I set num_leaves to prevent the tree from growing to much. I hand-tuned num_leaves during single model runs and made sure it is relevant to the number of features used. Before this competition, I have never used trees with depth > 9, so this was a new experience for me as well. Using max_depth = -1 was a very bespoke and unusual choice, and I don't think I will ever use this in real life ever! Younus_Mohamed Posted 2 months ago ¬∑ 454th in this Competition arrow_drop_up 10 more_vert I learned so much from this one post about competition submission than anywhere else. Thank you all for intresting conversation @ravi20076 @cdeotte @optimistix paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 7 more_vert How do you hand tune a model like lightGBM? Ravi Ramakrishnan Topic Author Posted 2 months ago ¬∑ 4th in this Competition arrow_drop_up 7 more_vert Just hand tune it based on the CV score - I don't use any library for this @paperxd emoji_people Ali_Haider_Ahmad Posted 2 months ago ¬∑ 19th in this Competition arrow_drop_up 7 more_vert Very cool, congratulations! I have a question: I think you're saying you used 380 features as inputs for the Ridge model. Have you tried using models like XGBs or NNs? Shouldn't they perform better with such a large number of inputs? Edit: Maybe because level 1 models generalize so well? Johannes Heller Posted 2 months ago ¬∑ 3rd in this Competition arrow_drop_up 7 more_vert Congratulations to scoring 4th place! What was your approach to feature selection with 1_265 total features? Has AutoGluon improved your overall score? MohamedMostafa259 Posted 2 months ago ¬∑ 203rd in this Competition arrow_drop_up 7 more_vert Thank you so much for your helpful insights! This was my first competition, and I really learned a lot from your approach. I appreciate it! invisible Posted 2 months ago ¬∑ 37th in this Competition arrow_drop_up 6 more_vert No , my friend. Thank you of course, but could you not tell me in your own words and give me a notebook? Swandip Singha Posted 2 months ago ¬∑ 129th in this Competition arrow_drop_up 5 more_vert @ravi20076 can you please public your solution? Ravi Ramakrishnan Topic Author Posted 2 months ago ¬∑ 4th in this Competition arrow_drop_up 4 more_vert Please peruse the post, we have 2 links to the models trained @swandipsingha Optimistix Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 7 more_vert Congratulations! 382 models is a lot of models even by my standards üòÄ Do you have a rough idea of the overall cost of using GPUs for this episode? Thanks! Ravi Ramakrishnan Topic Author Posted 2 months ago ¬∑ 4th in this Competition arrow_drop_up 8 more_vert I have a Colab Pro+ membership and used my full monthly quota for the same! I also spent more than 150USD on cloud subscriptions I have a local PC with 3090 as well and used it a lot through the month as well! All in all, a big hole in the pocket @optimistix !! Optimistix Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 5 more_vert Well spent as long as you enjoyed it, I reckon. I'm vacillating between buying a GPU or a subscription - with Kaggle's new initiative, I guess I'll just start with Colab Pro. Thanks! Oscar Aguilar Posted 2 months ago ¬∑ 54th in this Competition arrow_drop_up 8 more_vert Agree. I believe that $150 for training 382 models is quite reasonable in my opinion. Congrats @ravi20076 ! Ravi Ramakrishnan Topic Author Posted 2 months ago ¬∑ 4th in this Competition arrow_drop_up 3 more_vert @oscarm524 I used up a lot of my local 3090, Colab compute points and runpod.io GPUs to complete my work here. Had I used runpod.io all throughout, I would have become a pauper by now üòÄ Chris Deotte Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 10 more_vert 382 models is a lot of models even by my standards This is a good Kaggle technique. In each Kaggle competition, we should save OOF and test PRED for every experiment. Then by the end of the month we have 100s or 1000s of OOF. We can then put them all into our ensemble/stack. Our L2 meta model will ignore any OOF that don't help and use the others. And we see that even old previous weak experiments will boost CV and LB score. (In one comp I had 1000s of experiments and put them all into hill climbing) Optimistix Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 4 more_vert Yes, of course - that's what I always do as well; it's just that usually I end up with somewhere between 50-100 models/OOFs. 382 is a lot, but maybe one day I'll train that many as well üòÄ Chris Deotte Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 7 more_vert Ah gotcha. Yeah i always have that many. Some comps I have over 1000! Optimistix Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 3 more_vert Now I have two contrasting goals - a solo model strong enough to place well; and well over 100 OOFs! Hamed Abedi Posted 2 months ago ¬∑ 425th in this Competition arrow_drop_up 3 more_vert Thanks for the detailed insights and the figure, it helps a lot! Mehedi Posted 2 months ago ¬∑ 7th in this Competition arrow_drop_up 3 more_vert Congratulations! I have a question: did you set positive=True in the Ridge parameters? This comment has been deleted. Cyril Bourgeois Posted 2 months ago ¬∑ 20th in this Competition arrow_drop_up 3 more_vert Thanks for sharing, well done, beautiful evolution of your last year strategy. You have construct a beautiful  powerfull pipeline üöÄ Denver Magtibay Posted 2 months ago ¬∑ 599th in this Competition arrow_drop_up 3 more_vert Congrats for the win sir! Thank you for sharing your knowledge! Looking forward for more! Abish Pius Posted 2 months ago ¬∑ 1022nd in this Competition arrow_drop_up 4 more_vert Do you have a code example of how you do the ridge blend? paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 4 more_vert What is the benefit of using Ridge as the stacker instead of other, more complicated models? Ravi Ramakrishnan Topic Author Posted 2 months ago ¬∑ 4th in this Competition arrow_drop_up 4 more_vert CV- LB relation was key here - ridge helped me with that @paperxd Bandersnatch Kumar Posted 2 months ago arrow_drop_up 2 more_vert wow it's great This comment has been deleted. Appreciation (2) Roman@Ahmed Posted 2 months ago ¬∑ 1696th in this Competition arrow_drop_up 2 more_vert thanks lot, for sharing your solution This comment has been deleted. Too many requests error Too many requests",
      "Predict Podcast Listening Time | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 months ago Late Submission more_horiz Predict Podcast Listening Time Playground Series - Season 5, Episode 4 Predict Podcast Listening Time Overview Data Code Models Discussion Leaderboard Rules Optimistix ¬∑ 5th in this Competition  ¬∑ Posted 2 months ago arrow_drop_up 12 more_vert 5th place: 100 OOFs, laziness, and a blunder or two TLDR version: My overall approach was the same as in most playground episodes - see, for example, the Gather, Ridge, Repeat post from S4E9. I gathered 100 OOFs, focusing on model diversity, and nearly had a precipitous fall down the leaderboard due to not clipping one of my final two submissions. I also got a bit lazy and didn't put many ideas into action. Nonetheless, it was a lot of fun, especially in the last week where I first chased the Top 3 & got into it, and was later pipped by @greysky and @stopwhispering . Before going into details, I'd like to acknowledge the generosity of those who shared their insights, findings and code, including but not limited to @greysky , @masayakawamata , @yekenot , @pirhosseinlou , @crisbebop , @act18l , @cdeotte , @siukeitin , @ravaghi and @mikhailnaumov Phase 1: Easing in After a few unusual months with time-series, nearly random data, and very little data, the playground series was back in the \"comfort zone\" - plenty of data, and plenty of signal. A little too much data for the GPU-poor among us - I'm one of them, as the only GPU I use is Kaggle's. So like a few episodes last year, there was a lot of running out of GPU quota, and running out of 12 hours or memory this time as well. Anyway, I've finally started spending a little more time on the prize competitions on Kaggle, so I didn't spend much time on this episode for the first week or so, just playing around with a few Autogluon submissions and getting a feel for the data. It followed the recent trend of the original data itself being synthetic (and generated by an Indian Kaggler - also quite common of late!), but thankfully the creator @ysthehurricane had done a good job of designing a signal-rich dataset. Phase 2: Getting into it: I'd say this started about the time when @greysky blew the competition open - until this point, most solutions were in the range of the best Autogluon solutions, scoring in the 12.35-12.40 range. @greysky 's notebook made an impressive leap to 12.15, and opened the door to lots of target encoding and pair encoding. I experimented with his code a fair bit, modifying it to save OOFs, and tweaking parameters to gather several useful OOFs. I also did the same using XGBoost instead of LGBM. @masayakawamata also contributed several very useful notebooks - the ones with selected features were particularly useful as they could be built upon without using the precious GPU quota. Soon, I had several dozen OOFs, and I was on my way, moving up the leaderboard without quite being near the absolute top. Phase 3: Chasing the Top 3, and getting a bit lazy: Even as I was assiduously gathering more OOFs, I was also being lazy about not pursuing various ideas (more on that in a bit). Nonetheless, I kept moving up, and found that running some of the stronger models for 30 folds gave pretty significant improvements - my strongest solo model was an updated version of @pirhosseinlou 's strong solo XGB , with added features, using median rather than mean for encoding, and 30-fold CV. This model scored 11.75387 on the public - on the private, it scored 11.67004, which would have placed 34th, not bad for a solo model. There were several models in the 11.75-12 range, and this collection helped me finally break into the Top 3, though it was in and out, as things got interesting towards the end. Finale - A near-crash, but a close escape: As I moved in and out of the Top 3, things were looking good on the second-last day, when due to various reasons, I ended up making only one submission. My computer also started groaning under the brunt of all the ensembling, and several iterations took much longer than expected. There was an exciting race to for the second and third places (@cdeotte was too far ahead at no. 1), and it was a lot of fun, even if @greysky and @stopwhispering eventually overtook me, along with @ravi20076 (congrats, guys!). Throughout the competition, and notably in a discussion post by @hamedabedi , we'd discussed how there were some outlier values in the test data, so maybe it was a good idea to clip either those values, or all predictions to be submitted, or both. Even so, I'd been careless enough not to clip what was my best submission on the public LB (11.62534) - to my horror, I only realized it when the RMSE absolutely ballooned on the private LB (177.25179)! Fortunately, my other selected submission still got me 5th place (it turned out to be my fourth best score on the private LB - the best would have also finished 5th). Afterwards, I clipped the best public LB submission, and discovered that it would have placed 4th. D-uh! I should have been more careful - I would have felt like kicking myself if both submissions had been unclipped! A short list of things I was lazy about: Using the parquet format: This saves a lot of space compared to csv, which in turn saves time, especially during uploads and downloads. I waited a long time while uploading OOFs to Kaggle (to assemble using Autogluon), yet didn't around to using parquet until the last two days. Using LightAutoML: Something I usually do early on, for ensembles of GBDTs. Using Automated feature generation: Kept thinking about this yet only used OpenFE on the last day (didn't help much). Using TabNet Using the various NN architectures provided by LightAutoML Feature selection: Didn't do it systematically, just some limited analysis for a handful of features. Model selection/pruning: I could have probably pruned my OOF collection a fair bit with the same or slightly better performance. In the end, I just did it once, dropping all models with a low weight assigned by Ridge regression for the submission which landed me 5th place (70 OOFs instead of 100). As it turned out, the corresponding submission with all 100 OOFs scored better, and would have also placed 5th if selected. Experimenting with @masaishi 's finding that data points with more than 2 decimal digits in Episode_Length_minutes were strongly correlated with the actual Listening_Time_minutes Modeling the residuals: I'd meant to this in the next regression competition, but didn't get around to doing it. In retrospect, it might not have mattered much, since the vast majority of the models I used were GBDTs. A note on ensembling: I don't always go into details about this, as it feels like repeating myself, and stuff that's probably known to many reading my posts. But there are always newcomers, so it might make sense to elaborate on a few points. For those new to it - building a strong ensemble involves looking for model diversity. For instance, GBDTs and NNs tend to combine well, so it's always a good idea to have them in the mix. You can also generate diversity by running the same model for a different number of folds, or using different feature subsets. Another way is to play around with model parameters, e.g. using the various options for bootstrapping, scoring function, sampling strategy, loss function, etc. Ultimately, you want each model to get something right, that was missed by the others, so that collectively they get as much right as possible. You also want ensembling approaches that do a good job of combining all the models into a strong final solution - Ridge regression is always a good bet, but I also use Lasso, Hill Climbing, and Autogluon (occasionally I'll try out other methods, e.g. LightAutoML, or individual GBDTs). Sometimes, I toss in an OOF generated by an ensembler - technically, this is controversial, as you're throwing an L2 OOF into the mix with L1 OOFs, but it often works well in the playground solution. For instance, in this competition, my best scoring solutions always contained an OOF from enmslbing using Autogluon (AG). Usually, I'd use the AG OOF along with all the OOFs fed to it - towards the end, this got out of sync, and my final submission used 99 OOFs along with an AG OOF based on 95 of them. In the end, I'd like to congratulate @cdeotte , @greysky , @stopwhispering , @ravi20076 , @masaishi and everyone who finished strong. Happy Kaggling, everyone! Please sign in to reply to this topic. comment 6 Comments Hotness Mahog Posted 2 months ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert Congrats on 5th place! One question: when using OpenFE, how would you suppress those annoying lgbm logs? They just freeze my session every time I try to use it. Optimistix Topic Author Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Thanks! And congrats  on your 9th place as well! This was only my second time running OpenFE, so I don't really know, but cell 8 here seems promising: did you already try it? Mahog Posted 2 months ago ¬∑ 9th in this Competition arrow_drop_up 1 more_vert That works, thanks a ton! One more thing, could you check this out? (as I suppose you have experience with working with residuals) Optimistix Topic Author Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Thanks for confirming that it works. About the residuals - have you tried it with Linear Regression? If it works well there, then you aren't doing anything wrong, it's probably just that GBDTs already take care of it and we don't need to. paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 1 more_vert If you are using XGBoost,  why would you need to do predicting the residuals? I thought GBDTs already did that? Optimistix Topic Author Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert I used many different models, not just XGBoost - but since the vast majority of them were GBDTs, you're right, it probably didn't matter much. I've updated my statement to reflect that. Johannes Heller Posted 2 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Congratulations for 5th place and thanks for the great explanation of your approach.  For me, it was experimenting with stacking for ensembling that bumped me up to third place in the final hours of the competition üòÄ Just one question - what exactly do you mean by predicting the residuals? Optimistix Topic Author Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Thanks! I meant modeling the residuals to improve your regression predictions, something I'd planned to do but didn't get around to. But given that the vast majority of my ensemble were GBDTs, it probably didn't matter much - I've updated my statement to reflect that. Too many requests error Too many requests",
      "Predict Podcast Listening Time | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 months ago Late Submission more_horiz Predict Podcast Listening Time Playground Series - Season 5, Episode 4 Predict Podcast Listening Time Overview Data Code Models Discussion Leaderboard Rules masaishi ¬∑ 6th in this Competition  ¬∑ Posted 2 months ago arrow_drop_up 27 more_vert 6th Place: Select Feature Combinations based on RMSE Scores Repo: https://github.com/masaishi/kaggle-s5e4-6th-solution Update: I published the training code, and it can achieve an 11.63 private score with a single LightGBM model using 5-fold cross-validation. Code Public 11.70 | Private 11.63 | Single LGB Fold 5 TL;DR Identified data leaks(?) and applied targeted corrections Original Data as New Rows Select feature combinations based on RMSE scores Data Leak 1: More than 2 decimal digits in Episode_Length_minutes I verified the data leak originally shared by AngelosMar in this discussion post . My own verification EDA can be found here: Decimal Digits Analysis EDA The analysis revealed that data points with more than 2 decimal digits in Episode_Length_minutes were strongly correlated with the actual Listening_Time_minutes. After ensemble prediction, I overwrote these values using the formula: Episode_Length_minutes * 0.9554. df_train. filter (pl.col( \"Episode_Length_minutes_Decimal_Len\" ) > 2 ) content_copy https://www.kaggle.com/code/masaishi/decimal-digits-analysis-eda?scriptVersionId=236025089&cellId=7 Data Leak 2: Abnormal Number_of_Ads values Normally, Number_of_Ads ranges from 0-3. However, I found 7 instances where this value exceeded 3, and these abnormal values closely matched the Listening_Time_minutes. For these cases, I applied Number_of_Ads * 1.0588 after ensemble prediction. df_train. filter (pl.col( \"Number_of_Ads\" ) > 3.0 ) content_copy https://www.kaggle.com/code/masaishi/decimal-digits-analysis-eda?scriptVersionId=236025089&cellId=8 Data Leak 3: Records with identical features have similar listening times Since this competition used synthetic data (as mentioned in the description), I hypothesized that certain feature combinations would have consistent listening times. I systematically evaluated feature combinations to identify groups with low variability in listening times (low RMSE when predicting with group means). I will write how to calc it later. The following four feature combinations showed significant LB improvement when overwriting predictions with group means: [ \"Host_Popularity_percentage-Guest_Popularity_percentage-Episode_Num\" , \"Host_Popularity_percentage-Guest_Popularity_percentage-ELen_Int\" , \"Publication_Day-Guest_Popularity_percentage-ELen_Int-HPperc_Int\" , \"Guest_Popularity_percentage-Episode_Num-ELen_Int-HPperc_Int\" ,\n] content_copy Feature Engineering I implemented numerous feature transformations including: Cyclical Day/Time Features - Converting time-based features to sin/cos representations Ratio Features - Creating meaningful ratios between existing features Integer/Decimal Separation - Splitting numerical features into integer and decimal components Sentiment Features - Encoding categorical sentiment data numerically Polynomial Features - Creating squared and cubed versions of important variables df = df.with_columns( # Day features pl.col( \"Publication_Day\" ).cast(pl_f_type).mul( 2 * np.pi / 7 ).sin().alias( \"Day_sin\" ),\n      pl.col( \"Publication_Day\" ).cast(pl_f_type).mul( 2 * np.pi / 7 ).cos().alias( \"Day_cos\" ),\n      pl.col( \"Publication_Day\" ).cast(pl_f_type).mul( 4 * np.pi / 7 ).sin().alias( \"Day_sin2\" ),\n      pl.col( \"Publication_Day\" ).cast(pl_f_type).mul( 4 * np.pi / 7 ).cos().alias( \"Day_cos2\" ), # Time features pl.col( \"Publication_Time\" ).cast(pl_f_type).mul( 2 * np.pi / 4 ).sin().alias( \"Time_sin\" ),\n      pl.col( \"Publication_Time\" ).cast(pl_f_type).mul( 2 * np.pi / 4 ).cos().alias( \"Time_cos\" ),\n      pl.col( \"Publication_Time\" ).cast(pl_f_type).mul( 4 * np.pi / 24 ).sin().alias( \"Time_sin2\" ),\n      pl.col( \"Publication_Time\" ).cast(pl_f_type).mul( 4 * np.pi / 24 ).cos().alias( \"Time_cos2\" ), # Ratio features (pl.col( \"Episode_Length_minutes\" ) / (pl.col( \"Number_of_Ads\" ) + 1 )).fill_null( 0 ).alias( \"Length_per_Ads\" ),\n      (pl.col( \"Episode_Length_minutes\" ) / (pl.col( \"Host_Popularity_percentage\" ) + 1 )).fill_null( 0 ).alias( \"Length_per_Host\" ),\n      (pl.col( \"Episode_Length_minutes\" ) / (pl.col( \"Guest_Popularity_percentage\" ) + 1 )).fill_null( 0 ).alias( \"Length_per_Guest\" ), # Episode length features pl.col( \"Episode_Length_minutes\" ).floor().alias( \"ELen_Int\" ),\n      (pl.col( \"Episode_Length_minutes\" ) - pl.col( \"Episode_Length_minutes\" ).floor()).alias( \"ELen_Dec\" ),\n      pl.col( \"Host_Popularity_percentage\" ).floor().alias( \"HPperc_Int\" ),\n      (pl.col( \"Host_Popularity_percentage\" ) - pl.col( \"Host_Popularity_percentage\" ).floor()).alias( \"HPperc_Dec\" ), # Sentiment features (pl.col( \"Episode_Sentiment\" ) == \"2\" ).cast(pl.Int8).alias( \"Is_Positive_Sentiment\" ),\n      pl.when(pl.col( \"Episode_Sentiment\" ) == \"2\" ).then( 0.75 ).otherwise( 0.717 ).cast(pl_f_type).alias( \"Sentiment_Multiplier\" ), # Squared features (pl.col( \"Episode_Length_minutes\" ) ** 2 ).alias( \"Episode_Length_squared\" ),\n      (pl.col( \"Episode_Length_minutes\" ) ** 3 ).alias( \"Episode_Length_squared2\" ),\n  )\n\n  df = df.with_columns(\n      (np.sin( 2 * np.pi * pl.col( \"Episode_Num\" ) / 100 )).alias( \"Long_Term_Cycle_Sin\" ),\n      (np.cos( 2 * np.pi * pl.col( \"Episode_Num\" ) / 100 )).alias( \"Long_Term_Cycle_Cos\" ),\n      (pl.col( \"Episode_Length_minutes\" ) * pl.col( \"Sentiment_Multiplier\" )).alias( \"Expected_Listening_Time_Sentiment\" ),\n  )\n\n  df = df.with_columns(\n      (\n          (pl.col( \"Episode_Length_minutes\" ) - pl.col( \"Episode_Length_minutes\" ).median()). pow ( 2 )\n          + (pl.col( \"Host_Popularity_percentage\" ) - pl.col( \"Host_Popularity_percentage\" ).median()). pow ( 2 )\n          + (pl.col( \"Guest_Popularity_percentage\" ) - pl.col( \"Guest_Popularity_percentage\" ).median()). pow ( 2 )\n          + (pl.col( \"Number_of_Ads\" ) - pl.col( \"Number_of_Ads\" ).median()). pow ( 2 )\n      ).alias( \"Diff_Squared\" )\n  ) content_copy The most impactful feature was simply pl.col(\"Episode_Length_minutes\").floor().alias(\"ELen_Int\") - extracting the integer part of the episode length. Utilizing the Original Dataset I incorporated this original dataset in two ways: Direct concatenation with the competition training data; ‚ÄúOriginal Data as New Cols‚Äù Adding specific rows as \"Original Data as New Rows\" For the second approach, I was inspired by Chris Deotte's solution post from the Binary Prediction with a Rainfall Dataset competition (Playground Series - Season 5, Episode 3) shared here . The technique involved finding rows in the original dataset that matched the same feature group combinations I identified in my target encoding process. When I found matching feature groups, I added those original rows with their actual Listening_Time_minutes values as new training data. This essentially allowed me to enrich my dataset with additional ground truth values from the original source. for cols in combinations_list:\n        n = f\"pte_ { '_' .join(cols)} \" means = df_pltpd.group_by(cols).agg(pl.col( \"Listening_Time_minutes\" ).mean().alias( \"mean_listening_time\" ))\n        df = df.join(means, on=cols, how= \"left\" ).with_columns(pl.col( \"mean_listening_time\" ).fill_null(m).alias(n)).drop( \"mean_listening_time\" ) content_copy Select Feature Combinations for Target Encoding Code: https://www.kaggle.com/code/masaishi/select-feature-combinations-by-rmse I'd like to share my approach for finding optimal feature combinations through target encoding. This method systematically evaluates different feature groupings to identify which combinations best predict podcast listening time. Here's the step-by-step process I implemented: First, split the dataset into training (80%) and validation (20%) sets: Generate feature combinations to test (1-4 features at a time): For each combination, create groups by concatenating feature values: # For first feature in combination if comb[ 0 ] in df.select(cs.numeric()).columns:\n    concat_expr = pl.col(comb[ 0 ]). round (round_num).cast(pl.Utf8) else :\n    concat_expr = pl.col(comb[ 0 ]).cast(pl.Utf8) # Add remaining features to create group identifier for col_name in comb[ 1 :]: if col_name in df.select(cs.numeric()).columns:\n        concat_expr = concat_expr + \"_\" + pl.col(col_name). round (round_num).cast(pl.Utf8) else :\n        concat_expr = concat_expr + \"_\" + pl.col(col_name).cast(pl.Utf8) content_copy Calculate the mean listening time for each group in the training data: df_train = df_train.with_columns(\n    concat_expr.alias( \"group\" ).cast(pl.Categorical)\n)\n\ndf_group = df_train.group_by( \"group\" ).agg(\n    pl.col( \"Listening_Time_minutes\" ).count().alias( \"count\" ),\n    pl.col( \"Listening_Time_minutes\" ).std().alias( \"std_listening_time\" ),\n    pl.col( \"Listening_Time_minutes\" ).mean().alias( \"mean_listening_time\" ),\n    pl.col( \"Episode_Length_minutes\" ).mean().alias( \"mean_episode_length\" ),\n) content_copy Apply these group means to the validation set as predictions: df_valid = df_valid.with_columns(\n    concat_expr.alias( \"group\" ).cast(pl.Categorical)\n)\ndf_valid = df_valid.join(\n    df_group[[ \"group\" , \"mean_listening_time\" ]],\n    on= \"group\" ,\n    how= \"left\" ,\n) content_copy Calculate RMSE and other statistics for each combination: std_distributes.append({ \"group\" : \"-\" .join(comb), \"cover_rate\" : df_group[ \"count\" ]. sum () / df_train[ \"Listening_Time_minutes\" ].count(), \"len\" : len (df_group), \"mean_count\" : df_group[ \"count\" ].mean(), \"median_count\" : df_group[ \"count\" ].median(), \"std_count\" : df_group[ \"count\" ].std(), \"mean_std_listening_time\" : df_group[ \"std_listening_time\" ].mean(), \"rmse\" : calculate_rmse(df_valid[ \"Listening_Time_minutes\" ], df_valid[ \"mean_listening_time\" ]),\n}) content_copy Sort results by RMSE to find the best feature combinations: std_distributes = std_distributes.sort( \"rmse\" ) content_copy This approach is essentially a systematic way to perform target encoding across different feature combinations. The best combinations provide valuable insights about which podcast attributes most strongly influence listening time. Model Ensemble For my final submission, I created an ensemble combining several different regression models: HistGradientBoostingRegressor LGBMRegressor SVR (Support Vector Regression) TabNetRegressor XGBRegressor Experiment Management Effective experiment management was crucial to my success in this competition. I tracked experiment parameters using Wandb, while implementing an automated commit system that included validation scores and Wandb experiment names in commit messages. This made it easy for me to identify which code changes produced the best results. With over 1,000 experiments conducted throughout the competition, establishing this structured environment from the beginning proved invaluable. The screenshot examples show how my tracking system organized results visually. I also carefully structured my code architecture to maximize flexibility: src /\n‚îú‚îÄ‚îÄ config .py ‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ data_class .py ‚îÇ   ‚îú‚îÄ‚îÄ data_process .py ‚îÇ   ‚îú‚îÄ‚îÄ default_selecteds .py ‚îÇ   ‚îú‚îÄ‚îÄ encoders .py ‚îÇ   ‚îú‚îÄ‚îÄ feature_eng .py ‚îÇ   ‚îú‚îÄ‚îÄ simple_data_process .py ‚îÇ   ‚îú‚îÄ‚îÄ simple_feature_eng .py ‚îÇ   ‚îî‚îÄ‚îÄ tabnet_feature_eng .py ‚îú‚îÄ‚îÄ main .py ‚îú‚îÄ‚îÄ models\n‚îÇ   ‚îú‚îÄ‚îÄ hgbr .py ‚îÇ   ‚îú‚îÄ‚îÄ lgb .py ‚îÇ   ‚îú‚îÄ‚îÄ svr .py ‚îÇ   ‚îú‚îÄ‚îÄ tabnet .py ‚îÇ   ‚îú‚îÄ‚îÄ test .py ‚îÇ   ‚îî‚îÄ‚îÄ xgb .py ‚îî‚îÄ‚îÄ utils .py content_copy Each model followed a consistent interface with standardized input/output formats: def train_model ( fold: int , datasetXy: DatasetXy ) -> tuple [ float , list | None ]: # Model implementation @dataclass class DatasetXy :\n    X_train: pl.DataFrame\n    y_train: pl.Series\n    X_valid: pl.DataFrame\n    y_valid: pl.Series\n    X_test: Optional [pl.DataFrame] = None y_test: Optional [pl.Series] = None content_copy This design made it easy for me to test new models or apply successful feature engineering techniques across different algorithms. My configuration system also allowed for quick switching between prediction and evaluation modes. While these details might seem minor, they significantly enhanced my ability to iterate efficiently throughout the competition. Special Thanks to: AngelosMar - https://www.kaggle.com/angelosmar1 Chris Deotte - https://www.kaggle.com/cdeotte Panagiota Moraiti - https://www.kaggle.com/giotamoraiti Pranshu Bahadur - https://www.kaggle.com/pranshubahadur Spiritmilk - https://www.kaggle.com/act18l Ravi Ramakrishnan - https://www.kaggle.com/ravi20076 Masaya Kawamata - https://www.kaggle.com/masayakawamata Chinmaya - https://www.kaggle.com/chinmayadatt Farukcan Saglam - https://www.kaggle.com/greysky Carl McBride Ellis - https://www.kaggle.com/carlmcbrideellis Thomas Mei√üner - https://www.kaggle.com/thomasmeine Please sign in to reply to this topic. comment 16 Comments Hotness Johannes Heller Posted 2 months ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Congratulations for your 6th place. Thanks for the detailed explanation, especially of your approach to Feature Selection. I went for a very greedy traditional forward feature selection. How long did your feature selection process take? By the way, I'm also a fan of WandB. I've often used it for hyperparameter optimization in other competitions - quite easy to use and great visualizations. Maybe I'll try it for experiment tracking in the future, too. masaishi Topic Author Posted 2 months ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thank you! I spent almost the entire last week on feature selection. https://www.kaggle.com/code/masaishi/select-feature-combinations-by-rmse I selected features in the notebook above, and I probably ran more than 50 experiments in total. AngelosMar Posted 2 months ago ¬∑ 166th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for sharing your approach! Hamed Abedi Posted 2 months ago ¬∑ 425th in this Competition arrow_drop_up 1 more_vert Great work ! Thanks for sharing your detailed solution! Optimistix Posted 2 months ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Congrats! Cool experiments, and a very systematic approach with experiment tracking - something I've been meaning to look into, without quite getting around to doing it. masaishi Topic Author Posted 2 months ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Thanks! I still don't have any best practices for the ratio of time to EDA and experiments, but if I run five experiments and they don't go well, I try to allocate time to EDA. Denver Magtibay Posted 2 months ago ¬∑ 599th in this Competition arrow_drop_up 1 more_vert Congrats sir! Following you now! Hopefully you share more like this on future competitions! masaishi Topic Author Posted 2 months ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thank you! It's good experience for me to write in English, so I still share code and discussion posts. paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 1 more_vert I don't understand the point of cubing or squaring a feature. GBDTs only care about splits, so the order would still remain the same. As the idea of splitting decimals by the integer and fractional part seems interesting, could you elaborate on that idea? masaishi Topic Author Posted 2 months ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Regarding the first point about squaring, I'm not very familiar with LightGBM either, but since decision trees use histogram algorithms for making decisions, I think that when you square or cube values, the distribution changes, so information beyond just the order might be important. I don't know the structural reasons exactly, but I simply added these features because they improved CV results in experiments. For the second point about splitting decimals by the integer and fractional part, I added these because they improved CV calculations. My hypothesis is that since noise was added to the original data when creating this competition, the integer part may have been effective as a Fuzzy Feature. paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 1 more_vert Ok, thanks for your valuable insights! Oscar Aguilar Posted 2 months ago ¬∑ 54th in this Competition arrow_drop_up 2 more_vert Congrats @masaishi and thanks for sharing your detailed approach! masaishi Topic Author Posted 2 months ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Thank you! Chris Deotte Posted 2 months ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Congratulations on 6th place! Great detective work, great feature engineering, great diverse ensemble! masaishi Topic Author Posted 2 months ago ¬∑ 6th in this Competition arrow_drop_up 2 more_vert Thank you! Your idea for previous competitions helps me a lot. paperxd Posted 2 months ago ¬∑ 340th in this Competition arrow_drop_up 0 more_vert What is cover_rate? masaishi Topic Author Posted 2 months ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert When grouping, I excluded unique items where data was not available, and calculated the percentage of remaining rows compared to the total as the 'cover_rate'. Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 5, Episode 4 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Podcast Listening Time Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 94.81 MB csv Apache 2.0 Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 94.81 MB sample_submission.csv test.csv train.csv 3 files 25 columns  Too many requests",
    "data_description": "Predict Podcast Listening Time | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 2 months ago Late Submission more_horiz Predict Podcast Listening Time Playground Series - Season 5, Episode 4 Predict Podcast Listening Time Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2025 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: Your task it to predict listening time of a podcast episode. Start Apr 1, 2025 Close May 1, 2025 Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = ( 1 N N ‚àë i = 1 ( y i ‚àí ÀÜ y i ) 2 ) 1 2 where ÀÜ y i is the predicted value and y i is the original value for each instance i . Submission File For each id in the test set, you must predict the Listening_Time_minutes of the podcast. The file should contain a header and have the following format: id ,Listening_Time_minutes 750000 , 45 . 437 750001 , 45 . 437 750002 , 45 . 437 etc . content_copy Timeline link keyboard_arrow_up Start Date - April 1, 2025 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  April 30, 2025 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Citation link keyboard_arrow_up Walter Reade and Elizabeth Park. Predict Podcast Listening Time. https://kaggle.com/competitions/playground-series-s5e4, 2025. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 8,639 Entrants 3,454 Participants 3,310 Teams 26,079 Submissions Tags Beginner Tabular Mean Squared Error Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s5e5",
    "discussion_links": [
      "/competitions/playground-series-s5e5/discussion/582611",
      "/competitions/playground-series-s5e5/discussion/582700",
      "/competitions/playground-series-s5e5/discussion/582848",
      "/competitions/playground-series-s5e5/discussion/582594",
      "/competitions/playground-series-s5e5/discussion/582564",
      "/competitions/playground-series-s5e5/discussion/582518",
      "/competitions/playground-series-s5e5/discussion/582591",
      "/competitions/playground-series-s5e5/discussion/582631",
      "/competitions/playground-series-s5e5/discussion/582596",
      "/competitions/playground-series-s5e5/discussion/582546",
      "/competitions/playground-series-s5e5/discussion/582604",
      "/competitions/playground-series-s5e5/discussion/582892",
      "/competitions/playground-series-s5e5/discussion/582556",
      "/competitions/playground-series-s5e5/discussion/582806"
    ],
    "discussion_texts": [
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules Chris Deotte ¬∑ 1st in this Competition  ¬∑ Posted a month ago arrow_drop_up 136 more_vert 1st Place - GPU Hill Climbing! Thanks Kaggle for another fun playground competition. This month was different than past months because the features were all numeric without NANs. GPU Hill Climbing My final solution is the result of feeding hundreds of GBDT, NN, and NVIDIA cuDF cuML models into my GPU Hill Climbing starter notebook here . Over the month of May 2025, I used the speed of GPU and NVIDIA cuDF cuML to build as many diverse models as possible. Hill climbing is great because it automatically selects models for us. From the hundreds of candidate models, hill climbing selected the following 7 models: Weight Model Notes CV score 1/12 XGBoost cuML target encoded features1 0.06084 1/12 XGBoost cuML target encoded features2 0.06061 1/12 XGBoost cuML target encoded features3 0.06053 1/4 XGBoost product features 0.05951 1/6 CatBoost binned features and groupby features 0.05937 1/6 NN over LinearRegression NN is here 0.05999 1/6 XGB over NN NN is here 0.05989 Final Ensemble CV and LB Scores The final hill climbing ensemble has RMSLE CV = 0.05880 Public LB = 0.05677 Private LB = 0.05841 XGBoost with cuML Target Encoder (CV=0.06XX) In my final ensemble 25% of the weight is XGBoost with NVIDIA cuML TargetEncoder features. This demonstrates that diversity is more important than single model CV score. My XGBoost with TE features each have a poor CV score of 0.06XX but they improved the final ensemble CV 0.05890 => 0.05880 and Private LB 0.05847 => 0.05841 XGBoost with Product Features (CV = 0.05951) For each feature, i created a log1p version, i.e. df[f'log1p_{c}'] = log1p( df[c] ) . Then i created all products, divisions, sums, and differences between all pairs of features. CatBoost with Binned Features and GroupBy Features (CV=0.05937) CatBoost loves categorical features, so I converted each numerical feature into 9 equal width binned values. I also created log1p versions of all features and converted those into 9 binned values. Afterward I created combinations of all pairs of columns. The resultant new columns had 81 unique values and were also categorical. We then use cat_features = CATS . For groupby features, I would pick a group of people like Sex and Age using the bins, then I would compute each person's z-score about their height . And then their z-score about their weight , i.e. for a male in their 40's how does their weight compare with other males in their 40's. I created 26 of these features. I would make groups from 1 to 3 features then compute z-score of another feature. Another example is [ [\"Sex\",\"Weight_bin\",\"Body_Temp\"], [\"Heart_Rate\"] ], . This means i make groups from Sex, Weight, Body_Temp and compute z-score for Heart_Rate. NN over Linear Regression (CV=0.05999) I trained my public notebook NN on the residuals from a NVIDIA cuML LinearRegression model. This improved CV 0.0608 => 0.0599 . The linear regression model does a great job capturing the linear relationships in the data. And then it helps the NN learn these relationships better. train LinearRegression with 5 Kfold seed 42. Make OOF predictions and test PRED predictions. make a new target with new_target = old_target - LinearRegression_OOF train NN with train data and new target (with 5 Kfold seed 42) infer NN on test, then final_pred = NN_PRED + LinearRegression_PRED . XGB over NN (CV=0.05989) I trained my XGB with Product Features on the residuals from my public NN notebook. This did not improve the XGB CV score but it made a new diverse model that improved my hill climbing ensemble CV score. We implement this using similar bullet points as above in \"NN over Linear Regression\" section. Icing on the Cake - Retrain Using 100% Train I use the following trick in all my Kaggle competitions. My OOF are 5-Kfold and we use hill climbing to find the model weights from these OOF. Then we retrain all the models using 100% train data and use fixed number of iterations equal to 25% ( which equals to 1/(K-1) ) more than average number of iterations from 5-Kfold early stopping. Then we take a weighted average of these 100% test preds using the weights found from hill climbing based on 5-Kfold OOF. This gives a nice boost in every Kaggle competition! (I also train K of these using a different seed each time and average the predictions). 29 12 12 1 Please sign in to reply to this topic. comment 60 Comments 10 appreciation  comments Hotness AISHA KHANUM Posted 7 days ago arrow_drop_up 0 more_vert Thank you for sharing @cdeotte ! Interesting idea Can you please guide me on the proper way to approach this competition and get started on it? It would be of great help to me to learn from grandmaster. Daniel Bekker Posted 10 days ago ¬∑ 1384th in this Competition arrow_drop_up 3 more_vert Thank you for sharing @cdeotte ! What heuristic do you recommend (if any) for selecting the max number of models you may add to the ensemble when hill climbing or is this just something you test empirically on a case-by-case basis? Evgeniy Borovoy Posted 14 days ago arrow_drop_up 1 more_vert Congratilations! Urvish Ahir Posted 19 days ago arrow_drop_up 1 more_vert Congrates !! @cdeotte PavelGorbachev1 Posted 21 days ago ¬∑ 3549th in this Competition arrow_drop_up 1 more_vert Interesting idea!Congratulations Muhamad Awais Posted 22 days ago arrow_drop_up 2 more_vert It looks great, Congrats Chirs. Can you guide me on the proper way to approach this competition and get started on it? It would be of great help to me to learn from grandmaster. Harsh Gupta Posted a month ago ¬∑ 1150th in this Competition arrow_drop_up 3 more_vert This detailed discussion will be very helpful for future competition for me, and also Thank you so much for this information. Pratham Jain Posted a month ago ¬∑ 1265th in this Competition arrow_drop_up 3 more_vert Insane work! Love how you combined GPU brute force with clever ensemble strategy. That NN over residuals trick + product features + CatBoost bins is gold. Also, really cool to see \"bad\" CV models still shine in the ensemble. Respect for pushing cuML to its limits Congrats. Henry D Posted a month ago ¬∑ 687th in this Competition arrow_drop_up 1 more_vert Thank you so much for sharing, I've learned a lot from your explanations. I noticed that you have over 100 entries per competition. How did you structure your workflow to manage that effectively? Khushi Yadav Posted a month ago arrow_drop_up 1 more_vert Thank you for your detailed explanation.‚ú® Frederic Nicholson Posted a month ago ¬∑ 13th in this Competition arrow_drop_up 3 more_vert thanks for sharing your strategy and congratulations to another first place. Just for my info, what was your best private score?  my best models private score beat your winning score, just wanted to see if you are  in the boat. Chris Deotte Topic Author Posted a month ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert My best private score is this hill climbing ensemble (0.05841). Congrats on achieving 12th place and congrats on building models better than private LB 0.05841 !! Oscar Aguilar Posted a month ago ¬∑ 171st in this Competition arrow_drop_up 4 more_vert Congratulations on another win with a strong approach! I have a quick question: Are those results based on 5-fold or 10-fold cross-validation? In my experiments, I found that I achieved slightly better results with a 10-fold cross-validation strategy over a 5-fold strategy. During the last week of the competition, I retrained the CatBoost model, which has been my best single model, using a 15-fold cross-validation strategy. This improved my Public LB score from 0.05730 to 0.05708, which was the best solution based on the 10-fold cross-validation strategy. These results were obtained using raw data without any feature engineering. Chris Deotte Topic Author Posted a month ago ¬∑ 1st in this Competition arrow_drop_up 10 more_vert Hi @oscarm524 Thanks! I use a trick in all my Kaggle competitions. My OOF and hill climbing use 5-Kfold to find the model weights. Then I retrain all the models using 100% train data and use fixed number of iterations equal to 25% ( which equals to 1/(K-1) ) more than average number of iterations from 5-Kfold early stopping. Then I take a weighted average of these 100% test preds using the weights found from hill climbing based on 5-Kfold OOF. This gives a nice boost in every Kaggle competition! Oscar Aguilar Posted a month ago ¬∑ 171st in this Competition arrow_drop_up 2 more_vert Interesting. Thanks for sharing, @cdeotte . I do remember now that you mentioned in a YouTube video with Abhishek that you always retrain your models on the entire dataset. 5 more replies arrow_drop_down Mar√≠lia Prata Posted a month ago arrow_drop_up 4 more_vert We're already waiting you (e.g. topics/Notebooks/ solutions) on the New Fertilizer Playground. Thank you Deotte! Johannes Heller Posted a month ago ¬∑ 72nd in this Competition arrow_drop_up 4 more_vert Congrats on another well-deserved win ü§© Two things I find especially interesting in your description: Binning and combining binned features into new categories. ‚Üí Do you think that works particularly well for synthetically generated data (somehow matching the generation mechanism), or also with real-life data? Using linear models as input for neural networks. ‚Üí Are you using the residuals from the linear model simply as an additional feature for the NN beside original and other generated features? Chris Deotte Topic Author Posted a month ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert Hi @stopwhispering . Converting numerical features into categorical features with binning helps in real life too. Because often the numerical feature can behave somewhat categorical to begin with. Take Age for example. In real life there are big differences between babies, infants, toddlers, children, college age, adults, elderly etc. So binning Age into 0's, 10's, 20's, 30's, 40's, 50's often groups people into categories where they share a lot of properties with others in those categories. That being said, i think all categorical tricks work better with Kaggle playground synthetic data. (Note we can tune the number of bins as a hyperparameter. I tried many and 9 worked best). When i say \"NN over LinearRegression\", I'm talking about using the residuals: train LinearRegression with 5 Kfold seed 42. Make OOF predictions and test PRED predictions. make a new target with new_target = old_target - LinearRegression_OOF train NN with train data and new target (with 5 Kfold seed 42) infer NN on test, then final_pred = NN_PRED + LinearRegression_PRED . Also, in addition to training NN with residuals as described above. We can optionally/additionally add LinearRegression_OOF as a new feature column too. Because sometimes the residual is related to the magnitude of the LinearRegression_OOF. I forget if i added this column or not. paperxd Posted a month ago ¬∑ 17th in this Competition arrow_drop_up 2 more_vert is it a good idea to use like xgboost and another xgboost predicting residuals or is it generally not a good idea for GBDTs 3 more replies arrow_drop_down Nikolay Pasevich Posted a month ago ¬∑ 832nd in this Competition arrow_drop_up 1 more_vert Super! Thank you that you share your steps. Briliant  work!!! Fazliddin Karimov Posted a month ago arrow_drop_up 1 more_vert Thank you for your detailed explanation :) Yasir Hussein Shakir Posted a month ago ¬∑ 2450th in this Competition arrow_drop_up 1 more_vert Congratulations sir. Your work is impressive Marpini Pavan Srikar Posted a month ago arrow_drop_up 1 more_vert Congratulations!‚Ä¶Thanks for sharing your solution. Rookie Posted a month ago arrow_drop_up 1 more_vert simple and amazing solution! Afridi Jubair Posted a month ago ¬∑ 142nd in this Competition arrow_drop_up 1 more_vert Nice work man, also great insights for future similar competitions‚Ä¶ Roman@Ahmed Posted a month ago ¬∑ 1368th in this Competition arrow_drop_up 1 more_vert Congrats @cdeotte - Your work is always amazing, thanks for sharing Derya Umut Kulali Posted a month ago ¬∑ 216th in this Competition arrow_drop_up 1 more_vert Congratulations! Nice work :) Sarah Arshad Posted a month ago arrow_drop_up 1 more_vert Congratulations!‚Ä¶..Amazing overview Chamod Kalupahana Posted a month ago ¬∑ 3585th in this Competition arrow_drop_up 1 more_vert Nice work, congratulations! üéâ narsil (jobs-in-data.com) Posted a month ago ¬∑ 775th in this Competition arrow_drop_up 1 more_vert Interesting that you didn't normalize the target via dividing by minutes. Target was super strongly linearly correlated with minutes (which makes sense because you if train x2 longer you need roughly x2 more energy), but I understand models were able to model that linear relationship based on provided target, without additional normalization. Chris Deotte Topic Author Posted a month ago ¬∑ 1st in this Competition arrow_drop_up 5 more_vert I did try using ratio targets (similar to what I did in last month's competition about podcast listening times) but it didn't help my hill climbing ensemble CV this month. (Whereas last month training with ratio targets did improve my stack's ensemble CV). narsil (jobs-in-data.com) Posted a month ago ¬∑ 775th in this Competition arrow_drop_up 1 more_vert I imagined you must've tried ratio targets - what is interesting for me is that sometimes they help and sometimes they don't - in seemingly very similar contexts , i.e. clear drivers that correlate almost perfectly linearly with the target. And of course - if we think \"why\" - there is no good answer. That is why Data Science / Kaggle is an art actually. Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules Mahog ¬∑ 2nd in this Competition  ¬∑ Posted a month ago arrow_drop_up 18 more_vert 2nd place - Trust CV and diversity The higher of my 2 selected subs was a rather stereotypical ensemble: 74 OOFs ensembled with Ridge. But when I looked back through my submissions, the highest one on the private LB was an 11 model ensemble, ensembled with HC (positive weights only) and it contained the following: 2 Autogluons, 1 without FE and 1 with autofeat features (thanks @masayakawamata !) 2 Catboosts without FE 1 LGBM with TE features 1 Catboost with TE features 1 LGBM goss with huber loss, no FE 1 Linear Regression model based on this notebook (thanks @angelosmar1 !) 1 ResMLP (thanks again @masayakawamata !) 1 LNN (thanks @nikitamanaenkov !) This one's my favorite (HC agreed with me :D), it is a Catboost trained on the original features + Catboost classifier probs (after splitting the target into bins), then I train another Catboost on the residuals (idea from here , thanks @cdeotte !) Please sign in to reply to this topic. comment 6 Comments 1 appreciation  comment Hotness Vladimir Demidov Posted a month ago ¬∑ 53rd in this Competition arrow_drop_up 1 more_vert Glad for your result. LNN is a strong, progressive model - it needs to be more explorable! Nikita Manaenkov Posted a month ago ¬∑ 1050th in this Competition arrow_drop_up 1 more_vert I'm slowly doing my research now, looking at different variations, comparing them on noisy and longtailed data, LNN has me completely hooked Mahog Topic Author Posted a month ago ¬∑ 2nd in this Competition arrow_drop_up 0 more_vert Thanks! I agree, it definetely has potential Karthik Gowda Posted a month ago ¬∑ 3278th in this Competition arrow_drop_up 0 more_vert LNN - Lagrange Neural Networks ? Derya Umut Kulali Posted a month ago ¬∑ 216th in this Competition arrow_drop_up 0 more_vert Congratulations. Thanks for the solution. Appreciation (1) This comment has been deleted. Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules nice kazusan ¬∑ 3rd in this Competition  ¬∑ Posted a month ago arrow_drop_up 11 more_vert Ôºìrd Place - Diversity and Hill Climbing First of all, I would like to express my sincere thanks to the organizers and participants of this competition, which was a valuable learning experience. Also, congratulations to @cdeotte for winning 1st place twice in a row and @mahoganybuttstrings for winning 2nd place. I‚Äôm especially grateful to @cdeotte for the helpful information he provides us every time. Outline Step1 Now, in this competition, I was troubled by unstable CV-LB correlations and ignored public LB for consideration. The original competition dataset was initially considered for use as part of the training data. However, I could not improve the CV as much as I would have liked and decided not to adopt it. The first work considered was a Meta-model combining a total of 18 models (5kfold) of Catboost, LGBM, XGB, NN and AutoGluon. For these 18 models, I created multiple conditions using AutoFeat and Optuna to focus on model diversity. CV score was 0.05893. Step2 In particular, I referred to this notebook and made additional considerations regarding the features.(thanks @onurkoc83 ) And finally, I made an ensemble using Hill Climbing. In addition to the Meta-model above, the Hill Climbing selected the following six models at this time. This improved the CV to 0.05885. And with this model, which had the best CV, I was able to come in third. Concluding thoughts I was fortunate enough to take 3rd place this time, but I‚Äôm still lacking in knowledge and will continue to learn more. Thank you for your continued guidance. 1 Please sign in to reply to this topic. comment 2 Comments 1 appreciation  comment Hotness This comment has been deleted. Appreciation (1) Derya Umut Kulali Posted a month ago ¬∑ 216th in this Competition arrow_drop_up 0 more_vert Congratulations! Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules AngelosMar ¬∑ 4th in this Competition  ¬∑ Posted a month ago arrow_drop_up 14 more_vert 4th Place Solution - Ridge Ensemble of 12 Models First of all, congratulations to everybody for their efforts! A 4th place in my 4th playground competition is really a very pleasant surprise for me. Below i will mention some things from my approach. A tricky thing about this competition was the CV - LB relationship. A better CV would often result in a worse LB and this was clear from the start. At times, i felt that the correlation was more negative than it was positive and throughout the competition I was never able to achieve a good LB score. At some point i decided to completely disregard the LB and focus on only optimizing the CV score. This turned out to be a good decision! My final two submissions with the highest CV score had the best private LB scores too. They were: Ridge ensemble with 12 models: CV = 0.05868, public LB = 0.05698, private LB = 0.05846, 4th place Ridge ensemble with 11 models: CV = 0.05870, public LB = 0.05688, private LB = 0.05847, would have finished in the 7th-10th range. The ensemble which finished 4th included 3 'level 2' models (extra trees, neural network and LGBM trained on the rest of my OOFs). I got this idea for the level 2 models from the way Autogluon works. When i did it i was not sure if it's a good practice to mix level 1 and level 2 models but in the end it gave me an extra 5th decimal for the 4th place. To select the final models for my Ridge ensembles out of 30+ OOF predictions, i used a sequential feature selector with cross validation. Below i will mention some individual models which I worked on, most of which were part of the final ensembles. All of them were trained with the same 5 folds, with the exception of Autogluon which i think uses 8 folds and a single XGBoost which i trained with 15 folds. Autogluon Autogluon trained for 15 hours and no feature engineering had the best single CV = 0.058800, but at the same time a disappointing public LB score 0.05712. In both my final ensembles this model had a weight > 0.5. I consider it a key part of my solution. GBDT Feature engineering did not work at all for me for GBDT models and so I used only the initial features. The best CV scores achieved for each type of model were: - Catboost:  CV = 0.05916 - XGBoost:  CV = 0.05937 - LGBM with 'GOSS' option:  CV = 0.05965 I trained variations of the above, in some of which i added the original data and in some of them i predicted a transformed target like Calories divided by Duration. I also trained 2 XGBoost models with nested cross validation, using linear regression in the inner folds. One of them used the predictions of linear regression as a feature and the other predicted the residuals of linear regression. In another XGBoost i used the per sample weight option to give a big weight to some points which appeared as outliers in the Duration vs Calories scatter plot. Linear Regression Linear regression was strong! My best model had CV = 0.05976 and used ~400 features. It's an improved version of this notebook i posted during the competition. This was my favourite model in this competition. Also thanks to @dantetheabstract for a nice linear regression starter notebook . Neural Networks The architecture for all my NNs was based on this notebook of @masayakawamata . Thanks for this notebook! I have posted my best NN (CV = 0.05954) in this notebook and a few more details about the training in this comment. Also thanks to @cdeotte for a nice starter NN notebook . My ensembles included also a NN in which i treated the problem as multi-label classification over 277 labels. The final prediction of this model was the average value, weighted with the predicted probability distribution. Some other NN's I tried treated all variables as categorical and used one hot encoding or embeddings but generally did not work as good as the other ones. Even though the initial features had a very small number of unique values, treating them as categorical did not work very well. Please sign in to reply to this topic. comment 7 Comments 2 appreciation  comments Hotness Mykyta Barkalov Posted a month ago ¬∑ 969th in this Competition arrow_drop_up 1 more_vert Nice work. In which way you made an ensembling? I also made Ridge Ensembling (but only 3 models) and maybe I did it a bit wrong. Because my best Private Score is 0.05893, and CV for this was 0.0596. On the other hand, when I have my CV = 0.0591, my Private Score was 0.0590. AngelosMar Topic Author Posted a month ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert For ensembling you keep out-of-fold (OOF) predictions and test predictions from all your models. Then you create two datasets where the OOF and test predictions are columns. You fit Ridge on the first one (OOF predictions) and with the fitted model you predict the second one (test predictions). Other than Ridge you can use other methods like hill-climbing for ensembling. Mykyta Barkalov Posted a month ago ¬∑ 969th in this Competition arrow_drop_up 0 more_vert Thanks for answer. I did it in the same way. But I noticed that when random_state = 42 , my CV score is 0.0591, but when I remove a fixed random_state, my CV score became 0.0601. But I don't know why‚Ä¶ I didn't think that it can make an impact on model quality AngelosMar Topic Author Posted a month ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert I think that's normal, your CV score will have small variations over different seeds. Also i had a typo on the CV scores above for my final submissions, now i fixed them. Denver Magtibay Posted a month ago ¬∑ 501st in this Competition arrow_drop_up 1 more_vert Congrats! Well done! Mahog Posted a month ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Your LR model beat mine :D, mine had a CV of 0.05984 (also thanks a lot to your notebook!) AngelosMar Topic Author Posted a month ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Congratulations for 2nd place! Your CV is great too. I wonder how far could the score of linear regression be pushed. Appreciation (2) DIVYANSH RAI Posted a month ago arrow_drop_up 0 more_vert Nice idea..thanks for sharing it Anab Farooq Posted a month ago ¬∑ 1852nd in this Competition arrow_drop_up 0 more_vert really insightful. Thank you Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules Alan1305 ¬∑ 5th in this Competition  ¬∑ Posted a month ago arrow_drop_up 8 more_vert 5th Place Solution: Ensemble of 68 models Introdution: In this competition, the main challenge is unstable CV-LB correlation and FE is not working very well. So I decided to bulid an ensemble and fully ignore public LB. Workflow: In the beginning, I focus FE with forward selection by brute-force column-wise aggregation. But most models keeps at <20 features without improvements. Then, I switched to ensemble. Hyperparameters(HPs): I don't use Optuna or Grid Search either. Instead, I save all the OOFs(and test predictions) of all tested HP combinations. Since FE is not effective, I decided to train (too) many models with different HPs. In the last week of the competition, I have trained LGB, XGB, CatBoost(GPU), HGB, YDF, Random Forests, a total of more than 2000 tree models and 10 MLPs(GPU) Hill Climbing (HC): I like HC because it can perform model selection by forcing a model weight to exactly zero. (Lasso can also do that but in general it is too hard to tune optimally.) Then, I perform HC sequentially as a model selection tool until a smaller set of OOFs does not give better CV. Note that this process is order-variant, I permuted the OOFs for thousands of times to get the final subset of 79 OOFs. Then, I used cvxpy (quadratic programming) to perform HC again with the remaining 79 OOFs. cvxpy gives deterministic result that should be order-invariant and generally better and faster than sequential HC. I didn't use it in the beginning because it has very high memory usage. It ends with 68 OOFs. Result: Final submissions CV: 0.0588017, Public LB: 0.05671, Pivate LB: 0.05846 ( cvxpy select 68 OOFs from 79 OOFs) CV: 0.0588070, Public LB: 0.05671, Pivate LB: 0.05846 (Sequential HC) Best unselected submission CV: Forgotten, around 0.05885~0.05890, Public LB: 0.05692, Pivate LB: 0.05845 In the last few days, I also found that CV-LB correlation is much more stable with ensemble prediction than single model prediction. In my case, via HC, better ensemble CV means better LB. The final 2 submissions are also my best CV & public LB. Ensemble is the key in this competition, and NN is the key of the key. I had trained > 2000 tree models and only 10 MLPs However, in my final ensemble of 68 models, 7 of them are MLPs, they also contribute the most in my ensemble! Takeaways: 1: Try different strategies in the beginning. I discovered a stable CV-LB correlation and power of NN in ensemble too late. So I don't have much time to try more NNs and more ensemble techniques. 2: Bulid a good diversified ensemble than combining strong single models. 3: Trust CV. Blending public works is fine, but if the weights are determined by luck/LB probing, then shake up is coming when CV-LB is unstable. Since FE is not important in this competition, I don't provide the process of FE and model fitting, the actual code is just a standard routine of 5-Fold CV with a outer for loop of different HPs. The usage of cvxpy and also my solution is provided here Please sign in to reply to this topic. comment 4 Comments Hotness Denver Magtibay Posted a month ago ¬∑ 501st in this Competition arrow_drop_up 1 more_vert Congrats! Well done sir! Thank you for sharing your technique! Krzysztof Strzelecki Posted a month ago ¬∑ 69th in this Competition arrow_drop_up 1 more_vert Thanks Alan and congrats for your score.  I have just discovered the difference between Public and Private evaluation :) KS Posted a month ago ¬∑ 300th in this Competition arrow_drop_up 1 more_vert Thanks for sharing! Besides the algorithm, did your models in the ensemble mainly differ in hyper parameters? Or how did you got so many models? Alan1305 Topic Author Posted a month ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Yes, mainly in HPs. The whole process took about 10 days. Since with only 1X features, LGB/XGB/HGB train very fast. We can train 20~70 models (depends on HPs) per notebook within 12 hours. For Catboost with GPU, we can train 100~300 models within 12 hours. For RF, YDF, I only trained < 100 because they are slow. Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules OMID BAGHCHEH SARAEI ¬∑ 6th in this Competition  ¬∑ Posted a month ago arrow_drop_up 23 more_vert 6th Place Solution I'm absolutely thrilled to announce my 6th place finish in the competition! Huge congrats to all the top rankers. For my final submissions, I used an ensemble of 30 models. Here's a breakdown of my two key submissions: 1. Ridge Ensemble (My Best Submission - 6th Place!): 5 Kfold CV Score: 0.05884 Public LB: 0.05669 Private LB: 0.05846 (This was the one that secured the 6th rank!) 2. Hill Climbing Ensemble: 5 Kfold CV Score: 0.05879 Public LB: 0.05670 Private LB: 0.05848 (This submission would have placed around 10th-13th) My approach is something like my main notebook here in this link: Ridge Ensemble for Calorie Expenditure Prediction . I'll probably share more detailed information from my specific models soon. Updates: I want to give a special thank you to @pirhosseinlou for his notebook Simple Single XGBoost Model , which added great diversity and weight to my overall ensemble. Here are screenshots of my final submissions. All the best, OMID BAGHCHEH SARAEI 14 Please sign in to reply to this topic. comment 14 Comments Hotness Oscar Aguilar Posted a month ago ¬∑ 171st in this Competition arrow_drop_up 1 more_vert Congrats on achieving a top 10 solution! paperxd Posted a month ago ¬∑ 17th in this Competition arrow_drop_up 1 more_vert When doing hill climbing does it matter if the amount of folds stays the same for each OOF and the random state stays the same? OMID BAGHCHEH SARAEI Topic Author Posted a month ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert I think, yes, it matters. Personally, I recommend keeping the same fold splits for all models. Martin Mohn Posted a month ago ¬∑ 876th in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for sharing your solution and giving us the chance to learn from it :) Swandip Singha Posted a month ago ¬∑ 1059th in this Competition arrow_drop_up 1 more_vert First of all congratulations..I have a question ,you mentioned about 30 model,but in your notebook i find only 10 models ,,can you please clear it.Thanks in advance OMID BAGHCHEH SARAEI Topic Author Posted a month ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Hi @swandipsingha , Thank you for your kind words. You're right to point that out. In my main (public) notebook, the ensemble shown directly includes 12 models, as you can see in the following screenshot: The reason my final submissions include 30 models is because many of them, particularly powerful ones like FLAML and AutoGluon, took a very long time to train (each often taking nearly 10 hours). To manage this, I trained them in separate notebooks and then linked their outputs (as CSV prediction files) into my main ensemble notebook. For example, the FLAML ExtraTrees model (which achieved a CV of 0.06114 and LB of 0.05892) was trained in a dedicated notebook like this one: FLAML ExtraTrees: 0.06114 CV & LB of 0.05892 . Similarly, various AutoGluon models (like ag_best, ag_cat, ag_fastai) were also trained individually in other notebooks for the same reason. The screenshots of my final ensemble show the full 30 models contributing to the predictions. I hope this clarifies things! Thanks again for your interest. Swandip Singha Posted a month ago ¬∑ 1059th in this Competition arrow_drop_up 0 more_vert Thanks for your answer Optimistix Posted a month ago ¬∑ 40th in this Competition arrow_drop_up 1 more_vert Congrats! It's great to see you back in TPS, and in the Top 10. Roman@Ahmed Posted a month ago ¬∑ 1368th in this Competition arrow_drop_up 1 more_vert Thank you for sharing your solution‚Äîit‚Äôs a great learning opportunity for me. Keep up the amazing work! Ali_Haider_Ahmad Posted a month ago ¬∑ 34th in this Competition arrow_drop_up 1 more_vert Congratulations! I never trusted Hill Climbing during this competition. Well done for relying on Ridge. OMID BAGHCHEH SARAEI Topic Author Posted a month ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Hi, thanks for the congratulations! @alihaiderahmad I appreciate your comment on Hill Climbing. However, I actually found Hill Climbing very useful, especially early on, for understanding the diversity and unique contributions of each individual model in my ensemble. While Ridge regression ultimately edged it out for my best submission, Hill Climbing performed very similarly on both the CV and private leaderboard, as my results show. It proved to be a nearly equally strong ensembling method in this competition. Abish Pius Posted a month ago ¬∑ 166th in this Competition arrow_drop_up 2 more_vert Was feature engineering less important in this comp? I remember the last playground everyone was making 100s of features to score high, how do you decide whether or not to focus on that versus model design/ensembling? Swandip Singha Posted a month ago ¬∑ 1059th in this Competition arrow_drop_up 1 more_vert same question,please give us your opinion OMID BAGHCHEH SARAEI Topic Author Posted a month ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Hi @abishpius , I'd argue that feature engineering was still critically important for diversity reasons through varied preprocessing. If you look at the following screenshots of my 30-model ensemble, please focus on two specific AutoGluon models: ag_best and ag_cat. For ag_cat, I specifically preprocessed and treated all relevant columns as categorical features. For ag_best, I used another preprocessing strategy, which I adapted from public notebooks. Normally, two very similar models might not add much value to an ensemble; one often makes the other redundant. However, as you can see in the screenshots, the ag_cat model received a surprisingly good weight in the Hill Climbing ensemble. Why? This is precisely because of its different preprocessing strategy. By treating features distinctly ( as categorical instead of numerical for ag_cat), it captured different patterns, thus adding valuable diversity to the overall ensemble. So, while I didn't necessarily create many new engineered features for every single model, the strategic variation in how existing features were preprocessed and interpreted by different models was a key form of feature engineering for ensemble performance. Hope this clarifies my perspective! Ahoy in Chen Posted a month ago ¬∑ 376th in this Competition arrow_drop_up 1 more_vert Congratulations! Thanks a lot for sharing your solutions. I really got inspired from your notebook as a newbie! I noticed you ensembled two CatBoost models: Model A: AutoGulon-style (convert numeric ‚Üí categorical) Model B: Raw numeric features Since feature engineering can make or break ensembles, I wonder whether this preprocessing difference may cause prediction conflicts?Additionally, Would you recommend this approach generally? Thanks! üôè OMID BAGHCHEH SARAEI Topic Author Posted a month ago ¬∑ 6th in this Competition arrow_drop_up 0 more_vert Hi! Thanks for the kind words @ahoyinchen . My idea here is to prioritize diversity and validate with Cross-Validation (CV) score. If an idea, including a distinct preprocessing approach, consistently improves my CV score (and ideally aligns with the Public LB), I keep it in the ensemble. The nice thing is, Hill Climbing often picks up on these improvements automatically. The ensemble's job is to combine these diverse perspectives to get a more robust final prediction. I recommend experimenting with diverse preprocessing strategies within your ensemble. Hope this helps! This comment has been deleted. Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules Mahdi Ravaghi ¬∑ 7th in this Competition  ¬∑ Posted a month ago arrow_drop_up 25 more_vert 7th place solution First off, congratulations to all who survived the massive shake-up. I can't say that it took me by surprise. I wrote a post about a potential upcoming shake-up here and shared my thoughts on the matter. The dataset this month was very large, and while I didn't expect a shake-up at first, that quickly changed after I made my first few submissions. Data Preprocessing I trained models both with and without the original dataset, but most were trained without it. As I mentioned here , the original dataset didn't bring much improvement. Models trained with additional data were primarily part of early experiments to evaluate its usefulness. Aside from that, I didn't do much preprocessing. I trained my models without any feature engineering or preprocessing, except for the Sex column, which I converted from categorical to integer values. Modeling I used both standard gradient boosted models and AutoGluon. The latter wasn't very competitive this month, so I only used four models from early experiments. Most of my models were CatBoost, as it proved to be the strongest single model. Ensembling I experimented with Ridge, Lasso, AutoGluon, and hill climbing for ensembling. Hill climbing showed the best CV score, but Ridge turned out to be the winner in the end. AutoGluon didn't perform well in terms of CV or LB scores. Interestingly, I had a submission where I used AutoGluon as an ensembler, and it could have secured 3rd place. However, I didn't choose it because it had neither strong CV nor a strong LB score. Final Words If you take anything away from this discussion, let it be this: trust your CV and avoid mindless blending . Blender notebooks have become a significant problem in playground competitions. They heavily overfit to the LB and may mislead beginners into thinking they're effective strategies for winning. It's becoming increasingly difficult to find quality public notebooks, as many of the top ones are blends with manually tuned weights aimed at overfitting to the public LB. I hope this trend changes, and that the community begins to focus more on learning and adhering to data science best practices instead of mindlessly blending others' work. I wish you all the best. Please sign in to reply to this topic. comment 12 Comments 1 appreciation  comment Hotness Shubham Jain Posted a month ago ¬∑ 274th in this Competition arrow_drop_up 1 more_vert I strongly agree with you about using Blender notebooks, @ravaghi ! I could have achieved the ranking in the top 70, if I had used only an ensemble model! Mahdi Ravaghi Topic Author Posted a month ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert There is nothing to be learned from blending other people's work, and most of the time they don't even survive the shakeup. I guess as long as you‚Äôve learned this, you‚Äôre doing better than most of them. paperxd Posted a month ago ¬∑ 17th in this Competition arrow_drop_up 1 more_vert What was your tolerance for hill climbing? Mahdi Ravaghi Topic Author Posted a month ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert I didn't use tolerance. Trained until there was no improvements made. Oscar Aguilar Posted a month ago ¬∑ 171st in this Competition arrow_drop_up 1 more_vert Congratulations on achieving a top 10 solution! I had a similar experience with AutoGluon . I saved the out-of-fold predictions from my individual models and used them as inputs in AutoGluon , but I wasn't able to achieve good local cross-validation scores or a strong public LB score. Gabriel Vasconcel Posted a month ago ¬∑ 1915th in this Competition arrow_drop_up 1 more_vert I'm new here. Can someone explain what are these \"blender notebooks\"? Ravi Ramakrishnan Posted a month ago ¬∑ 93rd in this Competition arrow_drop_up 0 more_vert Blender Notebooks are kernels that simply weigh public Notebooks and submit @gabrielvasconcel OMID BAGHCHEH SARAEI Posted a month ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Blender notebooks have become a significant problem in playground competitions. They heavily overfit to the LB and may mislead beginners into thinking they're effective strategies for winning. It's becoming increasingly difficult to find quality public notebooks, as many of the top ones are blends with manually tuned weights aimed at overfitting to the public LB. I completely agree. I believe a separate category for single models would be very helpful. It would make it much easier to identify and learn from strong individual baselines or components, saving time when searching for robust solutions. Optimistix Posted a month ago ¬∑ 40th in this Competition arrow_drop_up 2 more_vert Congrats! I wasn't surprised to see the shakeup or you and @omidbaghchehsaraei making big strides on the private LB either üòÄ Ravi Ramakrishnan Posted a month ago ¬∑ 93rd in this Competition arrow_drop_up 2 more_vert @ravaghi blind blends is a serious issue - agreed. I hope Kaggle makes such notebooks private and classifies them as vote manipulation. This is perhaps the only way to curb the soft spam. Mahdi Ravaghi Topic Author Posted a month ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert That's a very good idea. There should also be a downvote button for notebooks. These kinds of notebooks don't bring any value to the community. There is a report function for notebooks, but either nobody uses it or the reports aren't taken seriously. There's also the option @omidbaghchehsaraei suggests, but I think that would require a lot more work for Kaggle to implement. Appreciation (1) Derya Umut Kulali Posted 4 days ago ¬∑ 216th in this Competition arrow_drop_up 0 more_vert Thanks for sharing. Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules pinoystat ¬∑ 8th in this Competition  ¬∑ Posted a month ago arrow_drop_up 10 more_vert [8th] Place Solution for the [Predict Calorie Expenditure] Competition The last submission I made in this compeition was 16 days ago. Hence, I was surprised that I made it in the 8th place.  My solution is a simple ensemble with weights determined by Hill Climbing.  The final ensemble is composed of 5 test predictions from 5 notebooks. The models selected by the algo are 3 Tensorflow , 1 Catboost and 1 XGBoost.  In this competition, I mainly focused in building Tensorflow models to gain deeper understanding (by doing) about tensors, tensor manipulations and this library. Notebooks Ensemble of Predictions Summary Notebook # F_1027 is an XGBoost public notebook you can find here . Thanks to @jiaoyouzhang Notebook #F_1023 -> CatBoost public notebook. Many thanks to @chrisk321 .You can find it here. The rest below are all tensorflow from my private notebooks: Notebook #F_1025, F_1021 and F_1024 -> Tensorflow I did not bother optimizing or doing some unique stuff on the public notebooks. I just mixed this up with my Tensorflow models and call it a day. Logs from Hill Climbing algo: Current Best model:\nF_1023\nModels to add to the best model: \nStarting RMSE:  [0.058812503]\nIteration: 1, Model added: F_1025, Best weight: 0.41, Best RMSE: 0.05860217\nIteration: 2, Model added: F_1027, Best weight: 0.27, Best RMSE: 0.05848809\nIteration: 3, Model added: F_1021, Best weight: 0.12, Best RMSE: 0.05847134\nIteration: 4, Model added: F_1024, Best weight: 0.05, Best RMSE: 0.05846814\ncomplete Please sign in to reply to this topic. comment 4 Comments Hotness ChrisK321 Posted a month ago ¬∑ 695th in this Competition arrow_drop_up 1 more_vert Appreciate the shout out @pinoystat and congrats on 8th place -- well done! paperxd Posted a month ago ¬∑ 17th in this Competition arrow_drop_up 0 more_vert What was your tolerance you used for hill climbing? pinoystat Topic Author Posted a month ago ¬∑ 8th in this Competition arrow_drop_up 0 more_vert I did not put any tolerance (minimum metric improvement). As long as the mix improve the metric, its fine with me. This comment has been deleted. Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules Iqbal Syah Akbar ¬∑ 9th in this Competition  ¬∑ Posted a month ago arrow_drop_up 15 more_vert 9th Place Solution, 9 Models in the Ensemble It's been at least a year since I've done Kaggle competition. I was about to commit until the end but eventually I had to leave midway, partly because work priority took over, and partly because that Playground Series is the same as ever for good and for bad. Good to know that I'm not rusty at all though :) Feature Engineering There are four feature engineering that I've done: I encoded Sex feature with my trusty encoder for all models: M-Estimate Encoder. For neural network and linear regression, I added the inverse of each feature into the model. For example: 1 S e x , 1 H e i g h t , etc. For linear regression specifically, I had to use scikit-learn's PolynomialFeatures to generate the polynomial combinations of the features including their interactions. This is because linear regression needs a lot of features to perform relatively well, while it's unnecessary for neural network since you can just set the units in the first layer as high as you want for that purpose. On the other hand, for ridge, I used standard scaling and Nystroem with polynomial kernel for similar reason as above. If you want to generate thousands of features in relatively quick speed without killing your kernel (as long as you don't go overkill on picking the parameter), you can use random forest embedding. Since it's unsupervised, you can train it on concatenation of train and test dataset (and also the original dataset if you want). I've only had the chance to use this on plain CatBoost sadly. The goal here isn't to find the absolute best single model; it's just to provide diversity for the ensemble. Models As mentioned previously, I used neural network (Tensorflow), linear regression, ridge, and plain CatBoost. Other models that I use without any feature engineering except of encoding are Random Forest, XGBoost, LightGBM, and two CatBoost with different bootstrap types. Those 6 with the exception of Random Forest are tuned with Optuna. Overall, that's just 9 models. I certainly could've gone more like what others had done :) Ensembling I used Ridge to check the local CV score first and see what model I need to exclude. After that I used Optuna to find the actual optimal weight. I was about to try Hill Climbing but I got too lazy before I dropped. Things to Note Original dataset may not improve the CV by significant amount, but I always find that it's better to include it than not. Sometimes it's not always good to tune all your models to their absolute best. While improving CV of single model is good, some models shouldn't have the best parameter to give the best diversity for the ensemble. I had this experience with Random Forest. Always make sure your CV is robust. If you want to include original dataset, always concatenate it after you split the dataset into train and validation instead of before. You really want to make sure the validation dataset reflects the test dataset. I don't know how it is for others but my neural network performs at its best without residuals. Its layer structure is just [256 > 64 > 16 > 4 > 1 (output)] anyway with batch normalization before activation function in each layer. Please sign in to reply to this topic. comment 10 Comments 2 appreciation  comments Hotness Frederic Nicholson Posted a month ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert great insights. Jorgen9B Posted a month ago ¬∑ 197th in this Competition arrow_drop_up 1 more_vert Congratulations on your results, and thanks for your useful comments! DolorNeto Posted a month ago arrow_drop_up 0 more_vert @iqbalsyahakbar could be share your notebook? Yasir Hussein Shakir Posted a month ago ¬∑ 2450th in this Competition arrow_drop_up 0 more_vert Congratulations sir Mohammed Salem Lanqes Posted a month ago ¬∑ 2334th in this Competition arrow_drop_up 0 more_vert Congratulations Bro, my question what is wrong when we include the original dataset before split the data. Also, could you please share the code. Kh√°nh V≈© Posted a month ago ¬∑ 22nd in this Competition arrow_drop_up 0 more_vert What is your M-estimate encoder? Is it just some forms of maximum-likelihood? Iqbal Syah Akbar Topic Author Posted a month ago ¬∑ 9th in this Competition arrow_drop_up 0 more_vert It's this: https://contrib.scikit-learn.org/category_encoders/mestimate.html Kh√°nh V≈© Posted a month ago ¬∑ 22nd in this Competition arrow_drop_up 0 more_vert Cool, thanks! This comment has been deleted. Appreciation (2) Anab Farooq Posted a month ago ¬∑ 1852nd in this Competition arrow_drop_up 1 more_vert This is very insightful, Thank you! potter xu Posted a month ago arrow_drop_up 0 more_vert Congratulations Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules paperxd ¬∑ 17th in this Competition  ¬∑ Posted a month ago arrow_drop_up 7 more_vert 17th place - Poor Public LB scores My LB score was extremely bad to start things off, and for some reason my LB score was almost never below 0.05700, which demotivated me. For the first two weeks I did a Hill Ensemble of over 200 models which resulted in Private LB score of 0.05848, but I didn't submit this because I wasn't too confident with the LB score. For the last week I created a single model which had a Public LB score of 0.05697 but a Private LB score of 0.05850! Thanks to @siukeitin for helping me answer a lot of questions! Please sign in to reply to this topic. comment 5 Comments Hotness Derya Umut Kulali Posted a month ago ¬∑ 216th in this Competition arrow_drop_up 0 more_vert How did you generate the 200 models for hill climbing? Congrats! paperxd Topic Author Posted a month ago ¬∑ 17th in this Competition arrow_drop_up 1 more_vert I wrote some code which allowed me to switch to different models and preprocessing for more diversity. I ran the majority of the code locally on a 4070 Derya Umut Kulali Posted a month ago ¬∑ 216th in this Competition arrow_drop_up 0 more_vert Thanks for the answer. Optimistix Posted a month ago ¬∑ 40th in this Competition arrow_drop_up 0 more_vert Congrats on a great finish! Abhirup Choudhury Posted a month ago ¬∑ 479th in this Competition arrow_drop_up 0 more_vert Congrats on the rank @paperxd ! Could you please elaborate on how you achieved the final model? Also how did you generate the 200 models for hill climbing in your earlier attempt? Personally, I was stuck in ~0.058 trying to feature engineer with 3 models, then I tried to using an ensemble of diverse models, each having different FE, which let me public score of ~0.0567, being the best attempt, and in private ~0.05870, I fell off by 200 ranks. I would appreciate your input! paperxd Topic Author Posted a month ago ¬∑ 17th in this Competition arrow_drop_up 1 more_vert Thanks! To create 200 OOFs I just made like a skeleton model which allowed me to easily switch the model used, different preprocessing, etc. Also, the code below is the majority of the features I created, along with some additional target encoded features - `def simple_process(df): df = df.drop(columns = ['id']) sex_enc = {'female':-1, 'male':1} df['Age'] = df['Age'] ** 0.5 df['Heart_Rate'] = df['Heart_Rate'] - 30 df [ 'Weight' ] = 0.4 / df [ 'Weight' ] df [ 'Height' ] = 0.4 / df [ 'Height' ] df [ 'Body_Temp' ] = 70 / df [ 'Body_Temp' ] df = df.replace(sex_enc) return df.astype( 'float32' ) content_copy def preprocess(df): df = df.reset_index(drop = True).copy() df[ 'BIN_Weight' ] = pd.qcut(df[ 'Weight' ], q = 5 , labels = False )\ndf[ 'BIN_Height' ] = pd.qcut(df[ 'Height' ], q = 5 , labels = False )\n\nfor i in range( 2 , 11 ):\n    df[ 'Round_Duration' +str(i)] = np.round(df. Duration / i) * i\n    df[ 'Round_HR' + str(i)] = np.round(df. Heart_Rate / ( 2 *i)) * 2 * i\n    df[ 'Floor_Duration' +str(i)] = np.floor(df. Duration / i) * i\n    df[ 'Floor_HR' + str(i)] = np.floor(df. Heart_Rate / ( 2 *i)) * 2 * i\n    df[ 'Ceil_Duration' +str(i)] = np.ceil(df. Duration / i) * i\n    df[ 'Ceil_HR' + str(i)] = np.ceil(df. Heart_Rate / ( 2 *i)) * 2 * i\n\ndf[ 'Duration' ] = df[ 'Duration' ] * 2 df[ 'Round_Duration3' ] = df[ 'Round_Duration3' ] * 2 df[ 'Floor_Age' ] = np.floor(df. Age )\ndf[ 'Ceil_Age' ] = np.ceil(df. Age )\ndf[ 'Floor_Height' ] = np.floor( 10000 * df. Height )\ndf[ 'Ceil_Height' ] = np.ceil( 10000 * df. Height )\ndf[ 'Floor_Weight' ] = np.floor( 10000 * df. Weight )\ndf[ 'Ceil_Weight' ] = np.ceil( 10000 * df. Weight )\ndf[ 'Floor_BT' ] = np.floor( 100 * df. Body_Temp )\ndf[ 'Ceil_BT' ] = np.ceil( 100 * df. Body_Temp )\n\ndrop_columns = []\nfeatures = [ 'Sex' , 'Age' , 'Duration' , 'Heart_Rate' , 'Body_Temp' , 'Weight' , 'Height' , 'Round_Duration3' ]\n\ncombs_2 = combinations(features, 2 )\ncombs_3 = combinations(features, 3 )    \ncombs_4 = combinations(features, 4 )\n\ndf_cols_to_add = []\n\naggs = [ 'std' ]\nfor comb in combs_2:\n    new_df = pd. DataFrame ()\n    c1, c2 = comb\n    name_mult = '_mult_' .join(comb)\n    name_add = '_add_' .join(comb)\n    name_te = '_TE_' .join(comb)\n    new_df[name_mult] = df[c1] * df[c2]\n    new_df[name_add] = df[c1] + df[c2]\n    new_df[name_te] = df[c1].astype(str) + '_' + df[c2].astype(str)\n    for agg in aggs:\n        name_gby1 = f '_{agg}1_' .join(comb)\n        name_gby2 = f '_{agg}2_' .join(comb)\n        new_df[name_gby1] = df[c1].map(df.groupby(c1)[c2].agg(agg))\n        new_df[name_gby2] = df[c2].map(df.groupby(c2)[c1].agg(agg))\n\n    df_cols_to_add.append(new_df)\n    del new_df, c1, c2, comb, name_mult, name_add, name_te\n    gc.collect()\n\nfor comb in combs_3:\n    new_df = pd. DataFrame ()\n    c1, c2, c3 = comb\n    name_mult = '_mult_' .join(comb)\n    name_add = '_add_' .join(comb)      \n    new_df[name_mult] = df[c1] * df[c2] * df[c3]\n    new_df[name_add] = df[c1] + df[c2] + df[c3]\n    for agg in aggs:\n        name_gby1 = f '_{agg}1_' .join(comb)\n        name_gby2 = f '_{agg}2_' .join(comb)\n        name_gby3 = f '_{agg}3_' .join(comb)\n        new_df[name_gby1] = df.groupby([c1, c2])[c3].transform(agg)\n        new_df[name_gby2] = df.groupby([c1, c3])[c2].transform(agg)\n        new_df[name_gby3] = df.groupby([c2, c3])[c1].transform(agg)\n    df_cols_to_add.append(new_df)\n    del new_df, c1, c2, c3, comb, name_add\n    gc.collect()\n\nfor comb in combs_4:\n    new_df = pd. DataFrame ()\n    c1, c2, c3, c4 = comb\n    for agg in aggs:\n        name_gby1 = f '_{agg}1_' .join(comb)\n        name_gby2 = f '_{agg}2_' .join(comb)\n        name_gby3 = f '_{agg}3_' .join(comb)\n        name_gby4 = f '_{agg}4_' .join(comb)\n        new_df[name_gby1] = df.groupby([c1, c2, c4])[c3].transform(agg)\n        new_df[name_gby2] = df.groupby([c1, c3, c4])[c2].transform(agg)\n        new_df[name_gby3] = df.groupby([c2, c3, c4])[c1].transform(agg)\n        new_df[name_gby4] = df.groupby([c1, c2, c3])[c4].transform(agg)\n    df_cols_to_add.append(new_df)\n    del new_df, c1, c2, c3, c4, comb\n    gc.collect()\n\ndf_add = pd.concat(df_cols_to_add, axis = 1 )\ndf = pd.concat([df, df_add], axis = 1 )\ndf = df.drop(columns = drop_columns)\ndel df_cols_to_add, df_add\ngc.collect()\nreturn df ` content_copy Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules Kh√°nh V≈© ¬∑ 22nd in this Competition  ¬∑ Posted a month ago arrow_drop_up 8 more_vert 22nd place solution: I asked ChatGPT for feature transformation ideas Feature engineering & transformation I'm not an expert in workout so I asked ChatGPT for some feature transformations that are functions of the original features: def mifflin_st_jeor ( row ): \"\"\"Basal Metabolic Rate (kcal / day).  Height in cm, Weight in kg.\"\"\" if row[ \"Sex\" ] == \"male\" :\n        s = 5 else : # female s = - 161 return 10 * row[ \"Weight\" ] + 6.25 * row[ \"Height\" ] - 5 * row[ \"Age\" ] + s def boer_lbm ( row ): \"\"\"Lean Body Mass (kg).\"\"\" if row[ \"Sex\" ] == \"male\" : return 0.407 * row[ \"Weight\" ] + 0.267 * row[ \"Height\" ] - 19.2 else : return 0.252 * row[ \"Weight\" ] + 0.473 * row[ \"Height\" ] - 48.3 def body_surface_area ( row ): \"\"\"Mosteller BSA (m¬≤).\"\"\" return np.sqrt(row[ \"Height\" ] * row[ \"Weight\" ] / 3600 ) def body_fat_pct ( row ):\n    bmi = row[ \"BMI\" ]\n    adj = 10.8 if row[ \"Sex\" ] == \"male\" else 0 return 1.2 * bmi + 0.23 * row[ \"Age\" ] - adj - 5.4 def max_hr ( age ): \"\"\"Age-based Max Heart Rate.\"\"\" return 220 - age def vo2_est ( hr, age ): \"\"\"Very rough HR‚ÜíVO‚ÇÇ regression (ml¬∑kg‚Åª¬π¬∑min‚Åª¬π).\"\"\" return 14.0 + 0.37 * hr - 0.006 * age\n\ntrain_df[ \"BMI\" ] = train_df[ \"Weight\" ] / (train_df[ \"Height\" ] / 100 ) ** 2 train_df[ \"Total_Exertion\" ] = train_df[ \"Duration\" ] * train_df[ \"Heart_Rate\" ]\ntrain_df[ \"Heart_Effort\" ] = train_df[ \"Heart_Rate\" ] / train_df[ \"Duration\" ]\ntrain_df[ \"BMR\" ] = train_df.apply(mifflin_st_jeor, axis= 1 )\ntrain_df[ \"LBM\" ] = train_df.apply(boer_lbm, axis= 1 )\ntrain_df[ \"BSA\" ] = train_df.apply(body_surface_area, axis= 1 )\ntrain_df[ \"Body_Fat_Pct\" ] = train_df.apply(body_fat_pct, axis= 1 )\ntrain_df[ \"MHR\" ] = train_df[ \"Age\" ].apply(max_hr)\ntrain_df[ \"pct_MHR\" ] = train_df[ \"Heart_Rate\" ] / train_df[ \"MHR\" ]\ntrain_df[ \"Training_Load\" ] = train_df[ \"pct_MHR\" ] * train_df[ \"Duration\" ]\ntrain_df[ \"VO2\" ] = vo2_est(train_df[ \"Heart_Rate\" ], train_df[ \"Age\" ])\ntrain_df[ \"Heat_Index\" ] = (train_df[ \"Body_Temp\" ] - 37.0 ) * train_df[ \"Duration\" ] content_copy Cross-validation 5 folds splitting based on 100 quantile bins of sklearn.neighbors.LocalOutlierFactor.negative_outlier_factor_ . I'm not sure how much this helps but it does detect those non-physical data points and helped decreasing the variance of RMSE between folds. The plot below shows the data points in the bin number 0 (purple) vs. points outside it (yellow). Modeling Combining this with the original features, I tuned the three gradient boosting algorithms XGB, LGBM and CatBoost with Optuna on Kaggle notebook. The two submissions I made were RidgeCV ensembles. The best one got 0.05873 CV and 0.05851 private LB. Please sign in to reply to this topic. comment 4 Comments 1 appreciation  comment Hotness Shahporan Priyom Posted a month ago ¬∑ 2487th in this Competition arrow_drop_up 1 more_vert Thanks for your solution. Now i wonder why i did not think in this direction :3 aci.patlican Posted 25 days ago arrow_drop_up 0 more_vert I didn't understand of how all these user-defined functions are determined by gpt. Did you upload the train data and ask for gpt of which features can be created or for example \"lean body mass\" is a well-known feature and gpt just directed you. Kh√°nh V≈© Topic Author Posted 15 days ago ¬∑ 22nd in this Competition arrow_drop_up 0 more_vert I described to ChatGPT o3 what the dataset looks like, namely what the columns are, and also the objective of this contest. Appreciation (1) Frederic Nicholson Posted a month ago ¬∑ 13th in this Competition arrow_drop_up 1 more_vert outstanding idea ! thanks for sharing. Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules Supritha Kamalanathan ¬∑ 38th in this Competition  ¬∑ Posted a month ago arrow_drop_up 2 more_vert 38th place - 41 Models, Custom Features, and Hill Climbing Hey! This was my first proper attempt at a Kaggle competition and I'm super grateful for the supportive community we have here. I'm sharing my solution and learnings here in the hope that it helps others, just like many of your posts helped me! Approach Overview I explored a variety of models (like catboost, xgboost, random forest, extra trees, multi layer perceptron, ridge regression, bayesian regression, etc.) and experimented with different features like: Groupby aggregates features KMeans cluster assignments Time-series inspired features Multiscale binning Rank-based transformations Mathematical transformations PCA components added as features Domain specific features etc. Alongside exploring different sets of features, I also tried out different kinds of ensembles like simple weighted averages of CatBoost and XGBoost models, Ridge regression ensembles etc., adding in some creativity to the process as well. Eventually, I implemented a hill climbing ensemble and that turned out to be my most effective strategy. Final Submission After generating OOF predictions from 41 different models with different sets of features as written before, I passed them to the hill climbing algorithm, and it selected 16 models for the final prediction: 9 CatBoost models 3 Extra Trees models 2 XGBoost models 1 MLP model 1 Ridge model This gave me my best final CV score. Learnings & Improvements I wasn‚Äôt able to properly tune CatBoost and XGBoost models due to some kernel issues. Fine-tuning these models and using positive-only ensemble weights could have boosted my final score I guess, based on some discussions I have looked into. In hindsight, I should have trusted my CV score more than the public leaderboard. I have a few submissions that would've likely ranked between 20-25 but didn‚Äôt pick them because of leaderboard shakeups. A huge thanks to @cdeotte for patiently answering my questions and for sharing so many valuable insights, they helped a lot during this competition! As a complete beginner, I'm very happy to have participated. This community is fantastic and every question I posted was answered kindly and helpfully. I learned a ton through this competition and the discussions. Looking forward to participating in more! Please sign in to reply to this topic. comment 1 Comment 1 appreciation  comment Hotness Appreciation (1) Derya Umut Kulali Posted a month ago ¬∑ 216th in this Competition arrow_drop_up 0 more_vert Congratulations! Thanks for sharing. Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules Optimistix ¬∑ 40th in this Competition  ¬∑ Posted a month ago arrow_drop_up 15 more_vert Public #3, Private #40 - No ensemble small enough The CV-LB correspondence was quite wonky this month - can't recall ever having so many examples of CV improving but LB worsening. Somewhere along the way, I started pruning my ensembles according to the public LB, keeping only additions which improved both CV and LB - in many previous competitions, I've generally kept OOFs which improved CV without worsening the LB, but this month I often discarded them. In effect, I was overfitting to the public LB. I also ended up with far smaller ensembles than usual, though I did have large ones too, all the way to 122 OOFs. In the end, I chose my best public LB as one submission (26 OOFs, CV: 0.05878, LB: 0.05631, private: 0.05853) and my best CV as the other (122 OOFs, CV: 0.05855, LB: 0.05670, private: 0.05855). Interestingly, the largest OOF had CV = private LB, so can't really complain. On the other hand, I had several other submissions with better scores, which could have landed me somewhere in the 7-16 range (private scores: 0.05847-0.05849). Most of these came from Hill Climbing with only positive weights allowed, which typically chose 5-10 OOFs. In a few past competitions where I had concerns about Hill Climbing overfitting, I chose the solutions with only positive weights over ones which allowed negative weights (which tend to pick all or nearly all OOFs). As there was good reason to believe that a small number of strong models was preferable to larger ensembles this month, I should have gone back to prioritizing Hill Climbing without negative weights - but hey, hindsight is 20-20 and all that. #3 to #40 is a somewhat bad fall, but I've had much worse at least twice. I'd like to end by thanking all who generously shared their insights and code, including but not limited to @masayakawamata , @cdeotte , @ravaghi , @yekenot , @omidbaghchehsaraei , @ravi20076 , @suikeitin , @jiaoyouzhang , @pirhosseinlou , @onurkoc83 , @ricopue , @crisbebop , @elainedazzio , @sayyedfarrukhmehmood and a big congratulations to @cdeotte , @mahoganybuttstrings , @nicekazusan , @omidbaghchehsaraei , @ravaghi and all who finished strong. And now onto a multi-class classification challenge after a long time - hopefully it'll be a lot of fun. Happy Kaggling, everyone! 1 Please sign in to reply to this topic. comment 9 Comments 1 appreciation  comment Hotness Ali_Haider_Ahmad Posted a month ago ¬∑ 34th in this Competition arrow_drop_up 1 more_vert Congratulations.. I think that in such cases, luck plays a little. Optimistix Topic Author Posted a month ago ¬∑ 40th in this Competition arrow_drop_up 0 more_vert Thanks! Yes, it's part of the mix. Vladimir Demidov Posted a month ago ¬∑ 53rd in this Competition arrow_drop_up 1 more_vert Congrats on your Master title (after they've fixed medal awarding). Optimistix Topic Author Posted a month ago ¬∑ 40th in this Competition arrow_drop_up 0 more_vert Thanks so much! And thanks for sharing solid NN code and insights as usual. Giovanni Marco Dall'Olio Posted a month ago ¬∑ 209th in this Competition arrow_drop_up 1 more_vert Thanks for sharing your insights. You did very well, congratulations! Choosing the best submission is always a struggle - you never now how much you are overfitting to the public leaderboard. Optimistix Topic Author Posted a month ago ¬∑ 40th in this Competition arrow_drop_up 0 more_vert Thanks! Yes, it's always a bit tricky. Pang Luo Posted a month ago ¬∑ 26th in this Competition arrow_drop_up 1 more_vert I wonder what would happen if you changed the seed. Somehow, I feel that luck plays a role in this game. Optimistix Topic Author Posted a month ago ¬∑ 40th in this Competition arrow_drop_up 0 more_vert Congrats on a good rank. Yes, luck is usually a part of the equation. Appreciation (1) Ch ali hassan Posted a month ago arrow_drop_up 1 more_vert thanks for sharing these details Too many requests error Too many requests",
      "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules Jason! ¬∑ 43rd in this Competition  ¬∑ Posted a month ago arrow_drop_up 2 more_vert 43rd Place Solution for the Predict Calorie Expenditure Competition This is the plan I followed. Analyze the data. Make a preliminary model without looking at other approaches or using any libraries. Make a model w/ a good CV-score and send in two submissions. After analyzing the data , I determined that \"1) the variables most closely associated to Calories are Duration , Heart_Rate , and Body_Temp . 2) While the relationship between Duration and Calories seems to be positive and linear, the relationship between Heart_Rate / Body_Temp and Calories seems to be S-shaped‚Äù - quoted from my preliminary model notebook I then made a preliminary model using the following Assumption: We can treat the effects of Duration , Heart_Rate , and Body_Temp on Calories as additive without interaction terms. Reason Behind Assumption: I felt like it. I trained this model through simple gradient descent. I used all the data points to calculate the gradient for each update. In retrospect, my preliminary model did not perform much better than a linear model trained on least-squares regression (after a log1p transformation of the target) as the CV results in my notebook show. Next, I trained some models using standard libraries (e.g. XGBoost , tensorflow , etc.) and read the community discussion. From the community, I adopted the following insights. Feature engineering seems to be of limited use, although scaling other variables by Duration may be helpful. When creating voting rates, we can have different weights for different Sex and Age bins ( @harukikakinuma ) - I ultimately only had different weights for Sex and did not consider age. My final ensemble (weighted voting) included 5 forest-type models, 4 trained through boosting algorithms and one trained with a vanilla random forest algorithm, and 2 neural network models trained using gradient descent. Here are details of the training. The learning parameters for the forest-type algorithms were tuned through grid search. The architectures and learning parameters for the neural networks were tuned through vibes. The voting weights were tuned through optuna . I originally wanted to use a grid search, since I don‚Äôt yet understand how optuna works, but my grid was too large. The algorithms were scored via overall RMLSE of the OOF predictions, rather than the average RMLSE over folds - see my post HERE . My best CV score was around 0.5918. The unweighted average achieved a CV score was around 0.5919. I used 4 folds, mainly for time issues. I clipped the predictions of the neural network models to be in-range. No clipping is necessary for forest-type models to address in-range issues. I did not discover anything interesting about the features, model architectures, or learning parameters not discussed elsewhere in the community. My two submissions differed in the following manner. In Submission 1, I used the best number of trees for each forest-type model that I found while training. In Submission 2, I increased the number of trees for each forest-type model by 1/3 to account for the fact that my training sets in CV were 3/4 the size of the overall training set. They received almost the exact same score on the private LB (0.05855 and 0.05854, respectively). Main Thing I Learned: How I like to organize my workflow. Goals for Next Competition: Make more home brewed models. Interact w/ more community members. Please sign in to reply to this topic. comment 0 Comments Hotness Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 5, Episode 5 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Calories Burnt Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 49.49 MB csv Apache 2.0 Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 49.49 MB sample_submission.csv test.csv train.csv 3 files 19 columns  Too many requests",
    "data_description": "Predict Calorie Expenditure | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month ago Late Submission more_horiz Predict Calorie Expenditure Playground Series - Season 5, Episode 5 Predict Calorie Expenditure Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2025 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: Your goal is to predict how many calories were burned during a workout. Start May 1, 2025 Close Jun 1, 2025 Evaluation link keyboard_arrow_up The evaluation metric for this competition is Root Mean Squared Logarithmic Error . The RMSLE is calculated as: RMSLE = ( 1 n n ‚àë i = 1 ( log ( 1 + ÀÜ y i ) ‚àí log ( 1 + y i ) ) ) 1 2 where: n is the total number of observations in the test set, ÀÜ y i is the predicted value of the target for instance (i), y i is the actual value of the target for instance (i), and, log is the natural logarithm. Submission File For each id row in the test set, you must predict the continuous target, Calories . The file should contain a header and have the following format: id ,Calories 750000 , 93 . 2 750001 , 27 . 42 750002 , 103 . 8 etc . content_copy Timeline link keyboard_arrow_up Start Date - May 1, 2025 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  May 31, 2025 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Elizabeth Park. Predict Calorie Expenditure. https://kaggle.com/competitions/playground-series-s5e5, 2025. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 9,345 Entrants 4,486 Participants 4,316 Teams 37,192 Submissions Tags Beginner Tabular Mean Squared Log Error Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s5e6",
    "discussion_links": [
      "/competitions/playground-series-s5e6/discussion/587393",
      "/competitions/playground-series-s5e6/discussion/587398",
      "/competitions/playground-series-s5e6/discussion/587464",
      "/competitions/playground-series-s5e6/discussion/587405",
      "/competitions/playground-series-s5e6/discussion/587392",
      "/competitions/playground-series-s5e6/discussion/587414",
      "/competitions/playground-series-s5e6/discussion/587573",
      "/competitions/playground-series-s5e6/discussion/587739",
      "/competitions/playground-series-s5e6/discussion/587409",
      "/competitions/playground-series-s5e6/discussion/587476",
      "/competitions/playground-series-s5e6/discussion/587391"
    ],
    "discussion_texts": [
      "Predicting Optimal Fertilizers | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 days ago Late Submission more_horiz Predicting Optimal Fertilizers Playground Series - Season 5, Episode 6 Predicting Optimal Fertilizers Overview Data Code Models Discussion Leaderboard Rules Chris Deotte ¬∑ 1st in this Competition  ¬∑ Posted 5 days ago arrow_drop_up 133 more_vert 1st Place - Fast GPU Experimentation with RAPIDS cuDF cuML Thanks Kaggle for another fun playground competition! My June playground \"Predicting Optimal Fertilizers\" solution uses techniques from my previous 6 months of playground solutions. Below are links to old solutions with more info. And I will describe the techniques again briefly below. Techniques from Previous Playground Comps using GPU Acceleration! Date Name Metric Technique(s) Solution Link Rank Dec 2024 Insurance Comp RMSLE RAPIDS cuDF Feature Engineering link 1st Jan 2025 Forecasting Comp MAPE Boosting over Residuals link 2nd Feb 2025 Backpack Comp RMSE Use Original Data link 1st Mar 2025 Rainfall Comp AUC RAPIDS cuML link 2nd Apr 2025 Podcast Comp RMSE cuML Stacking link 1st May 2025 Calorie Comp RMSLE GPU HillClimbing link 1st RAPIDS cuDF Feature Engineering The secret sauce in this competition is cuDF feature engineering , specifically Target Encoding . The competition data has 8 features which can each be treated as categorical. First we make all pairs which is 28 new columns. Then we make all triples which is 56 new columns. Then we make all quadruples which is 70 new columns. From these 162 = 8+28+56+70 columns, we use cuML Target Encoder to encode the 7 binary targets, y==0?, y==1?, ..., y==6? . This produces 1134 new columns. Then we target encode using the original dataset to produce another 1134 new columns. We then train an XGBoost using all these 2268 columns! Use Original Data Using relevant external data helps in all Kaggle competitions. This technique is especially important in Kaggle's playground competitions because every competition is synthetic data generated from an original dataset. Therefore we want to utilize the original data. There are two ways to use external data like the original dataset. We can add the data as new rows . Or we can add the data as new columns . Most public notebooks in this competition used the former technique. But no public notebook used the latter. (In our final hill climbing ensemble we need models trained with former and models trained with later). To use the later, we first create a categorical column (by using original column or combination of original columns). Once we have a categorical column, we target encoding it using the original data. TE = original_data.groupby(CAT_COL)[TARGET].agg( \"mean\" ) TE.name = f \"TE_{CAT_COL}_orig\" train = train.merge(TE, on =CAT_COL, how= 'left' ) content_copy Stacking (with cuML) Stacking is especially beneficial for the MAP@3 metric which likes calibrated probabilities. We train an NN level 2 stacking model with categorical cross entropy loss and it doesn't matter if stage 1 models are calibrated or not. For stage 1 models, we don't even need multi-class. We can train individual models to predict single binary classification, Is the target y=0? , Is the target y=1? etc etc. We predict OOF probabilities using XGBoost, CatBoost, NN, cuML Linear Regression. Then we train a level 2 NN stacking model using categorical cross entropy loss to predict multi-class probabilities. This uses all the knowledge of stage 1 models and produces calibrated multi-class stage 2 predictions! We also build an GBDT level 2 stacking model using XGBoost and objective='multi:softprob' . The average of NN level 2 and GBDT level 2 is better than each stage 2 model individually! Repeated KFold Bizen250 published a beautiful public notebook here , which illustrates an important concept about MAP@3 metric. This metric is very sensitive to the randomness of training (and predicted probabilities). Therefore an easy way to boost MAP@3 metric is to train the same model again with different seed and average the predicted probabilities before selecting top3 for MAP@3. For example, I train all my NNs 100 times and average the probabilities before selecting top 3 fertilizers. And I train all my GBDT dozens of times using 100% data and average the probabilities before selecting the top 3. In the plot below, we see that each fold of each 5-Fold XGB has average MAP@3=0.376 but when we average the probabilities from 100 5-Fold XGB and then compute MAP@3, we achieve 0.380! Hill Climbing (with cuML) The effectiveness of hill climbing comes from creating a diverse set of models. Then when we combine them with weighted average, the result is better than any of the individual models. The key ingredients to tabular data hill climbing is building diverse GBDT, NN, and ML models. The fastest way to produce ML models is using RAPIDS cuML. Boosting over Residuals Note that we can give XGBoost starting predictions. First we train a cuML Linear Regression model. Then we use XGBoost's dtrain.set_base_margin(LINEAR_REGRESSION_LOGITS) to begin the boosting from the predictions of the linear regression model. This is an overlooked XGBoost trick! Pseudo Labeling A helpful trick in all competitions in the final days is to add test data labeled with your best ensemble. For classification, NN naturally handle the soft target probabilities (with their default cross entropy losses). For XGB, we write a custom objective to train multi-class with soft targets. Re-Train with 100% Train Another helpful trick in all competitions in the final days is to retrain all the most important models in your final ensemble using 100% train data. For XGBoost when going from 5-Fold to 100% we use fixed number of iterations equal to 25% more than average fixed during early stopping. And for NN, we just create a fixed step learning schedule from the average reduce on plateau of early stopping from 5-Fold. Trust Your CV For my final submission, I chose my best CV ensemble which was also my best LB score. My CV ensemble described below has CV MAP@3 = 0.386! Wow! Final Submission - CV 0.386 My final submission is an ensemble of 9 models where each is trained many times with different seeds. The CV score is 0.386 and Private LB is 0.38652 and public LB is 0.38450! XGBoost cuDF FE - use original data as columns w/ tree depth = 4 . (And train 10 models with 100% data) XGBoost cuDF FE - use original data as columns w/ tree depth = 10 . (And train 10 models with 100% data) Stacking NN with many stage 1 models (And train 25 models) XGBoost RepeatedStratifiedKFold - use original data as rows - Public notebook by @bizen250 here (Trains 50 XGB) Stacking XGB over LGBM - Public notebook by @ayushchandramaurya here (And train multiple versions different seeds) XGBoost - use original data as rows w/ tree depth = 18 - Public notebook by @elainedazzio here (And train multiple versions different seeds) NN - Public notebook by @ricopue here (And train 100 NNs) Stacking XGB with many stage 1 models and with pseudo label COLUMNS (i.e KNN features). NN with pseudo label ROWS (And train 25 NNs) So we use 9 models and train each many times with different seeds. Therefore our final ensemble is approximately combining 300 sets of predictions ! Weights (for the 9 models) are determined by GPU Hill Climbing ! Enjoy! Consider using all these techniques in Kaggle's July playground competition! 25 4 Please sign in to reply to this topic. comment 88 Comments 7 appreciation  comments Hotness Xixama Posted 10 hours ago arrow_drop_up 1 more_vert Congratulations, sir, @cdeotte Khushi Yadav Posted 18 hours ago arrow_drop_up 1 more_vert Thanks for sharing this fantastic approach.‚ú® HijabZahra Posted a day ago arrow_drop_up 3 more_vert U shared this I found it very useful thanks @cdeotte Sadia Shahid latif Posted a day ago ¬∑ 2079th in this Competition arrow_drop_up 3 more_vert thanks for sharing this with us . its really helpful @cdeotte bakhtawar shabbir Posted a day ago arrow_drop_up 1 more_vert Brilliant engineering‚ÄîGPU stacking and feature tricks shine! Sarah Arshad Posted a day ago arrow_drop_up 1 more_vert Thanks for sharing your approach! I find it really useful‚Ä¶. aimlVeera Posted 2 days ago ¬∑ 1471st in this Competition arrow_drop_up 1 more_vert Absolutely phenomenal work Chris!  I recently jumped back into a Kaggle comp after a long break, and this Playground comp was my first one. it‚Äôs been incredibly energizing to be part of the community again.  I had a few questions and curiosities from your approach: 1.Since target encoding creates so many features, have you ever tried dropping some of them early using feature importance or SHAP‚Äîjust to reduce the load a bit before training? Would love to know if that helps. Your Level 1 models were trained on binary targets (Is y==0?, etc.) rather than as multi-class models. From an ensemble perspective, do you find that this approach generates more decorrelated OOF predictions for the Level 2 meta-learner compared to using the OOFs from standard multi-class base models? It seems like a powerful way to force the base models to specialize. Thanks again for sharing your work and the knowledge you share with the community. Chris Deotte Topic Author Posted a day ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Hi. Welcome back. I did not try to remove features. I have tried in previous playground competitions after I do mass feature engineering like this but it never helps much. I think this is because of the nature of Kaggle synthetic data. In real life certain combination of features don't help. But since this comp's original data had no signal and we were only detecting the synthetic data artifacts, then every combo had equal importance. Regarding stage 1 models, yes training on binary targets makes more diverse and decorrelated models. I actually included both binary and multi-class in my stage 1 models. I observed that adding binary after already having the multiclass improved CV score, so the binary were decorrelated. Vignesh BalaChander Posted 2 days ago arrow_drop_up 1 more_vert Thanks Kaggle for another fun playground competition! My June playground \"Predicting Optimal Fertilizers\" solution uses techniques from my previous 6 months of playground solutions. Below are links to old solutions with more info. And I will describe the techniques again briefly below. Techniques from Previous Playground Comps using GPU Acceleration! Date Name Metric Technique(s) Solution Link Rank Dec 2024 Insurance Comp RMSLE RAPIDS cuDF Feature Engineering [link][1] 1st Jan 2025 Forecasting Comp MAPE Boosting over Residuals [link][2] 2nd Feb 2025 Backpack Comp RMSE Use Original Data [link][3] 1st Mar 2025 Rainfall Comp AUC RAPIDS cuML [link][4] 2nd Apr 2025 Podcast Comp RMSE cuML Stacking [link][5] 1st May 2025 Calorie Comp RMSLE GPU HillClimbing [link][6] 1st RAPIDS cuDF Feature Engineering The secret sauce in this competition is cuDF feature engineering , specifically Target Encoding . The competition data has 8 features which can each be treated as categorical. First we make all pairs which is 28 new columns. Then we make all triples which is 56 new columns. Then we make all quadruples which is 70 new columns. From these 162 = 8+28+56+70 columns, we use cuML Target Encoder to encode the 7 binary targets, y==0?, y==1?, ..., y==6? . This produces 1134 new columns. Then we target encode using the original dataset to produce another 1134 new columns. We then train an XGBoost using all these 2268 columns! Use Original Data Using relevant external data helps in all Kaggle competitions. This technique is especially important in Kaggle's playground competitions because every competition is synthetic data generated from an original dataset. Therefore we want to utilize the original data. There are two ways to use external data like the original dataset. We can add the data as new rows . Or we can add the data as new columns . Most public notebooks in this competition used the former technique. But no public notebook used the latter. (In our final hill climbing ensemble we need models trained with former and models trained with later). To use the later, we first create a categorical column (by using original column or combination of original columns). Once we have a categorical column, we target encoding it using the original data. TE = original_data.groupby(CAT_COL)[TARGET].agg( \"mean\" ) TE.name = f \"TE_{CAT_COL}_orig\" train = train.merge(TE, on =CAT_COL, how= 'left' ) content_copy Stacking (with cuML) Stacking is especially beneficial for the MAP@3 metric which likes calibrated probabilities. We train an NN level 2 stacking model with categorical cross entropy loss and it doesn't matter if stage 1 models are calibrated or not. For stage 1 models, we don't even need multi-class. We can train individual models to predict single binary classification, Is the target y=0? , Is the target y=1? etc etc. We predict OOF probabilities using XGBoost, CatBoost, NN, cuML Linear Regression. Then we train a level 2 NN stacking model using categorical cross entropy loss to predict multi-class probabilities. This uses all the knowledge of stage 1 models and produces calibrated multi-class stage 2 predictions! We also build an GBDT level 2 stacking model using XGBoost and objective='multi:softprob' . The average of NN level 2 and GBDT level 2 is better than each stage 2 model individually! Repeated KFold Bizen250 published a beautiful public notebook [here][8], which illustrates an important concept about MAP@3 metric. This metric is very sensitive to the randomness of training (and predicted probabilities). Therefore an easy way to boost MAP@3 metric is to train the same model again with different seed and average the predicted probabilities before selecting top3 for MAP@3. For example, I train all my NNs 100 times and average the probabilities before selecting top 3 fertilizers. And I train all my GBDT dozens of times using 100% data and average the probabilities before selecting the top 3. In the plot below, we see that each fold of each 5-Fold XGB has average MAP@3=0.376 but when we average the probabilities from 100 5-Fold XGB and then compute MAP@3, we achieve 0.380! Hill Climbing (with cuML) The effectiveness of hill climbing comes from creating a diverse set of models. Then when we combine them with weighted average, the result is better than any of the individual models. The key ingredients to tabular data hill climbing is building diverse GBDT, NN, and ML models. The fastest way to produce ML models is using RAPIDS cuML. Boosting over Residuals Note that we can give XGBoost starting predictions. First we train a cuML Linear Regression model. Then we use XGBoost's dtrain.set_base_margin(LINEAR_REGRESSION_LOGITS) to begin the boosting from the predictions of the linear regression model. This is an overlooked XGBoost trick! Pseudo Labeling A helpful trick in all competitions in the final days is to add test data labeled with your best ensemble. For classification, NN naturally handle the soft target probabilities (with their default cross entropy losses). For XGB, we write a custom objective to train multi-class with soft targets. Re-Train with 100% Train Another helpful trick in all competitions in the final days is to retrain all the most important models in your final ensemble using 100% train data. For XGBoost when going from 5-Fold to 100% we use fixed number of iterations equal to 25% more than average fixed during early stopping. And for NN, we just create a fixed step learning schedule from the average reduce on plateau of early stopping from 5-Fold. Trust Your CV For my final submission, I chose my best CV ensemble which was also my best LB score. My CV ensemble described below has CV MAP@3 = 0.386! Wow! Final Submission - CV 0.386 My final submission is an ensemble of 9 models where each is trained many times with different seeds. The CV score is 0.386 and Private LB is 0.38652 and public LB is 0.38450! XGBoost cuDF FE - use original data as columns w/ tree depth = 4 . (And train 10 models with 100% data) XGBoost cuDF FE - use original data as columns w/ tree depth = 10 . (And train 10 models with 100% data) Stacking NN with many stage 1 models (And train 25 models) XGBoost RepeatedStratifiedKFold - use original data as rows - Public notebook by @bizen250 [here][8] (Trains 50 XGB) Stacking XGB over LGBM - Public notebook by @ayushchandramaurya [here][9] (And train multiple versions different seeds) XGBoost - use original data as rows w/ tree depth = 18 - Public notebook by @elainedazzio [here][10] (And train multiple versions different seeds) NN - Public notebook by @ricopue [here][11] (And train 100 NNs) Stacking XGB with many stage 1 models and with pseudo label COLUMNS (i.e KNN features). NN with pseudo label ROWS (And train 25 NNs) So we use 9 models and train each many times with different seeds. Therefore our final ensemble is approximately combining 300 sets of predictions ! Weights (for the 9 models) are determined by GPU Hill Climbing ! Enjoy! Consider using all these techniques in Kaggle's July playground competition! [1]: https://www.kaggle.com/competitions/playground-series-s4e12/discussion/554328 [2]: https://www.kaggle.com/competitions/playground-series-s5e1/discussion/560549 [3]: https://www.kaggle.com/competitions/playground-series-s5e2/discussion/565539 [4]: https://www.kaggle.com/competitions/playground-series-s5e3/discussion/571176 [5]: https://www.kaggle.com/competitions/playground-series-s5e4/discussion/575784 [6]: https://www.kaggle.com/competitions/playground-series-s5e5/discussion/582611 [7]: https://www.kaggle.com/competitions/playground-series-s5e6/discussion/585000 [8]: https://www.kaggle.com/code/bizen250/xgboost-repeatedstratifiedkfold [9]: https://www.kaggle.com/code/ayushchandramaurya/predicting-fertilizer-name-stacking-ensemble [10]: https://www.kaggle.com/code/elainedazzio/20250618-pg6-xgb?scriptVersionId=246053236 [11]: https://www.kaggle.com/code/ricopue/s5e6-fertilizers-nn-keras-all-embedding Great Work!!Buddy. Vignesh BalaChander Posted a day ago arrow_drop_up 1 more_vert Yes, I often use SHAP or feature importance to prune features early ‚Äî it helps reduce overfitting and speeds up training. Training binary classifiers per class usually gives more decorrelated predictions, which improves the meta-learner‚Äôs performance. It forces base models to specialize more. Glad you‚Äôre enjoying the competition and happy to discuss more! Shuvojit Das Posted 2 days ago ¬∑ 642nd in this Competition arrow_drop_up 1 more_vert Hey @cdeotte , Absolutely phenomenal work your approach was a masterclass in structured experimentation and ensemble strategy. Big congratulations on the top spot well deserved! Learned a ton from your solution. Anshuman Bhoi Posted 2 days ago arrow_drop_up 1 more_vert Just wondering Roman@Ahmed Posted 2 days ago ¬∑ 1317th in this Competition arrow_drop_up 1 more_vert Hi @cdeotte , I hope you're doing well. First of all, I want to thank you for sharing your work ‚Äî I‚Äôve been learning a lot by studying your notebooks and solutions. I really admire how thoughtfully you apply different techniques to improve your scores. I have a small question that I‚Äôm a bit confused about, and I‚Äôd be really grateful if you could share your perspective. When starting a in data , should we focus first on applying various modeling and feature engineering techniques, and more more techniques  to improve the score step-by-step, or should we spend more time initially understanding the training and test data ‚Äî their behavior, characteristics, patterns, and possible challenges? It would be incredibly helpful to know how you usually approach a dataset. What are the main steps you follow when you start working on a new problem? Especially since we often see you using techniques so thoughtfully, it would mean a lot to understand how you decide what to try and when. Thanks Chris Deotte Topic Author Posted a day ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert This is a great question. Give me a few days and I'll respond with a thoughtful answer. Ole-Jakob Posted 3 days ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Congrats on the crushing victory, @cdeotte ! A bit late to the party, but I just finished going through your write-up and your many insightful answers‚Äîreally impressive work. I picked up several new gems like boosting over residuals , 100% retrain , pseudo-labeling , and your brilliant method of turning the original data directly into features. I‚Äôll definitely be incorporating these ideas in future Playground Series competitions even if I'm worried about the complexity of the workflow. I had a couple of follow-up questions: In some of your earlier write-ups involving target encoding (TE), you included stats like std , min , max , and skew , especially for 2- and 3-feature combinations. Was there a specific reason you chose to stick with just the mean in this competition? I noticed you didn‚Äôt seem to use TE between features‚Äîsuch as between numeric values and soil type or crop type. Since everything is treated as categorical, is there a theoretical reason not to apply TE across all feature pairs? Thanks again for the inspiration‚Äîlearned a lot from your work! Chris Deotte Topic Author Posted 2 days ago ¬∑ 1st in this Competition arrow_drop_up 0 more_vert Hi. Thanks. Congratulations on achieving 10th place! 1) Since the metric is multi-class, when we pick a single statistic (like mean), we need to TE every feature 7 times (once for each binary target (i.e. Is y==0 mean, y==1 mean, y==2 mean, etc). In this comp, I only used mean and that already produced 3000 features. If I used std, min, max, skew. Then I would have 15,000 features. That is too many :-) I did try a few of these but the improvement was not enough to justify so many new features. Also note, that many of these stats are un-informative for classification problems. The target (of y==0, y==1, etc) is 0 or 1. So the statistic min always equals 0 and max always equals 1. And some of the others don't have much info either. 2) I don't understand your question. I did create feature pairs. There are 8 original features. I treat them all as categorical and make all pairs which is 8_choose_2 = 28 . And I make all triples 8_choose_3 = 56 . And I make all quadruples which is 8_choose_4 = 70 . What other combinations are you talking about? Ole-Jakob Posted 2 days ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Hi ! I completely forgot that each feature needs to be target encoded seven times‚Äîthat makes perfect sense now. Please feel free to disregard my second (slightly clumsy) question :) Dayana Castillo Posted 3 days ago arrow_drop_up 1 more_vert Huge congrats! I'm still a beginner, but seeing your progress really inspires me ‚Äî I hope I can reach that level one day! Ayush chandra Maurya Posted 3 days ago ¬∑ 80th in this Competition arrow_drop_up 1 more_vert Congratulations @cdeotte , great work! Thanks for sharing your approach, I got to learn many things from you in last 3 months. Constantly improving my skill  and  in next competition I will try to be in top. Chris Deotte Topic Author Posted 3 days ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Awesome. Congratuations @ayushchandramaurya I used your stacking public notebooks in my final solution. They were diverse and strong models. Nice work designing them! Ayush chandra Maurya Posted 3 days ago ¬∑ 80th in this Competition arrow_drop_up 2 more_vert Thank you @cdeotte , I am glad it was helpful ! The mistake I have made is that I have not tried to blend more diverse model. Galaxy Posted 3 days ago arrow_drop_up 1 more_vert Respected Chris Deotte, I hope you're doing well. First of all, I would like to congratulate you on your recent win. I am a first-year undergraduate student at IIT, and I am deeply interested in Artificial Intelligence and Machine Learning. I have recently started learning Python and some basic mathematics such as Linear Algebra, Statistics, and Probability. However, I have many questions and confusions about AI and ML-like how to begin, where to learn from, and which books to study. I don‚Äôt yet have a clear roadmap. I spend a lot of time using ChatGPT to ask questions about AI and ML- day and night -but sometimes I feel like I‚Äôm progressing slowly. Since the field of computer science is still new to me, I used to feel overwhelmed by tools like Git, GitHub, Kaggle, VS Code, and so many Python libraries. But now, I‚Äôm getting used to them and even enjoying the process. I‚Äôm reaching out to you for some guidance on how to move forward in AI and ML. Any advice or suggestions from you would be incredibly valuable to me. Thank you so much for your time. Have a great day! Warm regards, Chris Deotte Topic Author Posted 3 days ago ¬∑ 1st in this Competition arrow_drop_up 3 more_vert Hi @siamagma Welcome to the Kaggle community. Like all learning endeavors, my suggestion is to make learning fun. Always strive to learn more (each day) and pick activities and topics that are interesting and fun to you. Also don't be intimidated by everything that is out there. Just keep learning something new each day while having fun! A suggested progression of learning new topics could be below. But feel free to jump around and follow what makes you excited: Statistics (means, std etc. How to make predictions simply from computations) ML models (like logistic regression, SVM, random forest, KNN, etc) KFold cross validation (what is it? how to use it) MLP simple NN models (fully connected NN nothing fancy) GBDT (like XGBoost, CatBoost, LGBM for regression and classification) CNN (basic computer vision models like EfficientNet etc) Transformers (Encoder LLM like DeBerta for text regression and classification) Advanced topics like LLM text generation (i.e. Llama etc) and fancy NN like timeseries WaveNet (CNN), GRU/LSTM (RNN), Time Transformers. And transformer image models (ViT) image object detection (YOLO) and image segmentation (UNET). And GBDT ranking. And AutoML This is sort of the progression that I took. And during that journey you will learn about topics needed like data augmentation, learning schedules, cross validation, hill climbing, stacking, pseudo labeling, hyperparameter tuning, etc etc. Galaxy Posted 3 days ago arrow_drop_up 1 more_vert Thank you very much for your valuable guidance and kind advice. I truly appreciate it and will certainly follow your suggestions. Once again, thank you for your support. Albina Chimidova Posted 3 days ago ¬∑ 665th in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for sharing your solution. It is very important for me to learn from best practices as a beginner in data science. J. Posted 3 days ago ¬∑ 863rd in this Competition arrow_drop_up 1 more_vert Thanks for sharing your approach! I find it really useful as I‚Äôm working on improving at competitions. :) Oleksandr Vyshnevskyi Posted 4 days ago ¬∑ 800th in this Competition arrow_drop_up 1 more_vert Wow great aproach interesting practice Karishma Battina Posted 4 days ago ¬∑ 626th in this Competition arrow_drop_up 1 more_vert Congratulations Chris, thanks for sharing your approach. Taekyung Posted 4 days ago arrow_drop_up 1 more_vert Thanks for sharing your method. By the way, I am a beginner Kaggler. By the way, how much does it help to know college level math (linear algebra, statistics) when playing Kaggle? Or do you need mathematics from another field? Did your mathematical intuition help you when you were doing this competition? I am interested in math, but the atmosphere around me does not study math well. I am curious. I'm not sure the way what to study. This is a sudden question.üòÖ Congratulations on winning the competition!üéâ Chris Deotte Topic Author Posted 4 days ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Personally, I feel math is very helpful. It is the language which helps me understand how all AI, DL, ML algorithms work. Knowing how things work helps me to utilize them to the fullest. Also it helps to understand metrics and losses and how to optimize them. Having said that, there are many Kagglers without a math background that perform very well. So we can also gain intuition by just using lots of algorithms and learning how they work. Taekyung Posted 4 days ago arrow_drop_up 1 more_vert Very Thanks.üòÄüòÑü•∞ Good Luck. KIMBUMJU Posted 4 days ago arrow_drop_up 2 more_vert Dear Dr. Chris Deotte, Greetings. I am a recent university graduate from South Korea, currently preparing to become a data scientist. I would like to express my sincere gratitude for the incredible solutions you have shared across various Kaggle competitions. Studying and trying to understand your work has provided me with not only a wealth of knowledge but also great inspiration. As I carefully reviewed numerous solutions and discussions from different competitions, I noticed one particularly striking pattern: the public/private score shake-up in your submissions tends to be remarkably minimal compared to other participants. This insight became especially meaningful to me recently when I competed in a challenge on another platform. While my public score was quite strong, my private score turned out to be very disappointing. It was a frustrating experience, but it reminded me of your consistent performance and resilience to such shake-ups. Through this, I realized the critical importance of cross-validation strategies. I‚Äôm very eager to learn how you approach this issue. Could you kindly share what techniques or considerations you use to minimize public/private score differences? Do you also believe that cross-validation plays a key role in addressing this, as I do? If so, would it be possible for me to learn about your cross-validation strategy? I had hoped to reach out to you personally, but I couldn‚Äôt find a way to contact you directly, so I decided to leave a comment on one of your recent posts instead. I would greatly appreciate it if you could share your thoughts. Thank you very much for your time and consideration. Warm regards, Bumju Kim Chris Deotte Topic Author Posted 4 days ago ¬∑ 1st in this Competition arrow_drop_up 2 more_vert Hi. Nice to meet you. The secret to avoiding shakeup is cross validation. In every competition, I select my best CV score for my final submission. My model which achieves best public LB is different than my model which achieves best private LB. I view each competition as two challenges. One challenge is best public LB and second challenge is best private LB. What is confusing to observers is that the same model that achieves best public LB is not the same model that achieves best private LB. To achieve best public LB, we basically Hill Climb the public LB. We add models to our ensemble and keep those that increase public LB score. However at the same time, we build a second ensemble where we use Hill Climbing to improve local CV score. We select this second model as our final submission because this second model will generalize and achieve best private LB score. KIMBUMJU Posted 4 days ago arrow_drop_up 2 more_vert @cdeotte Thank you again for your kind response. Your willingness to share your ideas has been incredibly helpful to me. I now have a few follow-up questions, if you don‚Äôt mind: When optimizing the local cross-validation (CV) score, do you also use a Hill Climbing approach similar to what you use for improving the public leaderboard score? I‚Äôve heard that using a large ensemble of models can actually help prevent overfitting and improve generalization‚Äîdo you agree with this idea? I‚Äôm also curious about what kind of cross-validation strategy you tend to use most often. There are many options such as KFold , StratifiedKFold , GroupKFold , and RepeatedKFold . Since different strategies often yield different CV scores, how do you decide which one to rely on? Do you apply the same CV method for both classification and regression problems? In particular, for regression problems, if the target distribution is not normal, it‚Äôs not straightforward to apply stratification like in classification tasks. In such cases, how do you ensure the CV splits are reliable? Thank you very much for taking the time to read my questions! Chris Deotte Topic Author Posted 3 days ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Yes I use Hill Climbing locally. I publish code here . Note that some people use this code incorrectly in this competition. When we load the OOF and TEST PRED into numpy arrays x_train and x_test. We must preserve order. For example, the 4th OOF we load into x_train needs to be the corresponding OOF for the 4th TEST PRED we load into x_test. Most public notebook in June playground comp got this wrong. I usually just use basic KFold. Most often Kagglers use unnecessary fancy KFold types. There are times to use each of the different ones, but not every time like most Kagglers do. When choosing which KFold to use it is mostly about picking KFold so that each validation fold looks like test data. For example if test data has new unseen customers, then we want our validation folds to have new unseen customers etc etc. (i.e. we use GroupKFold) ). (Another example is we use StratifiedKFold (1) if there exists a rare class and (2) test data will preserve the proportion of the rare class. Cross validation is all about mimicking test data. If class A has 3% in train data, but class A might have 20% in test data, then we don't want to validate and force validation folds to have 3% because our production test data may actually have 20%. So in other words not using stratified often gives us a validation score that shows us more about how our model will generalize to unknown proportions of class A in test). KIMBUMJU Posted 3 days ago arrow_drop_up 0 more_vert @cdeotte Thank you sincerely for taking the time to provide such a high-quality response. While reading your explanation, one question came to mind. You mentioned that \"the key to cross-validation is to mimic the test data,\" but I find it a bit difficult to fully grasp this concept. Since the target variable only exists in the training dataset and not in the test dataset, how can we make the validation data's distribution or characteristics resemble that of the test set? For instance, let's say both the training and test datasets consist of 3,000 rows. When creating a validation set using only a subset of the training data (around 300 to 500 rows), how is it possible to ensure that this much smaller validation set‚Äîcontaining 5 to 10 times fewer samples‚Äîstill resembles the test set in terms of distribution and characteristics? Are there any concrete methods or perspectives for building a validation dataset that closely mirrors the test set in order to perform more sophisticated cross-validation? Thank you again for your time and for reading this. I would sincerely appreciate it if you could share how an expert like yourself approaches this kind of problem. Optimistix Posted 4 days ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Congrats once again! One of the early ideas I had was to simply repurpose your solution from Feb - should probably have done it. Chris Deotte Topic Author Posted 4 days ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert Hi Optimistix. Congratulations achieving 5th place. Yes, this competition is just like February playground with the difference being the metric (because both comps have zero signal in the original dataset. And both comps require us to \"reverse engineer\" the synthetic data generation process). What surprised me is why an ensemble was needed here. In Februrary, I won with a single model. I thought I could build another single model here. But a single model was never strong enough here. Optimistix Posted 4 days ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Yes, the zero signal in the original dataset was exactly why I thought of adapting your solution from February. Interestingly, this month we seem to have gone from zero signal to zero original data üòÄ Chris Deotte Topic Author Posted 4 days ago ¬∑ 1st in this Competition arrow_drop_up 1 more_vert haha, yup this month looks like small data comp. There isn't much to do in small data playground comps. I suggest techniques from March Rainfall playground comp. Minimal to no feature engineering and equal weight ensemble of a few diverse models. And use original data in simple diverse ways. Optimistix Posted 4 days ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Thanks! Shall do something along those lines. Also good in a way, since I have some other important stuff lined up this month. Have a great July! Natsu.sv9 Posted 4 days ago ¬∑ 939th in this Competition arrow_drop_up 1 more_vert This is very well explained and knowable. Thank you for such explanation. Mohamed Adel Posted 4 days ago arrow_drop_up 1 more_vert Impressive work! I appreciate you sharing it here. statify Posted 4 days ago ¬∑ 58th in this Competition arrow_drop_up 1 more_vert @cdeotte really appreciated how clearly you explained. Too many requests error Too many requests",
      "Predicting Optimal Fertilizers | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 days ago Late Submission more_horiz Predicting Optimal Fertilizers Playground Series - Season 5, Episode 6 Predicting Optimal Fertilizers Overview Data Code Models Discussion Leaderboard Rules Masaya Kawamata ¬∑ 2nd in this Competition  ¬∑ Posted 5 days ago arrow_drop_up 47 more_vert 2nd Place Solution - L3 Ensemble of 100+ OOFs First of all, I want to express my gratitude to the organizers for hosting another incredibly fun Playground competition this month. I'd also like to thank everyone who shared discussions, code, and various insights. It was a very enjoyable and educational experience. My solution is a 3-layer ensemble: 100 OOF predictions (from a 5-fold SKF) ‚Üí L2 models (XGB, LGBM, LogReg, NN) ‚Üí L3 Hill Climb . All experiments and submissions were done exclusively using Kaggle Notebooks. L1 - The Model Zoo Multiple XGBoost models trained with different combinations of categorical features. Some were trained on the original dataset only, while others were trained on train + original (with a weight of 4.0 for the original data). An XGBoost model pre-trained on the original data and then fine-tuned on the train data. XGBoost models trained with additional engineered features, such as binned features, clustering features, and Target Encoding. An XGBoost model that included latent features generated by a Supervised AutoEncoder. XGBoost models using auc and merror as their eval_metric . ExtraTrees , RandomForest , and NN models trained using the predictions of XGBoost as features. Several LightGBM models, mirroring the XGBoost experiments. Logistic Regression and three types of Neural Networks (TabTransformer, simple NN). Here is a summary of the CV scores for the main L1 models: Model CV (mAP@3) All Cat XGB 0.37768 All Cat LGBM 0.37483 CB over XGB 0.37707 Finetune XGB 0.37753 NN 0.36067 XGB + FE 0.345 - 0.365 Most other models had scores between 0.34 and 0.369. While they didn't perform as well individually, they contributed to the diversity of the ensemble. Notably, models with added features like Target Encoding or latent features from the Supervised AutoEncoder scored around 0.35, but significantly improved the final ensemble performance. L2 & L3 - Stacking and Hill Climbing I used 100 of the OOF predictions to train L2 stacking models: XGBoost , LightGBM , Logistic Regression , and a Neural Network . For Logistic Regression and the NN , I found that using too many OOF features hurt both CV and LB performance. I used Optuna to select the best subset of OOFs for these models. I wanted to use forward selection, but opted for Optuna due to computational constraints. For XGBoost and LightGBM , I used all 100 OOF features without any parameter tuning. Finally, I blended the predictions of these L2 models using a L3 Hill Climb algorithm, optimizing directly for mAP@3 , to create my final submission. Here are the CV scores for the L2 and L3 models: Model CV (mAP@3) CV (logloss) L2_NN 0.38321 1.87040 L2_LGBM 0.38343 1.87014 L2_XGB 0.38348 1.86997 L2_LogReg 0.38362 1.87074 L3_Hill Climb 0.38418 1.86958 Discussion & Thoughts As many of us discovered, XGBoost performed exceptionally well in this competition. It was difficult to achieve a similar score with CatBoost. I haven't yet reached a satisfying conclusion as to why there was such a significant performance gap. One interesting observation was the optimal max_depth . When treating all columns as categorical , the optimal max_depth was around 7-8 . When treating numerical columns as numbers, the optimal max_depth was much higher, around 16-18 . Here's my hypothesis: By treating numerical columns as categorical, the model can learn non-linearities and interactions more efficiently with fewer splits, while also being less sensitive to outliers. This combination likely allowed for a more robust and generalizable model to be built with a shallower max_depth (7-8). The very high max_depth of 16 when using raw numerical features might have been a sign of the model trying to force-fit the data's complex relationships through a series of linear splits, potentially leading to overfitting. Categorization seems to have provided a \"hint\" that helped the model interpret these complexities more appropriately. A question for the community: I'm still uncertain about using Hill Climb as an L2 model. Since it's a purely linear combination, I suspect that cross-validation might not be strictly necessary, though it feels like the safest approach. Is it effective as an L2 model, or is it better reserved for a final L3 blend? I would be very grateful to hear your thoughts, experiences, or theoretical explanations on this topic. (As an aside, in my experiments, the selected OOF combinations were nearly identical across folds, which suggests CV might not have been essential). Key Takeaways from This Competition Ensembling is Key: When using Hill Climb, it's more important to have diverse models (even with varying scores) than a collection of similar high-scoring models. Stacking with Tree Models: XGBoost and LightGBM are relatively robust to noise when used as stackers, making OOF feature selection less critical. Stacking with Linear/NN Models: Logistic Regression and NNs are more sensitive to noise, and their performance often improves with careful OOF feature selection. Weighting Original Data: Giving a significantly larger weight to the original dataset can sometimes lead to a performance boost. Optimization Target: While directly optimizing mAP@3 is difficult (optimizing mlogloss is usually a good proxy), Hill Climbing seemed more stable when optimizing for mAP@3 directly rather than for logloss . Feature Engineering: Treating all features as categorical (or binned) can be a very effective strategy, but be aware that it can change the optimal hyperparameters, like max_depth . The Final Week: It's best to ignore the Public Leaderboard in the final week to avoid overfitting and second-guessing your CV strategy. 4 Please sign in to reply to this topic. comment 23 Comments Hotness Sadia Shahid latif Posted 4 hours ago ¬∑ 2079th in this Competition arrow_drop_up 3 more_vert Thanks u shared ur work . It's very useful @masayakawamata Taylor S. Amarel Posted 19 hours ago ¬∑ 1002nd in this Competition arrow_drop_up 1 more_vert Thanks for sharing, the diagram really helped me grasp the hill climb implementation. Anand Vashishtha Posted 20 hours ago ¬∑ 1874th in this Competition arrow_drop_up 1 more_vert awesome, it helps me a lot from learning perspective Pratik Singh668 Posted 2 days ago ¬∑ 1168th in this Competition arrow_drop_up 1 more_vert congratulations , good work Ole-Jakob Posted 2 days ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Congrats on 2nd place, @masayakawamata , and thanks for a very well-written solution description! When treating all columns as categorical, the optimal max_depth was around 7‚Äì8.\nWhen treating numerical columns as numbers, the optimal max_depth was much higher, around 16‚Äì18. I noticed something similar‚Äîvery high max_depth values in XGBoost when treating features as numerical. However, I also applied those high depths to categorical features, which in hindsight might have been a mistake. For some of my models, I ended up using max_depth=17 , and I was surprised XGBoost needed such deep trees. At that depth, the potential number of leaf nodes is 2^17 = 131,072 , which is enormous. Of course, max_depth just sets the upper bound; the model typically uses far fewer nodes if that leads to better performance. I did a quick analysis and found that the average depth across all trees was around 16.8 , and the average number of leaf nodes was about 1,500 . That suggests most samples follow very narrow paths through deep tree structures. In the Playground Series, I haven't seen much discussion around why XGBoost prefers such deep trees in some cases. With the help of LLMs generating introspective code, I might explore this further in an upcoming PGS round. Denver Magtibay Posted 3 days ago ¬∑ 1044th in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for sharing your knowledge to us! Albina Chimidova Posted 3 days ago ¬∑ 665th in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for sharing your solution ideas. It is very important for me to learn from best practices as a beginner in data science. Ty-Yuki Posted 4 days ago ¬∑ 138th in this Competition arrow_drop_up 1 more_vert Thank you for sharing. I learned a lot from your discussion of XGBoost's max_depth . Haruki Kakinuma Posted 4 days ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Is HC effective as an L2 model, or is it better reserved for a final L3 blend? In my approach, I used HC and Ridge as the models before taking the final weighted average. This choice was based on the fact that it resulted in the best CV score, compared to using other stacking models in the weighted average. However, I must admit I had some doubts about this decision. Here is how I see it: Unlike stacking models, HC allows the use of MAP as the loss function, which happened to work well in this competition. (Minimizing log loss is not a necessary and sufficient condition for minimizing MAP.) I wasn't able to fully tune the stacking method, and HC ended up playing a compensatory role. I hope this comment helps you solve your question. Masaya Kawamata Topic Author Posted 4 days ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Thanks a lot for sharing, @harukikakinuma ! I really appreciate your valuable insights. I've been thinking about the same question myself. My understanding is that to evaluate an L2 model accurately with cross-validation, it's crucial to use the same fold splits as the L1 models to prevent data leakage, which can lead to overly optimistic CV scores. I recognize this is especially important for complex, non-linear models like XGBoost or LightGBM that can easily overfit to leaks. I was wondering if this same principle strictly applies to Hill Climbing. After analyzing my submissions, I found that my best private LB score of 0.3853 (though I didn't end up selecting it) came from a final L3 ensemble that included an L2 Hill Climb model created without cross-validation. When I looked at the performance of the L2 Hill Climb models individually, there was almost no difference between the one built with CV and the one without. It seems the difference was within the margin of error. This leads me to believe that cross-validation might be unnecessary when using Hill Climbing as an L2 model. Haruki Kakinuma Posted 4 days ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert My understanding is that to evaluate an L2 model accurately with cross-validation, it's crucial to use the same fold splits as the L1 models to prevent data leakage, which can lead to overly optimistic CV scores Ah, I see! That makes sense now. Sorry ‚Äî it seems I misunderstood your point and gave a slightly off-topic response. When I mentioned including HC in L2, I was approaching it under the assumption that you were questioning whether it‚Äôs a good idea from a methodological perspective ‚Äî something like, ‚ÄúIf we‚Äôre going to use HC at the final layer, is it really appropriate to include HC in the input layer as well? Wouldn‚Äôt it make more sense to feed the OOF results from the input layer‚Äôs HC directly into the L3 HC instead?‚Äù That‚Äôs the angle I was answering from. This choice was based on the fact that it resulted in the best CV score Also, as you pointed out, it was incorrect of me to refer to the HC result as a ‚ÄúCV score.‚Äù I should‚Äôve said the overall MAP value instead. Haruki Kakinuma Posted 4 days ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Let me once again share my interpretation below: In this competition, there was a significant amount of artificial noise added, so there were many cases where even similar outputs resulted in different answers. Because of this, data leakage didn‚Äôt occur even without using CV for evaluation. Unlike stacking models, HC allows the use of MAP as the loss function, which happened to work well in this competition. (As I mentioned) For the above reasons, the risk of data leakage was quite low in this competition, which amplified the advantages of HC. I believe that‚Äôs why achieving the top score on the Private LB was not just a coincidence, but rather a natural outcome. statify Posted 4 days ago ¬∑ 58th in this Competition arrow_drop_up 1 more_vert @masayakawamata congratulations! Thanks for your clear and step by step explanation Roman@Ahmed Posted 4 days ago ¬∑ 1317th in this Competition arrow_drop_up 1 more_vert heartfelt congratulations, @masayakawamata Faryal Rifaz Posted 4 days ago ¬∑ 1551st in this Competition arrow_drop_up 1 more_vert Congratulations! Your work is truly appreciable and inspiring. I'm a new user on Kaggle, and going through your solution has helped me learn a lot, especially your clear approach and smart techniques. Thank you for sharing your knowledge with the community. H. Buƒüra Eken Posted 5 days ago ¬∑ 22nd in this Competition arrow_drop_up 1 more_vert Thanks for the detailed and insightful write-up. Your approach of using different OOF selection strategies for tree-based vs. linear/NN models in the L2 stacking layer was a very clever one. Congrats!! Optimistix Posted 5 days ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Congrats! You've been making great contributions with your code as well as posts, and it's wonderful to see you right at the top. Very well deserved! Masaya Kawamata Topic Author Posted 5 days ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Hi @optimistix , Thank you for your always kind words! And a huge congratulations to you on your 5th place finish. Seeing you join midway through the competition and immediately start climbing the leaderboard was a great source of motivation for me. I look forward to seeing you again in the next competition! Optimistix Posted 4 days ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Same here - looking forward to your contributions. Best to limit efforts given the small and seemingly easy dataset, though. All the best! DanteTheAbstract Posted 5 days ago ¬∑ 20th in this Competition arrow_drop_up 1 more_vert Excellent wright up @masayakawamata and well done on the 2nd place finish. Could you explain this method you mentioned, I'm not familia with it An XGBoost model pre-trained on the original data and then fine-tuned on the train data. About your question on hill climbing, I think using it as an L2 model works really well, for example my solution was just hill climbing on about 40 hand crafted models, most just using default params. Only 19 of which we selected, but still achieved a decent result (it could have been much better but was unlucky with submission selection). Masaya Kawamata Topic Author Posted 5 days ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Hi @dantetheabstract , Thanks for the kind words, and congrats on your result! I really appreciate you sharing your experience with Hill Climbing‚Äîit's very insightful. Regarding the pre-training and fine-tuning method, it's a two-step transfer learning process designed to get the most out of both the small, original dataset and the larger, synthetic one. The core idea is to first learn general patterns from the high-quality original data, and then adapt that knowledge to the specific train data of the competition. Here‚Äôs what that looks like in code: # Step 1: Pre-train a model on the original (real) data to create a strong baseline. pretrain_model = xgb.XGBClassifier(**params)\npretrain_model.fit(X_orig, y_orig) # Step 2: Use the first model as a starting point for fine-tuning on the competition data. # This would typically be inside a cross-validation loop. finetune_model = xgb.XGBClassifier(**params) # The `xgb_model` parameter transfers the knowledge from the pre-trained model. finetune_model.fit(\n    X_train_fold, \n    y_train_fold,\n    xgb_model=pretrain_model # <-- This is the key part! ) content_copy By passing xgb_model=pretrain_model, we're telling the new model to start with the wisdom of the pre-trained one, which is much more effective than learning from scratch. Your success using Hill Climbing is very impressive and a great example of its power. Thanks again for sharing! Hope this helps clarify the method! DanteTheAbstract Posted 5 days ago ¬∑ 20th in this Competition arrow_drop_up 1 more_vert Thanks, yeah I get it now, didn't know xgboost could do that. Also learned about dtrain.set_base_margin from @cdeotte 's solution that can begin the boosting from the predictions of another model. Derya Umut Kulali Posted 5 days ago ¬∑ 32nd in this Competition arrow_drop_up 1 more_vert Congratulations. Thanks for the solution. Haruki Kakinuma Posted 5 days ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Congratulations!! Thank you so much for sharing so many model improvements! „Ç¨„ÉÅ„Åä„ÇÅ„Åß„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅ Masaya Kawamata Topic Author Posted 5 days ago ¬∑ 2nd in this Competition arrow_drop_up 0 more_vert Congratulations on 7th place! That's a fantastic achievement. I also wanted to say that your discussion posts were incredibly helpful and insightful throughout the competition. I learned a great deal from them. „ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅÔºÅÔºÅ paperxd Posted 5 days ago ¬∑ 6th in this Competition arrow_drop_up 2 more_vert Hi! Your model structure with 4 L2 models is really interesting! I am intrigued by how you managed to even use NN, XGBoost, and LightGBM has a L2 model due to their complexity. Did you opt for a lower depth on the GBDTs? How did you make your Neural Network into an effective ensembler? How did Optuna help choose which OOFs would be good to feed into the NN and LogReg? Also, seeing your question, I am slightly confused because I never used cross-validation with hill climbing. I frequently use hill climbing  as a L2 model and I like to reserve L3 model for a simple mean or anything of similar simplicity. However, I think you should just trust your gut since you are one of the best ML coders I know. paperxd Masaya Kawamata Topic Author Posted 5 days ago ¬∑ 2nd in this Competition arrow_drop_up 1 more_vert Hi paperxd, Thanks for your interest and questions! And a huge congratulations to you on finishing in 6th place! Achieving that with a short submission period is truly impressive. You're spot on about the GBDTs. I set max_depth=3 for both XGBoost and LightGBM. I found that anything deeper than that seemed to degrade the performance. For the NN, it's a single-layer model with 128 units. I used Optuna to find the top 3 combinations of OOFs, and then created the oof and test_pred for each of those combinations. Regarding the feature selection with Optuna, it gave a nice boost to the CV scores: LogReg: improved from 0.3828 to 0.3836 NN: improved from 0.3825 to 0.3832 I performed feature selection for these two models because I noticed that simply adding more OOFs would sometimes decrease the CV score. This process ultimately led to a better Private LB score, taking it from 0.38472 to the 0.38518-0.38527 range. And thank you for the kind words! I really appreciate it. paperxd Posted 4 days ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert That is a smart idea; using optuna to mix your OOF predictions so you end up with less noise from worse predictions is revolutionary! Too many requests error Too many requests",
      "Predicting Optimal Fertilizers | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 days ago Late Submission more_horiz Predicting Optimal Fertilizers Playground Series - Season 5, Episode 6 Predicting Optimal Fertilizers Overview Data Code Models Discussion Leaderboard Rules Mahog ¬∑ 3rd in this Competition  ¬∑ Posted 5 days ago arrow_drop_up 15 more_vert 3rd place - Ridge and CV are all you need TLDR: Slowly built an ensemble of 60+ models over the month Searched for a good ensembler in the first 2 weeks Relied on the various XGB hyperparameters in public notebooks Trust CV This month I focused on having a wide variety of different models (like XGB vs NN) instead of the same model with different FE (there is still some FE in here though). Here are some of the tricks I used: Ridge as ensembler At the beginning of the comp, I used HC as the ensembler. But it had an issue: adding a model often decreased the CV score. Then I tried genetic algorithms (GA). It worked pretty well up to about 20 models, but it had the same issue as HC after going beyond that. After searching for a while, I settled on Ridge. It was very fast (about 1 minute for my 60 OOFs) and adding a model almost always improved the CV score, so I used it to the end. Some notes on using Ridge: The oofs were stacked in the shape of (n_samples, n_models * 7) One-hot encode the target and use multi-target regression Turn classification into regression I trained a few XGBs and NNs using multi-target regression with MSE loss by one-hot encoding the target. I would have trained more regression models like this but ran out of time. XGB with product features, label encoded Late into the competition, I decided to use product features to add diversity, but I accidentally label encoded the new features. It worked surprisingly well and boosted CV by 0.0001. Diverse libraries During my quest for the most amount of unique models, I found some pretty cool libraries: pytabkit pytorch-tabular rtdl_num_embeddings Without these libraries I would probably not be in top 10 Conclusion I first want to thank all the people who have helped me reach this far, including but not limited to: @cdeotte , @optimistix , @siukeitin , @ravi20076 , @masayakawamata , @omidbaghchehsaraei , @ravaghi , @yekenot (I probably missed a lot of people). Through these past 3 months, I have managed to get 3 top 10 finishes, some Kaggle Swag, and an important learning: \"CV and Ridge is all you need\". Additional notes My final list of models: 3 \"all-embedding\" NNs based on here 1 Autogluon 5 Catboosts 1 Gandalf NN from pytorch-tabular 2 GRNs based on here Some LAMA NNs 5 LGBMs 1 LNN based on here 3 Logistic Regression models based on here 1 normal NN from here OOFs from here 1 RealMLP from pytabkit 3 Pytorch NNs with rtdl_num_embeddings 1 SAINT model 1 TabTransformer model from here 3 TabMs from pytabkit 20 XGBs 1 ExtraTrees model 1 GradientBoostedTreesLearner from ydf I had also managed to select my 2 best submissions on the private LB, and funnily enough, my best CV was the second best on private LB, and my best public LB was my best private. Never seen that happen before. Please sign in to reply to this topic. comment 8 Comments Hotness Ole-Jakob Posted 2 days ago ¬∑ 10th in this Competition arrow_drop_up 0 more_vert Congrats on 3rd place, @mahoganybuttstrings ! Impressive work‚Äîsuch a diverse and extensive set of models, and a well-deserved top finish. Thanks also for introducing some fresh libraries. It‚Äôs always inspiring to explore new tools and avoid getting stuck in the same old modus operandi that's been around forever. Roman@Ahmed Posted 4 days ago ¬∑ 1317th in this Competition arrow_drop_up 0 more_vert Huge shoutout to  winner Sujal Neupane Posted 4 days ago ¬∑ 23rd in this Competition arrow_drop_up 0 more_vert Congratulations!!! Ridge worked the best for me as well.. Haruki Kakinuma Posted 5 days ago ¬∑ 7th in this Competition arrow_drop_up 0 more_vert Congratulations!! You're truly embodying trustCV ‚Äî I really admire you! Thank you so much for all your advice throughout this competition! Mahog Topic Author Posted 4 days ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thanks! Congrats to you too! Optimistix Posted 5 days ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Congrats! This is the way üòÄ Mahog Topic Author Posted 4 days ago ¬∑ 3rd in this Competition arrow_drop_up 1 more_vert Thanks! Congrats to you too! It's insane how consistent you are. Optimistix Posted 4 days ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Thanks! It's getting harder, since you and a few others have joined our group of assiduous ensemblers üòÄ Mahog Topic Author Posted 4 days ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Fair point :D Taylor S. Amarel Posted 5 days ago ¬∑ 1002nd in this Competition arrow_drop_up 0 more_vert Great write up! Really appreciate you walking us through the thought process. Too many requests error Too many requests",
      "Predicting Optimal Fertilizers | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 days ago Late Submission more_horiz Predicting Optimal Fertilizers Playground Series - Season 5, Episode 6 Predicting Optimal Fertilizers Overview Data Code Models Discussion Leaderboard Rules hahahaj ¬∑ 4th in this Competition  ¬∑ Posted 5 days ago arrow_drop_up 15 more_vert 4th Place - Stacking Ensemble using XGB only Thanks Kaggle for this fun playground competition!  As my first playground competition, this experience has been incredibly valuable. I've learned numerous ML techniques from the generous Kaggle community members who shared their approaches. Final Submissions Best CV (0.3842 CV score) : 50-model ensemble using XGB Best LB : (0.38384 public score and 0.38454 private score) Ensemble Architecture The winning approach was an ensemble with XGBoost only and it contained the following: 1. TabTransformer Models (3 total) 2 TabTransformers with feature engineering 1 TabTransformer without feature engineering Reference: TabTransformer 2. Gradient Boosting Models YDF : 2 models (no FE) CatBoost : 3 models (2 with FE, 1 with feature selection) LightGBM : ~10 models including: Standard LGBM with/without FE LGBM GOSS Different feature combinations XGBoost : Multiple configurations: Original data only Augmented data Repeated KFold Diverse feature sets 3. Neural Networks (6 total) Various architectures with different feature engineering Reference: NN 4. Other Models 2 AutoGluon models with FE 2 Random Forests (no FE) Linear models: 1 Linear Regression 1 SGD model Reference: Linear Regression Please sign in to reply to this topic. comment 16 Comments 2 appreciation  comments Hotness Ole-Jakob Posted 2 days ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Congrats and really impressive for your first playground competition. Welcome! Thanks for sharing the models. I will look into using YDF as an additional tool in the next PGS. Ty-Yuki Posted 4 days ago ¬∑ 138th in this Competition arrow_drop_up 1 more_vert Thank you for sharing! Your XGBoost-only ensemble is very impressive. Faryal Rifaz Posted 4 days ago ¬∑ 1551st in this Competition arrow_drop_up 1 more_vert Congratulations üéâ Very Insightful , I learned so many things. Thanks for sharing with us Upcoming Grandmaster Posted 4 days ago ¬∑ 158th in this Competition arrow_drop_up 1 more_vert Congrats ,Impressive XGB-only stack‚Ä¶‚Ä¶great diversity via data and folds Roman@Ahmed Posted 4 days ago ¬∑ 1317th in this Competition arrow_drop_up 1 more_vert Celebrate your well-deserved success Yx Posted 5 days ago ¬∑ 141st in this Competition arrow_drop_up 1 more_vert Congrats! Your notebook of XGB is also easy-to-follow and impressive, it really helped me get started with this competition. Thanks so much for your kind sharing and help. Optimistix Posted 5 days ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Congrats! That's very impressive for your first playground competition! Soumyojit Sen Posted 5 days ago ¬∑ 298th in this Competition arrow_drop_up 1 more_vert Congratulations for your dominating leaderboard position. Since I'm new to competitions and this was my first one, I just wanted to ask if you could actually guide on some notebooks about how do we implement ensemble models in competitions. I'm also seeing a lot of top kaggle users uses \"Hill Climbing\" technique, which is also unfamiliar to me, if you could suggest some introductory notebooks for building ensembles regarding competitions, that would be much helpful :) hahahaj Topic Author Posted 5 days ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert Stacking Ensemble comes from @ravaghi Ôºõ Hill Climbing comes from @cdeotte Haruki Kakinuma Posted 5 days ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Congratulations! I used some of your work as a reference when building my model‚Äîthank you for the help! hahahaj Topic Author Posted 5 days ago ¬∑ 4th in this Competition arrow_drop_up 0 more_vert You are welcome. Congrats to you too! paperxd Posted 5 days ago ¬∑ 6th in this Competition arrow_drop_up 2 more_vert Hello! Congrats on your top finish! You seem to have a very diverse model layout, so I think you are best suited for this question. How did your NN models compared to your GBDT models? NN models seem to always perform a lot worse, so I didn't incorporate NNs into my ensemble. Also, what made you decide to have XGBoost as your meta-predictor? How could such a complex model be used as an L2 model? Thanks, paperxd Komil Parmar Posted 5 days ago ¬∑ 261st in this Competition arrow_drop_up 1 more_vert Great question, I have the same to ask hahahaj Topic Author Posted 5 days ago ¬∑ 4th in this Competition arrow_drop_up 1 more_vert Hi paperxd! Congrats to you too. My NN model isn‚Äôt as good as my GBDT models (CV scores between 0.350-0.365), but it still improves my CV score when ensembled. I chose XGBoost as my meta-predictor because I can tune its parameters to control complexity and it generally performs best. paperxd Posted 4 days ago ¬∑ 6th in this Competition arrow_drop_up 1 more_vert Thanks for your reply! I previously thought NN models were worthless but I guess I was proven wrong. Appreciation (2) H. Buƒüra Eken Posted 5 days ago ¬∑ 22nd in this Competition arrow_drop_up 1 more_vert congrats!! Derya Umut Kulali Posted 5 days ago ¬∑ 32nd in this Competition arrow_drop_up 1 more_vert Thanks for the solution. Too many requests error Too many requests",
      "Predicting Optimal Fertilizers | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 days ago Late Submission more_horiz Predicting Optimal Fertilizers Playground Series - Season 5, Episode 6 Predicting Optimal Fertilizers Overview Data Code Models Discussion Leaderboard Rules Optimistix ¬∑ 5th in this Competition  ¬∑ Posted 5 days ago arrow_drop_up 22 more_vert 5th Place Solution - An ensemble of 53 OOFs TLDR: Mostly stayed away for the first two weeks, participated more or less normally for the next 10-12 days, had many plans for the last few days but ran out of time as other things came up. I had three solutions which scored > 0.385 on the private LB, the one with the highest CV scored 0.38509 and would have secured 4th place. But I didn't choose it, the one with the best public LB (0.38337) scored 0.38502 and got me fifth place. I watched with bemusement as I dropped about a 100 places from #2 to about #102 or so over the past few days, as the blendy blenders went about systematically overfitting the public LB, and the floaty followers formed a solid block (or few) of several dozens, pushing the rest of us artificially behind for a few days - so there's a certain satisfaction in being on the right side of the shakeup. Drivers of diversity in the ensemble Model type: While most of my models were XGBs or LGBMs, the ensemble also included CatBoost, HGB, Neural Nets, Random Forests and Logistic Regression. I didn't get around to adding other models like SVMs. Playing around with Hyperparameters: Many people use GOSS and DART variants of LGBMs, but one can also play around with various hyperparameters e.g. depth, number of estimators, max leaves, scoring function, evaluation metric etc. Augmentation with the original data : I used varying number of copies of the original dataset, from 1-6. I avoided 7 and above because after a point, you're biasing away from the competition data towards the original (e.g. 8 copies was literally more than the training data). Using numerical data as categorical: Treating all data as categorical as well as treating numerical features as both numerical and categorical helped, with or without binning. Varying fold numbers: Varying the number of folds in k-fold CV can also add to diversity. Things I started too late Treating numerical data as categorical and using models other than XGBs and LGBMs (esp. NNs) all boosted the CV and usually LB as well - I should have added more such models. I should also varied more hyperparameters, esp. with XGBs, and added more CatBoost models. Things I never got around to doing A long list, but one that I considered early and probably should have prioritized was to repurpose @cdeotte 's winning solution from February, since the original dataset had nearly no signal, just like that month. Finally,  I'd like to thank those who shared code and insights, including but not limited to @ravaghi , @suikeitin , @masayakawamata , @hahahaj , @omidbaghchehsaraei , @pirhosseinlou , @ayushchandramaurya , @patrykszcz , @gauravduttakiit and congratulate @masayakawamata , @mahog , @hahahaj , @paperxd , our resident Topper-in-chief @cdeotte , and everyone who finished strong. Happy Kaggling! 1 Please sign in to reply to this topic. comment 11 Comments Hotness Ty-Yuki Posted 4 days ago ¬∑ 138th in this Competition arrow_drop_up 1 more_vert Congratulations! I learned a lot about ensemble techniques. Faryal Rifaz Posted 4 days ago ¬∑ 1551st in this Competition arrow_drop_up 1 more_vert Congratulations! That's amazing. Roman@Ahmed Posted 4 days ago ¬∑ 1317th in this Competition arrow_drop_up 1 more_vert heartfelt congrats Hamza Bashir Posted 4 days ago ¬∑ 2249th in this Competition arrow_drop_up 2 more_vert heartfelt congrats H. Buƒüra Eken Posted 5 days ago ¬∑ 22nd in this Competition arrow_drop_up 1 more_vert The \"ensemble of 53 OOFs\" is an incredible effort. If you get the time, it would be great if you could briefly share what the source of diversity was for these models (different algos/features?) and how you combined them (a simple blend or a meta-model?). Congrats!! Optimistix Topic Author Posted 4 days ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Thanks! Usually, I'd compile a bigger collection, but had scaled back participation in June. I was in a hurry yesterday, but have added some details about ensemble diversity now - let me know if you have any questions. H. Buƒüra Eken Posted 3 days ago ¬∑ 22nd in this Competition arrow_drop_up 1 more_vert Thank you so much for taking the time to update your post with detail. The new section clarifies exactly what I was curious about. Congrats again! Optimistix Topic Author Posted 3 days ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Glad to hear that, and thanks so much! Haruki Kakinuma Posted 5 days ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Congratulations! Your consistency is truly impressive. Optimistix Topic Author Posted 5 days ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Thanks so much, and congrats on your 7th place finish! Ali_Haider_Ahmad Posted 5 days ago ¬∑ 206th in this Competition arrow_drop_up 1 more_vert Congratulations ü•≥ Optimistix Topic Author Posted 5 days ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Thanks so much! Ole-Jakob Posted 2 days ago ¬∑ 10th in this Competition arrow_drop_up 0 more_vert Well done @optimistix , and congrats on yet another top placement! You continue to be impressively consistent among the top contenders in the PGS. \"blendy blenders went about systematically overfitting the public LB, and the floaty followers formed a solid block\" I had a very similar experience. It seems like this has become the norm‚Äîoverfitting the public leaderboard in the final days, followed by dramatic shake-ups. At this point, it's probably wise to ignore the public LB entirely during the last week. It‚Äôs also a bit unfortunate how this trend clutters the code repositories, making it harder to discover the genuinely insightful notebooks created by experienced contributors. \"Augmentation with the original data: I used varying numbers of copies of the original dataset, from 1 to 6‚Ä¶\" I did something similar using a 4x duplication. But honestly, I still don't fully understand why this helps‚Äîespecially if the original dataset has no clear signal. Intuitively, if there's no useful information in the original data, we might just be amplifying noise in an already noisy setting. Too many requests error Too many requests",
      "Predicting Optimal Fertilizers | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 days ago Late Submission more_horiz Predicting Optimal Fertilizers Playground Series - Season 5, Episode 6 Predicting Optimal Fertilizers Overview Data Code Models Discussion Leaderboard Rules paperxd ¬∑ 6th in this Competition  ¬∑ Posted 5 days ago arrow_drop_up 13 more_vert 6th place - 1 week rush Wow! I am impressed that I somehow managed to somewhat make a comeback. My Scuffed Solution Early on I think everyone noticed CatBoost was really bad, so I never used CatBoost in my ensemble. My ensemble was only made of XGBoost (70% of the oofs) and LightGBM (30% of the oofs). I collected 135 oofs and preds that were all created by myself, because one thing I learned was that a single bad oof or pred prediction sourced from a public notebook without careful check could result in a really bad leaderboard score. My final model architecture was a simple mean between a logistic regression and hill climb of all my oofs. I trained all my models locally on my 4070 super, works really well. Some Secrets I might have gatekeeped I discovered that setting all the data types to categorical improved the score early on Adding original data also drastically improved the score XGBoost sampling_method = gradients somehow worked better even with high subsample? XGBoost refresh_leaf = 0 also slightly improved the score, might be overfitting Things that sucked CatBoost Feature Engineering Slow Training Times, I think 95% of the models use a learning rate of 0.05 üòÖ Summer School starting so I only had like 1 week before I couldn't prioritize coding anymore Logistic Regression taking 3 hours to run on the CPU Adding additional OOF predictions and somehow the Hill Climbing score becomes worse Happy Summer Break Unless you also have summer school PS How do you know if you won a prize? Please sign in to reply to this topic. comment 6 Comments 1 appreciation  comment Hotness Ole-Jakob Posted 2 days ago ¬∑ 10th in this Competition arrow_drop_up 0 more_vert Congratulations and thanks for sharing your approach! Roman@Ahmed Posted 3 days ago ¬∑ 1317th in this Competition arrow_drop_up 0 more_vert congrats and thanks for sharing your approach Optimistix Posted 5 days ago ¬∑ 5th in this Competition arrow_drop_up 0 more_vert Congrats on leading for a long time, and returning towards the end for a great finish. AngelosMar Posted 5 days ago arrow_drop_up 0 more_vert If you are prize winner they will send you an email in the next days Haruki Kakinuma Posted 5 days ago ¬∑ 7th in this Competition arrow_drop_up 0 more_vert Congratulations! You came up with a revolutionary approach early on‚Äîvery impressive! It was memorable how you stayed at the top of the leaderboard for such a long time early in the competition. Appreciation (1) Faryal Rifaz Posted 4 days ago ¬∑ 1551st in this Competition arrow_drop_up 0 more_vert Congratulations! Too many requests error Too many requests",
      "Predicting Optimal Fertilizers | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 days ago Late Submission more_horiz Predicting Optimal Fertilizers Playground Series - Season 5, Episode 6 Predicting Optimal Fertilizers Overview Data Code Models Discussion Leaderboard Rules Haruki Kakinuma ¬∑ 7th in this Competition  ¬∑ Posted 4 days ago arrow_drop_up 9 more_vert 7th place solution - HC + Ridge Thank you for the past month! It was an honor to achieve this ranking in a competition I participated in wholeheartedly from start to finish. In every competition, I make it a point to explore and deeply experiment with new techniques. In this competition, I challenged myself with multi-level ensembling, something I had never tried before ‚Äî up until now, I had only done simple weighted averaging of multiple models at most. Below, I will share my 1. model,  2. reflections, 3. acknowledgments. 1. model L1 For the first 20 days or so, I focused on implementing base models for ensembling. To increase diversity, I varied the following 3 aspects, tuned the remaining hyperparameters for optimal performance, and kept only the models that contributed to improving the overall prediction. how many original datas added to the training data treat numerical features as cat or int reg_alpha, reg_lambda params As a result, 7 XGB models(‚ÜíRIDGE1) and 6 LGBM models(‚ÜíRIDGE2) remained. I also made NN model (refer to here , changed some points like num of folds). L2 I used 2 public codes( one two ) in order to improve the score. I pruned the ensemble candidates that used approaches different from mine as much as possible. L3 At this layer, I ultimately used both HC and RIDGE. I also experimented with stacking models like NN and XGB, but they didn‚Äôt help improve the score at all, so I ended up abandoning them. Although I was initially hesitant to use HC at this layer, the cross-validation score for the final submission was clearly better, so I decided to go with it. L4 HC(L3) OOF : 0.38396, private LB : 0.38460 RIDGE(L3) CV : 0.38368, private LB : 0.38449 ‚Üílast submission(L4) CV : 0.38412, private LB : 0.38486 2. reflection positive aspect Reflecting on the previous competition and heeding the warnings from Discussions, I was able to fully trust my CV this time. To be honest, since I‚Äôm still relatively new to Kaggle, I used to get overly excited or discouraged by the public LB, but in the final week, I managed to stay sane :) negative aspect First of all, I completely forgot that ridge regression has a hyperparameter. It happened to work well with the initial setting, so I left it as is and got caught up in building the stacking model. I only realized this just now after looking at other people‚Äôs solutions‚Ä¶ Also, I had been naming all my submission files the same (ensemble_submission.csv), so I lost track of which submission was which. As a result, I ended up taking a bit of a gamble for the final submission. Miraculously, I submitted the one with the best CV score, but I definitely need to reflect on this. 3. acknowledgments I was able to come this far thanks to all the things I learned from everyone. While I can't mention every single person here, I‚Äôd like to express my gratitude to the following individuals. @siukeitin , @tilii7 , @richardjana , @masayakawamata @cdeotte , @robschieber , @paperxd , @mahoganybuttstrings @act18l , @gauravduttakiit , @ravi20076 , @yunsuxiaozi , @gowthamdd thanks!! Please sign in to reply to this topic. comment 9 Comments 1 appreciation  comment Hotness Ole-Jakob Posted 2 days ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Well done, and thanks for sharing your solution! I‚Äôm curious‚Äîwas there a specific reason you included the neural network output in the XGBoost-based Ridge model for L2 in your diagram? Or do you create a separate Ridge model for each model type in the L1 layer? Haruki Kakinuma Topic Author Posted 2 days ago ¬∑ 7th in this Competition arrow_drop_up 0 more_vert Thank you so much for your thorough feedback‚Äîit's very much appreciated! This might get a bit long, but to briefly explain how I ended up with the current structure: it was more a result of the development flow than a deliberate design choice. Honestly, I haven't tried other structures yet, but I suspect this current hierarchy isn't optimal. Haruki Kakinuma Topic Author Posted 2 days ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert To clarify, it's not that \"I intentionally included the NN in Ridge1\", but rather that \"the LGBM couldn‚Äôt be included in Ridge1 in the first place\". Initially, I implemented L2 using HC (hierarchical combination), not Ridge, and I was planning to use that version as my final submission. At that time, I tried to include LGBM in HC, but adding its OOF predictions didn‚Äôt improve performance. As a result, only XGBT and NN remained in the HC model. To ensure LGBM could still be used in the overall model, I created a separate ensemble (Ridge2) composed solely of LGBM models, and then used its output as one of the inputs to HC. Later on, after reviewing past competition solutions, I discovered more complex ensemble structures and decided to implement one of them. By that point, the LGBM in Ridge2 had already been optimized specifically for a standalone LGBM ensemble, so I left it as-is and included it in L2. As a result, the current setup consists of a Ridge model (Ridge2) made only from LGBMs, and another Ridge model (Ridge1) built from XGBT and NN. I believe there were still many aspects that could have been improved, including the points I mentioned as negative aspects in the solution and the structure I explained here. Moving forward, I aim to build more refined models and better overall processes Ole-Jakob Posted 2 days ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Thanks for the explanation. Having an iterative process is great‚Äîit helps us stay flexible and responsive to what works and what doesn‚Äôt. Roman@Ahmed Posted 3 days ago ¬∑ 1317th in this Competition arrow_drop_up 1 more_vert Congratulations! and ,thanks for sharing your approach, Haruki Kakinuma Topic Author Posted 3 days ago ¬∑ 7th in this Competition arrow_drop_up 0 more_vert ThanksÔºÅ I'll keep working hard to aim for a higher rankingÔºÅ Ty-Yuki Posted 4 days ago ¬∑ 138th in this Competition arrow_drop_up 1 more_vert Congratulations! Your HC + Logistic Regression approach was very impressive. Haruki Kakinuma Topic Author Posted 3 days ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Thanks! I‚Äôll make the most of what I‚Äôve learned from this competition and keep pushing forward. Faryal Rifaz Posted 4 days ago ¬∑ 1551st in this Competition arrow_drop_up 1 more_vert Congratulations ! insightful Appreciation (1) H. Buƒüra Eken Posted 4 days ago ¬∑ 22nd in this Competition arrow_drop_up 1 more_vert Congratulations!! Too many requests error Too many requests",
      "Predicting Optimal Fertilizers | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 days ago Late Submission more_horiz Predicting Optimal Fertilizers Playground Series - Season 5, Episode 6 Predicting Optimal Fertilizers Overview Data Code Models Discussion Leaderboard Rules Ole-Jakob ¬∑ 10th in this Competition  ¬∑ Posted 3 days ago arrow_drop_up 5 more_vert 10th Place Solution - ~350 oofs => 9 hillclimbing versions => Final Autogluon ensemble Introduction First and foremost, I want to extend a massive thank you to Kaggle for hosting the competition and to the community for sharing knowledge and creating interesting discussions. This was my first time working with the map@3 metric, and I found that ranking predictions give room for more creativity in the solutions. Congratulations to the maestro himself @cdeotte for winning yet another playground competition and to @masayakawamata , @mahoganybuttstrings , @hahahaj and @optimistix for giving him a run for his money. TL;DR Used standard models: XGBoost, CatBoost, LightGBM, NN, and Logistic Regression with minimal feature engineering. Generated ~350 Out-of-Fold (OOF) predictions. The best single model was an XGBoost with CV=0.37799 , tuned with Optuna, using 6 fixed folds and sample weight bumping. Created 20 versions of hill climbing ensembles, with the best achieving a CV=0.383830 . Used AutoGluon to ensemble the 9 best hill climbing results, reaching a final CV of 0.383918 and a Private LB score of 0.38472 . Solution Overview Each new version of hillclimbing had a larger set of oofs and the last one had ~350. Base Models XGBoost: XGBoost performed the best, with the top single model achieving a CV=0.37799 . Thanks to @ravi20076 for pointing out here in the early stage of the competition. The best parameters found were: Params = { 'max_depth' : 17 , 'min_child_weight' : 4 , 'subsample' : 0.8932774942420912 , 'colsample_bytree' : 0.40035993059700825 , 'gamma' : 0.44089845615519774 , 'reg_alpha' : 3.0189326628791378 , 'reg_lambda' : 1.0463133005441632 , 'max_delta_step' : 5 } content_copy This was achieved using sample weight bumping for 2nd, 3rd, and 4th places as described below. It is surprising that a depth of 17 was optimal, which I attribute to the synthetic nature of the data. LightGBM/CatBoost: Both were included for diversity, but they generally performed worse than XGBoost. Neural Networks (NN): I used a modified version of the excellent notebook by @paddykb : No Keras, No Loan . The best I could achieve was CV=0.35097 . I tried both cases using numeric features as numeric and categorical, and found the latter to perform best. Logistic Regression: This was adapted from @siukeitin 's What if you can only use logistic regression‚Ä¶ . With sample weight bumping, I achieved a CV=0.37468 . Ensembling Strategy Hill Climbing: I created 20 different versions using the GPU hill-climbing method by @cdeotte . My best hill climbing ensemble consisted of the following 14 OOF predictions: - oof_model -1 -15 -w11-xgboost_trial_3_map3_0.37799.npy\n- oof_model -13 -3 -w10-logisticregression_trial_0_map3_0.37530.npy\n- oof_model -4 -3 -w10-NN_trial_0_map3_0.35016.npy\n- oof_model -5 -5 -w10-catboost_trial_1_map3_0.34875.npy\n- oof_model -12 -1 -w10-xgboost_index_0_map3_0.36555.npy\n- oof_model -1 -15 -w11-xgboost_trial_0_map3_0.37737.npy\n- oof_model -4 -2 -w10-NN_trial_3_map3_0.34908.npy\n- oof_model -5 -5 -w10-catboost_trial_2_map3_0.35196.npy\n- oof_model -12 -1 -w10-xgboost_index_67_map3_0.36579.npy\n- oof_model -1 -7 -w10-xgboost_trial_0_map3_0.37640.npy\n- oof_model -13 -1 -w10-logisticsregression_trial_0_map3_0.37411.npy\n- oof_model -12 -1 -w10-xgboost_index_39_map3_0.36198.npy\n- oof_model -3 -2 -w10-lightgbm_trial_2_map3_0.35041.npy\n- oof_model -5 -3 -w10-catboost_trial_2_map3_0.34606.npy content_copy This resulted in CV=0.383830 and Public LB=0.38268 . As @cdeotte has pointed out several times, the key is diversity in the models rather than high-performing single models. On a side note, I think there is more to investigate with hill climbing (ideas, not used in this competition): Hill climb each fold separately, apply to the test set, and average the results. Experiment with starting the hill climb from different models, not just the best-performing one. Implement a tree-based search (e.g., using Alpha-beta pruning) to find an optimal set of models, rather than relying on a purely greedy approach. During this competition, I noticed that sometimes adding a new OOF to the hill climb would significantly worsen the score, which makes me curious about potential improvements to the method. Final Ensemble with AutoGluon: The 9 best hill climbing ensembles were further ensembled using AutoGluon to produce the final submission with CV=0.383918 and Private LB=0.38472 . Thanks to @ravaghi for pointing out how to use map@3 in AutoGluon here . Other Techniques 1. Use Sample_Weight x4 for original data Following a common technique from public notebooks, I set the sample_weight of the original data to 4x, which improved the score. 2. Use Sample_Weight to bump up 2nd, 3rd, and 4th placements I analyzed OOF predictions to identify instances where the correct class was predicted as 2nd, 3rd, or 4th. I then retrained the models with increased sample_weight for these specific instances to encourage the model to rank them higher. The potential gain for moving up one place is: 2nd => 1st : 0.5 (from 0.5 to 1.0) 3rd => 2nd : 0.17 (from 0.33 to 0.5) 4th => 3rd : 0.33 (from 0.0 to 0.33) I experimented with bumping the sample_weight for predictions where the probability difference between the two places where below a threshold (e.g., 0.02 ). This technique yielded a ~0.001 CV improvement for many models. I tried bumping only the 2nd place predictions, as well as bumping all three ranks. I considered automating this with Optuna, but that will be a project for a future playground competition. 3. Using the same CV folds for everything I used a fixed 6 Stratified-K-Fold strategy for all experiments. To ensure consistency and prevent data leakage between stages, I added a verification step to my scripts: def verify_first_fold ( val_idx ): # Hardcoded first 10 indices of the first validation fold val_idx_expected = np.array([ 1 , 2 , 5 , 8 , 9 , 14 , 16 , 23 , 27 , 30 ]) if np.array_equal(val_idx[: 10 ], val_idx_expected): return print ( \"First fold validation indices do not match the expected values.\" ) print ( \"Expected:\" , val_idx_expected) print ( \"Actual:\" , val_idx[: 10 ]) raise ValueError( \"Fold mismatch detected. Halting execution.\" ) # In the training loop for fold, (train_idx, val_idx) in enumerate (kf.split(X=train_df, y=...)): if fold == 0 :\n        verify_first_fold(val_idx) content_copy This is not a bulletproof method, but it saved me from polluting my OOFs on several occasions. 4. Optuna - Tricks I consolidated all hyperparameter tuning runs in the Optuna Dashboard for better tracking. All the results are stored in local sqlite3 databases for each notebook and I use a separate consolidation script to merge into a single db: def merge_optuna_studies ( source_storages: List [ str ], \n    output_storage: str , \n    min_trials: int = 10 ,\n    name_prefix: str = \"\" ) -> int : \"\"\"Merge studies from multiple Optuna storages into one.\"\"\" copied_studies = 0 for source_storage in source_storages: try : # Get all study summaries from the source storage study_summaries = optuna.study.get_all_study_summaries(storage=source_storage) if len (study_summaries) == 0 : print ( f\"No studies found in {source_storage} . Skipping.\" ) continue print ( f\"Found { len (study_summaries)} studies in {source_storage} \" ) # Process each study for summary in study_summaries:\n                study_name = summary.study_name\n                n_trials = summary.n_trials if n_trials < min_trials: print ( f\"Skipping study ' {study_name} ' with only {n_trials} trials (minimum required: {min_trials} )\" ) continue print ( f\"Copying study ' {study_name} ' with {n_trials} trials\" )\n\n                optuna.study.copy_study(from_study_name=study_name, \n                                        from_storage=source_storage, \n                                        to_storage=output_storage, \n                                        to_study_name=study_name)\n\n                copied_studies += 1 except Exception as e: print ( f\"Error processing {source_storage} : { str (e)} \" ) return copied_studies content_copy I used WilcoxonPruner to reduce computational effort, though this resulted in fewer OOFs being generated. Here is the setup: # Pruner setup p_threshold = 0.08 # p-value threshold for pruning n_startup_steps = 2 # Number of trials to complete before pruning begins pruner = optuna.pruners.WilcoxonPruner(\n    p_threshold=p_threshold,\n    n_startup_steps=n_startup_steps\n) # Create the Optuna study study = optuna.create_study(\n    direction= \"maximize\" , \n    pruner=pruner\n) def objective ( trial ): # ... # Within the CV loop, report the intermediate score to Optuna # ... train and evaluate on fold ... trial.report(map3_score, step=fold) # Prune the trial if it is unpromising unless on the last fold. if trial.should_prune() and fold < total_folds - 1 : raise optuna.TrialPruned() return final_cv_score content_copy This setup also populates the CV scores (intermediate Values)  values in the Optuna Dashboard: I always enqueued default parameters or other strong baseline configurations as the first trial. 5. Feature Engineering Search Mid-competition, I fixed my best XGBoost parameters and systematically tested ~200 engineered features, calculating the map@3 for each. Some examples: { \"tested_features\" : { \"Potassium_binned\" : 0.36687777777820074 , \"Humidity_x_Moisture_x_Potassium\" : 0.36101355555603537 , \"Humidity_x_Potassium\" : 0.36245444444491326 , \"Humidity_x_Moisture\" : 0.36123777777825505 , \"log_Temparature\" : 0.3660744444448719 , \"Temparature_x_Nitrogen_div_Humidity\" : 0.36036377777826734 , \"sqrt_Phosphorous\" : 0.36733622222264173 , \"Temparature_x_Humidity_div_Nitrogen\" : 0.3603313333338274 , content_copy Not a single feature improved the result over the baseline without feature engineering. This is consistent with other competitors' findings. However, some of these OOFs were still selected by the hill climbing algorithm due to the diversity they added. I am aware of @cdeotte 's advice to add all features and then remove them one by one, but I lacked the computational resources for that approach. I had to remove some of the oofs due to memory limitation of the number of oofs possible to hillclimb on my pc. What Did Not Work / Future Improvements TabTransformer: I spent some time trying to get a network based on the TabTransformer library to work, but could only achieve a CV=0.31 . I later discovered the excellent notebook by @omidbaghchehsaraei TabTransformer but did not have time to incorporate his work. Identify DAP/Urea in a separate model: Analyzing the per-class map@3 score from several of my OOFs, I found that DAP and Urea had poor results compared to other classes: CLASS-WISE MAP@3 CONTRIBUTION: (final oof) Class 14-25-14: MAP@3=0.432626 , Contribution=0.066011 (114,436 samples) Class 10-26-26: MAP@3=0.408642 , Contribution=0.062052 (113,887 samples) Class 17-17-17: MAP@3=0.426334 , Contribution=0.063923 (112,453 samples) Class 28-28: MAP@3=0.372506 , Contribution=0.055209 (111,158 samples) Class 20-20: MAP@3=0.369162 , Contribution=0.054581 (110,889 samples) Class DAP: MAP@3=0.341869 , Contribution=0.043240 (94,860 samples) Class Urea: MAP@3=0.315330 , Contribution=0.038814 (92,317 samples) I tried to train a separate binary classification model to identify DAP/Urea and use its predictions as a feature for the main models, but this did not improve my score. I also tried using sample_weight to specifically bump up DAP/Urea, but that only resulted in a much worse CV. Lessons Learned Trust your local CV. Prioritize model diversity for ensembling. Learn from discussion and other notebooks I used the same seed for all models. However, as @cdeotte mentioned in his 1st place solution , map@3 can be sensitive to randomness in training, so perhaps there was an element of luck involved. :) This was my first attempt at a write-up. Thank you to the Kaggle Community for all the lessons learned, and I'm looking forward to the next playground competition. Happy Kaggling! Please sign in to reply to this topic. comment 4 Comments Hotness Haruki Kakinuma Posted 3 days ago ¬∑ 7th in this Competition arrow_drop_up 0 more_vert Thanks for sharing! I think it's amazing that you've taken on so many challenges. The fact that you have 350 OOFs is proof of that. By the way, how did you create those 350? What variables did you use to differentiate between the models? Ole-Jakob Topic Author Posted 3 days ago ¬∑ 10th in this Competition arrow_drop_up 1 more_vert Thank you, and congrats on your 7th place finish! I have developed a structured framework for managing multiple models. Each model is built and run in a dedicated notebook, all following the same standardized template: Support for running locally, on Kaggle, or in Google Colab (with dynamic path handling) Flexibility to use any combination of the two original datasets (this competition unusually had two) Built-in implementation for CV splitting, MAP\\@3 scoring, and generation of OOF and submission files Model-specific configuration (e.g., XGBoost, LightGBM, etc.) Optuna integration for hyperparameter optimization Consistent saving of OOF and submission files as .npy per Optuna trial, with a clear folder and naming convention Here‚Äôs a glimpse of my notebook setup in VSCode: Model -1 -1 -XGBoost.ipynb  \nModel -1 -2 -XGBoost.ipynb  \n...  \nModel -2 -1 -XGBoost.ipynb  \nModel -3 -1 -LightGBM.ipynb  \n... (40 notebooks total) content_copy Each Optuna trial (unless pruned) generates a new set of OOF and submission files, which accumulate quickly: Why this setup works well: I can quickly duplicate and rename a notebook (e.g., to Model-4-1-CatBoost.ipynb ), then ask GitHub Copilot (or any LLM) to \"Replace XGBoost with CatBoost\"‚Äîand let the magic happen. It‚Äôs easy to track what each model variant does. New ideas can be prototyped rapidly in a clean environment. Any new OOFs are automatically included in the final hill-climbing ensemble. What differentiates the models? Most variation comes from the Optuna search space. Here‚Äôs an example for XGBoost: param = { 'objective' : 'multi:softprob' , 'num_class' : len (tgt_mapper), 'eval_metric' : 'mlogloss' , 'n_estimators' : 10000 , 'learning_rate' : 0.025 , 'early_stopping_rounds' : 75 , 'max_depth' : trial.suggest_int( 'max_depth' , 14 , 18 ), 'min_child_weight' : trial.suggest_int( 'min_child_weight' , 4 , 6 ), 'subsample' : trial.suggest_float( 'subsample' , 0.7 , 0.9 ), 'colsample_bytree' : trial.suggest_float( 'colsample_bytree' , 0.3 , 0.5 ), 'gamma' : trial.suggest_float( 'gamma' , 0.1 , 0.5 ), 'reg_alpha' : trial.suggest_float( 'reg_alpha' , 2 , 4 ), 'reg_lambda' : trial.suggest_float( 'reg_lambda' , 1 , 3 ), 'max_delta_step' : trial.suggest_int( 'max_delta_step' , 4 , 6 ), 'device' : 'gpu' , 'random_state' : 42 , 'enable_categorical' : True } content_copy Any change to the search space or which parameter to include in the search adds a new model. Other model variations included: Different combinations of the two original datasets (first, second, both, or none). (Model 1-1, 1-2, 1-3 and 1-4 are the same XGBoost script with this difference) Usage of sample_weight , as described earlier For neural nets, Optuna explored embedding sizes, dropout rates, and dense layer configurations Several OOFs came from different feature engineering paths I tested Not sure if this was the best explanation, but I main idea is to store oof's and submission files from trials in Optuna to create diversity. Mahog Posted 3 days ago ¬∑ 3rd in this Competition arrow_drop_up 0 more_vert Congrats! During this competition, I noticed that sometimes adding a new OOF to the hill climb would significantly worsen the score, which makes me curious about potential improvements to the method. I noticed the same as well, which made me switch to GA and then finally Ridge. I suspect it's because of HC converging to a non-optimal result due to the noisy nature of MAP@3. Ole-Jakob Topic Author Posted 3 days ago ¬∑ 10th in this Competition arrow_drop_up 0 more_vert Thank you, and congratulations on securing 3rd place! I‚Äôm really looking forward to going through your solution‚Äîand the other top entries‚Äîtomorrow. I haven‚Äôt used GA myself yet, but from what I understand, GA and hill climbing sit on opposite ends of the exploration-exploitation spectrum, with Ridge somewhere in between. If I want to increase exploration to avoid getting trapped in local optima, hill climbing might not be the best fit. I‚Äôll definitely look into this more during the next PGS. Appreciate the insight! Too many requests error Too many requests",
      "Predicting Optimal Fertilizers | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 days ago Late Submission more_horiz Predicting Optimal Fertilizers Playground Series - Season 5, Episode 6 Predicting Optimal Fertilizers Overview Data Code Models Discussion Leaderboard Rules Oscar Aguilar ¬∑ 21st in this Competition  ¬∑ Posted 5 days ago arrow_drop_up 12 more_vert #21 Solution | Stacking + Hill Climbing First, I would like to start by sincerely thanking Kaggle for hosting this episode of the playground series. In this post, I will briefly outline my approach. Base Models The table below summarizes the various models that were built using the data: Model Number of Models Catboost 20+ LGBM 50+ LGBM-Goss 10+ XGBoost 50+ Logistic 10+ Multinomial NB 10+ GradientBoostedTreesLearner 5+ All the base model were trained over the same 10-fold cross-validation strategy, StratifiedKFold(n_splits=10, shuffle=True, random_state=42) . Stackers Most of the top-scoring public notebooks used LogisticRegression as the final_estimator in the StackingClassifier . However, in many of my experiments, I did not achieve good alignment between the local cross-validation score and the public leaderboard score. As a result, I decided to try a different model as the final_estimator . I found that using LinearDiscriminantAnalysis as the final_estimator in the StackingClassifier generated a consistent alignment between the local cross-validation score and the public leaderboard score. I also experimented with GaussianNB as the stacker and observed similar behavior as with LogisticRegression . Notably, I trained over 50 different stackers using LinearDiscriminantAnalysis as the final_estimator . Hill Climbing The table below displays the public leaderboard scores of my top four stackers: Stacker Public LB score 1 0.38239 2 0.38233 3 0.38224 4 0.38211 Then, I saved the out-of-fold predictions from the above stackers and used them to estimate the weights for the ensemble using hill climbing. The public leaderboard score of the hill climbing ensemble was 0.38247 and the private score was 0.38385. What worked Treating all features as categorical as pointed out in a couple of discussions. Adding multiple copies of the original dataset for training purposes. What didn't work Feature engineering Please sign in to reply to this topic. comment 1 Comment Hotness Optimistix Posted 5 days ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Congrats on sticking to your guns as usual, and earning a big jump up the LB! Too many requests error Too many requests",
      "Predicting Optimal Fertilizers | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 days ago Late Submission more_horiz Predicting Optimal Fertilizers Playground Series - Season 5, Episode 6 Predicting Optimal Fertilizers Overview Data Code Models Discussion Leaderboard Rules H. Buƒüra Eken ¬∑ 22nd in this Competition  ¬∑ Posted 5 days ago arrow_drop_up 6 more_vert Public: 3rd -> Private: 22nd | Trust Your CV As the dust settles on the competition, our final results‚Äîa Public LB score of 0.38428 dropping to a Private LB score of 0.38387 ‚Äîoffer a clear and valuable lesson. This post will detail our final ensemble strategy, but more importantly, it will reflect on how our journey validates the crucial advice famously shared by fellow competitor Ravi R : one must always be prudent and trust their own CV score. Our Winning Strategy in a Nutshell Our top solution was not a simple blend but a structured, 10-model hierarchical ensemble. We manually grouped high-performing public submissions into three distinct clusters: An \" Elite \" group with the top-scoring models. A \" Good \" group of strong, complementary models. A single \" Independent \" model, chosen for its diversity to cover the others' blind spots. The final prediction came from a carefully tuned, weighted vote between representatives from these three groups. This structure proved to be more effective than any other we tested. The Optimization Journey This final model was the result of dozens of systematic experiments. We started with simple blends, evolved to the hierarchical structure described above, and then began a long \"hill-climbing\" process. We meticulously tuned parameters ( rank_points , weight_power ) and made single, isolated model swaps to find the optimal 10-player roster. This journey taught us that ensemble chemistry is incredibly sensitive, and simply adding a higher-scoring model doesn't always guarantee a better result. The Final Lesson: \"Trust Your CV\" Our final scores‚Äîa Public LB of 0.38428 and a Private LB of 0.38387‚Äîare a perfect illustration of the advice shared by fellow competitor Ravi R during the competition. He wisely cautioned against chasing public LB scores with blends that are not backed by a solid cross-validation (CV) strategy. Since we were ensembling public submissions, the Public LB was our only form of validation. Our entire optimization process, as methodical as it was, was essentially fine-tuning our submission for that specific public dataset. The small drop in our private score is a classic \"shake-down\" and the price paid for not having an independent CV score. Our experience perfectly validates Ravi's point: the public LB is a guide, not the ultimate truth. Thank you to the community for the public work that made this possible. Happy Kaggling! Please sign in to reply to this topic. comment 3 Comments Hotness Ty-Yuki Posted 4 days ago ¬∑ 138th in this Competition arrow_drop_up 1 more_vert Congratulations! Your approach to grouping models is very impressive. Dhruv Devaliya Posted 4 days ago ¬∑ 955th in this Competition arrow_drop_up 1 more_vert Congratulations!!! I'm a newbie in kaggle and data science field and this was my first competition.. I had one doubt I used ensemble technique but still my score was pretty low.. and some of us are using 1-2 model but thier score was pretty high as compared to me.. so what the main reason of this ? H. Buƒüra Eken Topic Author Posted 4 days ago ¬∑ 22nd in this Competition arrow_drop_up 0 more_vert Hi! First of all, congratulations on your first competition! The reason for that usually comes down to two core principles: Quality > Quantity : A single, well-trained model with great features is almost always better than an ensemble of many mediocre models. Diversity is Key : The power of an ensemble comes from combining models that make \"different\" mistakes. If all the models make similar predictions, combining them won't improve the score much. In short, the goal isn't to combine a lot of models; it's to intelligently bring together models that are both high-quality and diverse . Too many requests error Too many requests",
      "Predicting Optimal Fertilizers | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 days ago Late Submission more_horiz Predicting Optimal Fertilizers Playground Series - Season 5, Episode 6 Predicting Optimal Fertilizers Overview Data Code Models Discussion Leaderboard Rules Ravi Ramakrishnan ¬∑ 28th in this Competition  ¬∑ Posted 5 days ago arrow_drop_up 15 more_vert Rank 28 approach - diversity and CV prevail! Hello all, I am thankful to Kaggle for a reasonably good playground episode the past month. I am sure a lot of us worked with the MAP@3 metric for the first time and I think the experience was quite good! I wish to extend sincere thanks to the forum contributors for their generosity as well and congratulate the high scoring solution participants as well. My approach herewith tried to infuse as much diversity as feasible, keeping CV-LB relation in mind. Diversity can be ensued with features and model choices and I chose both these elements for my pipeline. Details are illustrated in the figure below- Feature Engineering As indicated in the figure, I chose to ensure- No feature engineering and just the training features as is, but with string datatype Multiple train + multiple original datasets appended, ranging from 1 train, 0 original to 2 trains + 19 originals Limited experiments with bigrams + trigrams and target encoder using CuML Model training I used a 10-fold cv scheme as below- KFold(10, random_state = 42, shuffle = True) I trained a lot of varied boosted tree models across feature set options and built more than 70 single models with varied feature sets I observed that a single xgboost model with depth = 8, learning_rate = 0.01 on category represented dataset performed the best. The model elicited the best cv score when I appended 1 training data + 6-8 original datasets Most of the models performed poorly with early stopping with auc/ logloss proxy metrics. Using a higher estimator number (aka 10000) helped a lot, albeit with a longer training time Catboost was the worst performer with training features, but performed very well as a stacker model LightGBM and LightGBM goss performed moderately well and provided needed diversity to the ensemble I also used an Autogluon pipeline with 1 training + 1 original data and secured a cv score of 0.345. I used a few single models from this pipeline for diversity, despite its poor individual performance Ensemble This did not yield a significant benefit here. I choose the below approaches with varying elements of success- Autogluon stacker - built 3 autogluon staker models using different model choices Torch NN - built 4 staker models using different model choices Finally, I chose to blend the ensemble models and all the single models with a CuML Hill Climber. It is the same as any other hill climber but it uses cupy and cudf instead of pandas and numpy. I have a private setup for this and often use it across competitions CV details I shall highlight a few CV score ranges across my models as below- Model type/ algorithm CV score Selected for blending Starter public work 0.355 N Catboost 0.325- 0.33 Y LightGBM 0.34 - 0.375 Y LightGBM  goss 0.335 - 0.3725 Y XgBoost 0.34 - 0.3795 Y Torch NN 0.31-0.32 N Autogluon - direct application 0.345 Y Autogluon stacker model 0.3810 - 0.3812 Y Torch NN model 0.3790 - 0.3805 Y Logistic blend 0.3815-0.3818 N CuML Hill Climber blend 0.3820 - 0.3830 Y GPUs used I used the below GPU suite for the model training - Model type/ algorithm GPU Catboost A5000 LightGBM A5000 LightGBM  goss A5000 XgBoost A6000 Ada Torch NN A5000 Autogluon - direct application L4 - Google Colab Autogluon stacker model L4 - Google Colab Torch NN model L4 - Google Colab Logistic blend None CuML Hill Climber blend 4090 local GPU Key learnings Stick to your CV, don't chase non-CV backed blends and stay diligent and success will be yours! Concluding remarks Wishing you the best for the upcoming competitions and in your professional life also! Happy learning and best regards! References https://www.kaggle.com/datasets/ravi20076/playgrounds5e6models -- trained models https://www.kaggle.com/code/ravi20076/playgrounds5e6-public-baseline-v1 -- baseline work, most private models were trained with this code Regards, Ravi Ramakrishnan Tabular Classification Agriculture 1 Please sign in to reply to this topic. comment 16 Comments Hotness Taylor S. Amarel Posted 2 days ago ¬∑ 1002nd in this Competition arrow_drop_up 1 more_vert Awesome post, I learned a lot from the diagram alone! Maheen Khurshid Posted 2 days ago arrow_drop_up -1 more_vert congrats üéä @ravi20076 Ty-Yuki Posted 4 days ago ¬∑ 138th in this Competition arrow_drop_up 1 more_vert Congratulations! I learned a lot from your model training techniques. Harsh Gupta Posted 4 days ago ¬∑ 1216th in this Competition arrow_drop_up 1 more_vert Congratulations! And thank you shared resources for this competition previously. Abhishek Awasthi Posted 4 days ago arrow_drop_up 1 more_vert congrats for this! Faryal Rifaz Posted 4 days ago ¬∑ 1551st in this Competition arrow_drop_up 1 more_vert Congratulations! That's really insightful, appreciable. Sadia Shahid latif Posted 4 days ago ¬∑ 2079th in this Competition arrow_drop_up 2 more_vert Congratulations! And thanks u shared this to us @ravi20076 Optimistix Posted 5 days ago ¬∑ 5th in this Competition arrow_drop_up 1 more_vert Congrats on another good finish! I didn't use Autogluon much this month, as the performance seemed subpar - looks like you made good use of it. H. Buƒüra Eken Posted 5 days ago ¬∑ 22nd in this Competition arrow_drop_up 1 more_vert Hi Ravi, Congratulations on the great result and thanks for this incredibly detailed write-up! Your systematic approach to building diversity and trusting your CV is a fantastic lesson. On a different note, I'm planning a local PC build for Kaggle and was curious about your hardware. How are you finding the 4090 for this kind of workload, and what CPU/RAM do you pair it with? Any insight would be a great help. Thanks again for sharing and good luck in your next competitions! Best, Ravi Ramakrishnan Topic Author Posted 5 days ago ¬∑ 28th in this Competition arrow_drop_up 1 more_vert I am using the below- 4090 GPU Intel 13700 CPU 128 GB RAM ASUS ROG Z-790 motherboard 4090 is fine for non-LLM workloads. LLMs are a different game altogether! @hbugrae H. Buƒüra Eken Posted 5 days ago ¬∑ 22nd in this Competition arrow_drop_up 1 more_vert Thanks so much for sharing your specs, I have a quick follow-up, if you don't mind. I've heard some reports about stability issues with Intel's 13th/14th gen CPUs under sustained heavy loads. Have you run into any problems with your 13700? Also, since I'm about to buy the 14700, I was curious if you'd still choose the 13700 today or feel the upgrade is worth it. Thanks again for your time and insights! Ravi Ramakrishnan Topic Author Posted 4 days ago ¬∑ 28th in this Competition arrow_drop_up 1 more_vert @hbugrae I have not faced such issues but my PC is about 18 months old and these specs are now upgraded to 14th gen for Intel and 5090 for GPU. If your budget allows, then please prefer the latest versions. Also consider AMD processors for CPU. their performance is good as CPU. H. Buƒüra Eken Posted 4 days ago ¬∑ 22nd in this Competition arrow_drop_up 1 more_vert Got it, thank you so much for the follow-up and the extra advice Gerald Schwartz Posted 5 days ago ¬∑ 225th in this Competition arrow_drop_up 1 more_vert Just for clarification, are you adding the original data only at the the fitting stage? Ravi Ramakrishnan Topic Author Posted 5 days ago ¬∑ 28th in this Competition arrow_drop_up 0 more_vert Yes of course, why would I validate using this if it is an extra dataset @geraldschwartz ? Haruki Kakinuma Posted 5 days ago ¬∑ 7th in this Competition arrow_drop_up 1 more_vert Thank you for sharing so many helpful insights during the competition! They really helped me make a strong submission. Komil Parmar Posted 5 days ago ¬∑ 261st in this Competition arrow_drop_up 1 more_vert Congratulations üéâ There are so many GPUs mentioned here. I see local only on 4090. Can you share where did you use A5000 and A6000 GPUs from? Are they local too or rented on some platform? If rented, can you share like what's the procedure of testing the hyperparameters and model performance first locally and then using the rented GPUs for final training to minimize cost. Thanks Ravi Ramakrishnan Topic Author Posted 5 days ago ¬∑ 28th in this Competition arrow_drop_up 1 more_vert I rented them from runpod.io . These are easy to use and budget friendly @komilparmar I usually don't do too much tuning - I do some hand tuning with a single fold and prefer to use essential parameters only. Ali_Haider_Ahmad Posted 5 days ago ¬∑ 206th in this Competition arrow_drop_up 1 more_vert Great as always Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 5, Episode 6 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Fertilizer prediction dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. 3 files 50.37 MB csv CC0: Public Domain Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 50.37 MB sample_submission.csv test.csv train.csv 3 files 21 columns  Too many requests",
    "data_description": "Predicting Optimal Fertilizers | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ 5 days ago Late Submission more_horiz Predicting Optimal Fertilizers Playground Series - Season 5, Episode 6 Predicting Optimal Fertilizers Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2025 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: Your objective is to select the best fertilizer for different weather, soil conditions and crops. Start a month ago Close 5 days ago Evaluation link keyboard_arrow_up Submissions are evaluated according to the Mean Average Precision @ 3 (MAP@3): M A P @ 5 = 1 U U ‚àë u = 1 m i n ( n , 5 ) ‚àë k = 1 P ( k ) √ó r e l ( k ) where U is the number of observations, P ( k ) is the precision at cutoff k , n is the number predictions per observation, and r e l ( k ) is an indicator function equaling 1 if the item at rank k is a relevant (correct) label, zero otherwise. Once a correct label has been scored for an observation , that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is A for an observation, the following predictions all score an average precision of 1.0 . [ A , B, C, D, E]\n[ A , A , A , A , A ]\n[ A , B, A , C, A ] content_copy Submission File For each id in the test set, you may predict up to 3 Fertilizer Name values, with the predictions space delimited.  The file should contain a header and have the following format: id,Fertilizer Name \n750000,14 -35 -14 10 -26 -26 Urea\n750000,14 -35 -14 10 -26 -26 Urea \n... content_copy Timeline link keyboard_arrow_up Start Date - June 1, 2025 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  June 30, 2025 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Elizabeth Park. Predicting Optimal Fertilizers. https://kaggle.com/competitions/playground-series-s5e6, 2025. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 7,070 Entrants 2,779 Participants 2,648 Teams 19,969 Submissions Tags Beginner Tabular MAP@{K} Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s5e7",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Playground Series - Season 5, Episode 7 The dataset for this competition (both train and test) was generated from a deep learning model trained on the Extrovert vs. Introvert Behavior dataset. Feature distributions are close to, but not exactly the same, as the original. Feel free to use the original dataset as part of this competition, both to explore differences as well as to see whether incorporating the original in training improves model performance. Note - This is a relatively small dataset, so a one to use for comparing different modeling approaches, making visualization, etc. 3 files 1.05 MB csv CC BY-SA 4.0 Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 1.05 MB sample_submission.csv test.csv train.csv 3 files 19 columns  Too many requests",
    "data_description": "Predict the Introverts from the Extroverts | Kaggle menu Skip to content Create search ‚Äã explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ‚Äã Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle ¬∑ Playground Prediction Competition ¬∑ a month to go Join Competition more_horiz Predict the Introverts from the Extroverts Playground Series - Season 5, Episode 7 Predict the Introverts from the Extroverts Overview Data Code Models Discussion Leaderboard Rules Overview Welcome to the 2025 Kaggle Playground Series! We plan to continue in the spirit of previous playgrounds, providing interesting and approachable datasets for our community to practice their machine learning skills, and anticipate a competition each month. Your Goal: Your objective is to predict whether a person is an Introvert or Extrovert, given their social behavior and personality traits. Start 5 days ago Close a month to go Evaluation link keyboard_arrow_up Submissions are evaluated using Accuracy Score between the predicted value and the observed target. Submission File For each id in the test set, you must predict the target Personality . The file should contain a header and have the following format: id ,Personality 18524 ,Extrovert 18525 ,Introvert 18526 ,Introvert\netc. content_copy Timeline link keyboard_arrow_up Start Date - June 30, 2025 Entry Deadline - Same as the Final Submission Deadline Team Merger Deadline - Same as the Final Submission Deadline Final Submission Deadline -  July 31, 2025 All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About the Tabular Playground Series link keyboard_arrow_up The goal of the Tabular Playground Series is to provide the Kaggle community with a variety of fairly light-weight challenges that can be used to learn and sharpen skills in different aspects of machine learning and data science. The duration of each competition will generally only last a few weeks, and may have longer or shorter durations depending on the challenge. The challenges will generally use fairly light-weight datasets that are synthetically generated from real-world data, and will provide an opportunity to quickly iterate through various model and feature engineering ideas, create visualizations, etc. Synthetically-Generated Datasets Using synthetic data for Playground competitions allows us to strike a balance between having real-world data (with named features) and ensuring test labels are not publicly available. This allows us to host competitions with more interesting datasets than in the past. While there are still challenges with synthetic data generation, the state-of-the-art is much better now than when we started the Tabular Playground Series two years ago, and that goal is to produce datasets that have far fewer artifacts. Please feel free to give us feedback on the datasets for the different competitions so that we can continue to improve! Prizes link keyboard_arrow_up 1st Place - Choice of Kaggle merchandise 2nd Place - Choice of Kaggle merchandise 3rd Place - Choice of Kaggle merchandise Please note: In order to encourage more participation from beginners, Kaggle merchandise will only be awarded once per person in this series. If a person has previously won, we'll skip to the next team. Citation link keyboard_arrow_up Walter Reade and Elizabeth Park. Predict the Introverts from the Extroverts. https://kaggle.com/competitions/playground-series-s5e7, 2025. Kaggle. Cite Competition Host Kaggle Prizes & Awards Swag Does not award Points or Medals Participation 2,897 Entrants 1,148 Participants 1,129 Teams 5,362 Submissions Tags Beginner Tabular Accuracy Score Table of Contents collapse_all Overview Evaluation Timeline About the Tabular Playground Series Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "playground-series-s5e8",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "You can search Kaggle above or visit our homepage .",
    "data_description": "Kaggle: Your Home for Data Science Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. We can't find that page. You can search Kaggle above or visit our homepage ."
  },
  {
    "competition_slug": "playground-series-s5e9",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "You can search Kaggle above or visit our homepage .",
    "data_description": "Kaggle: Your Home for Data Science Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. We can't find that page. You can search Kaggle above or visit our homepage ."
  },
  {
    "competition_slug": "playground-series-s5e10",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "You can search Kaggle above or visit our homepage .",
    "data_description": "Kaggle: Your Home for Data Science Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. We can't find that page. You can search Kaggle above or visit our homepage ."
  },
  {
    "competition_slug": "playground-series-s5e11",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "You can search Kaggle above or visit our homepage .",
    "data_description": "Kaggle: Your Home for Data Science Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. We can't find that page. You can search Kaggle above or visit our homepage ."
  },
  {
    "competition_slug": "playground-series-s5e12",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "You can search Kaggle above or visit our homepage .",
    "data_description": "Kaggle: Your Home for Data Science Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. We can't find that page. You can search Kaggle above or visit our homepage ."
  }
]