[
  {
    "competition_slug": "home-credit-credit-risk-model-stability",
    "discussion_links": [
      "/competitions/home-credit-credit-risk-model-stability/discussion/508337",
      "/competitions/home-credit-credit-risk-model-stability/discussion/508113"
    ],
    "discussion_texts": [
      "Home Credit - Credit Risk Model Stability | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Code Competition · a year ago Late Submission more_horiz Home Credit - Credit Risk Model Stability Create a model measured against feature stability over time Home Credit - Credit Risk Model Stability Overview Data Code Models Discussion Leaderboard Rules yuuniee · 1st in this Competition  · Posted a year ago arrow_drop_up 170 more_vert [1st Place Solution] My Betting Strategy Foreword Hello, this is yuuniee. Thank you to the competition host Home Credit and staff, Kaggle staff and everyone involved. The image below is a summary of my solution. The maximum Private LB Score for my model pool without Metric Hacking is close to about 0.53. As everyone knows, this competition takes place in two phases. Phase 1 was ML (Machine Learning), and Phase 2 was MH (Metric Hack). I will briefly introduce each part. (If you are only curious about ML, please read only Phase 1.) I. Phase 1 - ML (Machine Learning) Thank you for writing a great notebook for EDA and solution construction at the beginning of the competition. @sergiosaharovskiy - https://www.kaggle.com/code/sergiosaharovskiy/home-credit-crms-2024-eda-and-submission @greysky - https://www.kaggle.com/code/greysky/home-credit-baseline Additionally, I would like to thank the many participants who provided various insights. The CV strategy is StratifiedGroupKFold, and has been tested and trained in various forms: No Shuffle and Shuffle. In my case, differences in CV of around 0.001 to 0.005 had a low correlation with LB, while differences above 0.01 had a high correlation with LB. In addition, CV improvement through model parameter adjustment had a relatively low correlation with LB, and CV improvement through FE had a relatively high correlation with LB. There's nothing special about Feature Engineering. Max, Min, Avg, Var, First, Last and Max-Min Difference of each item were utilized. (I will release the code after cleaning it up.) LGBM was a bit lacking compared to Catboost, but was good for the ensemble. The reason why the number of features is smaller is because some items (mainly categorical types) were excluded because they caused performance degradation and overfitting in LGBM, and other slightly different types of features were added and deleted. DNN is a Denselight model and utilizes the LightAutoML library. There were some bugs, so I had to fix some of them myself, but overall, I think it is a good library. At first, I planned to lightly test with Denselight and then build the final model with a larger model (FT-Transformer, etc.). Surprisingly, I haven't created or found a model that beats Denselight performance. Perhaps I made a mistake, or the nature of the data is sensitive to overfitting, so I guess a simpler model worked better. Catboost was the model that performed best in this competition and I guess it performed well on a large number of categorical features (about 117). During this period, my public LB was close to top 5 and the competition seemed to be going very smoothly. But… + What didn't work Transformer type models such as Tabnet, TabTransformer, FT-Transformer, etc. Income, expenditure, and tax statistics by period (month, week, etc.). Differences between individual income, expenses, and taxes. K-means clustering II. Phase 2 - MH(Metric Hack) Pointed out problems with MH at the beginning of the competition, @at7459 - https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/discussion/476449 Announced the revival of MH in the latter half of the competition, @johnpateha - https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/discussion/497167 Also, thanks to the many participants who suggested various other comments and solutions. Thanks to you, my knowledge has expanded. In particular, the two people above are true scientists and journalists who could have hidden this discovery and profited from it, but pursued the public interest by exposing it to everyone. (Why not consider Kaggle giving a special award for this sharing?) As a result, the problem was not solved, but we learned something(?) from it and at least I was able to take minimal measures. After @johnpateha 's \"Metric hack again, sorry\" post, I tried Data Reversing and found that the difference between date_decision and min_refreshdate_3813885D had a high correlation (about 0.9 or more) with WEEK_NUM. (More information can be found at https://www.kaggle.com/code/eivolkova/how-to-restore-the-dates?scriptVersionId=180157891 , published by @eivolkova immediately after the competition.) And the way suggested by @at7459 : DEVIDE = 1 / 2 REDUCE = 0.02 condition = df[ 'WEEK_NUM' ] < (df[ 'WEEK_NUM' ]. max ()-df[ 'WEEK_NUM' ]. min ())*DEVIDE+df[ 'WEEK_NUM' ]. min ()\ndf.loc[condition, 'score' ] = (df.loc[condition, 'score' ] - REDUCE).clip( 0 ) content_copy Based on this, the results of examining the score details submitted by changing the REDUCE and DEVIDE values ​​were as follows. The public/private division of test data spans the entire period and it is difficult to identify a specific distribution, and the optimal value in private will probably be distributed around 1/4 to 3/4. The optimal public LB value of REDUCE is 0.03 (at least in my model), and the optimal value in private is probably distributed around 0.02 to 0.04. The difference in pure model performance between top competitors was 0.00X, Since the difference due to the above adjustment value was 0.0X, At this stage it became gambling for me. Some will bet low, others will bet high. Here, I trusted my model and chose neutral, and chose DEVIDE=1/2, REDUCE=0.03, which is the median of the expected distribution. I expected someone who bet high or low to take the prize, and I expected about a 50% chance of finishing in the top 10. III. Conclusion The first selected submission is the No Hack model with CV Best Score created in Phase 1, and the second selected submission is the application of Metric Hack to it. (I thought there was a high probability that Metric Hack would work, but I also submitted No Hack just in case) Rather than simply sharing solutions, I thought it would be better to share my experience in detail. To improve the bad parts that happened in this competition. My ranking was due to luck, and although it is the Best Private Score submitted, it probably won't be Best Solution. Although the competition ended on a gloomy note due to the metric hacking issue, the host and staff worked hard to make it a meaningful and successful competition. Based on this, we expect a more developed system in the future. thank you Edit : I posted the FE code here. 644(for lgbm) - https://www.kaggle.com/code/yuuniekiri/fork-of-home-credit-risk-lightgbm 661(for cat & dnn) - https://www.kaggle.com/code/yuuniekiri/fork-of-home-credit-catboost-inference 55 20 22 Please sign in to reply to this topic. comment 54 Comments 4 appreciation  comments Hotness JK-Piece Posted a year ago · 1754th in this Competition arrow_drop_up 3 more_vert Hi @yuuniekiri , I cannot find the final submission notebook for the first place. Can you please share it? Alexander Ryzhkov Posted a year ago arrow_drop_up 5 more_vert Hi @yuuniekiri , Congrats with the great performance and amazing solution! Also I would like to thank you for using LightAutoML models in your solution 😎 DNN is a Denselight model and utilizes the Lightautoml library. There were some bugs, so I had to fix some of them myself, but overall, I think it is a good library. Could you please share the problems you had with our library using GitHub issue mechanism ? It will help us to make LightAutoML better 😊 P.S. If you already have fixes for the problems and you would like to share them, please use the Pull Request mechanism Alex (Head of LightAutoML team) yuuniee Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 2 more_vert Thank you so much. LightAutoML is a really convenient and useful library. The problem I had was not fatal, but I will share it there soon. :) GPU Poor ⵣ Posted a year ago arrow_drop_up 3 more_vert Congrats @yuuniekiri for the winning. May I ask you what approach did you choose to find the best ensemble weights? yuuniee Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 4 more_vert Thank you for your congrats. My approach is linear regression and weighted testing in OOF. Interestingly, the optimal weights of OOF score was close to the optimal weights of LB. nonchokablexd Posted a year ago arrow_drop_up 1 more_vert Congratulations troostiticgh Posted a year ago arrow_drop_up 1 more_vert Congratulations Krens Posted a year ago arrow_drop_up 1 more_vert Thank you for sharing such a detailed and insightful breakdown of your solution, yuuniee! Your approach to combining LGBM, DNN, and Catboost models with a weighted ensemble clearly highlights the importance of diversity in modeling techniques. I particularly appreciate the transparency in discussing both phases of the competition—Machine Learning and Metric Hacking. Your use of StratifiedGroupKFold and various feature engineering techniques, along with your observations about the correlation between CV improvements and LB performance, are incredibly valuable for the community. The post-processing strategy and the thoughtful discussion on the impact of WEEK_NUM adjustments and the choices around DEVIDE and REDUCE values offer practical insights into handling similar challenges. It's also commendable how you acknowledged the contributions and insights from other participants like @sergiosaharovskiy and @greysky for their foundational notebooks, and @at7459 and @johnpateha for their critical discussions around Metric Hacking. The collaboration and knowledge sharing within this community are what make these competitions so enriching. Despite the challenges posed by Metric Hacking, your balanced approach to selecting submission strategies demonstrates a clear understanding of risk management in competition settings. Cheng Zhao Posted a year ago · 36th in this Competition arrow_drop_up 1 more_vert thank you for the summary. Very inspiring kerenshang Posted a year ago arrow_drop_up 1 more_vert Thanks! Very useful article for fresh man. Mr RRR Posted a year ago · 2130th in this Competition arrow_drop_up 1 more_vert Thanks! Very useful article for fresh man Manav Trivedi Posted a year ago · 761st in this Competition arrow_drop_up 1 more_vert Smart way of using the hacks! xianZ_waikato Posted a year ago · 1231st in this Competition arrow_drop_up 2 more_vert Nice share, Congratulations for your first solo gold. catdoghu Posted a year ago arrow_drop_up 1 more_vert helpful for me! TomarAman Posted a year ago arrow_drop_up 1 more_vert Great approach Matous Famera Posted a year ago · 1067th in this Competition arrow_drop_up 1 more_vert Congratulations! My team had similar idea - Ensemble model of LightGBM, CatBoost and Tensorflow. We were really close to developing the ensemble model. However after the change of rules and allowing the metric hack, our focus shifted on the metric hack. Nirav Posted a year ago arrow_drop_up 1 more_vert Interesting antony githinji Posted a year ago · 2831st in this Competition arrow_drop_up 1 more_vert great hacking man emoji_people Anil Kumar Reddy Posted a year ago · 689th in this Competition arrow_drop_up 1 more_vert Congratulations , thanks for sharing your wonderfull journay!! narsil (jobs-in-data.com) Posted a year ago · 2932nd in this Competition arrow_drop_up 1 more_vert Thanks for sharing - this is great. One question out of curiosity: have you managed to use credit bureau A data successfully? (for context - I haven't - my post with details here https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/discussion/478360 ) yuuniee Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 2 more_vert This is quite a mystery to me too. As you can see from the code and comments for LGBM posted above, I had quite a bit of trouble with it. As a result, many features were deleted from bureau A and only some were retained. (And I experienced something similar with person data.) Because test data cannot be seen, it is difficult to determine the cause. Eduardo Toloza Posted a year ago · 2781st in this Competition arrow_drop_up 1 more_vert From Credit Bureau data I just used the feature related to the financial institution that approved the loan. Other features degraded the stability metric. I also thought it was better not to use debit card data. On the other hand, the best way I  found to use tax data, was the sum of all deduction tax of each client, it makes sense because it represents the tax deductions in the last year. That feature boosted my model's performance. narsil (jobs-in-data.com) Posted a year ago · 2932nd in this Competition arrow_drop_up 1 more_vert My interpretation is that hosts downsampled this source of data heavily for the test set, and hence it led to overfitting. Tomas Jelinek Competition Host Posted a year ago arrow_drop_up 2 more_vert Hi, we will have to look into it, in production models credit bureau contributes quite significantly… in dataset should be data as they are in our real processes (except for anonymization/masking), and there were no adjustments like downsampling in test set. Andrey Nesterov Posted a year ago · 57th in this Competition arrow_drop_up 3 more_vert Let me add my 5 cents. With MLP model I also had problems with overfitting, so I used only top 20 features (by gain importance of LGBM). But GBDT models worked pretty well with cred_b_a_* tables, especially one version of CatBoost with a private score of 0.520. Of course some features were filtered, but nothing special (maybe with the exception of additional aggregators that are applied to some num cols). In ensemble, these models (using cred_b_a_* tables) worked well. And also in early stages I filtered out some rows with null cols greater then some threshold, but then I think I just found out combination of aggregators that worked well with all tables and rows of cred_b_a_* data. THUNDER THUNDER Posted a year ago · 53rd in this Competition arrow_drop_up 1 more_vert I use credit_bureau_a_1_3 for training instead of credit_bureau_a_1_*. Eduardo Toloza Posted a year ago · 2781st in this Competition arrow_drop_up 1 more_vert Can you elaborate? ishlove7 Posted a year ago arrow_drop_up 1 more_vert Your idea is so amazing. Thank you for sharing awesome idea. Doyeong Posted a year ago · 71st in this Competition arrow_drop_up 1 more_vert Congratulations! I learn a lot~ C R Suthikshn Kumar Posted a year ago · 182nd in this Competition arrow_drop_up 1 more_vert Congratulations on winning this competition. Thanks for detailed explanation of your solution and sharing valuable comments on topic of Metric Hacking. Can you comment on execution time of your notebook and share best practices for the competition participants. yuuniee Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 0 more_vert Thank you very much. The execution time of my notes is approximately 2 to 3 hours. (I have never measured it accurately.) Best practice - I don't know if I'm qualified to comment on this. It's difficult to pinpoint something, but since financial data has a lot of noise and is vulnerable to overfitting, I think we need to pay special attention to generalization. For example, you should be very careful about randomly inserting unexplained data (but this may not always be accurate). In addition, we explore various methods to prevent overfitting. ChrisChan0204 Posted a year ago · 1325th in this Competition arrow_drop_up 1 more_vert Congratulations! This notebook is very impressive for me. Vishrut Grover Posted a year ago arrow_drop_up 1 more_vert This is awesome! I'll be reading this in detail soon! :) Great work @yuuniekiri ! wajyjpku Posted a year ago arrow_drop_up 2 more_vert Congrats @yuuniekiri for the winning. How do we filter out effective features and toxic features?Thank you for your response in advance. yuuniee Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 1 more_vert As you can see in the article above, features that have a significant advantage in the CV score are kept, and features that have no advantage are removed. Additionally, even if there is no significant gain in score, logically meaningful and stable features can be kept. This can also apply in the opposite case. wuuthraad Posted a year ago · 345th in this Competition arrow_drop_up 2 more_vert Great work man!",
      "Home Credit - Credit Risk Model Stability | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Code Competition · a year ago Late Submission more_horiz Home Credit - Credit Risk Model Stability Create a model measured against feature stability over time Home Credit - Credit Risk Model Stability Overview Data Code Models Discussion Leaderboard Rules Yuya Shintani · 13th in this Competition  · Posted a year ago arrow_drop_up 37 more_vert 13th place solution - pmts_year_1139T postprocess First of all, thanks to Kaggle and the host of the competition. To be honest, we don't fully understand what was effective, let me briefly describe our solution. Thanks @kentookumura @pegasus27 for the collaboration. Context Business context: https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/overview Data context: https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/data Overview of the approach In feature engineering, we created 772 handcrafted features using tables other than credit_bureau_b1, other_1, deposit_1, debitcard_1. We performed manual and correlation-based feature selection and reduced the number of features to 411. In modeling, we built 10 models using LightGBM, XGBoost, CatBoost, and HistGradientBoostingClassifier. I stacked these outputs with a RidgeClassifier and applied probability calibration to create predicted values. Then, we created the final predicted values using random seed averaging. In postprocess, we added a simple process to take the maximum value of pmts_year_1139T in the credit_bureau_a_2 table and apply a negative correction to the score for each year. Details of the submission Feature Engineering The feature engineering was mainly done by @kentookumura . Initially, we decided not to use the credit_bureau_b_2, other_1, deposit_1, debitcard_1 tables which had a high rate of missing case_id. We created handcrafted features based on the results of EDA and the solutions from past competitions. We reduced the number of features from 772 to 411 through manual and correlation-based feature selection. Below, we will describe the points that we believe were effective. Era df_base = df_base.with_columns(\n        ((pl.col( \"first_birth_259D\" ) / 10 ).floor() * 10 ).alias( \"era\" ).cast(pl.Int32),\n) content_copy Age at Start of Employment df = df.with_columns(\n        ((pl.col( \"empl_employedfrom_271D\" ) - pl.col( \"birth_259D\" )).dt.total_days() // 365 ).cast(pl.Int32).alias( \"agestartofemploymentA\" ), \n) content_copy Employment Period df_base = df_base.with_columns(\n        (pl.col( \"first_birth_259D\" ) - pl.col( \"first_agestartofemploymentA\" )).alias( \"durationofemploymentA\" ),\n) content_copy Date processing other than suffix D def handle_dates ( df ): for colin df.columns: if col[- 1 ] in ( \"D\" ,):\n            df = df.with_columns(pl.col(col) - pl.col( \"date_decision\" ))\n            df = df.with_columns(pl.col(col).dt.total_days())\n            df = df.with_columns(pl.col(col).cast(pl.Float32)) elif \"year\" in col:\n            df = df.with_columns(pl.col(col) - pl.col( \"date_decision\" ).dt.year())\n            df = df.with_columns(pl.col(col).cast(pl.Int32)) content_copy Merge tax_registry tables From some case_id with multiple provider information, we inferred the correspondence of each table column and made it into one table. Aggregation of String type (mode and n_unique) We used the process pl.col(col).drop_nans().drop_nulls().mode().sort().first() for reproducibility in polars. Removal of features that fluctuate greatly during the training data period We manually checked and removed features that fluctuate greatly with each WEEK_NUM. Modeling @uplus26e7 mainly handled the modeling. We used StratifiedGroupKFold(k=5) based on WEEK_NUM for CV. We tried multiple GBDT models that do not require scaling of features or missing value completion, as these did not go well. In order to create diverse models, we created 10 models with multiple parameters and stacked them with RidgeClassifier. Finally, we corrected the predicted values using Scikit-Learn's CalibratedClassifierCV. The above model performed random seed averaging (5 seeds) and used it for the final inference. Model Local CV AUC (average 5 seeds) Main Parameters XGBoost 0.8569388045 CatBoost 0.8543810988 LightGBM 0.8576141787 boosting=”gbdt”, extra_tree=True LightGBM 0.8569003931 boosting=”gbdt” LightGBM 0.8068982316 boosting=”rf” LightGBM 0.7993733596 boosting=”rf”, extra_tree=True LightGBM 0.8546620954 boosting=”dart” LightGBM 0.8517172791 boosting=”dart”, extra_tree=True HistGradientBoostingClassifier 0.8496922895 LightGBM 0.8574351195 boosting=”gbdt”, extra_tree=True, data_sample_strategy=”goss” CalibratedClassifierCV (RidgeClassifier) 0.859322 Postprocess @kentookumura 's thorough EDA and experiments revealed that the 'pmts_year_1139T' in the 'credit_bureau_a_2' table is likely the most recent 'date_decision' year. It was also observed that no date column transformations were added in data changes. Based on these findings, we implemented post-processing to decrease the predicted value based on the maximum 'pmts_year_1139T' value. submission = pd.read_csv( \"submission.csv\" )\npmts_year = ... # max pmts_year_1139T group by case_id submission.loc[pmts_year == 2020 , \"score\" ] = (submission.loc[pmts_year == 2020 , \"score\" ] - 0.07 ).clip( 0 )\n    submission.loc[pmts_year == 2021 , \"score\" ] = (submission.loc[pmts_year == 2021 , \"score\" ] - 0.06 ).clip( 0 )\n    submission.loc[pmts_year == 2022 , \"score\" ] = (submission.loc[pmts_year== 2022 , \"score\" ] - 0.02 ).clip( 0 )\n    submission.to_csv( \"submission.csv\" , index= False ) content_copy 20 7 Please sign in to reply to this topic. comment 7 Comments Hotness NicholasYong216 Posted a year ago arrow_drop_up 1 more_vert Thank you for your sharing.  I learned a lot from your post. However, apart from using pseudo-labeling, are there any techniques I can use? Yuya Shintani Topic Author Posted a year ago · 13th in this Competition arrow_drop_up 2 more_vert In my opinion, this competition is not suitable for learning materials because the postprocess effects are too big. Andreas Bisiadis Posted a year ago · 1694th in this Competition arrow_drop_up 1 more_vert Nice writeup! However, I have some questions: Did you use pseudo-labeling/meta-features? For the LightGBM models, was 'dart' better than any other boosting method (gbdt, rf)? For how long did training with dart last? Yuya Shintani Topic Author Posted a year ago · 13th in this Competition arrow_drop_up 4 more_vert Thank you for your comment. Did you use pseudo-labeling/meta-features? We tried both, but since they didn't work on the public lb, we didn't include them in the final submission. For the LightGBM models, was 'dart' better than any other boosting method (gbdt, rf)? For how long did training with dart last? The best LightGBM model in local cv, public lb was gbdt with extra_tree set to true. In dart training, we fixed n_estimators to 1000. Andreas Bisiadis Posted a year ago · 1694th in this Competition arrow_drop_up 1 more_vert @uplus26e7 How did you implement pseudo-labels? As OOF predictions on the validation or train set? Yuya Shintani Topic Author Posted a year ago · 13th in this Competition arrow_drop_up 3 more_vert In each fold, we used the predictions of the base model to the validation data as targets. C R Suthikshn Kumar Posted a year ago · 182nd in this Competition arrow_drop_up 2 more_vert Congratulations on securing 14th place in this competition. Thanks for sharing details of your solution. Varuni Rao Posted a year ago · 147th in this Competition arrow_drop_up 2 more_vert Glad to see you have used Calibrated results. My mistake was that I did not select the calibrated response for the final submission, else it would have boosted me up by at least 50. I learned a good lesson in this first competition - to trust my instincts. I manually implemented the isotonic calibration on the final results of my LGB and CatBoost ensemble and it gave me a 0.09 point boost on the private LB but did not select it as final submission because it did not give me any boost in public LB. luyao.li Posted a year ago arrow_drop_up 0 more_vert Thank you for your sharing， however i have one question： in Postprocess stage,how do you determine the decrease value Thank you for your response in advance."
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Create a model measured against feature stability over time In this competition, you will be predicting default of clients based on internal and external information that are available for each client. Scoring is performed using custom metric that not only evaluates the AUC of predictions but also considers the stability of predictions model across the data range of the test set. To better understand this metric, please refer to the Evaluation tab . This dataset contains a large number of tables as a result of  utilizing diverse data sources and the varying levels of data aggregation used while preparing the dataset. Note: All files listed below are found in both .csv and .parquet formats. Base tables Base tables store the basic information about the observation and case_id . This is a unique identification of every observation and you need to use it to join the other tables to base tables. Train FIles: Test Files: static_0 Properties: depth=0, internal data source Train Files: Test Files: static_cb_0 Properties: depth=0, external data source Train Files: Test Files: applprev_1 Properties: depth=1, internal data source Train Files: Test Files: other_1 Properties: depth=1, internal data source Train Files: Test Files: tax_registry_a_1 Properties: depth=1, external data source, Tax registry provider A Train Files: Test Files: tax_registry_b_1 Properties: depth=1, external data source, Tax registry provider B Train Files: Test FIles: tax_registry_c_1 Properties: depth=1, external data source, Tax registry provider C Train Files: Test Files: credit_bureau_a_1 Properties: depth=1, external data source, Credit bureau provider A Train Files: Test Files: credit_bureau_b_1 Properties: depth=1, external data source, Credit bureau provider B Train Files: Test files: deposit_1 Properties: depth=1, internal data source Train Files: Test Files: person_1 Properties: depth=1, internal data source Train Files: Test Files: debitcard_1 Properties: depth=1, internal data source Train Files: Test Files: applprev_2 Properties: depth=2, internal data source Train Files: Test Files: person_2 Properties: depth=2, internal data source Train Files: Test Files: credit_bureau_a_2 Properties: depth=2, external data source, Credit bureau provider A Train Files: Test Files: credit_bureau_b_2 Properties: depth=2, external data source, Credit bureau provider B Train Files: Test Files: Please be aware that the same naming conventions apply to the test files. It's worth noting that some external data providers might not be available for future (test) evaluations, which is anticipated. Each group of tables can comprise one or more individual tables. If a group contains more than one table, they are divided based on WEEK_NUM . This division was implemented to restrict the maximum size of the tables. Depth values: You can read more about Credit bureau (CB) here https://en.wikipedia.org/wiki/Credit_bureau . Special columns: All other raw columns in the tables serve as predictors. Their definitions can be found in the file feature_definitions.csv . For depth=0 tables, predictors can be directly used as features. However, for tables with depth>0, you may need to employ aggregation functions that will condense the historical records associated with each case_id into a single feature. In case num_group1 or num_group2 stands for person index (this is clear with predictor definitions) the zero index has special meaning. When num_groupN =0 it is the applicant (the person who applied for a loan). Various predictors were transformed, therefore we have the following notation for similar groups of transformations Please note that transformations within a group are denoted by a capital letter at the end of the predictor name (e.g., maxdbddpdtollast6m_4187119P ). We hope that this will simplify the manipulation with predictors. Edits: Test dataset was closed, please read our post with details in https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/discussion/482474 138 files 26.77 GB csv, parquet Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 26.77 GB csv_files parquet_files feature_definitions.csv sample_submission.csv 138 files 2588 columns ",
    "data_description": "Home Credit - Credit Risk Model Stability | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Code Competition · a year ago Late Submission more_horiz Home Credit - Credit Risk Model Stability Create a model measured against feature stability over time Home Credit - Credit Risk Model Stability Overview Data Code Models Discussion Leaderboard Rules Overview The goal of this competition is to predict which clients are more likely to default on their loans. The evaluation will favor solutions that are stable over time. Your participation may offer consumer finance providers a more reliable and longer-lasting way to assess a potential client’s default risk. Start Feb 5, 2024 Close May 28, 2024 Merger & Entry Description link keyboard_arrow_up The absence of a credit history might mean a lot of things, including young age or a preference for cash. Without traditional data, someone with little to no credit history is likely to be denied. Consumer finance providers must accurately determine which clients can repay a loan and which cannot and data is key. If data science could help better predict one’s repayment capabilities, loans might become more accessible to those who may benefit from them the most. Currently, consumer finance providers use various statistical and machine learning methods to predict loan risk. These models are generally called scorecards. In the real world, clients' behaviors change constantly, so every scorecard must be updated regularly, which takes time. The scorecard's stability in the future is critical, as a sudden drop in performance means that loans will be issued to worse clients on average. The core of the issue is that loan providers aren't able to spot potential problems any sooner than the first due dates of those loans are observable. Given the time it takes to redevelop, validate, and implement the scorecard, stability is highly desirable. There is a trade-off between the stability of the model and its performance, and a balance must be reached before deployment. Founded in 1997, competition host Home Credit is an international consumer finance provider focusing on responsible lending primarily to people with little or no credit history. Home Credit broadens financial inclusion for the unbanked population by creating a positive and safe borrowing experience. We previously ran a competition with Kaggle that you can see here . Your work in helping to assess potential clients' default risks will enable consumer finance providers to accept more loan applications. This may improve the lives of people who have historically been denied due to lack of credit history. Evaluation link keyboard_arrow_up Submissions are evaluated using a gini stability metric. A gini score is calculated for predictions corresponding to each WEEK_NUM . gini = 2 ∗ AUC − 1 A linear regression, a ⋅ x + b , is fit through the weekly gini scores, and a falling_rate is calculated as min ( 0 , a ) . This is used to penalize models that drop off in predictive ability. Finally, the variability of the predictions are calculated by taking the standard deviation of the residuals from the above linear regression, applying a penalty to model variablity. The final metric is calculated as stability metric = m e a n ( g i n i ) + 88.0 ⋅ m i n ( 0 , a ) − 0.5 ⋅ s t d ( residuals ) Submission File For each case_id in the test set, you must predict a probability for the target score . The file should contain a header and have the following format: case_id ,score 57543 , 0 . 1 57544 , 0 . 9 57545 , 0 . 5 etc . content_copy Timeline link keyboard_arrow_up February 5, 2024 - Start Date. May 20, 2024 - Entry Deadline. You must accept the competition rules before this date in order to compete. May 20, 2024 - Team Merger Deadline. This is the last day participants may join or merge teams. May 27, 2024 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Prize: $ 25,000 2nd Prize: $ 20,000 3rd Prize: $ 15,000 4th Prize: $ 10,000 5th – 9th Prize: $ 5,000 Special Prize: $ 10,000 - Special prize for consistent model stability over time Stability Prize link keyboard_arrow_up We are hosting a second track of this competition that focuses on stability. Credit scoring models typically have a lifecycle of a year or more, and their consistent performance over time is key. In this competition, you will work with a new metric that encounters both AUC and stability over weeks on the future sample (test set). For the Stability Prize, we will evaluate submissions based on how well they included this custom stability metric directly into model training. We can't wait to see the innovative approaches you come up with to tackle stability in your models. Requirements for qualification for special Stability Prize: Your solution is among top 20% on the private leaderboard Your approach to stability should be: 1) general, not dependent explicitly on used dataset, and 2) incorporate stability directly into your training model / loss function (not on the level of feature preparation or feature selection) Your notebook is public (can be made public at the end of the competition) Scoring Innovation (40%): Analyzes credit stability over time, specifically focusing on the competition evaluation metric beyond just feature selection. e.g. High average gini, non-decreasing performance over time, small oscillations… Quality (20%): Any tables, figures, and/or code samples are high quality (e.g. information-dense, easy-to-understand, and informative) Clarity (20%): The overall composition is articulate, concise, accurate, and easy-to-understand. Generalization (20%): The implementation of stability as a metric can be applied to non-competition datasets. Click here to go to the Submission form Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 12 hours run-time GPU Notebook <= 12 hours run-time Internet access disabled Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors. Citation link keyboard_arrow_up Daniel Herman, Tomas Jelinek, Walter Reade, Maggie Demkin, and Addison Howard. Home Credit - Credit Risk Model Stability. https://kaggle.com/competitions/home-credit-credit-risk-model-stability, 2024. Kaggle. Cite Competition Host Home Credit Group Prizes & Awards $105,000 Awards Points & Medals Participation 27,479 Entrants 5,315 Participants 3,856 Teams 91,975 Submissions Tags Tabular Banking Custom Metric Table of Contents collapse_all Overview Description Evaluation Timeline Prizes Stability Prize Code Requirements Citation"
  },
  {
    "competition_slug": "predict-energy-behavior-of-prosumers",
    "discussion_links": [
      "/competitions/predict-energy-behavior-of-prosumers/discussion/472793",
      "/competitions/predict-energy-behavior-of-prosumers/discussion/499938",
      "/competitions/predict-energy-behavior-of-prosumers/discussion/499397",
      "/competitions/predict-energy-behavior-of-prosumers/discussion/499649",
      "/competitions/predict-energy-behavior-of-prosumers/discussion/472537",
      "/competitions/predict-energy-behavior-of-prosumers/discussion/499364",
      "/competitions/predict-energy-behavior-of-prosumers/discussion/499475",
      "/competitions/predict-energy-behavior-of-prosumers/discussion/472565",
      "/competitions/predict-energy-behavior-of-prosumers/discussion/499358",
      "/competitions/predict-energy-behavior-of-prosumers/discussion/472598",
      "/competitions/predict-energy-behavior-of-prosumers/discussion/500243"
    ],
    "discussion_texts": [
      "Enefit - Predict Energy Behavior of Prosumers | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Eesti Energia · Featured Code Competition · a year ago Late Submission more_horiz Enefit - Predict Energy Behavior of Prosumers Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Enefit - Predict Energy Behavior of Prosumers Overview Data Code Models Discussion Leaderboard Rules hyd · 1st in this Competition  · Posted a year ago arrow_drop_up 175 more_vert 1st place solution Thanks to Enefit and Kaggle for hosting this great competition. And thanks to the great notebooks and discussions, I learned a lot. I have a feeling this comp is going to be slightly more volatile \nthan optiver one and wish I could end up in top place. I am so happy to win my third solo win! 😃😀😀 Overview My final solution consists of 4 XGBoost models (for two targets and two is_consumption) and 2 GRU models (for two targets). These models share same 600 features. model name validation set test set w/o online training test set w/ update one time test set w/ update three times XGBoost 41.5912(2 targets:42.2531/42.6607) 55.6231 54.1343 54.1213 GRU 43.0815(2 targets:43.7182/45.1687) 56.6651 54.9948 54.6960 XGBoost+GRU 40.6790(2 targets:40.9613/41.5294) 53.6627 52.4813 52.3090 Validation Strategy My validation strategy is pretty simple, train on first 500 days and set the rest days as my holdout validation set which has good indication with leaderboard score. And I only checked the final overall score and didn't print single model's score. Features I think my features are not special compared with other top teams. I simply merge all tables excluding gas_df with train_df and create lots of lagged features. I didn't spend much time to cutdown feature size since we have not much memory pressure in this competition. With careful feature design, I believe the final feature size can be reduced to around 100-200. Maybe generating different features and targets according to different conditions will achieve better model performance after reading other kagglers' solutions, but this is also very time consuming for me. fw_new_feature from this discussion help a bit(about 0.1~0.2), Thanks to @mohammadpakdaman0 # client_df\nclient_df = client_df. drop ([\"date\"]).sort([\"data_block_id\"], descending= False )\ndf = df. join (client_df. select ([\"county\", \"is_business\", \"product_type\", \"data_block_id\",\"eic_count\",\"installed_capacity\"]), how=\"left\", on =[\"county\", \"is_business\", \"product_type\", \"data_block_id\"])\n# electricity\nelectricity_df = electricity_df. drop ([\"origin_date\"]). rename ({\"forecast_date\":\"datetime\"}).with_columns([(pl.col(\"datetime\").str.to_datetime()+pl.duration(days= 1 )). alias (\"datetime\"), (pl.col(\"euros_per_mwh\").abs()+ 0.1 ). alias (\"euros_per_mwh\"),])\ndf = df. join (electricity_df[[\"data_block_id\",\"datetime\",\"euros_per_mwh\"]], how=\"left\", on =[\"data_block_id\",\"datetime\"])\n\n# fw_df \nfw_df = fw_df.with_columns(\n                (pl.col(\"origin_datetime\").str.to_datetime()+pl.duration(hours=\"hours_ahead\")). alias (\"datetime\"),\n                pl.col(\"latitude\").cast(pl.datatypes.Float32).round( 1 ),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32).round( 1 ),\n                pl.col(\"data_block_id\").cast(pl.datatypes.Int64),\n            ). join (weather_station_to_county_mapping. drop ([\"county_name\"]).with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32).round( 1 ),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32).round( 1 ),\n            ), how=\"left\", on =[\"longitude\",\"latitude\"]). drop ([\"longitude\",\"latitude\",\"origin_datetime\"])\nfw_df = fw_df.group_by([\"county\",\"datetime\",\"data_block_id\"]).agg([pl.mean(col). alias (\"fw_{}\".format(col)) for col in forecast_weather_cols]).with_columns([pl.col(\"county\").cast(pl.datatypes.Int64),    \n                                                                    pl.col(\"data_block_id\").cast(pl.datatypes.Int64),])\ndf = df. join (fw_df, how=\"left\", on =[\"county\",\"datetime\",\"data_block_id\"]).with_columns([(pl.col(\"installed_capacity\")*pl.col(\"fw_surface_solar_radiation_downwards\") / (pl.col(\"fw_temperature\") + 273.15 )). alias (\"fw_new_feature\"),])\n\n# hw_df \nhw_df = hw_df.with_columns(\n            (pl.col(\"datetime\").str.to_datetime()+pl.duration(days= 1 )). alias (\"datetime\"),\n                pl.col(\"latitude\").cast(pl.datatypes.Float32).round( 1 ),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32).round( 1 ),\n                pl.col(\"data_block_id\").cast(pl.datatypes.Int64),\n            ). join (weather_station_to_county_mapping. drop ([\"county_name\"]).with_columns(\n                pl.col(\"latitude\").cast(pl.datatypes.Float32).round( 1 ),\n                pl.col(\"longitude\").cast(pl.datatypes.Float32).round( 1 ),\n            ), how=\"left\", on =[\"longitude\",\"latitude\"]). drop ([\"longitude\",\"latitude\"])\nhw_df = hw_df.group_by([\"county\",\"datetime\",\"data_block_id\"]).agg([pl.mean(col). alias (\"hw_{}\".format(col)) for col in historical_weather_cols]).with_columns([                                                            pl. when (pl.col(\"datetime\").dt.hour()> 10 ). then (pl.col(\"datetime\")+pl.duration(days= 1 )).otherwise(pl.col(\"datetime\")). alias (\"datetime\"),\n                                                                pl.col(\"county\").cast(pl.datatypes.Int64),    \n                                                                pl.col(\"data_block_id\").cast(pl.datatypes.Int64),])\ndf = df. join (hw_df, how=\"left\", on =[\"county\",\"datetime\",\"data_block_id\"]) content_copy target (target-target_shift2)/installed_capacity. target/installed_capacity. target-target_shift2 raw target I only use 1&2 since 3&4 have no benefit to my final overall score. model I didn't spend much time to tune my xgboost models, just simply train and predict.  And I find there is no team using NN models yet, which really surprises me. My GRU models are also very simple.  input tensor's shape is (batch_size, 24 hours, 600 dense_feature_dim + 6*16 cat_feature_dim), followed by 2 layers GRU, output tensor's shape is (batch_size, 24 hours). Categorical features are ['county', 'product_type', 'hour', 'month', 'weekday', 'day']. NN models boost my final score by 0.9. online learning strategy Although online learning didn't help much in public leaderboard, I think it is crucial in private leaderboard since we will have additional 8 months‘ data. So I just gave up chasing higher public leaderboard score by ensembling with more models and decided to retrain less models every month. What not worked for me solar features from this notebook 1dcnn model and transformer model multi-days input instead of singe day input when applying GRU models split into more models given is_business equals 0/1. Thank you all! I will write more if necessary. 1 17 Please sign in to reply to this topic. comment 66 Comments 1 appreciation  comment Hotness paul rogov Posted a year ago arrow_drop_up 1 more_vert Hey Huang, thanks for another great content🥇 Two questions to you: how much time do you roughly spend on competitions mean() ? From first seeing a new dataset until training a good-enough model ensemble to be top 1-10 on Public Leaderboard Is pl.when() a polars or a spark sql function? If so, do you use Polars/Spark for handy dataset querying and feature engineering? Is it more flexible than pandas dataframe api? Have you noticed any difference in processing speed or memory usage against standard pandas? Thx hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 1 more_vert Thanks, I can't believe you know my last name is Huang. 2 hours per day, about 25 days, 10 days to reach 1-10 on lb. polars, much faster than pandas. paul rogov Posted a year ago arrow_drop_up 2 more_vert Haha, @whydata in one of the comments above called you Teacher Huang and I've guessed that he knows what is talking about. I lived in China for some time but for too short to learn Pǔtōnghuà. Thank you for the answer MaxUhl98 Posted 10 months ago · 2275th in this Competition arrow_drop_up 0 more_vert Hi Huang, may I know how you are able to consistently perform well with this small amount of time you are investing into kaggle? Yuan Xu Posted 8 months ago · 50th in this Competition arrow_drop_up 0 more_vert hahaha， 😁 Khushi Arora Posted a year ago arrow_drop_up 4 more_vert Thanks alottt. You too have done great work dude !!! Pawar Pallavi Posted a year ago arrow_drop_up 1 more_vert Interesting Edouard Gaudreau Posted a year ago arrow_drop_up 1 more_vert Congratulations! Sumit Pardhiya Posted a year ago arrow_drop_up 1 more_vert Amazing Solution!!! emoji_people WhyData Posted a year ago arrow_drop_up 1 more_vert Congratulations！Teacher Huang NB! I am also your fan~ I also tried a similar approach to target processing, but it was strange that I would lose points on the public leaderboard every time, so I ultimately chose not to handle the target in any way. SO I guess that your proprecessing for the installed_capacity and target may the key to get so higher score at lb with sample XGB & GRU. I noticed that you ended up using these two target processing methods, (target-target_shift2)/installed_capacity.\ntarget/installed_capacity. Are you using the first target for consumption and the second target for production? Or did they only construct duplicate models using two different objectives separately？ Anyway, thank you for bringing such a wonderful solution！ emoji_people WhyData Posted a year ago arrow_drop_up 0 more_vert Oh sorry, I find the answer 4 xgboost models (for two targets * two is_consumption) 🥹 hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 0 more_vert I think maybe we should generate different features when applying different targets. Proprecessing for installed_capacity is not the  key role. I am not sure. emoji_people WhyData Posted a year ago arrow_drop_up 0 more_vert WOW teacher, I just realized that you seem to have only filled the target and installed capacity in the last 1-4 hours by shift, rather than 1-4 days ? I estimate that there may be more than 7 consecutive missing items in the public list due to some bugs. Perhaps you are right, the difference and ratio features may be more suitable for the processed target. Additionally, high-frequency retraining may weaken the effectiveness of target processing. zhewei zhi Posted a year ago · 292nd in this Competition arrow_drop_up 0 more_vert Hello! I'd like to ask about the meaning of the variable target_shift_tmp. Does it mean shifting the data of each row in the target down by 2 + 3 + 4 rows? Thank you very much for your response! hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 0 more_vert Yes, you are right! JJ Krasnick Posted a year ago arrow_drop_up 1 more_vert Congratulations on your achievements! Great solution! emoji_people hahahaj Posted a year ago · 144th in this Competition arrow_drop_up 1 more_vert Congrats hyd! I would like to ask why you chose to use Xgboost instead of Lightgbm. hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 5 more_vert There is no big difference between these two models, I just randomly choose xgboost  and have no enough time to train lightgbm.😄 Timmy Juicehouse Posted a year ago · 238th in this Competition arrow_drop_up 1 more_vert Congratulation, I learn a lot from your post and am looking forward to your performance in next competition. I am your fan. client_df client_df = client_df.drop([\"date\"]).sort([\"data_block_id\"], descending=False) df = df.join(client_df.select([\"county\", \"is_business\", \"product_type\", \"data_block_id\",\"eic_count\",\"installed_capacity\"]), how=\"left\", on=[\"county\", \"is_business\", \"product_type\", \"data_block_id\"]) Do you notice high installed_capacity(about over 5000, I am not sure of the specific volume) in consumption (is_consumption == 1) is hard to predict, hopefully it's relatively not huge part in the whole datasets. I'm struggle to deal with high installed_capacity. (target-target_shift2)/installed_capacity. target/installed_capacity. Actually I tried them because I don't know how to solve high installed_capacity. hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 1 more_vert Thanks! I also find county 0 is very hard to predict because of its large consumption. Normalized by installed_capacity helps a bit. And I didn't take any other action, I think this represents the difficulty of the problem itself. Daisy Posted a year ago arrow_drop_up 1 more_vert You're transformer, but it seems not to be used in this competition. I'm looking forward to seeing you at 1st place in the Private Leaderboard! hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 1 more_vert Thank you! Yes, transformer does not hep in this competition~ emoji_people DongYK Posted a year ago arrow_drop_up 1 more_vert Amazing! Congratulations 1st in public score. I hope you keep the same place for the private leaderboard. We thought the GRU for production / installed_capacity and ANN for consumption, but we gave up because we are not used to those models for the data with many categorical features like segment, county, …, etc. I came up with (batch_size, 24 * 7, < 50 feature_dim). Wow. It's much easier to use features for 24 hours (maybe from start of the day to end of the day). If you don't mind, can you explain how to deal with the categorical features in GRU, NN (did you use one hot encoding? or some embedding layers?) and XGBoost? I have failed to build XGBoost models. So, I was wondering it is attributed to the difference of categorical features or hyperparameter. hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 2 more_vert For GRU, I just concat dense features with categorical features like this: cats = [embedding(x_cat[:, :, i]) for i, embedding in enumerate(self.cats)] x_cat_emb = torch.cat(cats, 2 ) x = torch.cat([x, x_cat_emb], 2 ) content_copy For GBDT, I didn't do anything special, because all categorical features are low-dimension, I think GBDT can learn very well. emoji_people DongYK Posted a year ago arrow_drop_up 1 more_vert Great architecture. It's very impressive to reduce the dimension of time. For GBDT, I didn't do anything special, because all categorical features are low-dimension, I think GBDT can learn very well. Can I understand your words as you didn't set county, weekday, … as categorical features in XGBoost because of the low number of dimensions? 4 more replies arrow_drop_down emoji_people sigint Posted a year ago · 253rd in this Competition arrow_drop_up 2 more_vert Congrats and thank you for sharing solution hyd! Why do you think transformers didn't perform as well as RNN this competition? hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 0 more_vert Thanks! I usually don't use transformer to learn time sequence information like RNN do but to learn information across stocks in optiver or counties in this competition. The relative target in optiver indicates there is some information you can learn by transformer models. But target here is very simple and there is no strong connection among different counties. emoji_people sigint Posted a year ago · 253rd in this Competition arrow_drop_up 2 more_vert Ah ok that makes sense. Thanks again hyd we learn a lot from your solutions and good luck on becoming kaggle #1. I'm a big fan of your work wasjaip Posted a year ago · 47th in this Competition arrow_drop_up 2 more_vert great job =) waiting for testing =))) hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 2 more_vert Good luck to you too! well well well Posted a year ago · 337th in this Competition arrow_drop_up 2 more_vert Congrats! 大佬 MrSimple Posted a year ago arrow_drop_up 0 more_vert Congrats! Great solution! silver Posted a year ago · 162nd in this Competition arrow_drop_up 0 more_vert \"So I just give up chasing higher public leaderboard score\" I am hurt😭 SUBHANI Posted 6 months ago arrow_drop_up 0 more_vert Fr! Loved the approach! faeqsu10 Posted 9 months ago arrow_drop_up 0 more_vert Thank you so much for the great insights into solving time series data problems. I've learned a lot thanks to you! :) João Mesquita Posted a year ago arrow_drop_up 0 more_vert Can you share your notebook? JK-Piece Posted a year ago · 334th in this Competition arrow_drop_up 0 more_vert Hello Hyd. I cannot find the code/notebook for the first place. Can you please share it? Aakash D Posted a year ago arrow_drop_up 0 more_vert I really appreciate you sharing this information. bc_bot Posted a year ago · 532nd in this Competition arrow_drop_up 0 more_vert Congratulations! Can you provide more details on how you combined your XGBoost and GRU models? Weighted ensemble or other method JK-Piece Posted a year ago · 334th in this Competition arrow_drop_up 0 more_vert Congratulations on the 1st place🎉 zhewei zhi Posted a year ago · 292nd in this Competition arrow_drop_up 0 more_vert Thank you very much for your sharing! I'd like to ask, what do target_shift_tmp and installed_capacity_tmp mean? I didn't quite understand the code. zhangwuji Posted a year ago · 189th in this Competition arrow_drop_up 0 more_vert Great solution",
      "Enefit - Predict Energy Behavior of Prosumers | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Eesti Energia · Featured Code Competition · a year ago Late Submission more_horiz Enefit - Predict Energy Behavior of Prosumers Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Enefit - Predict Energy Behavior of Prosumers Overview Data Code Models Discussion Leaderboard Rules loh-maa · 5th in this Competition  · Posted a year ago arrow_drop_up 28 more_vert 5th place solution A few words about myself I completed my PhD in AI in 2011, however from there, I opted for travel and adventure, rather than a career in R&D. Only in recent years I got back to ML, and despite all the buzz around AI, I'm still having a hard time finding a job. Perhaps living in Thailand as an expat complicates things a bit, so I resort to freelancing and that's also why I had plenty of time to dedicate to this competition. I enjoyed it a lot, first, because it was a great lesson, and also, because it was easy on hardware and thus allowed for plenty of experimentation. Solution overview After trying out some regression models from scikit-learn , out of curiosity, I turned to XGBoost and finally to LightGBM which gave the best baseline just out-of-the-box. I didn't have prior pleasure with Gradient Boosting methods in a real-world problem setting, so it was even more interesting. I came to conclusion that learning from notebooks shared by more experienced players is quite effective, when we have examples of how things can be done elegantly, such as Enefit│XGBoost│starter and Enefit Generic Notebook (I hope and believe these are the origin of the code which turns up modified-or-not in many other notebooks later on.) It was also the first time I saw polars in action. Thank you gentlemen for showing the way! As everybody knows, the problem was largely about feature selection and engineering. So it took lots of experimentation, in fact the whole process was one big loop akin to trial & error: idea & evaluation. The MLflow package was particularly helpful in that. Obviously I don't even remember all the ideas that I have tried out, so I will only describe what survived. More details on features in the next section. When it comes to the model, I opted for LightGBM and from there not much can be done but fine-tuning the parameters and eventually configuring an ensemble. Fine-tuning was done using a simple custom evolutionary search and a 2-stage cross-validation scheme. The 1st stage was time series cross-validation with 3 folds and each fold holding 2 months of testing data, so: training data until 2022-09-01, testing until 2022-11-01 training data until 2022-11-01, testing until 2023-01-01 training data until 2023-01-01, testing until 2023-03-01 This split was used primarily for intense algorithmic tuning and feature selection. The 2nd stage, i.e. a simple split with the remaining 3 months of data (until 2023-05-26) I used for less intense manual tuning and selection. The final model selection was naturally based on the public test data. Interestingly and admittedly, I was trying to beat the score on the public LB, but somehow I wasn't able to, ending up at 22nd place. Perhaps my local CV restricted me a bit in this pursuit, but anyway, I consider my entry to be very lucky. By looking at some other solutions, I think my proposal is more on the simple side. Crucially for the performance, \"production\" and \"consumption\" units are tackled separately. The production part is fed with only 75 features, while the consumption with 85. A scikit-learn's VotingRegressor ensemble of 3 LightGBM regression estimators is trained for each part, with slightly different parameters. Specifically: Production Consumption boosting_type :                        gbdt num_leaves : 358 max_depth : 10 learning_rate : 0 . 02 0 . 025 min_split_gain : 0 . 0 0 . 1 min_sum_hessian_in_leaf : 1 . 0 0 . 1 min_data_in_leaf : 120 360 bagging_fraction : 1 . 0 bagging_freq : 0 feature_fraction : 0 . 8 feature_fraction_bynode : 0 . 9 lambda_l1 : 0 . 01 lambda_l2 : 1 . 0 max_bin : 255 n_estimators : 2000 content_copy As you might have guessed, I cannot really justify those values -- they are just a result of fine-tuning. All training data is used at the submission evaluation with no validation set for early stopping. 2000 estimators didn't seem to give much advantage over let's say 1200, but on the other hand I didn't observe any heavy tendency for over-fitting at cross-validation, and also there was more data coming, so 2000 looked fine. Finally, important part of the solution is online re-training, which is done every 9 days of scoring data, so about 10 trainings along evaluation in total. As training of both ensembles is somewhat costly -- it takes about 20 minutes with GPU -- it was important to train only for scoring periods. Incoming data had to be collected for each day regardless. It was critical to understand how the submission evaluation works at all stages exactly and be careful to avoid unforeseen errors. I don't know if anyone's solution fell out at this turn, but it felt a bit slippery, for instance when switching between pandas and polars. I even encountered errors when using just different versions of these great libraries. Yet, maybe hosts took proper care to secure an easy pass for any solution passing the public data evaluation. I'm sure it could be very nasty if there were glitches in testing data. Feature selection and engineering Regarding features, I often wondered which approach is better: reduction or redundancy. I tended to reduce the number of features and simplify stuff wherever possible, but the dilemma comes back regardless. I mean, when a feature or a set of features seem to have little effect at cross-validation, do you keep it or remove it? The top solution by hyd apparently takes a very generous approach here, with lots of \"lagging\" features and reportedly not much care of what goes in, but was it actually for the better? LightGBM models can do their own feature selection and can handle random and useless features easily, however I imagine, in a multitude of features there is also risk of picking up \"false patterns\". Ultimately, it probably just depends on the problem and the features. For a while, I was using Optuna for feature selection, however later on, I was more comfortable with a simple custom evolutionary search, which worked for the model fine-tuning as well. Because there aren't too many of them, I can list all the selected features, for production: county, is_business, product_type, installed_capacity, euros_per_mwh,\nhours_ahead_fd, temperature_fd, dewpoint_fd, cloudcover_high_fd, cloudcover_low_fd, cloudcover_mid_fd, cloudcover_total_fd, 10 _metre_u_wind_component_fd, 10 _metre_v_wind_component_fd, direct_solar_radiation_fd, surface_solar_radiation_downwards_fd, total_precipitation_fd,\ntemperature_fl, dewpoint_fl, cloudcover_high_fl, cloudcover_low_fl, cloudcover_mid_fl, 10 _metre_u_wind_component_fl, 10 _metre_v_wind_component_fl, direct_solar_radiation_fl, snowfall_fl, total_precipitation_fl,\ntemperature_hl, dewpoint_hl, rain_hl, snowfall_hl, surface_pressure_hl, cloudcover_total_hl, cloudcover_low_hl, cloudcover_mid_hl, cloudcover_high_hl, windspeed_10m_hl, winddirection_10m_hl, direct_solar_radiation_hl, diffuse_radiation_hl, dewpoint_fl_dmean, cloudcover_high_fl_dmean, cloudcover_low_fl_dmean, cloudcover_mid_fl_dmean, cloudcover_total_fl_dmean, 10 _metre_u_wind_component_fl_dmean, 10 _metre_v_wind_component_fl_dmean, direct_solar_radiation_fl_dmean, surface_solar_radiation_downwards_fl_dmean, snowfall_fl_dmean, total_precipitation_fl_dmean, hours_ahead_fl_dmean, temperature_fl_dmean,\ntarget_6r_2, target_6r_3, target_6r_4, target_6r_5, target_6r_14, day, weekday, sin_dayofyear, cos_dayofyear, cos_hour, sin_hour, target_mean_ic_ratio_2, target_std_6r, target_delta_68_6r, target_7r_2, is_holiday, is_holiday_2, n_holidays_past_week, ssrd_t, ssrd_t_mean content_copy and consumption (diff): + target_2, cloudcover_total_fl, month, surface_solar_radiation_downwards_fl, target_4r_14, snowfall_fd, eic_count, target_delta_68_7r, target_4r_7, target_ratio_18_7r, installed_capacity_ratio, target_7r_7, target_std_7r, hours_ahead_fl, target_mean_2, target_4r_2, shortwave_radiation_hl, target_ratio_68_7r, year\n- target_6r_2, target_6r_3, target_6r_4, target_6r_5, target_6r_14, day, cos_dayofyear, ssrd_t, ssrd_t_mean content_copy Notably, the gas price has been dropped from both sets, as well as the country-wide historical weather. But instead, we have a bunch of *_fl_dmean features which are differences between \"forecast local\" weather and their average over the past week. There's also a couple of target_* features which are some transforms of the revealed targets (lags, ratios, averages, differences) and which survived the selection. The ssrd_* are based on the transform suggested here . is_holiday_2 is true if 2 days before was a holiday (maybe useful in conjunction with those past targets transforms that involve the most recent revealed target value, i.e. from 2 days before.) Another always-trailing dilemma were date-related features, like day , month or year , often on the border of significance at cross-validation. Having the trigonometric transform of the day of year should make day and month redundant, but of course it doesn't work like that. The biggest mystery though was the year , which in our case had very limited span. Sometimes you see general advice to keep such a feature, but honestly I can't figure out how a model can make use of a feature that is going to have a new, previously unseen value at prediction. Maybe it's already helpful for the model to see different years during training? It would be nice to find the right explanation. A big part of the solution and closely related to feature engineering was \"target transformation\". I did plenty of trials here, but ultimately I went with two simple transforms, for the production units: z = target / ( 1 + installed_capacity), content_copy where installed_capacity is obviously the most recent value available at prediction time, so lagging 2 days. And for the consumption units: z = target / ( 1 + target_avg), content_copy where target_avg is the average target from the most recent available 7 days (so actually day d-2 up to d-9 ), sampled at the same time as target , which is also from day d-2 . Interestingly, I tried to train the models assuming \"perfect information\", i.e. without the lags, as if all the information about the current datetime was available, but curiously it didn't make much difference to prediction. Anyway, there was additional processing and NaN filling to avoid any errors. The extra 1 in denominators is to prevent excess values in those hypothetical glitchy cases when the other term would be close to zero (it could be just clipping, too, though.) One more treatment which I will never know was ultimately helpful or not, was filling any missing values of the transformed target. The filling values were taken as median of similar units across the country. I guess it could be helpful during prediction for any unforeseen or interrupted prediction units, although according to cross-validation, filling up with just moderate constants was equally good. ^^ That's it, big thanks to the hosts for this great competition, congrats to the winners, cheers to all players, comments welcome and till next time! Please sign in to reply to this topic. comment 7 Comments Hotness hyd Posted a year ago · 1st in this Competition arrow_drop_up 2 more_vert Great solution! Good luck for finding a good job in Thailand very soon~ Freek Linssen Posted 5 months ago arrow_drop_up 0 more_vert Hi, great solution! I have a question about four variables. How did you construct the *_fd, target_6r_2, ssrd_t_mean,  and target_delta_68_7r variables? Thanks in advance! Icarus22 Posted 9 months ago arrow_drop_up 0 more_vert Thanks for your solution, it's very inspiring. I'm new to Kaggle and I wonder how online retraining is achieved in scoring data. @lohmaa loh-maa Topic Author Posted 9 months ago · 5th in this Competition arrow_drop_up 0 more_vert Hello @icarus22 , online retraining is basically retraining the model with data collected during the 3 months of evaluation, so updating the model with new data, done automatically within the submission, hence \"online\". You can probably find some implementation examples in the Code section. Bogdan Romanov Posted a year ago arrow_drop_up 0 more_vert Thanks for a detailed explanation of the solution! zhangwuji Posted a year ago · 189th in this Competition arrow_drop_up 0 more_vert Great! I love it! Octavi Grau Posted a year ago · 117th in this Competition arrow_drop_up 0 more_vert Great post and solution @lohmaa !",
      "Enefit - Predict Energy Behavior of Prosumers | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Eesti Energia · Featured Code Competition · a year ago Late Submission more_horiz Enefit - Predict Energy Behavior of Prosumers Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Enefit - Predict Energy Behavior of Prosumers Overview Data Code Models Discussion Leaderboard Rules Matt Motoki · 6th in this Competition  · Posted a year ago arrow_drop_up 20 more_vert 6th place solution I'd like to thank Enefit and Kaggle for hosting this competition. It was a fun competition and I learned a lot from it. Final submission: https://www.kaggle.com/code/mmotoki/submit-blend-v5-4-0/notebook Features Most of my features are the same or slightly transformed versions (to match the target) from the public notebooks. Thank you to everyone in the community who contributed to the public notebooks and discussions! The main difference in my features is that I also created simple baseline models for consumption and generation , which I retrained every day. Models I trained separate LightGBM models for production and consumption on 4 different targets (8 models total) target - target_lag48 target/(target_lag48_avg + 1) - baseline_pred target/(target_lag48_avg + 1) - target_lag48 target/(target_lag48_avg + 1) where target_lag48_avg is the daily average of the 2-day lagged target. I didn't think to scale by installed_capacity , but from other top solutions, that seems like a good target. Retraining I actually didn't retrain my LightGBM models at all. The only retraining was in my baseline models. Blending My final model was a weighted combination of the models with different targets, 10 seeds, and two hyperparameter settings. In total, I had 160 models = 8 (targets) × 10 (seeds) × 2 (hyperparameter settings) . In hindsight, I probably should have tried to allocate time to retraining LightGBM rather than creating such a big blend or retraining the simple baseline models so frequently. Target Consumption Weight Generation Weight 1. 0.13021 0.18193 2. 0.49116 0.10890 3. 0.19497 0.52331 3. 0.19300 0.19148 4 Please sign in to reply to this topic. comment 1 Comment Hotness C R Suthikshn Kumar Posted a year ago · 400th in this Competition arrow_drop_up 0 more_vert Congratulations on achieving the 6th rank in this competition. Thanks for sharing your solution details.",
      "Enefit - Predict Energy Behavior of Prosumers | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Eesti Energia · Featured Code Competition · a year ago Late Submission more_horiz Enefit - Predict Energy Behavior of Prosumers Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Enefit - Predict Energy Behavior of Prosumers Overview Data Code Models Discussion Leaderboard Rules Amedeo Biolatti · 7th in this Competition  · Posted a year ago arrow_drop_up 18 more_vert 7th place solution - aka the first of the losers 😄 Thanks to Enefit for hosting the competition! Target As many others I worked with two separate models for production and consumption. At first I noticed that dividing production by installed_capacity and consumption by eic_count improved the performance. In a second moment I tried, as many other, to predict the differnece with the target 48 hours ago, but I tried to explore the autoregressive settings a bit more in depth. In particular I considered the general case of an autoregressive model Y ( t ) = ϵ + α 1 Y ( t − 1 ) + α 2 Y ( t − 2 ) + α 3 Y ( t − 3 ) + … The final model is then trained to predict the residuals of this first AR model. This is a generalization of the simpler \"predict the delta\" approach. After extensive cross validation on a 5 fold sliding windows schema I ended up the the following setup For production: t a g e t _ p r o d u c t i o n = n o r m a l i z e d _ p r o d u c t i o n − n o r m a l i z e d _ p r o d u c t i o n _ l a g 48 h For consumption: t a g e t _ c o n s u m p t i o n = n o r m a l i z e d _ c o n s u m p t i o n − 0.5 n o r m a l i z e d _ c o n s u m p t i o n _ l a g 48 h − 0.1 n o r m a l i z e d _ c o n s u m p t i o n _ l a g 76 h − 0.1 n o r m a l i z e d _ c o n s u m p t i o n _ l a g 92 h − 0.1 n o r m a l i z e d _ c o n s u m p t i o n _ l a g 120 h − 0.1 n o r m a l i z e d _ c o n s u m p t i o n _ l a g 144 h − 0.05 n o r m a l i z e d _ c o n s u m p t i o n _ l a g 168 h − 0.05 n o r m a l i z e d _ c o n s u m p t i o n _ l a g 336 h Features The features used were the same as many public notebooks county, product_type, is_business + combinations hour, day, weekday, month + harmonic features eic_count, installed_capacity gas & electricity prices forecast weather + 7 days lag historical weather + 7 days lag holidays + days before/after releaved target (both production and consumption) with lags (2, 3, 4, 5, 6, 7, 14 days) + normalized, mean, std variations In total I used 192 features Stuff that didn't work difference between historical and forecasted weather more lags variance, covariance, slope, kurtosis of other features various aggregations of radiation features physics formula based on weather data Models I tried to optimize the parameters of LGBM, XGBoost, CatBoost and sklearn's HistGradientBoostingRegressor using Optuna on a 5 fold sliding windows CV schema with a kept aside test set. I also implemented a custom early stopping rule to prune cases that performed badly on the first few CV folds. At the end of the process I got a set of candidate models and I studied different ensembling strategies. The best one was to simply average the top 3 models. Doubling some models by repeating the training with different seeds was also useful The final ensemble was the following LGBM: best_params x 2 seeds LGBM: second_best_params XGBoost: best_params x 2 seeds The same parameters were used for production and consumption, since my experiments lead to the conclusion that a good set of parameters for one was generally good for the other (ie. optuna couln't improve it) Training The 10 models (5 production + 5 consumption) were trained just once at the start of February Post processing I also tried a few ideas for post processing but I didn't see any improvement in performance Please sign in to reply to this topic. comment 3 Comments Hotness Ayman Allawi Posted a year ago · 1927th in this Competition arrow_drop_up 1 more_vert What you have accomplished and learned in the competition is for sure a win C R Suthikshn Kumar Posted a year ago · 400th in this Competition arrow_drop_up 1 more_vert Congratulations on winning the 7th place in this competition. Thanks for your insights into the solution. emoji_people Ravi Ramakrishnan Posted a year ago · 142nd in this Competition arrow_drop_up 1 more_vert Your gold medal is testimony to your win @abiolatti Congratulations!",
      "Enefit - Predict Energy Behavior of Prosumers | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Eesti Energia · Featured Code Competition · a year ago Late Submission more_horiz Enefit - Predict Energy Behavior of Prosumers Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Enefit - Predict Energy Behavior of Prosumers Overview Data Code Models Discussion Leaderboard Rules Ahmet Erdem · 11th in this Competition  · Posted a year ago arrow_drop_up 78 more_vert Public 10th Place Solution (Private 11th) I expect some shake-up but this solution can probably land between 5th and 50th place if it doesn't throw an error. Here is what I did: Stacking Trained 2 separate models: Weather forecast features only model with target/installed_capacity as target Weather history features only model with target/installed_capacity as target Generated estimated production per capacity features and their interactions for my final model. Problem Translation Split the data into 4 and trained models for each (is_business, is_consumption) pair Created baseline prediction as max(target_lag2, target_lag4, target_lag7) New target becomes (target + 1)/(baseline_pred + 1) For each data partition, trained 3 models with different seeds and different sample weights (equal, log1p(baseline_pred) and sqrt(baseline_pred) Features 76 features. Nothing special. features = [ \"baseline_pred\" , \"dow\" , \"month\" , \"hour\" , \"capacity_change\" , 'eic_change' , \"est_per_capacity\" , \"est\" , \"prod_rate\" , \"cons_rate\" , \"cons_rate2\" , \"consumption_type\" , \"county\" , \"product_type\" , \"no_holiday\" , \"capacity_lag_2days\" , \"eic_lag_2days\" ] + weather_forecast_cols for day in [ 2 , 4 , 7 ]:\n    features += [ f\"target_lag_rate {day} \" , f\"target_sum_lag_rate {day} \" , f\"target_lag_ {day} days\" , f\"target_sum_lag_ {day} days\" , f\"target_lag_ {day} days_norm\" , f\"est_per_capacity_lag {day} _rate\" ] for day in [ 2 , 7 ]:\n    features += [ f\" {col} _lag {day} \" for col in weather_hist_cols] content_copy Postprocessing For each prediction_unit_id, calculated lag 2, 3, 4 errors and took their mean. 0.2*avg_error is added into the predictions as correction term. This way, I improve the performance for the cases where I constantly underpredict or overpredict. Re-training Every 8 days, I re-train my XGBoost models on GPU. This is especially useful when there are new prediction unit ids or increase in eics. Validation I split last 9 months into 3 validation folds. Also used the public LB as the 4th fold. While my 3-fold CV is 35.73, my public LB is 61.32. Ensemble Despite its relatively worse score, 80/20 ensembling with the public best kernel made my score 59.97. I think it is because my model was diverse and this problem benefits a lot from diverse ensembling. Btw I had an improvement also in my CV score by ensembling with the public best kernel but it was significantly lower. Please sign in to reply to this topic. comment 26 Comments Hotness Fnoa Posted a year ago · 152nd in this Competition arrow_drop_up 10 more_vert I really like this For each prediction_unit_id, calculated lag 2, 3, 4 errors and took their mean. 0.2*avg_error is added into the predictions as correction term. This way, I improve the performance for the cases where I constantly underpredict or overpredict. What improvement did it represent in terms of validation? Thanks for sharing and good luck on private. Ahmet Erdem Topic Author Posted a year ago · 11th in this Competition arrow_drop_up 3 more_vert Thank you. Improved around 0.4 both CV and LB. Good luck to you too! Timmy Juicehouse Posted a year ago · 238th in this Competition arrow_drop_up 1 more_vert @trasibulo Me too, never think of that. It's awesome. AbaoJiang Posted a year ago · 39th in this Competition arrow_drop_up 1 more_vert Hi @aerdem4 , So impressive, thanks a lot for sharing your solution! The postprocessing part is amazing. We try to do dynamic model selection based on lagged prediction error. That is, we choose to switch to the model having the lowest prediction error in the short term, or ensemble the one performing best with the pre-selected base prediction (the fixed one for all time steps). However, we fail to have a significant boost with this method. Hope you can stay in the gold zone in private LB! emoji_people DongYK Posted a year ago arrow_drop_up 1 more_vert For each data partition, trained 3 models with different seeds and different sample weights (equal, log1p(baseline_pred) and sqrt(baseline_pred) That's very interesting and novel approach. 😀 Every 8 days, I re-train my XGBoost models on GPU. Can you explain more detailed? Because we have failed to build a good retraining strategy. I don't know how everyone used retraining have found the appropriate strategy for retraining. +) You said that you used XGBoost. I have failed to build a good XGBoost. I wonder how you built it. I used the same features and the same categorical features. But, after switching XGBoost to LGBoost of the public notebooks, it showed a great improvement in CV and LB (2). So, I gave up to use XGBoost. Can you tell me the hyperparmeter or what you used categorical features? Ahmet Erdem Topic Author Posted a year ago · 11th in this Competition arrow_drop_up 1 more_vert I re-train from scratch every 8 days. Methods like partial fit doesn't usually work great. Since I don't have many features and XGB is fast on GPU, training on whole data every 8 days is feasible. XGB and LGB are very similar. Usually they perform almost the same for me. Did you use the same hyperparameters? emoji_people DongYK Posted a year ago arrow_drop_up 0 more_vert Thank you for answering. I used the same categorical features (segment, county, product_type, …, etc.) but I didn't use the exact same hyperparameters. (XGB -> LGB: 82.94 -> 79.34) So, I thought that XGB didn't work. Then, the hyperparameters could cause this difference in the performance. Yuan Xu Posted a year ago · 50th in this Competition arrow_drop_up 1 more_vert You're a great kagger Mahmoud saad Posted a year ago arrow_drop_up 1 more_vert Nice Job 🌟 Mikhail Golubchik Posted a year ago · 47th in this Competition arrow_drop_up 2 more_vert Very impressive. Sinan Calisir Posted a year ago · 37th in this Competition arrow_drop_up 2 more_vert Great solution and best of luck in the next months too! Anil Ozturk Posted a year ago · 157th in this Competition arrow_drop_up 2 more_vert Solid approach! Hope you'll stay there in the gold zone. 😌 Aleix Gimenez Posted a year ago · 59th in this Competition arrow_drop_up 2 more_vert Congrats on your solution! Could you comment on some of your choices? What's the rationale behind baseline = max(target_lag2, target_lag4, target_lag7), and do you know why it works better than picking just one, or taking their average? Naively it seems max should lead to a more noisy target. Also, why using the sample weights log1p(baseline_pred) or sqrt(baseline_pred)? Shouldn't this make your predictions to always overestimate real values? Ahmet Erdem Topic Author Posted a year ago · 11th in this Competition arrow_drop_up 5 more_vert Thank you! When I take the max, my target is usually between 0 and 1. When it makes an error, it doesn't cause such huge outliers this way. If I had only one day, I get more target values above 1.0 like 8 or 9 and mistakes become very costly. Since I divide the target by baseline_pred, I distort the original MAE metric. One could use baseline_pred as sampling weights to get the same metric on this target but experimentally it doesn't work great. log1p works the best  on average. Vitaly Kudelya Posted a year ago · 18th in this Competition arrow_drop_up 2 more_vert Did you use target/installed_capacity as target only for is_consumption = 0 or for all segments? As I remember,  for is_consumption = 1 changing the target didn't improve CV and Public score for me Ahmet Erdem Topic Author Posted a year ago · 11th in this Competition arrow_drop_up 2 more_vert Only for production and it is level 1 stacking model, so my final target is different. Ahmet Erdem Topic Author Posted a year ago · 11th in this Competition arrow_drop_up 0 more_vert Update: Ended up 11th place in Private LB. Happy that the solution was robust. Added some details about the features. silver Posted a year ago · 162nd in this Competition arrow_drop_up 0 more_vert I didn't go with normalized target (like target/installed capacity, target/some historical target) because they democratize target with different magnitude, and so not compatible with the metrics of the competition. Yet they perform so well :( Maksym Dolgikh Posted a year ago · 34th in this Competition arrow_drop_up 0 more_vert Very nice result! I tried to do target / installed capacity time and time again, but the submission always failed. Locally it worked well, but something in hidden data broke it. I tried removing nans, nulls, replacing zeros, but nothing worked. Did you have such trouble and how did you solve it? This is the function I used: def _fixNaN ( self, df_features ):\n\n    features = [ 'installed_capacity' , 'eic_count' ] for feat in features: # df_features = df_features.with_columns(pl.col(feat).replace(0.0, 1.0)) # df_features = df_features.with_columns(pl.col(feat).replace(np.inf, None)) mean_by_group = df_features.groupby( 'segment' ).agg(\n            pl.col(feat).fill_nan( None ).mean().alias( f' {feat} _mean' )\n        )\n\n        df_features = df_features.join(\n        mean_by_group,\n        on=[ 'segment' ],\n        how= 'left' ,\n        )\n\n        df_features = df_features.with_columns(\n            df_features.select(pl.col(feat)\n                               .fill_null(pl.col( f' {feat} _mean' ))\n                               .fill_nan(pl.col( f' {feat} _mean' ))\n                              )\n        )\n\n        df_features.drop_in_place( f' {feat} _mean' )\n\n        df_features = df_features.with_columns(pl.col(feat).fill_nan( None ))\n        df_features = df_features.with_columns(pl.col(feat).fill_null(strategy= \"mean\" )) return df_features content_copy Anil Ozturk Posted a year ago · 157th in this Competition arrow_drop_up 1 more_vert If you used segment as defined in the public kernels, any new segment will likely come with a null-state at its first occurence. So, you might be trying to fill NaNs with the same NaNs. 😅 I've also imputed my capacity NaNs, but not with the segments. I've extracted median values by grouping by on (business, consumption, productiontype). Combinations of all of these three columns exist in the training so it didn't raise any errors. Maksym Dolgikh Posted a year ago · 34th in this Competition arrow_drop_up 2 more_vert I too thought that segment might be null, that's why I added 2 last lines before return :) Perhaps I just messed up polars syntax somewhere… It was my first time using it, and I didn't have time to make enough tests. But I'm still happy with how much I learned while tying to make it work :) Daryna Ronska Posted a year ago · 36th in this Competition arrow_drop_up 0 more_vert Which model did you use as a final estimator for stacking, XGBoost as well or something else? Ahmet Erdem Topic Author Posted a year ago · 11th in this Competition arrow_drop_up 3 more_vert Both level 1 and level 2 models are XGB models on GPU. at7459 Posted a year ago · 54th in this Competition arrow_drop_up 0 more_vert thats some wild strats! btw, all the public kernels are overfitting by using the 24h forecast weather at training due to a syntax error, i dont think it really matters that much but i wonder how many people noticed and how it translates to the private lb Ahmet Erdem Topic Author Posted a year ago · 11th in this Competition arrow_drop_up 1 more_vert I didn't notice such a leak. It wouldn't work well in public too if there was such an issue I think. Can you share the code snippet that causes it? at7459 Posted a year ago · 54th in this Competition arrow_drop_up 1 more_vert df_forecast_weather = ( df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"}) .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45) doesnt actually filter anything because of missing parentheses it makes the cv a bit better than it should be iirc but it doesnt really matter too much at7459 Posted a year ago · 54th in this Competition arrow_drop_up 0 more_vert and yeah i spent a lot of time confused why that model performs so well even though its cv is relatively poor and its overfitting, in the end i hope for me that it's just some random fluke due to leaderboard data being an outlier time frame Ahmet Erdem Topic Author Posted a year ago · 11th in this Competition arrow_drop_up 0 more_vert Since effect of using sooner forecasts is small, this probably didn't cause any significant performance issue. I also couldn't figure out why it has much better CV-LB gap. It may be either a random fluke like you said like predicting a new prediction unit better or it may be that it just works good for summer. Julien Posted a year ago arrow_drop_up 0 more_vert Nice job ! Did you use gas and electricity prices ? I did not have any improvement with historical weather (but I did not ensemble as you did, I just added them as extra features) Ahmet Erdem Topic Author Posted a year ago · 11th in this Competition arrow_drop_up 1 more_vert Thanks. I couldn't get any value out of gas and electricity prices. This comment has been deleted.",
      "Enefit - Predict Energy Behavior of Prosumers | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Eesti Energia · Featured Code Competition · a year ago Late Submission more_horiz Enefit - Predict Energy Behavior of Prosumers Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Enefit - Predict Energy Behavior of Prosumers Overview Data Code Models Discussion Leaderboard Rules Takoi · 13th in this Competition  · Posted a year ago arrow_drop_up 26 more_vert 13th place solution 13th place solution First and foremost, I would like to express my gratitude to the organizers for hosting this competition and extend congratulations to all the winners. Here is a summary of the solution that secured the 13th place. Overview I developed separate models for production and consumption using LightGBM. Instead of using the raw target values, for production, I used the difference of the value two days prior divided by installed capacity. For consumption, I used the difference of the value two days prior divided by the eic count. I retrained the models three times during the inference period. Validation I used a single fold validation for the period from January 2023 to May 2023. When submitting, I retrained on all data with two seeds for both production and consumption models. Target Production: (target - target_lag2) / installed_capacity Consumption: (target - target_lag2) / eic_count Main Features Production: Date related features (e.g., holidays, months, weekdays) eic_count, installed_capacity Lag and differences of target (production) / installed_capacity, including statistical measures Lag and differences of target (production) / eic_count, including statistical measures Lag and differences of target (consumption) / installed_capacity, including statistical measures Lag and differences of target (consumption) / eic_count, including statistical measures Weather forecasts Difference between weather forecast and historical weather. Weather forecasts 2 hours before, 1 hour before, 1 hour after, and the average from 1 hour before to 1 hour after Averages of lagged target (production) / installed_capacity for weekdays and weekends (e.g., the average of the last two weekdays if the prediction day is a weekday, and the last two weekends if it's a weekend) https://www.kaggle.com/competitions/predict-energy-behavior-of-prosumers/discussion/468654 thanks @mohammadpakdaman0 Consumption Date related features (e.g., holidays, months, weekdays) Lag and differences of target (production) / installed_capacity, including statistical measures Lag and differences of target (consumption) / installed_capacity, including statistical measures Lag and differences of target (consumption) / eic_count, including statistical measures Weather forecasts Lag of historical weather Averages of lagged target (consumption) / eic_count for weekdays and weekends (e.g., the average of the last two weekdays if the prediction day is a weekday, and the last two weekends if it's a weekend) For computational efficiency, only the top 300 most important features were used in the production model and top 400 in the consumption model as determined by LightGBM. Re-training Re-training was conducted three times: at the end of January 2024, February 2024, and March 2024. At each of these times, the models for both production and consumption were trained with two seeds each. Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Enefit - Predict Energy Behavior of Prosumers | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Eesti Energia · Featured Code Competition · a year ago Late Submission more_horiz Enefit - Predict Energy Behavior of Prosumers Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Enefit - Predict Energy Behavior of Prosumers Overview Data Code Models Discussion Leaderboard Rules flaty · 26th in this Competition  · Posted a year ago arrow_drop_up 7 more_vert 26th place solution Thanks to kaggle and everyone involved for hosting such an exciting competition. Submission notebook is here . I'm referring to vitalykudelya’s excellent notebook .(Big thanks!!) Overview Weighted ensemble of LightGBM models with different targets (objective = \"regression_l1\"). The models are divided into production and consumption based on is_consumption. Validation Strategy I used a single fold validation for the period from Feb 2023 to May 2023. Target production(weights for ensemble) target - target_48hr(0.15) target - target_mean_5days(0.1) target - target_mean_7days(0.25) target / istalled_capacity(0.5) consumption(weights for ensemble) raw target(0.3) target - target_48hr(0.1) target - target_168hr(0.25) target - target_mean_5days(0.1) target - target_mean_7days(0.25) target_mean_5days = mean(target_48hr ~ target_120hr) target_mean_7days = mean(target_48hr ~ target_168hr) Features Basically, my feature is from this Notebook . I’ve excluded those derived from historical weather. I've added a few more, but there weren't any significant effects. Re-training Re-training was conducted only once: at the end of January 2024. Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Enefit - Predict Energy Behavior of Prosumers | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Eesti Energia · Featured Code Competition · a year ago Late Submission more_horiz Enefit - Predict Energy Behavior of Prosumers Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Enefit - Predict Energy Behavior of Prosumers Overview Data Code Models Discussion Leaderboard Rules Daryna Ronska · 36th in this Competition  · Posted a year ago arrow_drop_up 15 more_vert Public 30th Place Solution Notebook with the solution https://www.kaggle.com/code/darynarr/enefit-ensemble-target-diff/notebook Feature Engineering Client data 2 days ago. Forecast weather data for the current date and 7 days ago, by date and location and by date only. Historical weather 2 and 7 days ago, by date and location and by date only. Holidays. Datetime features. Target 2-21 days ago for both consumption and production, so for each row there are both target_consumption_{lag} and target_production_{lag}, and some statistics like mean, min, max, std, mean / std and ratios. is_consumption, is_business, product_type etc. CV Strategy Mainly used 2021-2022 for training and 2023 for validation, but as we chose to re-train the model we also validated on custom time series k-fold with n-days folds and gap between train and validation. But for feature selection and initial hypothesis testing validation on 2023 was used as it is just faster. Feature Selection After feature engineering we ended up with almost 300 features which did not contribute greatly to our model training time, and we decided that some feature selection is needed. We came up with something that I would call simplified forward feature selection. Adding features one by one to the model, if feature improved score on CV - add it, otherwise continue. Repeat until MAE stopes falling down. This left us with 83 features for consumption and 72 features for production, improving both training time and scores. This approach removed electricity and gas prices from our features, that is why there are no those features in Feature Engineering, but I still wonder why those prices worsen the results for us. Public LB 61.08 Model Ensemble of models: 1) by is_consumption; 2) by is_consumption and is_business; 3) by is_consumption and product_type; 4) 2-day diffs by is_consumption; Equal weights, single LightGBM for each model of the ensemble. Tweedie objective was a discovery. Re-training every 4 days. Do not re-train when currently scored==False to save the time, only add new data to training set . Training data is clipped to last 14 months to avoid training time growth with the time and it did not affect score for now, but will see. Public LB 61.49 Model Ensemble of models by is_consumption, LightGBM and CatBoost, equal weights. Re-training every 2 days. Saving the last 4 ensemble models and averaging them, so the most recent model has the highest weight and the least recent has the lowest weight. Also training on last 14 months only when currently scored==True. Fails I want to see those solutions with target / installed_capacity as a target, because in our case it improved the results on CV, but LB was a disaster. Suspecting outliers or change in pattern in test set, but not sure what is wrong. Credits Great Enefit Generic Notebook by @greysky was out starter notebook and modified version of it ended up to be our final submission My teammate @eddiekro 🙂 Please sign in to reply to this topic. comment 8 Comments Hotness Daryna Ronska Topic Author Posted a year ago · 36th in this Competition arrow_drop_up 1 more_vert Published notebook with the solution if anyone is interested https://www.kaggle.com/code/darynarr/enefit-ensemble-target-diff/notebook emoji_people DongYK Posted a year ago arrow_drop_up 1 more_vert This approach removed electricity and gas prices from our features, that is why there are no those features in Feature Engineering, but I still wonder why those prices worsen the results for us. I found that electricity and gas prices could explains the occasional spike of production and consumption. But, because of the lagged data, it wasn't helpful. Re-training every 2 days. Can you explain this strategy in detail because we have failed to retrain. Have you used refit or train(init_model) or just re-train? I want to see those solutions with target / installed_capacity as a target, because in our case it improved the results on CV, but LB was a disaster. Suspecting outliers or change in pattern in test set, but not sure what is wrong. We used `normalized_target = target / installed_capacity in production. In production, It shows great consistency between target and installe_capacity. And, It actually gave a good gain in CV and LB (1). But, In consumption, It shows improvement in CV but not in LB. As you said, it harmed LB greatly. After analysis, it didn't show enough consistency between target and capacity. So, we  couldn't use it for normalization. Daryna Ronska Topic Author Posted a year ago · 36th in this Competition arrow_drop_up 1 more_vert I found that electricity and gas prices could explains the occasional spike of production and consumption. But, because of the lagged data, it wasn't helpful. Yes, seems to be that lags are making those features useless. Can you explain this strategy in detail because we have failed to retrain. Have you used refit or train(init_model) or just re-train? We tried continuous training with initial model to spend less time on training, but re-training from scratch worked best for us. We update X with every new test set and y with new revealed targets on each iteration: def update_X_y ( X, y, X_revealed, y_revealed ):\n    X = pd.concat([X, X_revealed])\n    y = pd.concat([y, y_revealed])\n\n    X = X[~X.index.duplicated(keep= 'first' )]\n    y = y[~y.index.duplicated(keep= 'first' )] return X, y\nX, y = update_X_y(X, y, X_test, revealed_targets.set_index( 'row_id' )[ 'target' ].dropna()) content_copy and on train dropping those row_id s for which we do not have y yet (doing something like inner merge) def train_model ( X, y ):\n    idx_revealed = np.intersect1d(X.index, y.index).tolist()\n\n    X_train = to_categorical(X.loc[idx_revealed])\n    y_train = y.loc[idx_revealed]\n\n    timestamps = pd.to_datetime(X_train[[ 'year' , 'month' , 'day' , 'hour' ]])\n\n    mask = timestamps > (timestamps. max () - pd.DateOffset(months= 14 ))\n    X_train, y_train = X_train[mask], y_train[mask]\n\n    model = ModelCombined()\n    model.fit(X_train, y_train) return model\nmodel = train_model(X, y) content_copy Re-training was giving us about 2.5-4.5 boost in MAE on LB, I checked that occasionally. Which is huuuuge! But it is mainly because our model has this thick flaw of (almost) not being able to look outside the boundaries. I think, with target / installed_capacity for production model, re-training is not going to give such a boost because the model is initially better. We used `normalized_target = target / installed_capacity in production. In production, It shows great consistency between target and installe_capacity. And, It actually gave a good gain in CV and LB (1). But, In consumption, It shows improvement in CV but not in LB. As you said, it harmed LB greatly. After analysis, it didn't show enough consistency between target and capacity. So, we couldn't use it for normalization. Your explanations are reasonable and I think I was not persistent enough on that approach for production. I was trying to do normalisation only on production but also failed with LB and I gave up quickly. Surely, there was a mistake on our side somewhere emoji_people DongYK Posted a year ago arrow_drop_up 1 more_vert Thank you for answering. Almost everyone seems to choose a strategy for retraining from scratch. I was confused with the worse LB score for retraining. Looking at the many solutions, I realized that the models using absolute target + retraining give much gains in CV and LB. I speculate that the reason why retraining my models was not that good on LB is all my models used relative target. Winter is the season breaking the new highest consumption, it seems re-training will matter. I'm not that confident but I'll just leave it up to luck. 😁 Daryna Ronska Topic Author Posted a year ago · 36th in this Competition arrow_drop_up 1 more_vert At least you do not have to worry about getting any kind of errors associated with retraining) I am impressed that so many top solutions do not include frequent re-training and are getting high scores. Diversity of solutions is great in this competition AbaoJiang Posted a year ago · 39th in this Competition arrow_drop_up 1 more_vert Hi @darynarr , For your failed part, did you try to submit each target type separately and see the contribution? As our experiments show, production and non-business consumption perform well with target / installed_capacity on both CV and LB. And the culprit causing LB score drop is business consumption. Aleix Gimenez Posted a year ago · 59th in this Competition arrow_drop_up 1 more_vert I agree 100%. target / installed_capacity works very well for production, but doesn't help for consumption. I also observed that target / eic_count seems good for domestic consumption, but I didn't use it in the end. For me, predicting log(target / target_2d) for consumption worked well accross business/domestic cases. Daryna Ronska Topic Author Posted a year ago · 36th in this Competition arrow_drop_up 1 more_vert I tried using target for consumption and target / installed_capacity for production, but it did not lead to better results. For sure was doing something wrong. But honestly, even though it was logical assumption to use normalisation only for  production targets, I was still a little bit afraid to do so. For consumption CV was good, but LB was not, and I thought what if same happens on private LB but on production targets. Not likely, though, but still. So I gave up on that approach quickly, but seeing others solutions maybe I should have not) Coffeethefifth Posted a year ago · 151st in this Competition arrow_drop_up 1 more_vert Thx for sharing! I have a question about feature selection. When you adding a new feature, will you do some hyperparameter tuning job  before you refit the model or just refit the model with the same hyperparameters? Daryna Ronska Topic Author Posted a year ago · 36th in this Competition arrow_drop_up 0 more_vert No tuning between each feature added, even without that it took hours on GPU and I am not patient enough for that) I was trying to tune the model one more time after features being selected, but it was not making the results better, so ended up with parameters from the initial optuna cycle on full set of features. Coffeethefifth Posted a year ago · 151st in this Competition arrow_drop_up 1 more_vert Appreciate!",
      "Enefit - Predict Energy Behavior of Prosumers | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Eesti Energia · Featured Code Competition · a year ago Late Submission more_horiz Enefit - Predict Energy Behavior of Prosumers Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Enefit - Predict Energy Behavior of Prosumers Overview Data Code Models Discussion Leaderboard Rules Sinan Calisir · 37th in this Competition  · Posted a year ago arrow_drop_up 15 more_vert 37th Place Solution - Feature Engineering + Ensembling First, we would like to thank the organizers and Kaggle for hosting such an interesting challenge. Here is a brief overview of our solution with @sercanyesiloz that stayed robust over the 3 months. We finished the public part around 150th place and jumped to 45th and 30th in the first two updates respectively. And 37th place became our final position with the last update. We started this competition a little late so we used @vitalykudelya 's notebook as a starting point. We used the last 3 months of training data as our CV setup. For the submission, we used the whole data available. Here is the overview of the new features that helped the most both in CV and public LB: [ \"week\" , \"quarter\" ]\nis_morning=pl.col( \"hour\" ).is_in([ 6 , 7 , 8 , 9 , 10 , 11 ]),\nis_midday=pl.col( \"hour\" ).is_in([ 12 , 13 , 14 , 15 ]),\nis_afternoon=pl.col( \"hour\" ).is_in([ 16 , 17 , 18 , 19 ]),\nis_evening=pl.col( \"hour\" ).is_in([ 20 , 21 , 22 , 23 ]),\nis_night=pl.col( \"hour\" ).is_in([ 0 , 1 , 2 , 3 , 4 , 5 ]),\n\npl.when(pl.col( \"datetime\" ).dt.month_start() == pl.col( \"datetime\" )).then( 1 ).otherwise( 0 ).alias( \"is_month_start\" ),\npl.when(pl.col( \"datetime\" ).dt.month_end() == pl.col( \"datetime\" )).then( 1 ).otherwise( 0 ).alias( \"is_month_end\" ),\n\n[ \"sin(month)\" , \"cos(month)\" ]\n\ndf[ \"wind_magnitude\" ] = np.sqrt((df[ \"10_metre_u_wind_component\" ] ** 2 ) + (df[ \"10_metre_v_wind_component\" ] ** 2 ))\n\ndf[ \"shortwave_radiation/surface_pressure\" ] = df[ \"shortwave_radiation\" ] / df[ \"surface_pressure\" ]\n\ndf[ \"production_target\" ] = df[ \"installed_capacity\" ] * df[ \"surface_solar_radiation_downwards\" ] / (df[ \"temperature\" ] + 273.15 ) content_copy wind_magnitude feature is derived from this blog post: http://colaweb.gmu.edu/dev/clim301/lectures/wind/wind-uv The second important aspect of our solution was the ensembling. We combined the LGBM model with a neural net. Even though the score of the neural net was lower than the LGBM itself (3-4% on public LB), it gave us a significant boost in the final score. Here the second one is the ensemble with a neural net: The first one is the LGBM model with refitting every 31 days. We were not 100% sure about our refit strategy, so we kept the ensemble submission with no refitting (it was always stronger). Given our experiments gave a 1% boost overall with a refit, It would probably end up in the top 25 including that. We used target and target_diff=target-target_48h as our prediction objective. target / installed capacity objective did not work in our setup as some other solutions claim to be effective. Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Enefit - Predict Energy Behavior of Prosumers | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Eesti Energia · Featured Code Competition · a year ago Late Submission more_horiz Enefit - Predict Energy Behavior of Prosumers Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Enefit - Predict Energy Behavior of Prosumers Overview Data Code Models Discussion Leaderboard Rules AbaoJiang · 39th in this Competition  · Posted a year ago arrow_drop_up 14 more_vert Public 71st Solution Writeup (Private 39th) Public 71st Solution Writeup Firstly, thanks Kaggle and competition hosts for hosting this exciting competition. It's the exact first time I try to work as a team player, an amazing experience! Hence, thanks my teammates, @patrick0302 and @chunweishen for all the supports, discussion and hardworking. Also, thanks all kagglers for sharing your valuable insights, special thanks to @vitalykudelya for providing high-quality notebook, which our final solutions are highly based on. 1. Overview Split data into 3 target types for modeling, including prod , cons_c and cons_b . prod : is_consumption == 0 cons_c : is_consumption == 1 and is_business == 0 cons_b : is_consumption == 1 and is_business == 1 Transform target as shared in the public notebooks and add target / eic_count . Combined with target type splitting, we train different models with model_type summarized in the following table, (opens in a new tab)\"> \"raw\" stands for target , \"diff\" for target / target_lag2d , \"dcap\" for target / installed_capacity and \"deic\" for target / eic_count Train both xgb and lgbm versions of each model_type . Select custom feature set for each model, with the number of features in the range of 27 ~ 55. Use 3-fold time series CV with validation interval 202209 ~ 202211 , 202212 ~ 202302 , and 202303 ~ 202305 . Ensemble models from the model pool and retrain them in final evaluation phase (elaborated later). 2. Base Model Training As mentioned in overview, we train each model with custom selected features. Also, we train each model with different samples. Data Sampling We sample the data for model training with the following logics, p_x models use all production data, where x can be any target transformation in the table above. c_raw and c_diff use all consumption data and models are shared for cons_c and cons_b . cc_dcap and cc_deic use only cons_c data. cb_dcap and cb_deic use all consumption data. Because we observe cons_b data is a drag on cons_c performance when using the dcap and deic targets, we train cc_dcap and cc_deic with data only from the same target type. Feature Engineering and Selection Most of the features we use are from public notebooks. We fail to create other features which can boost CV scores except for the following one, target_lag?d_xpc : The plain lagged target features crossing production and  consumption . That is, we consider production/consumption lagged targets as the features for consumption/production models. We believe there exist more powerful features, waiting for more to be shared! Then, we do some experiments to verify that most of the features on public notebooks can be dropped directly. After all, less is more sometime. Hence, we use a 2-stage method to select feature set for each model. Set-wise forward selection We add features set-by-set ( e.g., local mean of the forecast weather) and keep them if the CV score boosts for all the 3 folds. For most models, we end up with a features set with 70 ~ 80 features. Element-wise backward elimination Then, we eliminate features element-by-element ( e.g., target_lag4d , target_lag5d ) simply based on feature importances. Again, features are dropped if removing these features doesn't worsen 3-fold CV scores (fold-by-fold, not average). Sometimes, we even observe CV boost in this stage. Finally, we get selected feature sets with 27 ( p_diff_lgb ) ~ 55 features. Model Training and Hyperparameters We just manually tune hyperparameters (mainly n_estimators without early stopping) in the very beginning, and fix them. About two weeks before the submission deadline, we revisit it with the final selected feature sets but can't observe CV/LB sync on hyperparameter tuning. Hence, we rollback to the fixed one at start. 3. Retraining and Ensemble Strategies Model Retraining To compensate for the 8-gap data and align CV setup with the final evaluation scenario, we retrain each model using one of the following methods, Retrain at start : Retrain models only once after the first currently_scored has been triggered. That is, the gap 8-month data is completely collected. Monthly retrain : Retrain models on the first day of each month. (opens in a new tab)\"> To determine which retraining strategy to choose, we compare CV scores of two CV setups, 3-fold (3 months for each) mimicking retrain at start and 9-fold (1 month for each) mimicking monthly retrain . Then we pick the one performs better for each single model. Model Ensemble We ensemble models based on the following logics, For ?_raw and ?_diff (where ? can be p , cc or cb ), we average them with the equal weight ( e.g., p_raw_xgb * 0.5 + p_diff_xgb * 0.5 , which is named as p_ens_xgb ). For each model_type , we average xgb and lgbm with the equal weight ( e.g., p_dcap_xgb * 0.5 + p_dcap_lgb * 0.5 , which is named as p_dcap_xl ). Each model is trained with 3 random seeds. Then, the prediction strategy is described as follows, For prod , dcap is selected as a base prediction. If installed_capacity is missing, we use raw * 0.5 + diff * 0.5 as prediction. If target_lag2d is again missing, we use only raw . For cc , d_cap * 0.5 + d_eic * 0.5 is used as base for segments seen in the training set. Otherwise, the same prediction hierarchy used in prod is applied. For cb , dcap or deic have decent CV boosts but fail to sync with LB. Hence, one submission uses raw * 0.5 + diff * 0.5 , and the other one takes deic into consideration (raw + diff + deic) / 3 . 4. Some More Studies Cascading or Rolling Training Set We observe that some models have slightly better performance when trained with the latest 1-year data, instead of all available data till 202109 . My question is which training set would you choose if both have similar performance? Retrain at Start or Monthly Retrain As described above, we choose retraining strategies by comparing performance of 2 CV setups, 3-fold 3-month and 9-fold 1-month. An example of the consumption raw model c_raw is illustrated as follows, (opens in a new tab)\"> In this case, monthly retrain performs better than retrain at start . As the month gap increases (from 0 to 2), the performance boost of monthly retrain becomes greater. Unseen Segment Test Considering there'll be new units in private LB, we test which target form is more suitable for unseen segments for each target type. As for implementation, we mask one unit per county in training set after data splitting is done. That is, we can validate on segments not appearing in the training set. For prod , we choose base prediction dcap . For cc , raw * 0.5 + diff * 0.5 performs better than base dcap * 0.5 + deic * 0.5 . And, we use base raw * 0.5 + diff * 0.5 for unseen cb segments. 5. What Remains We spend about 3 days to build a single NN model achieving LB 74 but have no time to work on this. It's based on WaveNet and co-train 24-hour production and consumption together, which can be seen as a multi-horizon forecasting problem. We try to select best models for prediction on the fly based on the historical error patterns, but observe only little improvement compared with simple blending. 6. What Didn't Work for Us Generate more powerful features. Tune hyperparameters. Increase the number of random seeds more than 3 for each model. Use dcap or deic for cb (CV boosts a lot, but LB is unstable). Please sign in to reply to this topic. comment 4 Comments Hotness silver Posted a year ago · 162nd in this Competition arrow_drop_up 1 more_vert \"We observe that some models have slightly better performance when trained with the latest 1-year data, instead of all available data till 202109.\" Observed same thing in cross validation, but the strategy yields worse results in public leaderboard for me. I capped my training data to 2-year in the final submission AbaoJiang Topic Author Posted a year ago · 39th in this Competition arrow_drop_up 0 more_vert Hi @yinanzhu , Thanks for your comment! Yeah, our public LB score worsens when using the latest 1-year data to train those model_type with CV boosts. Considering the estimated notebook runtime is much less than the limitation (we hope so), we finally choose to use all of the available training data for all models. emoji_people DongYK Posted a year ago arrow_drop_up 1 more_vert Hi, @abaojiang . Great job. We experimented target / eic_count , but it greatly harmed LB score. So, we only used models with target diff. How long do you think your retraining notebook take? I don't want to take much risk. So, I decided to retrain only once. But, I realized it could be a mistake. :) I don't know. AbaoJiang Topic Author Posted a year ago · 39th in this Competition arrow_drop_up 1 more_vert Hi @dongyk , Thanks for your appreciation! We experimented target / eic_count , but it greatly harmed LB score. Our production actually has worse CV scores with target / eic_count , so we never try to submit. As for consumption side, target / eic_count always performs better than target / installed_capacity . Then, a simple ensemble of these to provide another CV boost of around 0.2 for cons_c ( is_consumption == 1 and is_business == 0 ) and 1.5 for cons_b ( is_consumption == 1 and is_business == 1 ). How long do you think your retraining notebook take? As we test how much additional time cost it'll take to train with more data, one solution with 132 models to train takes around 8hr and the other with 114 takes 8.5hr (approximately). But, we use a more strict estimation to make it more tolerable. Hope you can survive on private LB!!",
      "Enefit - Predict Energy Behavior of Prosumers | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Eesti Energia · Featured Code Competition · a year ago Late Submission more_horiz Enefit - Predict Energy Behavior of Prosumers Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Enefit - Predict Energy Behavior of Prosumers Overview Data Code Models Discussion Leaderboard Rules Renato Reggiani · 42nd in this Competition  · Posted a year ago arrow_drop_up 2 more_vert 42th place solution 42th place solution I am grateful to Enefit and Kaggle for the opportunity to participate in this competition. It was a valuable learning experience and I am proud to have earned my first medal on a Kaggle competition. You can find my final submission here Feature Selection While my feature selection may not stand out compared to other teams, I made the decision to exclude some weather-related features that were not closely related to any county. Target Variables Original target Difference between target and its second shift Training Data Duration Most recent 1 year Most recent 2 years Model Building I built separate LightGBM models for both production and consumption, each with two different target variables and two different training data durations, resulting in a total of 8 models. Model Re-training All models underwent re-training twice: once after 20 days and again after 66 days. For models trained with 1 year of data, an additional re-training was conducted after 45 days. Please sign in to reply to this topic. comment 0 Comments Hotness"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Your challenge in this competition is to predict the amount of electricity produced and consumed by Estonian energy customers who have installed solar panels. You'll have access to weather data, the relevant energy prices, and records of the installed photovoltaic capacity. This is a forecasting competition using the time series API. The private leaderboard will be determined using real data gathered after the submission period closes. 💡 Nota bene: All datasets follow the same time convention. Time is given in EET/EEST. Most of the variables are a sum or an average over a period of 1 hour. The datetime column (whatever its name) always gives the start of the 1-hour period. However, for the weather datasets, some variables such as temperature or cloud cover, are given for a specific time, which is always the end of the 1-hour period. train.csv gas_prices.csv client.csv electricity_prices.csv forecast_weather.csv Weather forecasts that would have been available at prediction time. Sourced from the European Centre for Medium-Range Weather Forecasts . historical_weather.csv Historic weather data . public_timeseries_testing_util.py An optional file intended to make it easier to run custom offline API tests. See the script's docstring for details. You will need to edit this file before using it. example_test_files/ Data intended to illustrate how the API functions. Includes the same files and columns delivered by the API. The first three data_block_ids are repeats of the last three data_block_ids in the train set. example_test_files/sample_submission.csv A valid sample submission, delivered by the API. See this notebook for a very simple example of how to use the sample submission. example_test_files/revealed_targets.csv The actual target values from the day before the forecast time. This amounts to two days of lag relative to the prediction times in the test.csv . enefit/ Files that enable the API. Expect the API to deliver all rows in under 15 minutes and to reserve less than 0.5 GB of memory. The copy of the API that you can download serves the data from example_test_files/ . You must make predictions for those dates in order to advance the API but those predictions are not scored.  Expect to see roughly three months of data delivered initially and up to ten months of data by the end of the forecasting period. 19 files 1.07 GB csv, py, so + 1 other CC BY-NC-SA 4.0 Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 1.07 GB enefit example_test_files client.csv county_id_to_name_map.json electricity_prices.csv forecast_weather.csv gas_prices.csv historical_weather.csv public_timeseries_testing_util.py train.csv weather_station_to_county_mapping.csv 19 files 138 columns ",
    "data_description": "Enefit - Predict Energy Behavior of Prosumers | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Eesti Energia · Featured Code Competition · a year ago Late Submission more_horiz Enefit - Predict Energy Behavior of Prosumers Predict Prosumer Energy Patterns and Minimize Imbalance Costs. Enefit - Predict Energy Behavior of Prosumers Overview Data Code Models Discussion Leaderboard Rules Overview The goal of the competition is to create an energy prediction model of prosumers to reduce energy imbalance costs. This competition aims to tackle the issue of energy imbalance, a situation where the energy expected to be used doesn't line up with the actual energy used or produced. Prosumers, who both consume and generate energy, contribute a large part of the energy imbalance. Despite being only a small part of all consumers, their unpredictable energy use causes logistical and financial problems for the energy companies. Start Nov 2, 2023 Close May 1, 2024 Merger & Entry Description link keyboard_arrow_up The number of prosumers is rapidly increasing, and solving the problems of energy imbalance and their rising costs is vital. If left unaddressed, this could lead to increased operational costs, potential grid instability, and inefficient use of energy resources. If this problem were effectively solved, it would significantly reduce the imbalance costs, improve the reliability of the grid, and make the integration of prosumers into the energy system more efficient and sustainable. Moreover, it could potentially incentivize more consumers to become prosumers, knowing that their energy behavior can be adequately managed, thus promoting renewable energy production and use. About us Enefit is one of the biggest energy companies in Baltic region. As experts in the field of energy, we help customers plan their green journey in a personal and flexible manner as well as implement it by using environmentally friendly energy solutions. At present, Enefit is attempting to solve the imbalance problem by developing internal predictive models and relying on third-party forecasts. However, these methods have proven to be insufficient due to their low accuracy in forecasting the energy behavior of prosumers. The shortcomings of these current methods lie in their inability to accurately account for the wide range of variables that influence prosumer behavior, leading to high imbalance costs. By opening up the challenge to the world's best data scientists through the Kaggle platform, Enefit aims to leverage a broader pool of expertise and novel approaches to improve the accuracy of these predictions and consequently reduce the imbalance and associated costs. Evaluation link keyboard_arrow_up Submissions are evaluated on the Mean Absolute Error (MAE) between the predicted return and the observed target. The formula is given by: M A E = 1 n ∑ i = 1 n | y i − x i | Where: n is the total number of data points. y i is the predicted value for data point i. x i is the observed value for data point i. Submitting You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow the template in this notebook . Timeline link keyboard_arrow_up This is a future data prediction competition with an active training phase and a second period where selected submissions will be evaluated against future ground truth data. Training Timeline November 1, 2023 - Start Date. January 24, 2024 - Entry Deadline. You must accept the competition rules before this date in order to compete. January 24, 2024 - Team Merger Deadline. This is the last day participants may join or merge teams. January 31, 2024 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prediction Timeline: Starting after the final submission deadline there will be periodic updates to the leaderboard to reflect future data updates that will be evaluated against selected submissions. We anticipate 1-3 updates before the final evaluation. April 30, 2024 - Competition End Date Prizes link keyboard_arrow_up 1st Place - $ 15,000 2nd Place - $ 10,000 3rd Place - $ 8,000 4th Place - $ 7,000 5th Place - $ 5,000 6th Place - $ 5,000 Code Requirements link keyboard_arrow_up Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time Internet access disabled Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv and be generated by the API. Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors. Citation link keyboard_arrow_up Kristjan Eljand, Martin Laid, Jean-Baptiste Scellier, Sohier Dane, Maggie Demkin, and Addison Howard. Enefit - Predict Energy Behavior of Prosumers. https://kaggle.com/competitions/predict-energy-behavior-of-prosumers, 2023. Kaggle. Cite Competition Host Eesti Energia Prizes & Awards $50,000 Awards Points & Medals Participation 13,643 Entrants 2,715 Participants 2,731 Teams 3,955 Submissions Tags Tabular Energy Time Series Analysis Mean Absolute Error Table of Contents collapse_all Overview Description Evaluation Timeline Prizes Code Requirements Citation"
  },
  {
    "competition_slug": "optiver-trading-at-the-close",
    "discussion_links": [
      "/competitions/optiver-trading-at-the-close/discussion/487446",
      "/competitions/optiver-trading-at-the-close/discussion/486868",
      "/competitions/optiver-trading-at-the-close/discussion/462653",
      "/competitions/optiver-trading-at-the-close/discussion/486086"
    ],
    "discussion_texts": [
      "Optiver - Trading at the Close | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Optiver · Featured Code Competition · a year ago Late Submission more_horiz Optiver - Trading at the Close Predict US stocks closing movements Optiver - Trading at the Close Overview Data Code Models Discussion Leaderboard Rules hyd · 1st in this Competition  · Posted a year ago arrow_drop_up 305 more_vert 1st place solution Thanks to Optiver and Kaggle for hosting this great financial competition. And thanks to the great notebooks and discussions, I learned a lot. I am so happy to win my second solo win! 😃😀😀 Overview My final model(CV/Private LB of 5.8117/5.4030) was a combination of CatBoost (5.8240/5.4165), GRU (5.8481/5.4259), and Transformer (5.8619/5.4296), with respective weights of 0.5, 0.3, 0.2 searched from validation set. And these models share same 300 features. Besides, online learning(OL) and post-processing(PP) also play an important role in my final submission. model name validation set w/o PP validation set w/ PP test set w/o OL w/ PP test set w/ OL one time w/ PP test set w/ OL five times w/ PP CatBoost 5.8287 5.8240 5.4523 5.4291 5.4165 GRU 5.8519 5.8481 5.4690 5.4368 5.4259 Transformer 5.8614 5.8619 5.4678 5.4493 5.4296 GRU + Transformer 5.8233 5.8220 5.4550 5.4252 5.4109 CatBoost + GRU + Transformer 5.8142 5.8117 5.4438 5.4157 5.4030*(overtime) Validation Strategy My validation strategy is pretty simple, train on first 400 days and choose last 81 days as my holdout validation set. The CV score aligns with leaderboard score very well which makes me believe that this competition wouldn't shake too much. So I just focus on improving CV in most of time. Magic Features My models have 300 features in the end. Most of these are commonly used, such like raw price, mid price, imbalance features, rolling features and historical target features. I will introduce some features really helpful and other teams didn't share yet. 1 agg features based on seconds_in_bucket_group pl .when (pl .col ( 'seconds_in_bucket' ) < 300 ) .then ( 0 ) .when (pl .col ( 'seconds_in_bucket' ) < 480 ) .then ( 1 ) .otherwise ( 2 ) .cast (pl.Float32) .alias ( 'seconds_in_bucket_group' ), content_copy *[(pl.col(col).first() / pl.col(col)). over ([ 'date_id' , 'seconds_in_bucket_group' , 'stock_id' ]).cast(pl.Float32). alias ( '{}_group_first_ratio' .format(col)) for col in base_features],\n *[(pl.col(col).rolling_mean( 100 , min_periods= 1 ) / pl.col(col)). over ([ 'date_id' , 'seconds_in_bucket_group' , 'stock_id' ]).cast(pl.Float32). alias ( '{}_group_expanding_mean{}' .format(col, 100 )) for col in base_features] content_copy 2 rank features grouped by seconds_in_bucket *[(pl.col(col).mean() / pl.col(col)).over([ 'date_id' , 'seconds_in_bucket' ]).cast(pl.Float32).alias( '{}_seconds_in_bucket_group_mean_ratio' .format(col)) for col in base_features],\n *[(pl.col(col).rank(descending= True , method =' ordinal ') / pl . col (col) . count () ). over ([ 'date_id' , 'seconds_in_bucket' ]) . cast (pl.Float32) . alias ( '{}_seconds_in_bucket_group_rank' .format(col) ) for col in base_features ], content_copy Feature Selection Feature selection is important because we have to avoid memory error issue and run as many rounds of online training as possible. I just choose top 300 features by CatBoost model's feature importance. Model Nothing to say about CatBoost as usual, just simply train and predict. GRU input tensor's shape is (batch_size, 55 time steps,  dense_feature_dim), followed by 4 layers GRU, output tensor's shape is (batch_size, 55 time steps). Transformer input tensor's shape is (batch_size, 200 stocks,  dense_feature_dim), followed by 4 layers transformer encoder layers, output tensor's shape is (batch_size, 200 stocks). A small trick that turns output into zero mean is helpful. out = out - out .mean( 1 , keepdim= True ) content_copy 4 sample weight Online Learning Strategy I retrain my model every 12 days, 5 times in total. I think most teams can only use up to 200 features when training GBDT if online training strategy is adopted. Because it requires double memory consumption when concat historical data with online data. The data loading trick can greatly increase this. For achieving this,  you should save training data one file per day and also loading day by day. data loading trick def load_numpy_data (meta_data, features):\n    res = np. empty (( len (meta_data), len (features)), dtype=np.float32)\n    all_date_id = sorted (meta_data[ 'date_id' ]. unique ())\n    data_index = 0 for date_id in tqdm (all_date_id):\n        tmp = h5py. File ( '/path/to/{}.h5' . format (date_id), 'r' )\n        tmp = np. array (tmp[ 'data' ][ 'features' ], dtype=np.float32)\n        res[data_index:data_index+ len (tmp),:] = tmp\n        data_index += len (tmp)\n    return res content_copy Actually, my best submission is overtime at last update. I just skip online training if total inference time meets certain value. So there are 4 online training updates in total. I estimate that the best score would be around 5.400 if not overtime. Anyway, I am really lucky! Post Processing Subtract weighted-mean is better than average-mean since metric already told. test_df[ 'stock_weights' ] = test_df[ 'stock_id' ].map(stock_weights)\ntest_df[ 'target' ] = test_df[ 'target' ] - (test_df[ 'target' ] * test_df[ 'stock_weights' ]).sum() / test_df[ 'stock_weights' ].sum() content_copy What not worked for me ensemble with 1dCNN or MLP. multi-days input instead of singe day input when applying GRU models larger transformer, e.g. deberta predict target bucket mean by GBDT Thank you all! 121 38 32 26 Please sign in to reply to this topic. comment 70 Comments 5 appreciation  comments Hotness Ayman Allawi Posted a year ago arrow_drop_up 14 more_vert Excuse me for another question, Every 12 day you re-train the Catboost from scratch and fine-tune the two NNs am I right? hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 5 more_vert Right. ~~~~ WindClimber Posted a year ago · 23rd in this Competition arrow_drop_up 8 more_vert Thank you for sharing your great ideas! If I understand correctly, the GRU model is designed to capture time-series dynamics, while the Transformer is for the cross-sectional dynamics, and thus the data feeding process is different for these two. Please feel free to correct me if my interpretation is off. Thanks once again! hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 3 more_vert You are right! Natarajan Vijaikumar Posted a year ago arrow_drop_up 5 more_vert Awesome sharing @hydantess Igor Volianiuk Posted a year ago arrow_drop_up 3 more_vert Thank you for your sharing and gratz for the grade! Awesome work @hydantess ! lele Posted a year ago · 45th in this Competition arrow_drop_up 3 more_vert Thanks for your sharing and congrats again for the grade!  I am a little bit confused about the GRU and Transformer model input and output dimension in your model. I would appreciate a lot if you could elaborate more on below points. GRU input tensor's shape is (batch_size, 55 time steps, dense_feature_dim), followed by 4 layers GRU, output tensor's shape is (batch_size, 55 time steps). Transformer input tensor's shape is (batch_size, 200 stocks, dense_feature_dim), followed by 4 layers transformer encoder layers, output tensor's shape is (batch_size, 200 stocks). > In GRU, I am a little bit confused why the output shape is (batch size*55 timestamp)?  I think usually we just use the last timestamp(x = x[:, -1, :])  so the dimension will be (batch size * d) and then it will become (batch size * 1) with some fully connected layer? In transformer, if the input tensor's shape is  (batch_size, 200 stocks, dense_feature_dim), will the label still be the target for each stock at each timestamp or you predict the targets for 200 stocks all together? giampaolo1980 Posted a year ago · 106th in this Competition arrow_drop_up 1 more_vert Yes I tried to ask the same question. Apparently some seq2seq modelling was used but I am personally still a bit confused (how many targets were used for the seq2seq?) hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 3 more_vert You're right. Will take the last timestamp when inference. Yes. Leo Posted a year ago arrow_drop_up 3 more_vert Congratulations! @hydantess What are these operators: over , when , then , otherwise , over , cast , alias ? Are they from a third-party package or functions that you have designed yourself? I actually have some trouble understanding the function over . It looks like groupby in pandas . Can you show your magic features in LaTex? hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 4 more_vert This is polars. polars over == pandas groupby().transform() MJeremy Posted a year ago · 57th in this Competition arrow_drop_up 2 more_vert does this help a lot in preventing from overtime in submission? wen6Lev57q4 Posted a year ago arrow_drop_up 1 more_vert Congratulations！@hydantess I have two questions: How can we make the model engage in online learning during the prediction phase when there is no target? Why doesn't the transformer output directly, like GRU does (batchx55), but instead predicts the target of 200 stocks at that moment directly? hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 3 more_vert We can receive historical target info in inference phase, you can refer to this demo . I want transformer to learn info across different stocks, GRU to learn sequence info. You can combine these two modules in one model of course. zephyrus1 Posted a year ago arrow_drop_up 1 more_vert Thank you for your sharing it's amazing to see unique techniques. @hydantess WayneWang-8624 Posted a year ago · 54th in this Competition arrow_drop_up 1 more_vert Congrats and thank you for sharing this!! Ziyi Dong Posted a year ago arrow_drop_up 1 more_vert I am a novice who has just started with Kaggle, and I noticed that many competitions necessitate the use of an Online Learning Strategy. Could you explain the principles of this strategy or how to learn and apply it? hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 2 more_vert It means retrain models with more test data in prediction phase. Yousif Hajajj Posted a year ago arrow_drop_up 1 more_vert Congratulations! giampaolo1980 Posted a year ago · 106th in this Competition arrow_drop_up 1 more_vert Thanks a lot for sharing and congrats for the victory . Any chance you can share the winning code? Also why is the output of the GRU of size batch x time steps? Did you do seq2seq modelling? I thought the output would be batch size x 1( predict target for each data point? hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 1 more_vert Sorry, I don't plan to share my code. Yes, it's seq2seq model but not bidirection for avoiding label leak. Yisak Birhanu Bule Posted a year ago arrow_drop_up 1 more_vert congratulation man, I hope you will take first place in the Kaggle world rank next month when end  enefit  competition. can you share your notebook hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 1 more_vert Sorry, I don't plan to share my code.😂 Ayman Allawi Posted a year ago arrow_drop_up 1 more_vert Thanks for sharing your solutions it is a great one indeed. However, I am curious why you don't use meta model to combine and ensemble your models, I think that in the competition of predicting energy behaviour you didn't use meta model also hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 0 more_vert Thanks for your suggestion. I am not familiar with meta model. What does this mean? Stacking or what else? Ayman Allawi Posted a year ago arrow_drop_up 0 more_vert It can be done in countless ways, but in short it is when use the predictions of a model or group of models as feature to the single model that makes the final prediction, this model is the meta model hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 1 more_vert I know you mean. It's very time-consuming to train a second-level model and doesn't help much in fact. So i just use weighted sum. emoji_people bogoconic1 Posted a year ago · 14th in this Competition arrow_drop_up 1 more_vert Thanks for sharing! The weighted postprocessing improved our selected sub from 5.4457 to 5.4405, good for 11th place. This was the magic pp, zero-sum and zero-mean did not make much difference to our subs. We couldn’t make it work during the competition 😓 should have spent more time trying to fix it Somy2022 Posted a year ago arrow_drop_up 0 more_vert Can I ask you what u mean by sub from 5.4457 to 5.4405 emoji_people bogoconic1 Posted a year ago · 14th in this Competition arrow_drop_up 1 more_vert Our competition selected submission scored 5.4457 without any postprocessing Adding the weighted mean postprocessing, improved it from 5.4457 to 5.4405 Hina Ismail Posted a year ago arrow_drop_up 1 more_vert Great sir keep it up! Roberto Spadim Posted a year ago · 3110th in this Competition arrow_drop_up 1 more_vert gru should be trained and states reseted every day? hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 0 more_vert Just zero inited at the beginning of every day. skygoal9 Posted a year ago · 108th in this Competition arrow_drop_up 1 more_vert congratulations and thanks a lot for sharing achaosss Posted a year ago arrow_drop_up 1 more_vert Why the feature number you choose is 300, not 200 or 400? hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 1 more_vert based on CV score, 300 is better than 200. 400 will have memory issue. mhdaw Posted a year ago arrow_drop_up 1 more_vert Congratulations . Thanks for sharing your great idea . I'm kind of new to ML and if I got it correctly the input data for the Transformer part of your model was different with the input data of the GRU or the CatBoost . right ? And what do you mean by \"GBDT\" in \"predict target bucket mean by GBDT\" ? Thanks hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 0 more_vert Thanks~ Yes It means Gradient Boosting Decision Tree. Devang Giri Goswami Posted a year ago arrow_drop_up 1 more_vert Congrats @hydantess to be the top placeholder! JM Posted a year ago · 172nd in this Competition arrow_drop_up 1 more_vert Congratulations @hydantess , blew it away! Thanks for sharing the scores difference between the PP/OL additions too very informative.. Relocating to Netherlands/Optiver next step? 😅 hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 13 more_vert I love China. 😀 Jason Jiasting Posted 17 days ago arrow_drop_up 0 more_vert 大神你好，我有3个问题希望向您请教一下，1、您在上面提到用catboost的特征重要性top300的特征作为后续训练的特征，此处您在catboost训练时特征应该在300以上，后面还需要用catboost在筛选的300个特征上再次训练吗，还是300特征仅用在后续的GRU和Transform上了？2、每12天重新训练模型，是否也需要重新生成新的特征排序，重新选择300个特征；3、每次滚动训练时，您会对catboost重新进行超参数寻优吗？ IAmGroot Posted a year ago arrow_drop_up 1 more_vert Very informative Andreas Bisiadis Posted a year ago arrow_drop_up 1 more_vert Congratulations for your win! I did not have the chance to participate in the competition (I got into kaggle after the competition closed), however I am interested keeping up with the solutions. My final model(CV/Private LB of 5.8117/5.4030) was a combination of CatBoost (5.8240/5.4165), GRU (5.8481/5.4259), and Transformer (5.8619/5.4296), with respective weights of 0.5, 0.3, 0.2 searched from validation set. And these models share same 300 features. With Transformer, you mean Transformer models that are specific for tabular data, namely FT-Transformer or TabTransformer? hyd Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 7 more_vert No, It's torch.transformerencoder ~ Adnan Alaref Posted a year ago arrow_drop_up 1 more_vert Congratulations and continued success @hydantess",
      "Optiver - Trading at the Close | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Optiver · Featured Code Competition · a year ago Late Submission more_horiz Optiver - Trading at the Close Predict US stocks closing movements Optiver - Trading at the Close Overview Data Code Models Discussion Leaderboard Rules ADAM. · 9th in this Competition  · Posted a year ago arrow_drop_up 61 more_vert 9th Place Solution A big thanks to Optiver and Kaggle for hosting this competition. This competition has a really stable correlation between local cv and lb. Actually I entered this game a little late, about 30 days before its ends and I am not good at NN, so I only focus on Gradient Boosting tree models and its feature engineering. I noticed there are many top solutions using NN and it is really a good opportunity for me to learn NN. Model Xgboost with 3 different seeds and same 157 features There is not much difference between Xgboost and Lightgbm in lb score. But GPU Xgboost trains faster than GPU Lightgbm. Feature Engineering Firstly, create some \"basic features\" based on raw features(i.e. add, subtract, multiply, divide from raw features). Also, create some median-scaled raw size features. size_col = [ 'imbalance_size' , 'matched_size' , 'bid_size' , 'ask_size' ] for _ in size_col:\n    train[ f\"scale_ {_} \" ] = train[_] / train.groupby([ 'stock_id' ])[_].transform( 'median' ) content_copy Secondly, do further feature engineering/aggregation on raw features and \"basic features\" imb1, imb2 features market_urgency feateures I copied from public notebook diff features on different time window shift features on different time window rolling_mean/std features on different time window using history wap to calculate target of 6 second before. Then, do some rolling_mean some global date_id+seconds weighted features MACD feateures target rolliong_mean over stock_id + seconds_in_bucket Feature Selection Because we have limit on inference time and memory, it's essential to do some feature selection. I add features group by group and check whether the local cv improves. Each feature group usually have 10 - 30 features. If one groups make local cv improve, I add feature one by one insides this feature group and usually kept only 5-10 most effective features. I keep 157 features in my final model. Post-processing: Subtract weighted sum. From the definition of target, we can know weighted sum of target for all stocks should be zero. test_df[ 'pred' ] = lgb_predictions\ntest_df[ 'w_pred' ] = test_df[ 'weight' ] * test_df[ 'pred' ]\ntest_df[ \"post_num\" ] = test_df.groupby([ \"date_id\" , \"seconds_in_bucket\" ])[ 'w_pred' ].transform( 'sum' ) / test_df.groupby([ \"date_id\" , \"seconds_in_bucket\" ])[ 'weight' ].transform( 'sum' )\ntest_df[ 'pred' ] = test_df[ 'pred' ] - test_df[ 'post_num' ] content_copy Others: xgb mae objective xgb sample_weight 1.5 weight for latest 45 days data Online training. I only retrain model twice. one is N day (N is the start date of private lb), the other is N+30 day. polars and reduce_mem_usage function helps a lot Codes train: https://github.com/ChunhanLi/9th-kaggle-optiver-trading-close inference: https://www.kaggle.com/code/hookman/9th-submission 17 2 Please sign in to reply to this topic. comment 29 Comments 2 appreciation  comments Hotness Demon_1023 Posted a year ago · 1240th in this Competition arrow_drop_up 1 more_vert 你好，请问特征生成时shift是不是写错了，-mock_period好像是用到未来信息了 target mock系列 for mock_period in [1,3,12,6] :\n\n    df = df. with_columns ([\n        pl. col ( \"wap\" ). shift (-mock_period). over ( \"stock_id\" , \"date_id\" ). alias (f \"wap_shift_n{mock_period}\" )\n    ])\n    df = df. with_columns ([\n        (pl. col (f \"wap_shift_n{mock_period}\" )/pl. col ( \"wap\" )). alias ( \"target_single\" )\n    ]) content_copy ADAM. Topic Author Posted a year ago · 9th in this Competition arrow_drop_up 1 more_vert 应该不会出问题的。这里我仔细核对过的，具体细节确实有些忘了。 大致看了下是这个逻辑 比如10s 20s 30s shift(-3) -> 30s的wap移到10s位置，做一些计算得到特征A。 我后面要平移回去的 pl.col(\"target_mock\").shift(mock_period).over(\"stock_id\",\"date_id\").alias(f\"target_mock_shift{mock_period}\") 这里把10s的特征A shift到30s的位置，所以这里30s的特征A(10s shift过来的这个)其实还是基于30s以前的数据算的，没有穿越。 我这个写法读起来不太友好，把10s的wap移到30s来计算特征A其实和这个等价的，但是可读性更高。 Demon_1023 Posted a year ago · 1240th in this Competition arrow_drop_up 1 more_vert 感谢你的回复，的确target_mock在你的代码里shift回去会得到target_mock_shift{mock_period}特征，他没有泄露信息。 pl.col(\"target_mock\").shift(mock_period).over(\"stock_id\",\"date_id\").alias(f\"target_mock_shift{mock_period}\") 不过在构建target_mock_shift{mock_period}这些特征的过程中，也创造了wap_shift_n{mock_period}和target_single这些特征。这些特征没有删除掉，好像是用到了未来信息…… 即 30s的wap移到10s位置，这个wap就留在了10s位置，构造完特征后没有将中间过程的特征删除 ADAM. Topic Author Posted a year ago · 9th in this Competition arrow_drop_up 2 more_vert wap_shift_n{mock_period}和target_single 这些特征没有加入我的模型的。可以看下add_cols变量，只有这里面的变量，我才会加入模型。 Ethan Posted a year ago · 13th in this Competition arrow_drop_up 1 more_vert Congrats! Let‘s look forward next new GM. ADAM. Topic Author Posted a year ago · 9th in this Competition arrow_drop_up 0 more_vert Thanks. haha WindClimber Posted a year ago · 23rd in this Competition arrow_drop_up 1 more_vert Thank you for sharing! In my case, GPU XGboost trains much faster than GPU LightGBM as well. Funny Posted a year ago · 12th in this Competition arrow_drop_up 1 more_vert Thanks for sharing. Do you have insight how important \"1.5 weight for latest 45 days data\" is? I tried to put weight for lgb model but did not succeeded. I think it might be one of the keys providing the recent data is very relevant for prediciton ADAM. Topic Author Posted a year ago · 9th in this Competition arrow_drop_up 0 more_vert As far as I remembered, setting sample weight can improve round 0.001 in my local cv. hoo yuet Posted a year ago arrow_drop_up 2 more_vert 观摩学习！对于新手很大帮助 Zeyang Hao Posted 18 days ago arrow_drop_up 0 more_vert 😭跟着大佬就是能学到真东西 Somy2022 Posted a year ago arrow_drop_up 0 more_vert Can I ask you how you handle NaN value @hookman ADAM. Topic Author Posted a year ago · 9th in this Competition arrow_drop_up 0 more_vert fillna(-9e10) MengMai Posted a year ago arrow_drop_up 0 more_vert 请问最开始的权重 weight_df['weight'] =  [ 0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004, 0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004, 0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001, 0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004, 0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004, 0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008, 0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02, 0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002, 0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002, 0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002, 0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002, 0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002, 0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02, 0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04, 0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004, ] 是如何确定的 ADAM. Topic Author Posted a year ago · 9th in this Competition arrow_drop_up 0 more_vert Check this discussion . MeiYuxin Posted a year ago arrow_drop_up 0 more_vert Thanks for sharing。I have a question. Why do you build cv using days>390。 ADAM. Topic Author Posted a year ago · 9th in this Competition arrow_drop_up 0 more_vert Actually, I forgot it. Maybe some public discussion or notebook use this and I want to compare with them. I think It doesn't matter you use 45/60/90 as cv. It is all aligned with lb. MeiYuxin Posted a year ago arrow_drop_up 0 more_vert but I get result this: 15个特征时： 验证得分：5.89235 公榜得分：5.4095 66个特征时： 验证得分：5.90648 调参后验证得分：5.89542 公榜得分：5.3968 3 more replies arrow_drop_down Felix Levesque Posted a year ago arrow_drop_up 0 more_vert Any idea why your solution running on my computer crashes? It seems to work fine if I do something like : train = train[train[\"date_id\"] >= 350] to reduce the dataset size, but if not it crashes all the time. (I have 64 Gb RAM and 16 cores) ADAM. Topic Author Posted a year ago · 9th in this Competition arrow_drop_up 0 more_vert I have 128GB RAM so I didn't spend time reducing memory in training phase. Maybe you can use reduce_mem_usage function on the featured dataframe before going into Xgboost. Besides, you can also cast dtypes in generate_features_no_hist_polars (i.e. float64 -> float32) to save memory. Hina Ismail Posted a year ago arrow_drop_up 0 more_vert Nice work sir Somy2022 Posted a year ago arrow_drop_up 0 more_vert Can u explain why you do this step \" # 阶段1 (pl.col('ask_size') * pl.col('ask_price')).alias(\"ask_money\"), (pl.col('bid_size') * pl.col('bid_price')).alias(\"bid_money\"), (pl.col('ask_size') + pl.col(\"auc_ask_size\")).alias(\"ask_size_all\"), (pl.col('bid_size') + pl.col(\"auc_bid_size\")).alias(\"bid_size_all\"), (pl.col('ask_size') + pl.col(\"auc_ask_size\") + pl.col('bid_size') + pl.col(\"auc_bid_size\")).alias(\"volumn_size_all\"), (pl.col('reference_price') * pl.col('auc_ask_size')).alias(\"ask_auc_money\"), (pl.col('reference_price') * pl.col('auc_bid_size')).alias(\"bid_auc_money\"), (pl.col('ask_size') * pl.col('ask_price') + pl.col('bid_size') * pl.col('bid_price')).alias(\"volumn_money\"), (pl.col('ask_size') + pl.col('bid_size')).alias('volume_cont'), (pl.col('ask_size') - pl.col('bid_size')).alias('diff_ask_bid_size'), (pl.col('imbalance_size') + 2 * pl.col('matched_size')).alias('volumn_auc'), ((pl.col('imbalance_size') + 2 * pl.col('matched_size')) * pl.col(\"reference_price\")).alias('volumn_auc_money'), ((pl.col('ask_price') + pl.col('bid_price'))/2).alias('mid_price'), ((pl.col('near_price') + pl.col('far_price'))/2).alias('mid_price_near_far'), (pl.col('ask_price') - pl.col('bid_price')).alias('price_diff_ask_bid'), (pl.col('ask_price') / pl.col('bid_price')).alias('price_div_ask_bid'), (pl.col('imbalance_buy_sell_flag') * pl.col('scale_imbalance_size')).alias('flag_scale_imbalance_size'), (pl.col('imbalance_buy_sell_flag') * pl.col('imbalance_size')).alias('flag_imbalance_size'), (pl.col('imbalance_size') / pl.col('matched_size') * pl.col('imbalance_buy_sell_flag')).alias(\"div_flag_imbalance_size_2_balance\"), ((pl.col('ask_price') - pl.col('bid_price')) * pl.col('imbalance_size')).alias('price_pressure'), ((pl.col('ask_price') - pl.col('bid_price')) * pl.col('imbalance_size') * pl.col('imbalance_buy_sell_flag')).alias('price_pressure_v2'), ((pl.col(\"ask_size\") - pl.col(\"bid_size\")) / (pl.col(\"far_price\") - pl.col(\"near_price\"))).alias(\"depth_pressure\"), (pl.col(\"bid_size\") / pl.col(\"ask_size\")).alias(\"div_bid_size_ask_size\"), ])\" ADAM. Topic Author Posted a year ago · 9th in this Competition arrow_drop_up 1 more_vert I want to create some simple and useful features according to cross validation and business knowledge. Yisak Birhanu Bule Posted a year ago arrow_drop_up 0 more_vert thank you for your summary of solution, as well as code publication achaosss Posted a year ago arrow_drop_up 0 more_vert I still cannot understand \"Subtract weighted sum\" .Could you please provide more explanations? ADAM. Topic Author Posted a year ago · 9th in this Competition arrow_drop_up 1 more_vert Check the definition of target. Weighted sum of target for all stocks is 0. achaosss Posted 7 months ago arrow_drop_up 0 more_vert Thank you so much, I learn a lot from your code. Somy2022 Posted a year ago arrow_drop_up 0 more_vert Are research papers published for this competition, especially for the leaders? I want to read more about the subject Zi Posted a year ago arrow_drop_up 0 more_vert reading the code is like so much more efficient This comment has been deleted. This comment has been deleted. Appreciation (2) Onelux Posted a year ago · 183rd in this Competition arrow_drop_up 1 more_vert Congrats! and Thanks for sharing. max riffi Posted 10 months ago arrow_drop_up 0 more_vert Thanks for sharing",
      "Optiver - Trading at the Close | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Optiver · Featured Code Competition · a year ago Late Submission more_horiz Optiver - Trading at the Close Predict US stocks closing movements Optiver - Trading at the Close Overview Data Code Models Discussion Leaderboard Rules emoji_people bogoconic1 · 14th in this Competition  · Posted 2 years ago arrow_drop_up 24 more_vert 14th Place Solution for the Optiver - Trading at the Close Competition This post outlines our approach which got us 14th on the private leaderboard Firstly, thanks to @ravi20076 , @mcpenguin , @madarshbb , @cody11null for the collaboration, and Optiver for organizing Also, thanks to @wenxuanxx , @zhangyue199 , @lblhandsome , for upholding the Kaggle spirit and sharing your feature ideas and notebooks Context Business context: https://www.kaggle.com/competitions/optiver-trading-at-the-close Data context: https://www.kaggle.com/competitions/optiver-trading-at-the-close/data Overview of the approach To begin with, we fixed bugs in the public notebook, and made a small change to our local validation such that it does not have data leakage when handling global features df [ 'mid_price_movement' ] = df [ 'mid_price' ] .diff (periods= 5 ) .apply (lambda x : 1 if x > 0 else (- 1 if x < 0 else 0 ))\nto\ndf [ 'mid_price_movement' ] = df .groupby ( [ \"stock_id\" ] ) [ 'mid_price' ] .diff (periods= 5 ) .apply (lambda x : 1 if x > 0 else (- 1 if x < 0 else 0 )) content_copy (Btw this fix improved CV but worsened public LB score. Idk why LOL) Also, we wrote our own functions for RSI, MACD and bollinger bands indicators as the public one gave inaccurate results in inference What worked (impact on CV) Features based on revealed targets. We used lags 1,2,3 for target grouped by stock_id and seconds_in_bucket as features into our models (-0.005) Signed representation of imbalance_size (-0.003) Continuous model training with revealed targets. We refitted our LGB and CatBoost model at fixed intervals (elaborated later) (-0.006) Performing CV in streaming fashion. To do this, we saved the data each time period in _.csv and delivered them one by one in chronological order when calculating CV. This took much longer but correlated better with the public LB Zero sum post processing (-0.005) (but we are not sure in private LB so we only chose 1 submission with this) Global Features (-0.004). We have to re-initialize these values this whenever we want to retrain the model to keep it up to date Technical indicators RSI, MACD and bollinger bands. We had to rewrite them for good results (-0.002) What did not work (impact on CV) Group by date_id and stock_id for rolling features instead of stock_id (+0.003). We didn’t do this in the end Lagged features shift(x) where x is large. Made CV better but worsened LB Rolling features over a window x where x is large. Made CV better but worsened LB Sector features (+0.002) Neural Networks Triplet Imbalances (+0.001) realised this feature gave very unstable values because of precision issues so we decided to discard this even though they improved the public LB score (worsened from 5.3315 to 5.3327) Dropping features based on feature importances Zero mean postprocessing. It makes our ensembles and LB/CV correlation worse for some reason so we didn’t choose this. In fact the ensemble does worse with zero mean (5.333) compared to zero sum (5.3327) Some additional discussion points of the approach are outlined here Details of the submission Overall, after much consideration, the submissions that we chose were these two. Thank you @cody11null for tuning the parameters and testing it with your huge 91 model script 170 features, no postprocessing. Public 5.3384. Private 5.4457. LGB + CatBoost Refitting strategy (assuming day X is the first day where currently_scored is True) Day X: Refit LGB, CAT Day X+6: Refit LGB Day X+12: Refit CAT Day X+18: Refit LGB … Day X+54: Refit LGB Day X+60: Refit CAT 193 features, zero sum postprocessing. Public 5.3327. Private 5.4458. LGB + CatBoost Refitting strategy (assuming day X is the first day where currently_scored is True) Day X: Refit LGB, CAT Day X+9: Refit LGB Day X+18: Refit CAT Day X+27: Refit LGB … Day X+54: Refit CAT At each time period, we use the latest LGB and latest CAT and submitted the average prediction of the 2 models We tried to use the stock weights for a while but got submission scoring error when refitting so didn’t proceed 😢 Please sign in to reply to this topic. comment 11 Comments Hotness william.wu Posted a year ago · 129th in this Competition arrow_drop_up 1 more_vert Congratulations, great work! emoji_people bogoconic1 Topic Author Posted a year ago · 14th in this Competition arrow_drop_up 1 more_vert Updated with the private scores of the submissions we selected HW Posted 2 years ago · 30th in this Competition arrow_drop_up 1 more_vert Great work! We also tried to build some sector features. It improved in CV but got a worse LB like what you did. Did you figure out the reason? emoji_people Ravi Ramakrishnan Posted 2 years ago · 14th in this Competition arrow_drop_up 1 more_vert Some opinions- If you analyze the mae score across stock id, you may notice that 10-15 stocks cause huge errors leading to a lion's share of the OOF score. Mostly these stocks cause an issue in the predictions I tried some sector based features at the beginning of the competition (I loosely mentioned it in my ExtensiveEDA public notebook too), but they did not improve the CV score, so I rejected these features. All the best for the forecasting phase @wuhongrui HW Posted 2 years ago · 30th in this Competition arrow_drop_up 1 more_vert Good luck to your team too! Somy2022 Posted a year ago arrow_drop_up 0 more_vert are you share the code? emoji_people MAT Posted 2 years ago · 590th in this Competition arrow_drop_up 0 more_vert can you describe your approach to CV (beyond filtering through API)  in detail, please? it looks it was critical to you for your developments emoji_people bogoconic1 Topic Author Posted 2 years ago · 14th in this Competition arrow_drop_up 0 more_vert We saved each time bucket from dates 436-480 into CSV files with format 436_0.csv, 436_10.csv, 436_20.csv, etc Then when doing CV, load them one by one, doing feature generation and postprocessing bucket by bucket. It is very slow though, for us it took about 4 hours to get the CV for each experiment and hence we couldn't perform it too often the CV was 5.8117 with no postprocessing, 5.8080 with zero-sum and 5.8072 with zero-mean (without taking into account refitting) using simple average. Hold-out last 45 days. we didn't use the purging fold split in the public notebooks Somy2022 Posted a year ago arrow_drop_up 0 more_vert are you share the code? yuanzhe zhou Posted 2 years ago · 2nd in this Competition arrow_drop_up 0 more_vert It seems that everyone has trouble with large window size. emoji_people bogoconic1 Topic Author Posted 2 years ago · 14th in this Competition arrow_drop_up 0 more_vert Yeah, we restricted the window size to 450 or 470 dates to prevent this problem. QXYMP Posted 2 years ago arrow_drop_up 0 more_vert Thanks for sharing! Did you use the .refit command to refit the model? Will this bring the new risk, e.g., discussed in https://stackoverflow.com/questions/73664093/lightgbm-train-vs-update-vs-refit ? \"But it could lead to drastic changes in the predictions produced by the model, especially if the batch of newly-arrived data is much smaller than the original training data, or if the distribution of the target is very different.\" emoji_people bogoconic1 Topic Author Posted 2 years ago · 14th in this Competition arrow_drop_up 0 more_vert No, just feature generation and re-train, same as what we do with the original train data",
      "Optiver - Trading at the Close | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Optiver · Featured Code Competition · a year ago Late Submission more_horiz Optiver - Trading at the Close Predict US stocks closing movements Optiver - Trading at the Close Overview Data Code Models Discussion Leaderboard Rules O Yuksel · 15th in this Competition  · Posted a year ago arrow_drop_up 23 more_vert 15th place solution I used an ensemble of GBTs with a single online component. Published my training code here . Details Ensemble: 3 x LGB, offline 4 x XGB 1 x LGB, online (re-trained every 5 days with a window of 60 days) Training: Used n-fold cross validation by date_id with 5-day gaps between the folds. Hyperparameter selection: used public notebooks as a reference, optuna + manual tweaks as the final pick. Features that I haven't seen in public notebooks: Revealed_target Intraday revealed_target using the wap from previous time steps Features based on revealed stocks : sector_id embeddings based on historical open/close/high/low data Group features performance against the mean performance against the sector At-the-money call price estimate with \"expiry\" at the end of auction Inferred price based on tick size Post processing: Replaced \"zero-sum\" with subtraction by index-weighted mean targets. Other: Dropped stock_id and relied on the embeddings based on historical performance and sector to reduce the effects of delisting and other drastic changes. Stuff that didn't work: Most rolling features I introduced caused a drop in LB, so I omitted those such as: Rolling cross-correlation Rolling z-score Clustering stock_ids based on correlated wap, target Stock embeddings from neural networks I got the biggest public LB boost from online learning, post-processing and hyperparameter tuning. Acknowledgements I re-used feature engineering code from: https://www.kaggle.com/code/meli19/lgb-kf-baseline https://www.kaggle.com/code/zulqarnainali/explained-singel-model-optiver/notebook https://www.kaggle.com/code/judith007/lb-5-3405-rapids-gpu-speeds-up-feature-engineer https://www.kaggle.com/code/verracodeguacas/fold-cv Thanks to Kaggle and Optiver for the interesting competition. This is the first time I saw a competition through and learned a lot during the process. 4 Please sign in to reply to this topic. comment 2 Comments Hotness emoji_people Ravi Ramakrishnan Posted a year ago · 14th in this Competition arrow_drop_up 2 more_vert @lognorm I think proper online learning was key here, a simple model like LGBM gave a gold medal to us with good online retraining! Congratulations! C R Suthikshn Kumar Posted a year ago · 344th in this Competition arrow_drop_up 0 more_vert Congratulations on securing 15th place in this competition. Thanks for sharing the solution details."
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict US stocks closing movements This dataset contains historic data for the daily ten minute closing auction on the NASDAQ stock exchange. Your challenge is to predict the future price movements of stocks relative to the price future price movement of a synthetic index composed of NASDAQ-listed stocks. This is a forecasting competition using the time series API. The private leaderboard will be determined using real market data gathered after the submission period closes. [train/test].csv The auction data. The test data will be delivered by the API. All size related columns are in USD terms. All price related columns are converted to a price move relative to the stock wap (weighted average price) at the beginning of the auction period. sample_submission A valid sample submission, delivered by the API. See this notebook for a very simple example of how to use the sample submission. revealed_targets When the first time_id for each date  (i.e. when seconds_in_bucket equals zero) the API will serve a dataframe providing the true target values for the entire previous date. All other rows contain null values for the columns of interest. public_timeseries_testing_util.py An optional file intended to make it easier to run custom offline API tests. See the script's docstring for details. You will need to edit this file before using it. example_test_files/ Data intended to illustrate how the API functions. Includes the same files and columns delivered by the API. The first three date ids are repeats of the last three date ids in the train set, to enable an illustration of how the API functions. optiver2023/ Files that enable the API. Expect the API to deliver all rows in under five minutes and to reserve less than 0.5 GB of memory. The first three date ids delivered by the API are repeats of the last three date ids in the train set, to better illustrate how the API functions. You must make predictions for those dates in order to advance the API but those predictions are not scored. 7 files 646.9 MB csv, py, so Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 646.9 MB example_test_files optiver2023 public_timeseries_testing_util.py train.csv 7 files 44 columns ",
    "data_description": "Optiver - Trading at the Close | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Optiver · Featured Code Competition · a year ago Late Submission more_horiz Optiver - Trading at the Close Predict US stocks closing movements Optiver - Trading at the Close Overview Data Code Models Discussion Leaderboard Rules Overview In this competition, you are challenged to develop a model capable of predicting the closing price movements for hundreds of Nasdaq listed stocks using data from the order book and the closing auction of the stock. Information from the auction can be used to adjust prices, assess supply and demand dynamics, and identify trading opportunities. Start Sep 20, 2023 Close Mar 22, 2024 Merger & Entry Description link keyboard_arrow_up Stock exchanges are fast-paced, high-stakes environments where every second counts. The intensity escalates as the trading day approaches its end, peaking in the critical final ten minutes. These moments, often characterised by heightened volatility and rapid price fluctuations, play a pivotal role in shaping the global economic narrative for the day. Each trading day on the Nasdaq Stock Exchange concludes with the Nasdaq Closing Cross auction. This process establishes the official closing prices for securities listed on the exchange. These closing prices serve as key indicators for investors, analysts and other market participants in evaluating the performance of individual securities and the market as a whole. Within this complex financial landscape operates Optiver, a leading global electronic market maker. Fueled by technological innovation, Optiver trades a vast array of financial instruments, such as derivatives, cash equities, ETFs, bonds, and foreign currencies, offering competitive, two-sided prices for thousands of these instruments on major exchanges worldwide. In the last ten minutes of the Nasdaq exchange trading session, market makers like Optiver merge traditional order book data with auction book data. This ability to consolidate information from both sources is critical for providing the best prices to all market participants. In this competition, you are challenged to develop a model capable of predicting the closing price movements for hundreds of Nasdaq listed stocks using data from the order book and the closing auction of the stock. Information from the auction can be used to adjust prices, assess supply and demand dynamics, and identify trading opportunities. Your model can contribute to the consolidation of signals from the auction and order book, leading to improved market efficiency and accessibility, particularly during the intense final ten minutes of trading. You'll also get firsthand experience in handling real-world data science problems, similar to those faced by traders, quantitative researchers and engineers at Optiver. Evaluation link keyboard_arrow_up Submissions are evaluated on the Mean Absolute Error (MAE) between the predicted return and the observed target. The formula is given by: M A E = 1 n n ∑ i = 1 | y i − x i | Where: n  is the total number of data points. y_i is the predicted value for data point i. x_i is the observed value for data point i. Submitting You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow the template in this notebook . Timeline link keyboard_arrow_up This is a forecasting competition with an active training phase and a second period where models will be run against real market data. Training Timeline: September 20, 2023 - Start Date. December 13, 2023 - Entry Deadline. You must accept the competition rules before this date in order to compete. December 13, 2023 - Team Merger Deadline. This is the last day participants may join or merge teams. December 20, 2023 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Forecasting Timeline: Starting after the final submission deadline there will be periodic updates to the leaderboard to reflect market data updates that will be run against selected notebooks. Updates will take place roughly every two weeks. March 20, 2024 - Competition End Date Prizes link keyboard_arrow_up 1st Place - $25,000 2nd Place - $20,000 3rd Place - $15,000 4th Place - $10,000 5th - 10th Place - $5,000 Code Requirements link keyboard_arrow_up Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time Internet access disabled Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv and be generated by the API. Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors. Citation link keyboard_arrow_up Tom Forbes, John Macgillivray, Matteo Pietrobon, Sohier Dane, and Maggie Demkin. Optiver - Trading at the Close. https://kaggle.com/competitions/optiver-trading-at-the-close, 2023. Kaggle. Cite Competition Host Optiver Prizes & Awards $100,000 Awards Points & Medals Participation 20,564 Entrants 4,374 Participants 4,436 Teams 5,159 Submissions Tags Tabular Finance Mean Columnwise Mean Absolute Error Table of Contents collapse_all Overview Description Evaluation Timeline Prizes Code Requirements Citation"
  },
  {
    "competition_slug": "open-problems-single-cell-perturbations",
    "discussion_links": [
      "/competitions/open-problems-single-cell-perturbations/discussion/459258",
      "/competitions/open-problems-single-cell-perturbations/discussion/458738",
      "/competitions/open-problems-single-cell-perturbations/discussion/458750",
      "/competitions/open-problems-single-cell-perturbations/discussion/460191",
      "/competitions/open-problems-single-cell-perturbations/discussion/461363",
      "/competitions/open-problems-single-cell-perturbations/discussion/459623",
      "/competitions/open-problems-single-cell-perturbations/discussion/459618",
      "/competitions/open-problems-single-cell-perturbations/discussion/461015",
      "/competitions/open-problems-single-cell-perturbations/discussion/459961",
      "/competitions/open-problems-single-cell-perturbations/discussion/460858",
      "/competitions/open-problems-single-cell-perturbations/discussion/459588",
      "/competitions/open-problems-single-cell-perturbations/discussion/461225",
      "/competitions/open-problems-single-cell-perturbations/discussion/462031",
      "/competitions/open-problems-single-cell-perturbations/discussion/458661",
      "/competitions/open-problems-single-cell-perturbations/discussion/458663",
      "/competitions/open-problems-single-cell-perturbations/discussion/461159",
      "/competitions/open-problems-single-cell-perturbations/discussion/458764",
      "/competitions/open-problems-single-cell-perturbations/discussion/460059",
      "/competitions/open-problems-single-cell-perturbations/discussion/461649",
      "/competitions/open-problems-single-cell-perturbations/discussion/460911",
      "/competitions/open-problems-single-cell-perturbations/discussion/460567",
      "/competitions/open-problems-single-cell-perturbations/discussion/458888",
      "/competitions/open-problems-single-cell-perturbations/discussion/459365"
    ],
    "discussion_texts": [
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules JK-Piece · 1st in this Competition  · Posted 2 years ago arrow_drop_up 48 more_vert 1st Place Solution Writeup for Open Problems – Single-Cell Perturbations I would like to thank the organizers and Kaggle for hosting this exciting competition. I am also grateful to the participants who shared starter notebooks, datasets, and insightful ideas. Below is a more detailed writeup of my solution, including late findings. Competition Page https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview https://openproblems.bio/ 1. Integration of Biological Knowledge Since the input features only consisted of pairs of short keywords, that is, cell types and small molecule names, and given the large size of the target variable, I was quickly convinced  that I needed to somehow enrich the input feature space. I therefore dedicated my first days of the competition to this task. First, I searched for biological word/term embeddings in the literature, and found the paper  ''BioWordVec, improving biomedical word embeddings with subword information and MeSH’’ by Zhang et al [1]. The paper directed me to the code on Github where I could find pretrained embeddings for biological terms. This was motivated by the fact that 1) I would be able to find most cell types, and small molecule names in such embeddings, and 2) The embeddings would encode rich information about the general meaning of each term. With these embeddings, I created larger input features and trained a regression model. This achieved 0.767 on the public leaderboard. With a better hyperparameter search and feature engineering, I improved the score to 0.614. As this seemed to be a good direction to go, I decided to further enrich the input features. This time, I searched for the definition of each cell type and small molecule name on wikipedia. For this, I used the python library wikipedia https://pypi.org/project/wikipedia/ . I then represented each cell type and small molecule name by a few sentences describing it, then I bootstrapped an embedding from the descriptions. For example, Nk cells were described by: ''Natural killer cells, also known as NK cells or large granular lymphocytes (LGL), are a type of cytotoxic lymphocyte critical to the innate immune system that belong to the rapidly expanding family of known innate lymphoid cells (ILC) and represent 5–20% of all circulating lymphocytes in humans. The role of NK cells is analogous to that of cytotoxic T cells in the vertebrate adaptive immune response. NK cells provide rapid responses to virus-infected cell and other intracellular pathogens acting at around 3 days after infection, and respond to tumor formation.’’ I also explored different numbers of sentences to describe each cell type and small molecule. While this is interesting from the biological point of view, it did not improve the leaderboard score. In fact, the score became worse (0.656 vs. 0.614 previously). This can be explained by the fact that such natural language descriptions came with some noise, and pretrained embeddings were probably not computed to deal with this. Fine-tuning the embeddings on natural language descriptions of biological terms also fell short. Because my initial idea about input feature enrichment did not meet my expectations, I decided to look for alternatives. Thanks to the discussions in the forum, I came across a notebook proposing to use SMILES to encode chemical structures of small molecules. I immediately decided to use ChemBERTa embeddings of SMILES encodings and observed a significant improvement in the evaluation metric MRRMSE on the validation data splits (I used a 5 fold cross-validation setting throughout the competition). With this, I developed additional data augmentation techniques, including the mean, standard deviation, and (25%, 50%, 75%) percentiles of differential expressions per cell type and small molecule in the training data. 2. Exploration of the Problem As mentioned in the previous section, I started the competition by trying to build rich features for the input pairs (cell_type, sm_name). Ultimately, the use of ChemBERTa features of small molecules’ SMILES appeared to be an important step towards this goal.  Combined with the mean, standard deviation, and (25%, 50%, 75%) percentiles per cell type and small molecule, I achieved an optimal input feature representation. In my experiments, I used a 5-fold cross-validation setting with a fixed seed (42). It was hard to achieve a good score on the validation sets of the 2nd and 4th folds. On these folds, the MRRMSE on the validation set was approximately 1.19, and 1.15 on average, respectively. On the 1st, 3rd and 5th folds the average scores were 0.86, 0.86, and 0.90, respectively. The scores are the average across different model architectures (LSTM, 1d-CNN, GRU) and different input feature combinations (''initial’’, ''light’’, ''heavy’’). The three different input feature representations are as follows: “initial”: ChemBERTa embeddings, 1 hot encoding of cell_type/sm_name pairs, mean, std, percentiles of targets per cell_type and sm_name “light”: ChemBERTa embeddings, 1 hot encoding of cell_type/sm_name pairs, mean targets per cell_type and sm_name “heavy”: ChemBERTa embeddings, 1 hot encoding of cell_type/sm_name pairs, mean, 25%, 50%, 75% percentiles of targets per cell_type and sm_name The figure below shows the training curves (MRRMSE) per fold averaged over all three model architectures. The differences in the validation MRRMSE in the above figure motivated me to take a closer look into the validation sets, where I found different distributions of cell types. The figure below shows the predominant cell types per fold and the corresponding average (across models and different input feature representations) validation MRRMSE. In the validation sets of the 1st, 3rd, and 5th folds, the predominant cell types (in terms of percentage) are ''T regulatory cells’’, ''B cells’’, and ''Nk cells’’, respectively. On the 2nd and 4th folds, ''T cells CD8+’’ and ''Myeloid cells’’ are the most represented cell types in the validation sets, respectively. The percentage is computed as the number of occurrences of a cell type in the validation set divided by the number of occurrences of that cell type in the training set. From the bar-plots above, the cell types that are easier to predict are ''T regulatory cells’’, ''B cells’’, and ''Nk cells’’, while ''T cells CD8+’’ and ''Myeloid cells’’ are the hardest to predict. Based on this observation, an ideal training set should include more  ''T cells CD8+’’ and ''Myeloid cells’’ than the rest of the cell types. In this way, trained ML models would be able to generalize to other cell types. 3. Model Design Model Architecture I tried different model architectures, including gradient boosting models, MLP, and 2D CNN which did not work so well. I finally selected LSTM, GRU, and 1-d CNN architectures as they performed better on the validation sets. Below I show a rough implementation of the GRU model. dims_dict = { 'conv' : { 'heavy' : 13400 , 'light' : 4576 , 'initial' : 8992 }, 'rnn' : { 'linear' : { 'heavy' : 99968 , 'light' : 24192 , 'initial' : 29568 }, 'input_shape' : { 'heavy' : [ 779 , 142 ], 'light' : [ 187 , 202 ], 'initial' : [ 229 , 324 ]}\n                     }} class GRU (nn.Module): def __init__ ( self, scheme ): super (GRU, self ).__init__() self .name = 'GRU' self .scheme = scheme self .gru = nn.GRU(dims_dict[ 'rnn' ][ 'input_shape' ][ self .scheme][ 1 ], 128 , num_layers= 2 , batch_first= True ) self .linear = nn.Sequential(\n            nn.Linear(dims_dict[ 'rnn' ][ 'linear' ][ self .scheme], 1024 ),\n            nn.Dropout( 0.3 ),\n            nn.ReLU(),\n            nn.Linear( 1024 , 512 ),\n            nn.Dropout( 0.3 ),\n            nn.ReLU()) self .head = nn.Linear( 512 , 18211 ) self .loss1 = nn.MSELoss() self .loss2 = LogCoshLoss() self .loss3 = nn.L1Loss() self .loss4 = nn.BCELoss() def forward ( self, x, y= None ):\n        shape1, shape2 = dims_dict[ 'rnn' ][ 'input_shape' ][ self .scheme]\n        x = x.reshape(x.shape[ 0 ],shape1,shape2) if y is None :\n            out, hn = self .gru(x)\n            out = out.reshape(out.shape[ 0 ],- 1 )\n            out = torch.cat([out, hn.reshape(hn.shape[ 1 ], - 1 )], dim= 1 )\n            out = self .head( self .linear(out)) return out else :\n            out, hn = self .gru(x)\n            out = out.reshape(out.shape[ 0 ],- 1 )\n            out = torch.cat([out, hn.reshape(hn.shape[ 1 ], - 1 )], dim= 1 )\n            out = self .head( self .linear(out))\n            loss1 = 0.4 * self .loss1(out, y) + 0.3 * self .loss2(out, y) + 0.3 * self .loss3(out, y)\n            yhat = torch.sigmoid(out)\n            yy = torch.sigmoid(y)\n            loss2 = self .loss4(yhat, yy) return 0.8 *loss1 + 0.2 *loss2 content_copy In my late experiments, I realized that 1d-CNN and GRU are actually the best architectures as they achieve the best scores alone (0.733  for GRU and 0.745 for 1d-CNN on Private LB). LSTM alone achieves 0.839 on Private LB. With 0.25xLSTM + 0.65xCNN the Private LB is 0.725, and with 0.25xLSTM + 0.65xGRU the Private LB is 0.723. Loss Functions and Optimizer I simultaneously optimized 4 loss functions via weighted averaging: MSE, MAE, LogCosh, and BCE. The weights are 0.32, 0.24, 0.24, and 0.2, respectively. This was found to enhance the predictive performance of models. The Adam optimizer with learning rate 0.001 for LSTM and CNN, and 0.0003 for GRU was used to train the models. LogCosh is defined as: class LogCoshLoss (nn.Module): def __init__ ( self ): super ().__init__() def forward ( self, y_prime_t, y_t ):\n        ey_t = (y_t - y_prime_t)/ 3 # divide by 3 to avoid numerical overflow in cosh return torch.mean(torch.log(torch.cosh(ey_t + 1e-12 ))) content_copy LogCosh is similar to MAE with the difference being that it is a softer version that can allow smoother convergence. It was adapted from https://github.com/tuantle/regression-losses-pytorch . The BCE loss is indeed special as it is often used for classification tasks. However, I argue that it sends better signals to the models and optimizers when the target values are close to zero. To demonstrate this, consider the following two pieces of code: m1 = nn.Sigmoid()\nloss = nn.BCELoss() input = torch.tensor([ 0.05 ], requires_grad= True ).unsqueeze( 0 )\ntarget = torch.sigmoid(torch.tensor([- 0.05 ], requires_grad= False ).unsqueeze( 0 ))\noutput1 = loss(m1( input ), target) print (output1.item()) # 0.694 m2 = nn.Identity()\nloss = nn.MSELoss() input = torch.tensor([ 0.05 ], requires_grad= True ).unsqueeze( 0 )\ntarget = torch.tensor([- 0.05 ], requires_grad= False ).unsqueeze( 0 )\noutput2 = loss(m2( input ), target) print (output2.item()) # 0.010 content_copy With this example, one can observe that the MSELoss tells the model and optimizer that \"it is ok, there is no mistake here\". Obviously, there is a mistake, and BCELoss can see it as it returns a high loss value (0.694 compared to 0.010 for MSELoss). My choice of the BCELoss in this competition is motivated by the fact that most target values are from a Gaussian distribution with mean 0 as can be seen in the figure below. Hyperparameters 250 epochs of training Learning rate 0.001 for LSTM and CNN, and 0.0003 for GRU Gradient norm clip value: [5.0, 1.0, 1.0] for the three schemes ''initial'', ''light'', and ''heavy'' 4. Robustness I conducted 4 experiments using different subsets of the training data, and monitored the private leaderboard score. I considered subsets of the initial training data (de_train) with sizes 25%, 50%, 75%, and 100%. Below 25%, we cannot cover all small molecules in the test set (id_map) even with a stratified split on sm_name, and hence the one hot encoding algorithm cannot run. With 25%, I achieved 0.946. With 50%, I achieved 0.815. With 75% of the training data, it is 0.769, and with the full data the private leaderboard is 0.719 (which is better than my winning submission because I removed padding in the ChemBERTa model). The figure below shows the robustness of my approach as a decreasing curve, i.e., improvement of the MRRMSE with increasing training data amount. My second data augmentation technique can be regarded as noise addition. I randomly replace 30% of the input features’ entries with zeros, and add the resulting input feature together with the correct target as a new training datapoint. This has proven to improve the predictive performance of my models. In this sense, my models are robust to the noise as their performance is not hindered but rather improved. The biological motivation here is that we might not need to know the complete chemical structure of a molecule (assuming the dropped input features are from sm_name) to know its impact on a cell. Similarly, there might be a biological disorder in a cell, and we would still expect that cell to respond to a molecule (drug) in the same way as a normal cell. Below is the data augmentation function def augment_data ( x_, y_ ):\n    copy_x = x_.copy()\n    new_x = []\n    new_y = y_.copy()\n    dim = x_.shape[ 2 ]\n    k = int ( 0.3 *dim) for i in range (x_.shape[ 0 ]):\n        idx = random.sample( range (dim), k=k)\n        copy_x[i,:,idx] = 0 new_x.append(copy_x[i]) return np.stack(new_x, axis= 0 ), new_y content_copy 5. Documentation and Code Style The documentation and software dependencies are available on Github at https://github.com/Jean-KOUAGOU/1st-place-solution-single-cell-pbs 6. Reproducibility Code is available and well documented on Github at https://github.com/Jean-KOUAGOU/1st-place-solution-single-cell-pbs . Reproduction scripts are added. Sources [1] BioWordVec, improving biomedical word embeddings with subword information and MeSH Pytorch Regression Loss Functions [ChemBERTa]( https://huggingface.co/DeepChem/ChemBERTa-77M-MTR Please sign in to reply to this topic. comment 19 Comments Hotness JK-Piece Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 3 more_vert Reproduce 1st Place Private  Leaderboard Notebook: https://www.kaggle.com/code/jeannkouagou/1st-place-solution Note: Run ChemBERTa Model with random LM head (600-d) and no padding to obtain 0.719 on Private LB makio323 Posted 2 years ago · 24th in this Competition arrow_drop_up 4 more_vert Congratulations on the 1st place!  Thank you very much for sharing your detailed solution and what worked and what did not.  I see an exciting intersection of yours and 2nd one - using mean along with std of targets per cell_type and sm_name as features. JK-Piece Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 3 more_vert Thank you Makio. Indeed, there is an intersection, which is interesting! makio323 Posted 2 years ago · 24th in this Competition arrow_drop_up 2 more_vert I wish you success in future competitions as well! Yonggie Posted 2 years ago · 1013th in this Competition arrow_drop_up 1 more_vert Clear code in github and clear explanation in the post! Congrates! JK-Piece Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Thank you @zechengyin Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert Congratulations with the victory ! Would you be so kind to share your submission files for further research purposes , please. JK-Piece Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert Hi @alexandervc . Thanks. Here are the submissions: https://www.kaggle.com/code/jeannkouagou/submit-best Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert Thanks a lot ! jingfeng ou Posted 2 years ago arrow_drop_up 1 more_vert class LogCoshLoss(nn.Module): def init (self): super(). init () def forward (self, y_prime_t , y_t ) :\n    ey_t = ( y_t - y_prime_t )/ 3 # divide by 3 to avoid numerical overflow in cosh return torch. mean (torch. log (torch. cosh ( ey_t + 1e-12 ))) content_copy Is 1e-12 here misplaced? JK-Piece Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Hi @jingfengou . Adding 1e-12 is not necessary. But it can act as a small regularizer maxleverage Posted 2 years ago · 21st in this Competition arrow_drop_up 1 more_vert Congrats, what's the rationale behind the sigmoid output head? JK-Piece Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert Thanks for the question. Maybe I was not precise enough. In Section \"Loss Functions and Optimizers\", I wrote The BCE loss is indeed special as it is often used for classification tasks. However, I argue that it helps models better learn the signs of the predicted values as they are mostly close to zero. I actually meant that the BCE loss applied on the sigmoid of predictions and targets returns a higher loss (i.e., sends a better signal to the optimizer) when the values involved are close to zero; indeed, most values are close to zero, see plot above. To demonstrate this, let's consider the two pieces of code below: JK-Piece Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 4 more_vert m1 = nn.Sigmoid()\nloss = nn.BCELoss() input = torch.tensor([ 0.05 ], requires_grad= True ).unsqueeze( 0 )\ntarget = torch.sigmoid(torch.tensor([- 0.05 ], requires_grad= False ).unsqueeze( 0 ))\noutput1 = loss(m1( input ), target) print (output1.item()) # 0.694 m2 = nn.Identity()\nloss = nn.MSELoss() input = torch.tensor([ 0.05 ], requires_grad= True ).unsqueeze( 0 )\ntarget = torch.tensor([- 0.05 ], requires_grad= False ).unsqueeze( 0 )\noutput2 = loss(m2( input ), target) print (output2.item()) # 0.010 content_copy With this, one can observe that the MSELoss tells the model and optimizer that \"it is ok, there is no mistake here\" Oppositely, the BCELoss considers this to be a huge mistake. Meraj Hasan Posted 2 years ago · 1030th in this Competition arrow_drop_up 2 more_vert Congratulations! Excellent work JK-Piece Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Thank you Miraj. I appreciate Ankush H V Posted 8 months ago arrow_drop_up 0 more_vert Why do you think CNN was a good choice for this task Vinit Shah Posted a year ago arrow_drop_up 0 more_vert Good work, congrats! Abhishek Gupta Posted 2 years ago arrow_drop_up 0 more_vert Congrates sir JK-Piece Topic Author Posted a year ago · 1st in this Competition arrow_drop_up 0 more_vert Thank you @abhishekgupta18895 This comment has been deleted. JK-Piece Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thanks a lot @manavtrivedi",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules EliKal · 2nd in this Competition  · Posted 2 years ago arrow_drop_up 48 more_vert 2nd Place Solution for the Open Problems – Single-Cell Perturbations I am thrilled to finally unveil my solution to this competition! Context https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/data Overview of the approach My optimal model emerged as a composite of four models, where three were employed as a normalized sum, and the fourth acted as an amplifier, showcasing discernible impact, potentially on labels with alternating signs: weight_df1: 0.5 (utilizing std, mean, and clustering sampling, yielding 0.551) weight_df2: 0.25 (excluding uncommon elements, resulting in 0.559) weight_df3: 0.25 (leveraging clustering sampling, achieving 0.575) weight_df4: 0.3 (incorporating mean, random sampling, and excluding std, attaining 0.554) The resultant model is expressed as the weighted sum of these components: resulted_model = weight_df1 * df1 + weight_df2 * df2 + weight_df3 * df3 + weight_df4 * df4. All models were trained using the same transformer architecture, albeit with varying dimensional models (with the optimal dimension being 128). Data preprocessing and feature selection To train deep learning models on categorical labels, I employed one-hot encoding transformation. To address high and low bias labels, I utilized target encoding by calculating the mean and standard deviation for each cell type and SM name. During experimentation, I compared models with and without uncommon columns and discovered that including uncommon columns significantly improved the training of encoding layers for mean and standard deviation feature vectors, resulting in noticeable performance enhancement. EDA In my feature exploration, I performed truncated Singular Value Decomposition (SVD) specifically on the target variables. Despite experimenting with different SVD sizes, the models generated from this approach consistently fell short of replicating the performance observed in the full targets regression. Notably, as part of this analysis, I identified certain targets with significant standard deviation values. To leverage this insight, I strategically enriched the feature set by introducing the standard deviation as a dedicated feature for each cell type and SM name. This involved concatenating these standard deviation values into a unified vector format (std_cell_type, std_sm_name). Furthermore, as I delved into the dataset, I conducted a comprehensive examination, revealing a remarkable cleanliness characterized by the absence of NaN values and duplicates. This meticulous exploration not only extended to model training but also encompassed a nuanced analysis of the target variables, contributing to the informed enhancement of the feature set for improved model performance. Sampling strategy In devising an effective sampling strategy for partitioning the data into training and validation sets, I employed a sophisticated approach rooted in the clusters derived from a K-Means clustering analysis on the target values. The rationale behind this strategy lies in the premise that data points within similar clusters share inherent patterns or characteristics, thus ensuring a more nuanced and representative split. For each distinct cluster identified through K-Means, I executed the data partitioning using the train_test_split function from the scikit-learn library. This method facilitated a controlled allocation of data points to both the training and validation sets, ensuring that the models were exposed to a diverse yet stratified representation of the data. The validation percentage, a crucial parameter in this process, was meticulously chosen to strike a balance between model robustness and computational efficiency. By experimenting with validation percentages ranging from 0.1 to 0.2, I systematically assessed the impact on model performance. Ultimately, for the optimal model configuration, a validation percentage of 0.1 was deemed most effective. This decision was guided by a careful consideration of the trade-off between the need for a sufficiently large training set and the importance of a robust validation set to gauge model generalizability. In summary, this sampling strategy, grounded in cluster-based data splitting, not only acknowledges the underlying structure within the target values but also optimizes the partitioning process to enhance the model's ability to capture diverse patterns and generalize effectively to unseen data. Modeling This is my best architecture class CustomTransformer_v3 (nn.Module): # mean + std def __init__ ( self, num_features, num_labels, d_model= 128 , num_heads= 8 , num_layers= 6 , dropout= 0.3 ): super (CustomTransformer_v3, self ).__init__() self .num_target_encodings = 18211 * 4 self .num_sparse_features = num_features - self .num_target_encodings self .sparse_feature_embedding = nn.Linear( self .num_sparse_features, d_model) self .target_encoding_embedding = nn.Linear( self .num_target_encodings, d_model) self .norm = nn.LayerNorm(d_model) self .concatenation_layer = nn.Linear( 2 * d_model, d_model) self .transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout, activation=nn.GELU(),\n                                       batch_first= True ),\n            num_layers=num_layers\n        ) self .fc = nn.Linear(d_model, num_labels) def forward ( self, x ):\n        sparse_features = x[:, : self .num_sparse_features]\n        target_encodings = x[:, self .num_sparse_features:]\n\n        sparse_features = self .sparse_feature_embedding(sparse_features)\n        target_encodings = self .target_encoding_embedding(target_encodings)\n\n        combined_features = torch.cat((sparse_features, target_encodings), dim= 1 )\n        combined_features = self .concatenation_layer(combined_features)\n        combined_features = self .norm(combined_features)\n\n        x = self .transformer(combined_features)\n        x = self .norm(x)\n\n        x = self .fc(x) return x content_copy I modularized my model into two distinct segments to efficiently handle the complexity of the input data: Sparse Feature Encoding: For encoding sparse features, I utilized an embedding layer tailored to handle the sparsity inherent in certain feature types. Considering the nature of sparse features, I initially explored utilizing the nn.Embedding layer. However, due to computational constraints on my laptop GPU, I opted for an alternative approach using a linear layer (nn.Linear) to convert sparse features into a dense representation. Target Encoding Feature Encoding (Dense Features): Concurrently, I addressed the encoding of target encodings, which are inherently dense features. To accomplish this, I employed a separate linear layer (nn.Linear) designed specifically for target encodings, ensuring an effective transformation into a meaningful latent space. Following these individual encodings, I concatenated the resulting embeddings into a unified latent vector, fostering a comprehensive representation of both sparse and dense feature information. To further enhance the model's capacity to capture nuanced patterns, I employed normalization (nn.LayerNorm) on the concatenated features. Considering the computational challenges posed by sparse feature embedding using nn.Embedding, I strategically utilized nn.Linear for efficient processing on my laptop GPU. In addition, I introduced a nuanced approach for encoding dense features by breaking them into four distinct encoding layers, each dedicated to capturing nuanced patterns related to mean and standard deviation for both 'sm_name' and 'cell_type'. Furthermore, the model architecture leveraged the state-of-the-art Lion optimizer, recognized for its efficacy in optimizing transformer-based models. The specific model implementation, as exemplified in the CustomTransformer_v3 class, showcases a transformer architecture with multiple layers, heads, and dropout for optimal learning. In summary, this modularized and intricately designed model not only optimizes computational efficiency but also demonstrates a keen understanding of the unique characteristics of sparse and dense features, contributing to the overall effectiveness of the learning process. Hyperparameters The model training regimen was characterized by a judicious selection of hyperparameters, featuring an initial learning rate of 1e-5 and a weight decay of 1e-4, the latter serving as an additional regularization mechanism, particularly for dropout layers within the model architecture. This meticulous choice of hyperparameters aimed at striking a balance between effective training and prevention of overfitting. To dynamically adjust the learning rate during training, a learning rate scheduler of the ReduceLROnPlateau type was employed. The scheduler, configured with a mode of \"min\" and a reduction factor of 0.9999, adeptly adapted the learning rate in response to plateaus in model performance, ensuring an efficient convergence towards optimal results. The patience parameter, set to 500, determined the number of epochs with no improvement before triggering a reduction in the learning rate. The training process spanned 20,000 epochs, incorporating an early stopping mechanism triggered after 5,000 epochs of stagnation. This comprehensive training strategy underscored a thoughtful balance between fine-tuning the model's parameters and preventing overfitting, contributing to the robustness and efficiency of the learning process. Loss function While Mean Absolute Error (MAE) and Mean Squared Error (MSE) individually prove effective for gauging bias or variance, it's crucial to recognize their distinct roles in assessing model performance. To strike a balance between these metrics, I opted for Huber loss, amalgamating the strengths of both MAE and MSE. Despite utilizing the Mrrmse metric, the ultimate model selection hinged on the performance demonstrated by the optimal loss function on the validation set. Preventing overfitting In addressing the challenges posed by the limited size of the dataset, a critical consideration was ensuring the model's ability to effectively regularize. To achieve this, I incorporated Dropout layers within the transformer architecture and applied weight decay for L2 penalty on the model weights. Additionally, to mitigate the impact of exploding gradients, I implemented gradient norm clipping with a maximum norm of 1. Validation Strategy In pursuit of an optimal model configuration, I adopted a robust validation strategy by training with different seeds and employing k-fold cross-validation. The diverse set of performance metrics for each fold included: Achieving a score of 0.551 through the incorporation of standard deviation, mean, and clustering sampling. Attaining a score of 0.559 by excluding uncommon elements from the dataset. Realizing a score of 0.575 through clustering sampling. Securing a score of 0.554 by integrating mean, random sampling, and excluding standard deviation from the features. This validation approach not only facilitated the identification of the best-performing model but also guided the fine-tuning of hyperparameters to optimize overall model performance. Sources https://github.com/lucidrains/lion-pytorch https://www.kaggle.com/code/ayushs9020/understanding-the-competition-open-problems https://www.kaggle.com/code/alexandervc/op2-eda-baseline-s https://github.com/Eliorkalfon/single_cell_pb Please sign in to reply to this topic. comment 23 Comments 1 appreciation  comment Hotness Barry Posted 2 years ago · 51st in this Competition arrow_drop_up 6 more_vert Congratulations! @eliork 🎉🎉🎉Thank you very much for sharing. Your work is so creative. But due to my limited knowledge, I still cannot understand some parts well. I am already following your code sharing. Looking forward to your future achievements! EliKal Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Hi @songqizhou , I've uploaded the code to the Git repository. Please feel free to explore it and don't hesitate to reach out if you have any questions. Barry Posted 2 years ago · 51st in this Competition arrow_drop_up 1 more_vert Many thanks to you @eliork ! Your code is as elegant as poetry!🥰 IraQbot Posted 2 years ago · 19th in this Competition arrow_drop_up 3 more_vert congrats !! i thought none of the top solution would be transformer based EliKal Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 2 more_vert Thanks! I tried xgboost and simple mlp  but the results were the best for this model. makio323 Posted 2 years ago · 24th in this Competition arrow_drop_up 1 more_vert This competition made me realize the power of the \"generative\" NN model, generating 18211 points from chem with 155 ranks and cell with 6 ranks, though the smart feature engineering makes the difference. Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert Congratulations with the great result ! Would you be so kind to share the submission files of your solutions - for further research analysis  ? EliKal Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Thanks! The files are in my GitHub repository under submissions folder also check seq.py for more info. Octavi Grau Posted 2 years ago · 195th in this Competition arrow_drop_up 1 more_vert I followed you during the competition and I was sure you would be recognized in the private LB. Superb job @eliork ! :) Aya Posted 2 years ago · 43rd in this Competition arrow_drop_up 1 more_vert Thank you a lot for sharing your approach! And congratulations on the excellent result! C R Suthikshn Kumar Posted 2 years ago · 214th in this Competition arrow_drop_up 1 more_vert Congratulations on achieving the 2nd place. Thanks for sharing details about CustomTransformer model.  Very interesting indeed. Abhas Malguri Posted 2 years ago arrow_drop_up 1 more_vert Great Efforts Really! makio323 Posted 2 years ago · 24th in this Competition arrow_drop_up 2 more_vert Congratulations and thank you very much for sharing the solution!  Seeing your solution is indeed an aha moment; it is unbiased and summarizing both cell and chem features; wow, simple and strong in prediction, it is quite a learning. Liuda Cheldieva Posted 2 years ago · 207th in this Competition arrow_drop_up 2 more_vert I have been dreaming of seeing your solutions for a long time. EliKal Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 3 more_vert I hope I didn’t disappoint  :) VIKRAM MISHRA Posted 2 years ago arrow_drop_up 0 more_vert Your notebooks are truly commendable! I upvoted all of them. If you get a chance, I'd be grateful for your input on my latest work. Thanks yu3jun Posted a year ago arrow_drop_up 0 more_vert congrats !! I‘m confused about the discription : [My optimal model emerged as a composite of four models], aren't these models trained seperately? How can we conbine them together?  Or just compute the weighted sum of their separate results? Thanks a lot for your contributions! EliKal Topic Author Posted a year ago · 2nd in this Competition arrow_drop_up 0 more_vert The models were trained separately, with the same general architecture but with different features (mean,std,etc), I just ensembled the results using weighted sum, I hope it clarifies my approach. yu3jun Posted a year ago arrow_drop_up 0 more_vert Thanks for your reply! I've checked the seq.py，but I still have some questions about the specific features of weight_df. My understands are as follows, please correct me if I'm wrong: weight_df1 : Utilizing std, mean, and clustering sampling weight_df2 :  Same as weight_df1 but excluding uncommon elements. weight_df3 :  I'm not quite understand how to ' leveraging ' clustering sampling, are you using std and mean both here? What are the differences between Weight_ df1 and weight_df3 ? weight_df4 : Incorporating mean, random sampling, and excluding std Thanks again for your reply! EliKal Topic Author Posted a year ago · 2nd in this Competition arrow_drop_up 0 more_vert The difference between df1 to df3 is the sampling method. weight_df3: I clustered all of the data using k-means, and I sampled from each cluster instead of random sampling. Yonggie Posted 2 years ago · 1013th in this Competition arrow_drop_up 0 more_vert This is my brief summary on the work flow of the solution (correct me if I'm wrong🤣): data augmentation: utilized std and mean of the train_data and turned the train_data shape from (614,152) to (614,72996) , y shape to (614,18211) . then utilize K-means to clean the train data? then (check the model) train the transformer model (with 2 sperated MLP for sparse and dense features then a concatenate) with Lion and HuberLoss, output a rather high dim predictions. predict the test data using truancatedSVD to down sample the predictions to submission format. EliKal Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 1 more_vert I employed K-means to create samples from each cluster. A simple analogy is when dealing with significantly imbalanced data, it's crucial to ensure a representation of categories in both the training and validation sets. The optimal model was attained without truncated SVD. However, if you choose to apply this transformation, remember to also perform the inverse transform. Adlene Jenitta J Posted 2 years ago arrow_drop_up 0 more_vert Can you give me an example of when you utilized the Lion-Python optimizer? I have been waiting for the model. Deep learning models for the Kaggle's Open Problems – Single-Cell Perturbations competition. Coming soon. https://github.com/Eliorkalfon/single_cell_pb ) EliKal Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Hi, I've uploaded the code to the Git repository, check it out :) This comment has been deleted. Appreciation (1) hannah142 Posted 2 years ago arrow_drop_up 1 more_vert Congrats! Thanks for sharing",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Rafał Pawłowski · 3rd in this Competition  · Posted 2 years ago arrow_drop_up 32 more_vert 3rd Place Solution for the Open Problems – Single-Cell Perturbations 1. Integration of Biological Knowledge Generally, I treated this problem as a regression with 2 feature columns and 18211 targets, but I had tried to utilize SMILES sequences in neural network with LSTM unit. Both the sm_name and SMILES columns can be encoded exactly in the same way, so the sm_name column can be replaced with the SMILES column. Moreover, SMILES column is more informative, because every single character of the sequence can be encoded (not only a single value like in sm_name)  and the order of these characters provides extra information. Theoretically, In the worst case, the performance of the neural network using SMILES column instead of sn_name should be not worse than using original columns. Unfortunate, I have reached an unsatisfactory public score with this neural network and I stopped further research. 2. Exploration of the problem For simplicity, the analysis is done for the single best model without pseudolabeling. Since mrrmse metric is sensitive to outliers, a distribution of ranges for columns should be checked. The majority of columns have a range of values in an interval (4, 50). Naturally, the columns with high range lead to high mae or mse, so in order to determine genes which are easy and hard to predict, the standardized (divided by std) colwise mse is applied. The table below shows the hardest and easiest genes for prediction. The scheme of the applied cross validation.  Every fold contains one cell type chosen from NK cells, T cells CD4+, T cells CD8+, T regulatory cells and only sm_names being in public and private test was involved. The lowest value of this validation split corresponds to the lowest value on public and private dataset. In my opinion this is a reliable split and the perfect split depends on the model architecture, so every model can have its perfect validation split. The easiness of learning per cell types is shown below. The values of loss are different, because they are calculated in truncated space. T cells CD4+ and NK cells are learning well. T regulatory cells are harder for training. The T cells CD8 are weakly improving on validation dataset. This split uses about 25% of the dataset for validation, so I believe more reliable splits exist. My new proposition of split is: This split is similar to the previous one, but for each validation fold, randomly selected examples are moved to training. Let's check an impact of the new split for training. For each cell type, more data improved performance on this same, constant and small validation set. Metric is calculated on full dimension this time. The first value at 480 training examples corresponds to the previous split. Theoretically, decreasing the size of validation set to one example can lead to the best performance. Since it will be very similar to training on whole dataset, what is done finally. 3. Model design Solution The prediction system is two staged, so I publish two versions of the notebook. The first stage predicts pseudolabels. To be honest, if I stopped on this version, I would not be the third. The predicted pseudolabels on all test data (255 rows) are added to training in the second stage. Stage 1 preparing pseudolabels The main part of this system is a neural network. Every neural network and its environment was optimized by optuna. Hyperparameters that have been optimized: a dropout value, a number of neurons in particular layers, an output dimension of an embedding layer, a number of epochs, a learning rate, a batch size, a number of dimension of truncated singular value decomposition. The optimization was done on custom 4-folds cross validation.  In order to avoid overfitting to cross validation by optuna I applied 2 repeats for every fold and took an average. Generally, the more, the better. The optuna's criterion was MRRMSE. Finally, 7 models were ensembled. Optuna was applied again to determine best weights of linear combination. The prediction of test set is the pseudolabels now and will be used in second stage. Stage 2 retraining with pseudolabels The pseudolabels (255 rows) were added to the training dataset. I applied 20 models with optimized parameters in different experiments for a model diversity. Optuna selected optimal weights for the linear combination of the prediction again. Models had high variance, so every model was trained 10 times on all dataset and the median of prediction is taken as a final prediction.  The prediction was additionally clipped to colwise min and max. History of improvements: a replacing onehot encoding with an embedding layer a replacing MAE loss with MRRMSE loss an ensembing of models with mean a dimension reduction with truncated singular value decomposition an ensembling of models with weighted mean using pseudolabeling using pseudolabeling and ensembling of 20 models and weighted mean. What did not work for me : a label normalization, standardization a chained regression a denoising dataset a removal of outliers an adding noise to labels a training on selected easy / hard to predict columns a huber loss. 4. Robustness I have tested 3 types of the robustness: increasing dataset size, adding noise to labels and inputs. Adding the noise to inputs failed totally, it is logical for me, because the nominal and categorical values are changed and are behaving like the continues values, what is not beneficial. Let's see the performance on 40%, 50%,…, 100% of training dataset. It started from 40%, because singular values decomposition is limited by number of examples. The experiment was 5 times repeated, so the interval of uncertainty is visible. More data improves the performance significantly. The last test of robustness is adding noise to the labels. The random gaussian (a distribution with 0 mean and scale * std) noise was added to the labels. Adding some noise (0.01 * std) can even improve the model's performance.  Generally, the model is robust to the noise. 5. Documentation & code style The code on GitHub is documented. 6. Reproducibility GitHub code: repo Notebook. The version 264 is first a stage and 266 the second one: notebook . The code runs in approximately 1 hour using CPU Intel(R) Core(TM) i5-9300H CPU @ 2.40GHz and 8GB RAM. Please sign in to reply to this topic. comment 10 Comments 1 appreciation  comment Hotness Barry Posted 2 years ago · 51st in this Competition arrow_drop_up 6 more_vert Congratulations🎉🎉🎉 @jankowalski2000 ! Thank you very much for sharing! Your code is as beautiful as a poem, and reading it is a pleasure. From the updates on your notebook, I can see your perseverance in solving problems, and this honor is the best reward for your efforts. You are so young and talented, respect! Ankita Nain Posted 2 years ago arrow_drop_up 1 more_vert Great work 👏 johanna-galvis Posted 2 years ago · 857th in this Competition arrow_drop_up 1 more_vert congratulations @jankowalski2000 ! ,  will your model be presented (by you or someone of your team) in the NeurIPS workshop next week ? Rafał Pawłowski Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert I am not sure, I have to provide a lot of documents and refactor my code to fulfill all requirements for a leaderboard award. Rafał Pawłowski Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert The analysis and github code are added. C R Suthikshn Kumar Posted 2 years ago · 214th in this Competition arrow_drop_up 1 more_vert Congratulations on 3rd place in this competition. Thanks for sharing the details of your solution. VIKRAM MISHRA Posted 2 years ago arrow_drop_up 1 more_vert Great Work, I've gone through all. Maximus Power Posted a year ago · 805th in this Competition arrow_drop_up 0 more_vert It seems like pseudolabelling was a big part of the success of your solution. For classification, there is some explanation that pseudolabelling can help as a regularizer similar to entropy regularization (Lee 2013), but I've not seen it used before or studied in the regression setting. Why do you think it helps here, or have you seen it used in regression before? Thanks for the writeup! This comment has been deleted. Appreciation (1) Sixian Hsu Posted a year ago arrow_drop_up 0 more_vert Helps Me A lot, thanks!",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules paranoid · 4th in this Competition  · Posted 2 years ago arrow_drop_up 9 more_vert 4rd Place Solution for the Open Problems – Single-Cell Perturbations 1. Context https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/data 2. Overview of the approach Our approach is principally based on trial and error. Our optimal model is derived from several different models, of which the first model, RAPIDS SVR, was used to provide pseudo-labels but did not participate in the final ensembling: RAPIDS SVR - Used for training based on ChemEMB features to obtain pseudo-label results. Pyboost - Based on RAPIDS SVR pseudo-label features and other features. Public LB-0.581, Private LB-0.768. nn - Only utilized TruncatedSVD and Leaveoneout Encoder. Public LB-0.577, Private LB-0.731. (Yes, only nn can achieve 0.731 in Private LB) Open-source solution 0720 and open-source solution 0531 (late submission indicates that 0531 is not necessary) . The final solution's result is Public LB-0.566, Private LB-0.733~0.735. ((0.3*Pyboost + 0.7*nn)*0.9 + open-source solution 0720*0.1)*0.95 + open-source solution 0531*0.05 There is a score zone because our code has been saved in different versions in a messy manner, and there are some subtle parameter differences when reviewing our proposal, which makes it difficult to fully reproduce the final submission plan. This is our first time participating in the Kaggle competition, and in the future, we will be more meticulous in doing a good job of code version control. Different models employed different feature engineering strategies, which we will detail below. 3. Modeling 3.1 Cross-validation We believe that the method of cross-validation is a very important point for scoring improvement in this competition. A reasonable validation set allows us to trust our local CV instead of an overfitted LB. Initially, we used random k-fold cross-validation. This method's CV maintained a consistent trend with the LB in the early stages. However, as we further incorporated nn models and added more training strategies, we noticed a discrepancy between the CV and LB. Consequently, we experimented with public two other forms of cross-validation which come from AmbrosM and MT . Thanks for their sharing. Ultimately, we chose MT's method of cross-validation, which showed excellent consistency between CV and LB in our models. 3.2 RAPIDS SVR The first model we experimented with was RAPIDS SVR. It is frequently used in Kaggle competitions and has been part of multiple winning solutions. It is renowned for its fast training capabilities. 3.2.1 Feature Engineering The feature engineering for the RAPIDS SVR model primarily included generating embedding features from ChemBERTa-10M-MTR for SMILES and statistical features obtained by aggregating target data. During the competition, we noticed in the discussion forums that many mentioned the embedding features generated by ChemBERTa-10M-MTR did not yield positive results. In our trials, we found that these features had a negative impact on models like LGBM and CatBoost, but they improved performance in RAPIDS SVR. Therefore, we decided to use RAPIDS SVR to generate pseudo-labels as a basis for subsequent model training. Here are the features we used in our RAPIDS SVR model: One-hot encoding features of cell_type and sm_name. Embedding features generated by ChemBERTa-10M-MTR. We used a mean pooling method comes from Chris and set the maximum length for SMILES based on our analysis. def mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output.last_hidden_state.detach().cpu()\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min =1e-9\n    )\n\nclass EmbedDataset(torch.utils.data.Dataset):\n    def __init__(self,df):\n        self.df = df.reset_index( drop = True )\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,idx):\n        text = self.df.loc[idx, \"SMILES\" ]\n        tokens = tokenizer(\n                text,\n                None, add_special_tokens = True , padding = 'max_length' , truncation = True , max_length =150, return_tensors = \"pt\" )\n        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n        return tokens\n\n\ndef get_embeddings(de_train, MODEL_NM = '' , MAX_LEN =150, BATCH_SIZE =32, verbose = True ):\n    global tokenizer\n    # Extract unique texts\n    unique_texts = de_train[ \"SMILES\" ].unique()\n\n    # Create a dataset for unique texts\n    ds_unique = EmbedDataset(pd.DataFrame(unique_texts, columns=[ \"SMILES\" ]))\n    embed_dataloader_unique = torch.utils.data.DataLoader(ds_unique, batch_size =BATCH_SIZE, shuffle = False ) DEVICE = \"cuda\" model = AutoModel.from_pretrained( MODEL_NM )\n    tokenizer = AutoTokenizer.from_pretrained( MODEL_NM )\n\n    model = model. to (DEVICE)\n    model.eval()\n    unique_emb = [] for batch in tqdm(embed_dataloader_unique, total =len(embed_dataloader_unique)):\n        input_ids = batch[ \"input_ids\" ]. to (DEVICE)\n        attention_mask = batch[ \"attention_mask\" ]. to (DEVICE)\n        with torch.no_grad():\n            model_output = model( input_ids =input_ids,attention_mask=attention_mask)\n        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n        # Normalize the embeddings\n        sentence_embeddings = F.normalize(sentence_embeddings, p =2, dim =1)\n        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n        unique_emb.extend(sentence_embeddings)\n    unique_emb = np.array(unique_emb) if verbose: print ( 'unique embeddings shape' ,unique_emb.shape)\n\n    text_to_embedding = {text: emb for text, emb in zip(unique_texts, unique_emb)}\n\n    train_emb = np.array([text_to_embedding[text] for text in de_train[ 'SMILES' ]])\n    test_emb = np.array([text_to_embedding[text] for text in id_map[ \"SMILES\" ]])\n\n    return train_emb, test_emb\n\nMODEL_NM = 'DeepChem/ChemBERTa-10M-MTR' all_train_text_feats, te_text_feats = get_embeddings(df_de_train, MODEL_NM) content_copy 3.Statistical features for cell_type and sm_name by aggregated from the target column. We retained features like ['mean', 'min', 'max', 'median', 'first', 'quantile_0.4']. Interestingly, when experimenting with different features, we did not train on all targets but selected only the first target, A1BG, for training and validation. This approach of feature selection allowed us to screen all features in just a few minutes, recording the CV scores for different features. 3.3 Pyboost Our Pyboost solution is based on an open-source approach from Alexander Chervov . Thanks for your sharing. 3.3.1 Feature Engineering In our Pyboost model, we used four types of features: Pseudo-label features from RAPIDS SVR. Leaveoneout encoding features for cell_type and sm_name. We found in the Pyboost model that leaveoneout encoding was more effective than onehot encoding. Embedding features generated by ChemBERTa-10M-MTR for SMILES. Aggregated features for cell_type and sm_name against the target column, where we retained ['mean', 'max']. We reduced the dimensionality of 18,211 targets to 45 using TruncatedSVD. Similarly, we also reduced the dimensions of the above features to the same 45 dimensions. This dimensional reduction provided a certain improvement in our CV scores. 3.3.2 modeling model = GradientBoosting( 'mse' , ntrees =1000\n                    , lr =0.01\n                    , max_depth =10\n                    , subsample =1\n                    , colsample =0.2\n                    , min_data_in_leaf =1\n                    , min_gain_to_split =0\n                    , verbose =100) content_copy 3.4 nn 3.4.1 Feature Engineering In our nn model, we only used leaveoneout encoding features for cell_type and sm_name, as other features caused a decrease in CV scores. We also performed dimensionality reduction on the target data based on TruncatedSVD. 3.4.2 modeling Our nn model consisted of a 3-layer 1D convolutional layer + 1 fully connected layer + a non-pretrained ResNet18 network. This structure allowed us to achieve a LB score of around 0.57 based solely on leaveoneout encoding features for cell_type and sm_name, which seems quite tricky. Our initial nn model aimed to convert SMILES expressions into image data using the rdkit.Chem library and then input these images along with other features into the network for training, thus employing the ResNet network for image processing. However, we found that introducing SMILES image data did not improve training results. Despite this, we retained parts of the network structure and ended up with the aforementioned network. class ResNetRegression(nn.Module):\n    def __init__(self, input_size, output_size, pretrained = False , reshape_size =32, num_channels =16, dropout_rate =0.2):\n        super(ResNetRegression, self).__init__()\n        self.reshape_size = reshape_size\n\n        self.conv1d_layers = nn.Sequential(\n            nn.Conv1d(1, num_channels, kernel_size =3, stride =1, padding =1),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n\n            nn.Conv1d(num_channels, num_channels *2 , kernel_size =3, stride =1, padding =1),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n\n            nn.Conv1d(num_channels *2 , num_channels *4 , kernel_size =3, stride =1, padding =1),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n        )\n\n        self.fc_layers = nn.Sequential(\n            nn.Linear(num_channels * 4 * input_size, self.reshape_size * self.reshape_size),\n        )\n\n        self.resnet = models.resnet18( pretrained =pretrained)\n        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias = False )\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, output_size)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # Reshape x for Conv1d\n        x = self.conv1d_layers(x)\n        x = x.view(x.size(0), -1)  # Flatten for the linear layer\n        x = self.fc_layers(x)\n        x = x.view(x.size(0), 1, self.reshape_size, self.reshape_size)\n        x = self.resnet(x)\n        return x content_copy 3.5 The 0720 Open-source Solution In our final model, we assigned a weight of 0.05 to the 0720 open-source solution for ensembling. This is the result of a great notebook that uses the \"Autoencoder\" method. This integration resulted in an increase of 0.001 in our scores on both the Public LB and Private LB. We are grateful for the contribution shared by vendekagonlabs and discussion . 4 Parameter Tuning We are not enthusiasts of parameter tuning, as, in our experience, tuning does not bring qualitative improvements to the model. In this competition, we only experimented with tuning towards the end, using Optuna to try out different settings for 'n_components' and 'n_iter' in TruncatedSVD, as well as 'sigma' in LeaveOneOutEncoder. Ultimately, we selected a few sets of parameters that yielded the best CV scores. 5 Things That Did Not Work Normalization of target data. Converting SMILES into image features. Tree models such as LGBM and Catboost yielded average training results. Code https://github.com/paralyzed2023/4st-place-solution-single-cell-pbs.git Please sign in to reply to this topic. comment 3 Comments Hotness Pizzaboi Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert I'm scrolling down top solutions and let me tel you: I would never expect this competition be won with ResNet18 🤩 This comment has been deleted. This comment has been deleted.",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules LBSDXQ · 6th in this Competition  · Posted 2 years ago arrow_drop_up 6 more_vert 6th Place Solution for the Open Problems – Single-Cell Perturbations 1. Integration of biological knowledge I tried integrating different biological knowledge, but unfortunately, most didn't work very well. Firstly, I came up with the idea of using the GO term to reduce genes into modules. I adopted the KEGG 2016 genes set and filtered out gene sets with a P-value less than 0.05. However, genes from the same set exhibit considerably different differential expression (DE) values. As a result, I feel the hypothesis that genes from the same GO term would have similar DE values cannot stand. Then, I resorted to finding co-expressed genes based on the gene expression matrix. I found the DE values between co-expressed genes do have a high Pearson correlation. However, I checked the predicted DE values of co-expressed genes and found they yield strong Pearson correlations as well. In other words, adding the correlation constraints to the optimization process won't have much effect. Next is about the molecule representation. I tried using the features from the pre-trained molecule models including ChemBERT and MolCLR, as well as the TFIDF-transformed element count proposed in the public notebooks. In my experiments, using the pre-trained features led to slightly inferior results, while the TFIDF-transformed element count and simply learnable molecule features give similar performance. I wonder if it is because the pre-training models capture more coarse-grained differences between molecules, or because the molecules in the contest are quite different from those used for pre-training. Finally is the integration of ATAC data. I did not have a good idea of integrating ATAC data into the model. All that I had come up with is that genes corresponding to a small ATAC count would be less affected by molecules. Anyway, I am not sure about such a hypothesis, and in practice, I simply concatenated the ATAC feature and found it slightly improves the results. 2. Exploration of the problem According to the problem definition, I think three targets could be predicted to achieve the task. The first target is directly the DE values. The variables in DE value prediction would be cell types and compounds. A model could be trained to predict the DE value given cell type-compound pairs. The second target is the bulk expression values, namely, the input to the Limma model. The variables in bulk prediction would be only the compounds (as I will elaborate on later). I feel such a paradigm is the most promising and robust solution. The most important reason is that the results could be precisely validated. To be specific, we could send the predicted and provided bulk expressions to the Limma model, and check how close the predicted DE values are to the provided ones. I feel such a validation scheme would be much less risks in overfitting. The third target is the single-cell gene expression. Though the training data is most sufficient in such a paradigm, it is the most challenging paradigm since we do not have the paired before-perturbation and after-perturbation gene expression values. 3. Model design In this competition, I mainly explored the first two prediction paradigms, namely the DE values and bulk expression values. 3.1 DE value prediction 3.1.1 Preprocess No data preprocessing is applied. Common preprocessing strategies such as normalizing and scaling even harm the performance in my experience. 3.1.2 Model architecture The plainest model learns the cell type and compound representation, as well as makes the predictions at the same time. The model could be written within a few lines, namely, class Net (nn.Module): def __init__ ( self, type_num, compound_num, gene_num ): super (Net, self ).__init__() self .type_num = type_num self .compound_num = compound_num self .gene_num = gene_num self .type_embedding = nn.Embedding( self .type_num, 32 ) self .compound_embedding = nn.Embedding( self .compound_num, 32 ) self .predictor = nn.Sequential(\n            nn.Linear( 64 , 256 ),\n            nn.BatchNorm1d( 256 ),\n            nn.Dropout(),\n            nn.ReLU(),\n            nn.Linear( 256 , 1024 ),\n            nn.BatchNorm1d( 1024 ),\n            nn.Dropout(),\n            nn.ReLU(),\n            nn.Linear( 1024 , self .gene_num),\n        ) def forward ( self, type , compound ):\n        type_embedding = self .type_embedding( type )\n        compound_embedding = self .compound_embedding(compound)\n        embedding = torch.cat([type_embedding, compound_embedding], dim= 1 ) return self .predictor(embedding) content_copy Optionally, we could replace the learnable cell type representation with RNA or ATAC counts averaged by cell type, and replace the compound representation with features from pre-trained models such as ChemBERT and MolCLR. Training such a vanilla model gives 0.586 MRRMSE on the public test split and 0.785 MRRMSE on the private test split. I also tried Transformer architecture like the Performer used in the scBERT paper. The idea is to treat each gene as a token and replace the positional embedding with cell type and compound representation. The token of each gene is learned together with the Transformer model. Due to the space limitation, I will not attach the model configuration code here, please refer to the GitHub repository attached below. Sadly, though the Transformer architecture generally gives better performance, in my practice, I just cannot successfully train the model (i.e., the training loss stays very high), and achieved 0.611 MRRMSE on the public test split and 0.767 MRRMSE on the private test split. It was not until the competition ended that I found the Transformer architecture achieved a much better score on the private test split, despite its poor performance on the public test split. Such an interesting result is worth exploring (or it is simply due to randomness and luck). 3.1.3 Learning objective To directly predict the DE values, I tried the MSE loss, L1 loss, and MRRMSE loss. The MRRMSE loss gives the best performance in practice. I did not use fancy tuning on the optimizer but simply used a standard Adam optimizer without any learning rate scheduler. 3.2 Bulk expression prediction 3.2.1 Preprocess The bulk count data is first normalized following the classic scRNA-seq preprocessing pipeline, namely, i) scaled to have 1e4 counts per bulk, ii) applied the log1p transformation, and iii) scaled to have zero mean and unit variance. 3.2.2 Model architecture After many tries on the DE value prediction paradigm, I felt a bit frustrated since the performance gain only comes from the fancy tuning of the model architecture. Hence, I turned to try the bulk expression prediction paradigm in the later stage of the competition. According to the experiment design, I noticed that there is a negative control spot in each row on the plate. In other words, the only difference between the negative spot and the remaining spots lies in the added compound (please correct me if that is wrong). Consequently, we could predict the perturbed bulk expression values based on the baseline counts and the compound. To this end, I built a conditional autoencoder as follows, class Net (nn.Module): def __init__ ( self,\n        gene_num,\n        compound_num,\n        sm_feature= None ,\n        type_rna= None ,\n        type_atac= None , ): super (Net, self ).__init__() self .compound_num = compound_num self .gene_num = gene_num if sm_feature is None : self .sm_emb = nn.Embedding( self .gene_num, 64 ) self .sm_enc = None else : self .sm_emb = sm_feature self .sm_enc = nn.Sequential(\n                nn.Linear( self .sm_emb.shape[ 1 ], 512 ),\n                nn.BatchNorm1d( 512 ),\n                nn.ReLU(),\n                nn.Linear( 512 , 64 ),\n            ) self .type_atac = type_atac self .atac_enc = nn.Sequential(\n            nn.Linear( self .type_atac.shape[ 1 ], 512 ),\n            nn.BatchNorm1d( 512 ),\n            nn.ReLU(),\n            nn.Linear( 512 , 64 ),\n        ) self .encoder = nn.Sequential(\n            nn.Linear( self .gene_num, 512 ),\n            nn.BatchNorm1d( 512 ),\n            nn.ReLU(),\n            nn.Linear( 512 , 64 ),\n        ) self .decoder = ConditionalNet(\n            dim_feature= 64 ,\n            dim_cond_embed= 128 ,\n            dim_hidden= 1024 ,\n            dim_out= self .gene_num,\n            n_blocks= 4 ,\n            skip_layers=(),\n        ) def forward ( self, x, type , sm_name ): if self .sm_enc is None :\n            sm = self .sm_emb(sm_name) else :\n            sm = self .sm_enc( self .sm_emb[sm_name])\n        encode = self .encoder(x)\n        atac = self .atac_enc( self .type_atac[ type ])\n\n        x = sm\n        cond = torch.cat([encode, atac], dim= 1 )\n        pred = self .decoder(x, cond) return pred content_copy Notably, here I unnaturally chose the negative count and cell type-averaged ATAC count as conditions, while letting the compound be the input. In practice, such a configuration leads to better results than reversely setting negative count as input and compound as the condition. Such a result could probably be attributed to the over-fitting of compound representation for the later configuration. Likewise, I also tried the Transformer architecture but it does not work very well. 3.2.3 Learning objective The model was trained to predict the normalized bulk count value change after adding the compound. The predicted normalized value is then recovered to the raw count according to the previous scaling and normalization factors. I tried the MSE loss, L1 loss, Smooth L1 loss, and MRRMSE loss, and found the Smooth L1 loss leads to the best performance. The predicted bulk value was then concatenated with the known bulk value and fed into the Limma model to compute the DE values. 3.2.4 Post preprocessing Though the bulk expression prediction paradigm seems technically sound, its performance is not that satisfying. In my experiments, the prediction DE values differ a lot on the known compounds. Here, I would like to point out that comparing the DE values computed by Limma on the predicted bulk expression and the provided DE values is a solid validation metric, which would be less affected by the over-fitting problem. Anyway, I found the computed DE values have a larger mean compared with the provided ones. I wonder if it is because the Limma is applied on different sets of compounds (i.e., partially on the provided DE values but fully on the private DE values). Thus, I scaled the DE values computed by Limma to have the same mean on the known compounds. Such a paradigm ends up in the best MRRMSE of 0.587 on the public test split and 0.809 on the private test split. 3.3 Ensembling Ensembling the results of the above two different paradigms led to 0.563 MRRMSE on the public test split and 0.755 on the private test split. By further ensembling the EDA results of 0.567 public score, the performance further improved to have 0.552 MRRMSE on the public test split and 0.728 on the private test split. The performance gain by ensembling results of different paradigms is surprising. Yes, my best history submission is even better than the 1st place in the leaderboard. However, it does not correspond to the best public score so I did not choose it as the final submission. 4. Robustness For the DE value prediction paradigm, I tried removing compounds with extreme values such as \"MLN 2238\", but did not observe performance improvements. Adding noise to the inputs is less effective than adding the Dropout layers into the network. For the bulk expression paradigm, I tried removing the most dissimilar cell type \"T regulatory cells\" when training the model. Interestingly, the results were almost not influenced. The stronger robustness of the bulk expression paradigm could probably be attributed to the larger training data sample number. As for directly predicting the DE values, hundreds of data samples are too little to predict thousands of genes. 5. Documentation, code style, and reproducibility The code and documentation can be accessed from https://github.com/Yunfan-Li/PerturbPrediction . Please sign in to reply to this topic. comment 1 Comment Hotness Liuda Cheldieva Posted 2 years ago · 207th in this Competition arrow_drop_up 1 more_vert May I have access to the submission.csv  file 0.736?",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Zhijian Li · 7th in this Competition  · Posted 2 years ago arrow_drop_up 14 more_vert 7th Place Solution for the Open Problems – Single-Cell Perturbations Thank the organizers for hosting this interesting competition and congrats to the Winners. It was a big surprise for me to see my resolution finally achieve 7th place. Context https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/data Overview of the approach My approach has two major steps: i) learn embeddings for all cell types, small molecular, and genes,  ii) use embedding as features to train a deep learning model to predict the target while accounting for overfitting. Throughout the competition, I only used a fully connected network with three layers. My best submission is also based on a single FC model. Learning embeddings In this step, the goal is to learn a specific embedding for each cell type, molecular, and gene. I was strongly inspired by this paper , where the authors used deep tensor factorization to learn a dense, information-rich representation for cell type, experimental assay, and genomic position. Basically, I used the same approach but played around with the model architectures and parameters, such as the number of latent factors, and the number of dimensions of the network, as well as how to combine the features (e.g., concatenate vs. additive). In the end, my mode is the following: class DeepTensorFactorization (torch.nn.Module): def __init__ ( self, \n                 cell_types, \n                 compounds, \n                 genes, \n                 n_cell_type_factors: int = 4 , \n                 n_compounds_factors: int = 16 , \n                 n_gene_factors: int = 128 ,\n                 n_hiddens: int = 2048 ,\n                 dropout: float = 0.1 ): super ().__init__() self .cell_types = cell_types self .compounds = compounds self .genes = genes self .n_cell_types = len (cell_types) self .n_compounds = len (compounds) self .n_genes = len (genes) self .n_cell_type_factors = n_cell_type_factors self .n_compounds_factors = n_compounds_factors self .n_gene_factors = n_gene_factors self .cell_type_embedding = torch.nn.Embedding( self .n_cell_types, self .n_cell_type_factors) self .compound_embedding = torch.nn.Embedding( self .n_compounds, self .n_compounds_factors) self .gene_embedding = torch.nn.Embedding( self .n_genes, self .n_gene_factors) self .n_hiddens = n_hiddens self .dropout = dropout self .n_factors = n_cell_type_factors + n_compounds_factors + n_gene_factors self .model = nn.Sequential(nn.Linear( self .n_factors, self .n_hiddens),\n                                   nn.BatchNorm1d( self .n_hiddens),\n                                   nn.ReLU(),\n                                   nn.Dropout( self .dropout),\n                                   nn.Linear( self .n_hiddens, self .n_hiddens),\n                                   nn.BatchNorm1d( self .n_hiddens),\n                                   nn.ReLU(),\n                                   nn.Dropout( self .dropout),\n                                   nn.Linear( self .n_hiddens, 1 )) def forward ( self, cell_type_indices, compound_indices, gene_indices ):\n        cell_type_vec = self .cell_type_embedding(cell_type_indices)\n        compound_vec = self .compound_embedding(compound_indices)\n        gene_vec = self .gene_embedding(gene_indices)\n\n        x = torch.concat([cell_type_vec, compound_vec, gene_vec], dim= 1 )\n        x = self .model(x) return x content_copy To train this model, I used all the data from de_train.parquet and converted the table as follows: # convert to long df df = pd.read_parquet( 'de_train.parquet' )\ndf = df.sort_values([ 'cell_type' , 'sm_name' ])\ndf = df.drop([ 'sm_lincs_id' , 'SMILES' , 'control' ], axis= 1 )\ndf = pd.melt(df, id_vars=[ 'cell_type' , 'sm_name' ], var_name= 'gene' , value_name= 'target' ) content_copy The training data looks like: Here, I trained the model with 100 epochs without validation, because I will only the embedding layer for Step 2. To check if the model learns meaningful embedding for cell types, molecules, and genes, I also visualized the embedding with UMAP. For example, below is the 2D UMAP of gene embeddings: By eyeballing, it looks like there are some structures for the genes. Predicting target Once I obtained the embeddings, I trained another model to predict the target, and this model was also used to generate the final submission. Again, I used a FC network as follows: class PerturbNet (torch.nn.Module): def __init__ ( self, \n                 n_input: int = 148 ,\n                 n_hiddens: int = 2048 ,\n                 dropout: float = 0.5 ): super ().__init__() self .n_input = n_input self .n_hiddens = n_hiddens self .dropout = dropout self .model = nn.Sequential(nn.Linear( self .n_input, self .n_hiddens),\n                                   nn.BatchNorm1d( self .n_hiddens),\n                                   nn.ReLU(),\n                                   nn.Dropout( self .dropout),\n                                   nn.Linear( self .n_hiddens, self .n_hiddens),\n                                   nn.BatchNorm1d( self .n_hiddens),\n                                   nn.ReLU(),\n                                   nn.Dropout( self .dropout),\n                                   nn.Linear( self .n_hiddens, 1 )) def forward ( self, x ):\n        x = self .model(x) return x content_copy To prevent overfitting, I used each of the cell types as validation data, which means my model was based on 4-fold cross-validation. For the compounds, I followed the notebook here to select the private test compounds for validation: for key, cell_type in cell_type_names.items(): print (cell_type) # split data for training and validation, here we used the private test compounds for validation df_train = df[(df[ 'cell_type' ] != key) | ~df[ 'sm_lincs_id' ].isin(privte_ids)]\n    df_valid = df[(df[ 'cell_type' ] == key) & df[ 'sm_lincs_id' ].isin(privte_ids)]\n\n    df_train = df_train.sort_values([ 'cell_type' , 'sm_name' ])\n    df_valid = df_valid.sort_values( 'sm_name' )\n\n    df_train = convert_to_long_df(df_train)\n    df_valid = convert_to_long_df(df_valid)\n\n    df_train.to_csv( f'../../results/PerturbNet/splited_data/train_ {cell_type} .csv' )\n    df_valid.to_csv( f'../../results/PerturbNet/splited_data/valid_ {cell_type} .csv' ) content_copy For model training, I used the following strategies: # Setup loss and optimizer criterion = torch.nn.MSELoss()\n    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay= 1e-4 )\n    scheduler = ReduceLROnPlateau(optimizer, 'min' , min_lr= 1e-5 ) content_copy I trained one model by using each of the cell types as validation data, and for the final submission, I just averaged the predictions. What didn't work for me During the competition, I spent a lot of time including the features based on prior knowledge, such as single-cell data, molecular structure embedding, and gene embedding for other large models, such as geneFormer. However, they all didn't work out. So my final model was just based on the features learned from Step 1. Source https://github.com/lzj1769/7th_place_solution_Single-Cell-Perturbations https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-01977-6 https://github.com/jmschrei/avocado Please sign in to reply to this topic. comment 4 Comments Hotness Barry Posted 2 years ago · 51st in this Competition arrow_drop_up 5 more_vert Congratulations!!! Dr @zhijianli 🎉🎉🎉!!! You can't imagine how excited I was when I saw your plan!!! It's unbelievable that our thoughts are almost identical.  I think this method of encoding cells, compounds, and genes before making predictions can be more easily promoted, so I also have been spending time on it. The difference is that I tried other encoding methods to encode cells, compounds, and genes. In addition, I expanded the dataset by combining the cell annotation method ( scMMT , a method I wrote that is currently under review) with the 160k PMBCs dataset. However, due to a lack of scale control, the final expanded dataset seems to have had some negative impact on me. At the beginning, it seemed to be effective, but I didn't grasp the scale well, expanded too much, and ultimately had a negative impact. I also attempted to encode cell types using the principal components of RNA expression levels for each cell type in a multimodal dataset(As shown in the following figure), but it didn't work well and I didn't find a obvious better encoding method for cell types than onehot encoding.  So I used a combination of onehot and PCA1 methods. I see that you also used the \"sc.pl.embedding\" operation but I didn't, perhaps this is the reason why my cell coding is not ideal. I used to think they contained an equal amount of information, so I didn't try to it. I guess it's the effect of cell count, and I'm still searching for the real reason. In short, your method has been of great help to me, but due to my limited skills, I need to take the time to understand your coding methods. I will study your method carefully and hope to have the opportunity to learn and exchange more ideas with you in the future!!!🙏🙏🙏 Zhijian Li Topic Author Posted 2 years ago · 7th in this Competition arrow_drop_up 4 more_vert Hi @songqizhou Thanks for your comments, and I am glad that we converged on a similar approach. Regarding 'encode cell types using the principal components of RNA expression levels for each cell type in a multimodal dataset', this also didn't work out for me. I tried two different ways, i.e., 1) generate PCA for each single cell and then use average PCs the each cell type, 2) first get pseudo-bulk profiles for each cell type and then do the PCA. It turns out they just didn't boost my model. Sorry for the bad organization of my code repository; I am working on it to have a clean version, which should be much easier to understand. Finally, good luck with your submission! Barry Posted 2 years ago · 51st in this Competition arrow_drop_up 2 more_vert Thank you very much, Dr Li! Your code is as beautiful as poetry. Best regards to you! This comment has been deleted. This comment has been deleted.",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules BBco · 8th in this Competition  · Posted 2 years ago arrow_drop_up 7 more_vert 8th Place Solution for the Open Problems – Single-Cell Perturbations Competition Congrats to all the winners, and thank you to Kaggle for organizing such an interesting competition. And also thanks to the other kagglers who shared their ideas and notebooks. Context Competition Overview Competition Data Overview of the Approach Model For model development, I designed and fine-tuned a simple neural network through a series of experiments, aiming to reduce both CV and LB scores. I used augment features from the notebook - [3] OP2 - Feature Augment & Fragments of SMILES as model inputs. Here is the model architecture: class SingleCellModel (nn.Module): def __init__ ( self, dim_size, mul_ratio, labels= 18211 ): super ().__init__() self .ce_layer = nn.Linear(labels, dim_size) self .sm_layer = nn.Linear(labels, dim_size)\n\n        hidden_size = dim_size * 2 self .fc1 = nn.Linear(hidden_size, hidden_size*mul_ratio) self .fc2 = nn.Linear(hidden_size*mul_ratio, hidden_size) self .act = nn.GELU() self .out = nn.Linear(hidden_size, labels) def forward ( self, cell_type, sm_name ):\n        x1 = self .act( self .ce_layer(cell_type))\n        x2 = self .act( self .sm_layer(sm_name))\n\n        x = torch.concat([x1, x2], dim=- 1 )\n\n        x = self .act( self .fc1(x))\n        x = self .act( self .fc2(x))\n\n        x = self .out(x) return x content_copy Data Augmentation The training process invloved a strategic approach to data augmentation. Initially, I employed only mean values for the cell_type and sm_name, respectively. Subsequently, I explored various statistical values such as median, min, max and quantiles. And I found out that median values significantly improves the LB score. Moreover, I experimented with combinations of these features. I implemented 50% random selection between mean and median, and 25% random selection among mean, median, Q1 and Q2 for both cell_type and sm_name. Validation Strategy I used K-Fold cross-validation stratify on cell_type, trained 5, 10, 15, 20 splits. I reviewed that LB score increases with 10 and 15 splits. Details of the submission The results of augmented models are summarized below, along with their respective LB scores. median / 0.549 mean and median / 0.549 mean, median, Q1 and Q2 / 0.551 The final submission was a weighted average of these models by 0.35/0.35/0.3, which boosted up the LB score to 0.547. Since they had different prediction distributions with similiar LB scores, I believed that the ensemble would generalize well in the private. Things that didn't work pseudo labels dropout normalization data selection (control, etc.) Sources Learning single-cell perturbation responses using neural optimal transport https://www.kaggle.com/code/mehrankazeminia/3-op2-feature-augment-fragments-of-smiles Please sign in to reply to this topic. comment 5 Comments Hotness Alexander Chervov Posted a year ago · 13th in this Competition arrow_drop_up 3 more_vert Congratulations with great results ! Would it be possible to share more detailed code - something like Kaggle notebook (preferably) or github ? BBco Topic Author Posted a year ago · 8th in this Competition arrow_drop_up 3 more_vert No problem. Here's the link to the notebook. This comment has been deleted. BBco Topic Author Posted 2 years ago · 8th in this Competition arrow_drop_up 2 more_vert Thanks, glad you find it interesting. This comment has been deleted.",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Davezzq · 9th in this Competition  · Posted 2 years ago arrow_drop_up 10 more_vert 9th solution write-up: Pure NN model 9th solution: pure NN model Hi everyone! I am Dave, this is my first time completing a kaggle competition, and I feel honored to win a gold. I want to use a pure NN model to solve this problem. I would be happy if you find this solution interesting and helpful. Problem definition I have seen many good solutions using each row of the data as one sample, however, I view this problem in a different way. I extracted (cell,sm,gene,value) pairs from the dataset and for each time, the model will predict the object value for a given cell type, sm type, and gene type. The architecture of Model The overview of the model is shown as follows: You can see that I used three kinds of features: sm features, gene features and cell features. Let’s first check how to get the sm features: Here I involved many different kinds of features: MACC: Molecular ACCess System keys, are one of the most commonly used structural keys, check details here . ECFP: extended-connectivity fingerprints, are generated using a variant of the Morgan algorithm, check details here . WHIM: Weighted Holistic Invariant Molecular descriptors, are geometrical descriptors based on statistical indices calculated on the projections of the atoms along principal axes, check details here . sm type: the types of small molecules. sm hba: the number of H-bond acceptors for a molecule. sm hbd: the number of H-bond donors for a molecule. sm rotb: the number of rotatable bonds for a molecule. sm mw: the molecular weight for a molecule. sm psa: the Polar surface area for a molecule. sm logp: the log of the partition coefficient of a solute between octanol and water. Now let's check how to get the gene features: I have involved the PCA of the genes and the additional features. To calculate the PCA feature, I set the n_components as 10 so that I can get a 10-dimensional vector to represent each gene. Then let's check how to get the cell features: Something new here is that I used a GCN layer to help extract the feature contained in the relationship between different cells. The graph used here is very simple: cells are denoted as nodes, so there are only 6 nodes in this graph. Each pair of two nodes has an un-directed edge. I found it helpful in LB. What's more, given this problem is unbalanced, we need to improve the generalization on different cell types, so you may notice that I have involved random noise in the cell features. Another important part is the attention layers, the structure is shown as follows: Now we know the structure of the model, but before directly training them, to improve the performance, I built a larger model based on 3 models with the same structure mentioned earlier: Others I used MSE as the loss function, AdamW as the optimizer, the learning rate is 3e-4, weight decay is 1e-3, and the batch size is 128. loss function: MSE optimizer: AdamW learning rate: 3e-4 weight decay: 1e-3 batch size: 128 CV: 5-fold cross-validation Please sign in to reply to this topic. comment 5 Comments Hotness Alexander Chervov Posted a year ago · 13th in this Competition arrow_drop_up 1 more_vert Thanks for sharing and congratulations with gold medal ! Would it be possible to share code ? Kaggle notebooks (prefarbaly) or github ? Frenio Redeker Posted 2 years ago · 30th in this Competition arrow_drop_up 2 more_vert Thank you for sharing this detailed description of your model and congratulations on making it into the top ten! I also converted the data into long format with cell_type , sm_name , and gene as features and a single target value per row. Yours is the first solution I read that did that, too. Davezzq Topic Author Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Thank you! I'm glad that we share the same view on this problem, I think this way has more freedom, do you think so? Actually, by checking the private score of the history submissions, I found I could get the prize with a score of 0.732, but I didn't choose that submission… Frenio Redeker Posted 2 years ago · 30th in this Competition arrow_drop_up 1 more_vert I'm not sure about the superiority of our approach. Probably, an ensemble using both approaches would be best. If you're interested in my solution, here is the write-up. maxleverage Posted 2 years ago · 21st in this Competition arrow_drop_up 2 more_vert Very interesting, for linear mixture layer of the 3 models did you constrain the weights to sum to 1 or were they unconstrained? Davezzq Topic Author Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Thank you very much! In this competition, I found that unconstrainted weight and without bias for linear mixture layer can help me to improve the score, and the score will drop if I use softmax to constraint the weights, that's interesting😆",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules emoji_people Zhouning Du · 10th in this Competition  · Posted 2 years ago arrow_drop_up 10 more_vert 10th Place Solution for the Open Problems – Single-Cell Perturbations Thanks to Kaggle for organizing such an interesting competition. Thanks to the teammates who fought side by side. And other Kagglers who share various ideas. Context • Competition Overview • Competition Data Overview of the approach Overall, our final submitted result is an ensemble of two parts. Final submission = Part A×0.7 + Part B×0.3 We will explain the composition of Part A and Part B respectively. Part A It is an ensemble composed of neural networks with different structures. Feature engineering After many attempts, we finally adopted the following two features from the public notebook \"OP2: feature engineering\" as our training and testing feature. (1). PCA followed by target encoding (cell type + drug) without noise (pca_target_encoded_features) (2). PCA followed by target encoding (cell type + drug) with noise (pca_target_encoded_features_s0.1) (1) is subjected to PCA on 18,211 target variables and produced features representing cell type means for drugs and cell type means for compounds. And by using features (2) that add random noise, we believe this will make the model more generalizable. Models NN with Fully Connected Layers, as well as BatchNormalization, Dropout, ReLU, and Linear Activation Functions. The initial seed is set to 42 and is fixed. The loss function is mae, the optimizer is Adam. The structure of NN is shown below. tf.random.set_seed( 42 )\n\nmodel = Sequential([ \n    Dense( 1228 ),\n    BatchNormalization(),\n    Activation( \"relu\" ),\n    Dropout( 0.2 ),\n    Dense( 614 ),\n    BatchNormalization(),\n    Activation( \"relu\" ),\n    Dropout( 0.2 ),\n    Dense( 512 , activation= \"relu\" ),\n    Dropout( 0.2 ),\n    Dense( 256 , activation= \"relu\" ),\n    BatchNormalization(),\n    Dropout( 0.2 ),\n    Dense( 128 , activation= \"relu\" ),\n    Dropout( 0.1 ),\n    Dense( 18211 ,activation= \"linear\" )\n])\n\nmodel. compile (loss= \"mae\" , \n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[custom_mean_rowwise_rmse])\n\nhistory = model.fit(full_features, labels, epochs= 450 , verbose= 1 ) content_copy A simple Part A model training process is shown in this notebook \"Single-Cell Perturbations(Part A-Model Training)\" . Model ensemble We use feature (1) and feature (2) to train the network respectively. By changing the feature, the number of network layers, the number of nodes, and the number of training epochs, we successfully obtained a set of individual models scoring 0.567-0.582 on LB. Subsequently, we ensembled 7 models and use it as Part A (LB: 0.556/PB 0.741). In fact, if the weight of each model is determined based on CV during ensemble, we find that the above results can be further optimized to LB0.556/PB0.74. Moreover, the highest score we obtained was (LB: 0.557/PB 0.737) after combining 8 models through this method. Part B This part is also composed of NN and an ensemble of different models. Feature engineering The following two approaches are used to perform feature engineering. ・One-hot encoding on cell_type and sm_name ・SMILES(ChemBERTa-77M-MLM) Models class DnnV5 (nn.Module): def __init__ ( self, num_features, num_targets, hidden_size ): super (DnnV5, self ).__init__() self .conv1d1 = nn.Conv1d(\n            in_channels= 752 ,\n            out_channels= 256 ,\n            kernel_size= 1 ,\n            stride= 1 ,\n            padding= 0 ,\n            bias= True ) self .batch_norm1 = nn.BatchNorm1d( 256 ) self .dense1 = nn.utils.weight_norm(nn.Linear( 256 , 256 )) self .batch_norm2 = nn.BatchNorm1d( 256 ) self .dropout2 = nn.Dropout( 0.3 ) self .dense2 = nn.utils.weight_norm(nn.Linear( 256 , 512 )) self .batch_norm4 = nn.BatchNorm1d( 512 ) self .dropout4 = nn.Dropout( 0.1 ) self .dense4 = nn.utils.weight_norm(nn.Linear( 512 , num_targets)) def forward ( self, x ):\n        b,w = x.shape\n        x = x.reshape(b,w, 1 )\n        x = self .conv1d1(x)\n        x = x.reshape(b, 256 )\n        x = self .batch_norm1(x)\n        x = F.leaky_relu( self .dense1(x))\n\n        x = self .batch_norm2(x)\n        x = self .dropout2(x)\n        x = F.leaky_relu( self .dense2(x))\n\n\n        x = self .batch_norm4(x)\n        x = self .dropout4(x)\n        y = self .dense4(x) return y content_copy The highest score on LB when using model = DnnV5() for each fold is 0.579. For an ensemble of 5 fold + 3 seed, the best LB 0.568 for the above model structure can be obtained. Additionally, \"SCP Quickstart\" was referred to for fold creation. Model ensemble Multiple models were created with different smiles and model structures, and the final ensemble was created with the following ratio. ensemble_submission = 0.5 * (sub0568* 0.3 +sub0570* 0.2 + sub0571* 0.1 + sub0571_2* 0.2 + sub0573* 0.1 +sub0576_1* 0.05 + sub0576_2* 0.05 )+ lbsub567* 0.5 content_copy Among them, lbsub567 referred to the public notebook \"Blend for Single-Cell Perturbations\" . With the above model ensemble, we get a score of 0.560 on LB. We use this ensemble as part B . Based on the above process, our final submission is Part A×0.7 + Part B×0.3 ,     and (LB: 0.554/PB 0.741) is obtained. Things that didn't work •    Pseudo labels •    Feature normalization Please sign in to reply to this topic. comment 3 Comments Hotness Antonina Dolgorukova Posted 2 years ago · 13th in this Competition arrow_drop_up 6 more_vert Congratulation! It's quite amazing since I did use the same features and a very similar model as your part A ( this notebook ). But only with train augmentation (simple multiplication of rows by 50 with Gaussian noise), did I achieve min 0.569 score, and with ensembling 8 models - 0.566. Would be great if you publish a notebook with your part A solution, I have so much to learn! emoji_people Zhouning Du Topic Author Posted 2 years ago · 10th in this Competition arrow_drop_up 6 more_vert Thank you! Your feature engineering ideas are great! And I really appreciate you sharing your ideas and implementation process with us. I also have a lot to learn. I just updated the solution page and added a simple instruction notebook about training Part A: \"Single-Cell Perturbations(Part A-Model Training)\" . Alexander Chervov Posted a year ago · 13th in this Competition arrow_drop_up 1 more_vert Congratulations with gold medal ! Would it be possible to share the notebook(s) for Part \"B\" of your solution  ?",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Alexander Chervov · 13th in this Competition  · Posted 2 years ago arrow_drop_up 65 more_vert #13: U900 team - PYBOOST is what you need We would like to express great thanks to Kaggle and the organizers for creating that exciting (and quite difficult) challenge which is devoted to cutting-edge questions in bioinformatics. Research community will surely benefit from that. And great thanks to all participants and those who shared their ideas, notebooks, datasets, insights… Here is the report on U900 team approach. We follow the guidelines of the report provided by the organizers. The detailed Kaggle-style write-up of the solution is placed in the section 3.2 \"Model design. Details\" - Kagglers may prefer to jump to that subsection directly. Context Competition Overview: https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview Open Problems: https://openproblems.bio/ Table of contents We follow organizer's guideline : Integration of Biological Knowledge Exploration of the problem Model design Robustness Documentation & code style Reproducibility Highlights Main innovative tool  - new gradient boosting algorithm designed for MULTI-target tasks - PYBOOST - developed by team member A. Vakhrushev. Effectiveness to predict thousands targets  at once - distinguishes it from XGBoost, etc. E.g. aftermath: solo PYBOOST model can achieve private score 0.718 - better than top1 - 0.728. Openness and knowledge sharing. Team shared dozens notebooks, posts, datasets during the challenge - obtained: hundreds forks, thousands views, among 10 upvoted code notebooks 4 from the team (in particular top1 ). PYBOOST approach has been openly shared,     medal winning solutions incorporate it and as well as all top scored  publicly open solo-models. We also organized and shared on Youtube webinars around the challenge ( 1 , 2 , 3 ,) (as well as the one in 2022: 1 , 2 … ) - with thousand+ views. Not only PYBOOST:  several neural networks, in depth analysis of cross-validation schemes, methods to carefully control the diversity for models ensemble, non-standard approach to ensemble - forms the solution. Stability: 1) our public and private leaderboard rankings are approximately the same 2) aftermath: correlation between public and private scoring - 0.98. Thus our models are stable and generalize well on unseen data - thanks to careful cross-validation for solo models as well as diversity control of the entire ensemble. In-depth biological knowledge exploration: we performed and publicly shared standard single-cell pipelines analysis with Scanpy and Seurat , cell cycle analysis , top upvoted EDA notebook , created , benchmarked and analyzed 1 , 2 many features like ChemBert, molecular descriptors, Morgan fingerprints, etc… 1. Integration of Biological Knowledge 1.1 Did you use the chemical structures in your model?  Did you use other data sources? Which ones, why? Use of SMILES. One of our key Neural Networks (see section “Family of Neural Networks based on NLP-like SMILES embedding”)  use encoding for compounds based on their SMILES representation.  It  starts with Text Vectorization followed by Embedding layer and thus learns the embedding from the current data. We extended the training set with SMILES augmentation library , unfortunately - no score uplift. Use and benchmark Morgan Fingerprints and Molecular Descriptors, ChemBert embeddings. We encoded compounds by these techniques ( Notebook , Kaggle dataset , EDA1 , EDA2 ). Systematically compared these features with other encodings: ChemBert embeddings, pure machine learning encodings: one-hot, Helmert contrast encoding,  Backward Difference. The tables in the notebook show a bit surprising outcome  that the most simple one-hot encoding is the most effective among those. At  least among those encodings - which are  not incorporating targets,  target encoding techniques are more effective - benchmarked separately .(All these notebooks and datasets were openly shared during the challenge).  Final ensemble did not include these models. DrugBank We also analyzed and shared on Kaggle the DrugBank database ( Kaggle dataset ) with the idea - split compounds by similarity groups and use group indicators as additional features for our models. However due to technical reasons (not all challenge compounds found in DrugBank) and lack of time - that was not implemented.  Aftermath: team#43 reported uplift for Pyboost from the  similar idea. Our other models relied on pure ML technique for encoding compounds and cell types - target encoding. 1.2 What representation of the single-cell data did you use? Did you reduce genes into modules? Did you learn a gene regulatory network? Mainly we worked directly with the pseudo-bulk  differential expressions train dataset provided by the organizers ('de_train.parquet').  Various target encoding techniques (see “model design” section) were employed. Genes reduction by clustering - helps some models Two of  our models included the reduction genes into groups. The genes were clustered by K-means into 3 groups based on input train dataset. Features were constructed by target encoding techniques for  each group and neural networks were predicting each group independently. Concatenation was done at the final step. These models are among our top scored solo models (0.569, 0.570) as well as they allowed us to increase diversity in that family of our models.  See e.g. correlations clustermap for that family of models - the two mentioned above are: N3,4 (\"3kmeans\" in id). Use of raw scRNA-seq counts data Another two our models employed raw single-cell RNA sequencing data. That have been done using aggregation by cell-types and compound the raw counts expressions data, and further PCA and target encoding (see section 6.1. Make-features ).  Thus we created new features which have been used for training the neural networks. These features have been concatenated with the original one - we did not gain the performance, but we gained some diversity and so blend with the original one - brings uplift.   The performance of the original model and the one with raw count features is described in the table - pre-last raw (MLPv15 TE scaled_counts_features) - public score 0.583 - similar to other models. All the models from that table were averaged gaining score 0.573 and that entered as a component to the final ensemble (described in the next table ). 1.3 How did you integrate the ATAC data? Which representation did you use? Integration of single cell ATAC data, or any other single cell (e.g. CITE-seq) data can be done by exactly the same scheme as described and utilized above for raw single cell RNA sequencing count data - aggregation, dimensional reduction (PCA), target encoding. We did not have  enough time to explore these models. 1.4 If adding a particular biological prior didn’t work, how did you judge this and why do you think this failed? Prior bio-knowledge will always contain a kind of \"batch effect\" - different type of cells, donors, conditions, technology so on… Batch effect problem is no so solvable or even well-defined because what can be unwanted batch is one situation, is desired biological effect in the other.       During the Open Problems 2022 we studied a lot how to use various biological prior knowledge  - we and colleagues organized a kind crowd-source activity and participants openly shared with community solutions and datasets based on Reactome pathway database , Protein-protein interaction networks , and so on and so forth. The idea was constructing features based on aggregation by the biologically motivated groups of genes , pre-selecting those which related to targets based on prior knowledge. Followed by modified forward selection addition of these features if the cross-validation scores increases . However the outcomes were less prominent than pure ML approaches by the other teams. It resembles the situation with NLP where key successes of LLM are big models and large datasets - while prior knowledge (linguistic) approaches are not so effective.  As we can see from Open Problems 2021, 2022 and the current  challenges there are always teams on top who rely solely on ML methods. In some sense ML-models extract information from the train data more effectively than our prior knowledge databases. 2 Exploration of the problem 2.1 Are there some cell types it’s easier to predict across? What about sets of genes? Myeloid cells are more difficult to predict than B cells for the current challenge.  (Not surprising biologically). However that is most probably specific to the current dataset. That is quite natural from prior knowledge: B-cells and all cell types from the train - are lymphoid cells, while myeloid is different branch of the blood cells e.g. see hematopoiesis . So B-cells are more similar to train cells than myeloid cells and so it is natural  that prediction for B-cells goes better. Similar we can see from the data (without prior knowledge):  multiple evidence ( e.g. clustermap, umap, etc ) leads to the following picture - NK-cells are the most close to test set, and the most close to B-cells rather than to Myeloid cells, T-regs are the next close, while T-cells CD4+ and especially CD8+ least close . So since NK-cells a) are in the train b) closer to B-cells - hence we see translation goes better for B-cells.  If train set would contain other cell type which is close to Myeloid cells - than it would be opposite. By “most close” we mean with respect to the current data, not the prior biological knowledge. The analysis comparing predictability of B-cells and Myeloid cells is the following: There are 17 samples of each type in the train set - so one can compare local metrics for these samples and see that B-cells are better predicted For test samples we do not have ground truth - but we can compare disagreement between different models predictions  - we see that models quite more often disagree on Myeloid cells rather than on B-cells. See e.g. https://www.kaggle.com/code/antoninadolgorukova/op2-analysis-of-different-models-predictions#-Correlations-between-all-models-included-in-the-final-ensemble Genes The first order of magnitude effect controlling genes predictability   is, of course,   how big are their  values (more precisely how big are the values of their differential expression, since we are working with it)  - bigger values - everything is bigger - prediction errors, variations etc… The interesting question is what are the other effects. Figures here show the analysis. We see that, for each model, especially, Pyboost, there is a subset of genes with big SD and quite low variance, meaning that a model is quite confident in their prediction despite the high variability of DE. Also, each model gives highly variable predictions to a subset of genes with quite low SDs. More details on the analysis added in the post and notebook . Some highlights: All models are less confident in their predictions for myeloid cells compared to B cells (medians of prediction variability across genes and samples are higher). However, the highest bias (differences between predicted and true values) and variability of gene expression change predictions are associated with individual drugs rather than cell types. These drugs are mostly outliers - with the lowest number of cells (≤10 cells), or drugs that affected the cells in such a way that they were misclassified (discovered by @ambrosm in his Excellent EDA ). GO enrichment analysis suggests that the hard-to-predict genes are often related to immune cell activities, cytotoxicity, and cell death. Though it might be some artefact. 2.2 Do you have any evidence to suggest how you might develop an ideal training set for cell type translation beyond random sampling of compounds in cell types? As we understand the question - it is about planning new experiments to cover much higher number of the cell type, comparing to only 6 in the current challenge. With the goal to reduce expansive experiments costs in favor of a cheap computational computational approach. For that question -  the experience of the current challenge  suggests the following: Ideally we should take into account similarity distance between the cell types. Having the similarity - the strategy is the standard one - uniformly subsample train set with respect to similarity distance. In other words (simplified a bit): perform clustering of cell types with respect to similarity distance and choose say 1 representative from each cluster - that would be “ideal” training set.  That ensures that every cell type would have a  “neighbor cell type” belonging to the train set which is similar enough to it and so “translation” would go smoothly. So the key question - what similarity relation for cell-types to consider. We suggest: first run a preliminary experiment with SMALL number of drugs but LARGE number of cell-types - which allows to define similarity for cell types as similarity of their response to drugs. And take that similarity relation as a basis. Rationale and details  behind that suggestion are the following.  The clustermap of cell-types clearly suggests the relations described above: NK-cells close to B-cells and Myeloid, T-cells CD8+ are the most distinct, and the key points are the following: That similarity   is consistent with models results. So: it is defined without any modeling, but  models “respects” it:   e.g. exclude T-cells CD8+ often improves modeling quality - and that corresponds to the fact CD8+ cells are the most different from the others on the clustermap; NK-cells is the best validation fold for some models like Pyboost, etc. - and that corresponds to the fact that NK-cells most close to B-cells and Myeloid cells on the clustermap It is not evident from the prior biological knowledge. So it would be much more cost effective to define the similarity between cell types based on some prior biological knowledge (e.g. just the distance in some umap space for some atlas scale single cell dataset). But experience of the current challenge makes us doubt that such  similarity would perform well on drug response tasks. If experiments are planned “one by one”, but not “all at once”, it is worth considering “active learning” strategy - analyzing results after each step, and  choosing for the next step of experiment those cell types which are in the worst predicted clusters. 3 Model design. We split that section into two parts the first one is devoted to answers to organizer's questions. The second part is detailed write-up of the solution - Kaggler's may prefer to jump directly to the subsection 3.2 3.1 Answers to organizer's questions 3.1.1 Is there certain technical innovation in your model that you believe represents a step-change in the field? PYBOOST - a new innovative gradient boosting tool Which is developed for MULTI-target tasks by team member A. Vakhrushev - we believe an important step-change in a field. It is well-known that for tabular data with SINGLE target gradient boosting (XGBoost, LightGBM, CatBoost) are the top performers - showing better result than e.g. Random Forest, SVR, etc. and even  Neural Networks (neural works are best performing on images, audio, text - some kind of continuous, not tabular data). However these packages are not so effective when one needs to predict many targets simultaneously. PYBOOST resolves that issue providing an effective strategy to predict even thousands of targets at once by a gradient boosting approach. The innovative features of the PYBOOST consists of two parts: strictly-Pyboost - which is software library and the SketchBoost - which is algorithmic innovation which improves algorithmic part of gradient boosting on multi-target tasks. (But for brevity by PYBOOST we typically mean both parts). The software part - strictly-Pyboost - is software library which allows the efficient realization of the complicated boosting algorithms directly in Python utilizing GPU, that means we can write easy to deal Python code, but it will be almost as efficient as low level optimized C-code - because of utilizing the GPU. The second part is algorithmic innovation - \"Sketchboost\" - provides new strategy to speed up tree structure search in multioutput setup by approximating (\"sketching\") the scoring function used to find optimal splits. Approximation is made by reducing dimensions of the gradient and hessian matrices while keep other boosting steps without change, thus enables crucial speed-up for the main bottleneck in boosting algorithm. For more details we refer to the paper , and the webinar . We openly shared the PYBOOST approach with the community during the challenge Notebook , Post .  It gained hundred forks, becoming component of gold-zone solutions as well as other medal winning. Moreover aftermath shows that solo-Pyboost solution combined with ideas by the other teams provides better results than current top1.  Recent top2 solution for the CAFA5 challenge - prediction Gene Ontology terms is also based on Pyboost .  Thus PYBOOST is quite effective for such kind of MULTI-target biological tasks. 3.1.2 Can you show that top performing methods can be well approximated by a simpler model? It depends on the meaning of the “simpler”, let us try two variants for that meaning: Answer 1. Production ready solution expected  not to lose much compared to huge Kaggle-style ensemble 1) One side of the question seems to be: What is the estimated performance loss between Kaggle-style huge ensembles (not production ready) and production-ready reasonable  solutions ? In short - we think the performance loss would NOT be essential - some very rough and pessimistic estimation  can be  - let us say the top gives 0.558, then production ready (with ~2 solo models)  -  0.566, with 3 solo models - 0.563, with 4 solo models 0.559. We also think that appropriate modification of the PYBOOST solution deserves to be considered as the production ready solution, it is high performing, easy to use, maintain and modify. It is typically quite diverse from NN solutions and blend with any NN would uplift the scores. But … But it seems we are not ready to give more precise analysis, because -  strange and unusual things happened - just after the competition closure and based on published solutions and write-ups - new combined solutions breaking current top1 appeared (we followed that route - and demonstrated that solo PYBOOST model beats the top1 ). So in some sense we do not know what are the real  “top performing solutions” - almost surely combining approaches we can go quite further. Nevertheless we hope that it would not change the basic answer - the difference between the huge ensembles and production ready solutions is not expected to be essential. What seems to be essential - the setup with metric (MRRMSE) and preprocessing (LIMMA log-p-values) is not the perfect way. And so we recommend to update that first -  before making any further conclusions for production choices.  To give some details - it seems  that setup MRRMSE + log-p-values is very sensitive to outliers, and that is the reason why see:  solutions “Nothing but just multiplied a factor of 1.2” , leaderboard super-successful  probing  during the challenge and so on. Answer 2 If we understand the question in a slightly different manner: is it possible to approximate top solutions by some conceptually “trivial” ones ? Then the answer is: NO.   It is clear from write-ups that teams incorporate models like Neural Networks, Pyboost, and have non-trivial findings - so we would not call that “trivial”.  Also at the early stage of the competition we have tried more than 50 simple models+feature encodings:  Ridge, SVR, KernelRidge, Catboost, etc…  - but all of them showed results worse than 0.600 - so to break that  barrier one should already do something a bit non-trivial. (The predictions and the analysis were openly shared during the challenge Kaggle dataset link.) 3.1.3 Is your model explainable? How well can you identify what is causing your model to respond to certain inputs? PYBOOST has feature importance estimation as any other boosting  or Random Forest algorithm. For the Neural Networks we can apply the special techniques like activation maps technique to gain certain interpretation. 3.2 Model design. Details We constructed diverse models to gain the stability and better performance. Each has been carefully cross-validated. While ensembling we controlled diversity and preferred to rely on the most stable schemes.  The main innovative part of the solution - is PYBOOST - a new gradient boosting algorithm developed by team member (Kaggle grandmaser) Anton Vakhrushev. 3.2.0 Solution principal components: 1 Family of Pyboost/Catboost models 2 Family of MLP-like Neural Networks employing target encoding 3 Family of Neural Networks based on NLP-like SMILES embedding 4 Analysis of several cross-validation schemes and CV-LB correspondence 5 Multi-stage blend scheme with diversity control and  weights equal to 0.5 at each stage Below we report on each item one by one. 3.2.1 Family of Pyboost/Catboost models Here we describe construction of PYBOOST and CatBoost models - both by the same scheme. PYBOOST performs better, but CatBoost is diverse enough and provide uplift in blend.   The code: Pyboost: the basic baseline notebook ,  other versions of the PYBOOST are in the notebook . Catboost Notebook , version 64, scores 0.584, 0.776 Highlights: PYBOOST “out of box” gives quite good results  (better than “out of box”  our other models), but couple of tricks improves it: Target Encoding by Quantile 80 - that was found by systematic consideration of all target encoders and all their params Retraining on several “ALMOST ENTIRE” train subsets - the logic is simple: we have very few samples - so: retrain on entire train set - helps the model,  but we slightly improved it:  generate several “almost entire” train subsets, train on all of them, average the results. Thus we gain from both - larger train sets and diversity. CatBoost provides diverse enough solutions from Pyboost, even with less performance it is useful in blend. Modeling organization: The core Pyboost and Catboost models are organized as follows (TSVD + TargetEncoder scheme): TSVD reduction of targets to say 70 dimensions (components) Target encoding of cell type and compound by these components Train model to predict these components (NOT the original targets). (For PYBOOST - one model predicts all components at once, For CatBoost - train 70 models - one for each component - it is time consuming, but feasible) Predict TSVD components for the test set. And finally  use TSVD-inverse-transform - to obtain original (genes) targets from the predicted components. The key findings : Quantile 80 target encoder brings significant boost in performance e.g. 0.602->0.586 for Pyboost and CatBoost. Default value - Quantile 50 is significantly worse. That have been found by systematic  consideration of all possible category encoders and all their params. The notebook (openly shared) “Gentle tuner” provides a framework to tune params of the models and encoders together (employing several CV-schemes simultaneously).  First we found that effect for CatBoost ( notebook v61 linked figures )  and then employed for PYBOOST. The subsets for training - critically affect the scores. Idea - “train on multiple ALMOST ENTIRE train subsets”. Motivation: due to the small number of samples many of our models benefit if we retrain them on the ENTIRE train set (before the submission). But that is not the best way, which is - employ ALMOST ENTIRE train subsets, but  SEVERAL of them : I.e. retrain models on 5-10 subsets of the train (each sized  80-99%  of the entire train set) and average the predictions of all these models to get the submission. Thus models benefit from both - more information and diversity. The trick uplifts Pyboost from 0.584 to 0.577 Some details.  Let us emphasize one moment - “CV tuning and submit preparation are DIVORCED” in contrast to the usual Kaggle approach. I.e. The whole process is two staged - first one is standard -  we search for optimal params of the model using the cross-validation. At the second stage - submission preparation - we forget about CV folds and generate new training subsets (these “almost entire train” subsets). We train the model  with SAME params (found by CV) on these subsets and average the predictions. It is important that we do not use early stopping - number of trees/epochs was optimized by CV at the first stage and fixed on the second stage. That allows to retrain on (almost) entire train set - impossible with early stopping. So cross-validation and submission preparation are divorced in contrast to the usual Kaggle approach. The strategy works most probably due to  the small sample number. It is employed for boostings and one of our Neural Networks (Target Encoding based). Exclude T-cells CD8+ One small improvement 0.586->0.584 (but stably seen for other variants of the PYBOOST also) - exclude T-cells CD8+ from the training set. Notes: Pyboost  outperforms CatBoost about 0.010 for that task in equal setups, but their predictions are diverse enough to get uplift  in blend. The standard tuning experiments: Tuning the standard params for boostings - number of trees, max depth, learning rate, etc… as well as number of TSVD components - bring uplift from around 0.604 to 0.602 - so not that much crucial as ones above.  We tried a bit PCA/ICA instead but TSVD but got downlifts. Comments. Comment on TSVD-scheme. Employment of TSVD (or PCA, or ICA) reduction of targets is a more or less standard approach to treat mult-target tasks e.g. widely used in Open Problems 2022. Its obvious benefit is simplification - direct prediction of 18211 targets is not feasible for many  models (except NN). Less obvious benefit ( a bit surprisingly):  it often improves the performance, despite seemingly loss of information reducing 18211 targets to say 70. The reason is:  what is lost -  mostly noise, not the useful information and so reduction to say 70 components - kind of denoises the data and helps the model. We also experimented with PCA/ICA, but TSVD seems better for boostings, while for NN we used PCA.  (See our first notebooks for some experiments. And of course, that is not universal -  depends on the data). Remark (other models): the TSVD-scheme above can be applied for any model - we experimented a lot with Ridge,SVR, Kernel Ridge, LightGBM, Random Forest, ExtraTrees - but only Pyboost and Catboost showed good results for us. Somehow surprisingly, LightGMB was not effective, despite CatBoost was - typically it is not like that.  See “Gentle tuner” public notebook. Comment (Target Encoders - pay attention to LeaveOneOutEncoder): Target encoding is a standard way to treat  categorical features. The idea is to substitute the category by mean (median, quantile, etc) of target with respect to that category.  There are many modifications of target encoding and they have several parameters: Quantile Encoder (respectively), LeaveOneOutEncoder, CatBoost Encoder, James-Stein Encoder. We made systematic benchmarking of the encoders for that task for many models. As said above Quantile80 Encoder uplifts boostings a lot. We should also note that LeaveOneEncoder deserves special attention - for linear and close to linear (SVR, some Kernel Ridges) it stably outperforms other encoders ( tables ). For Boostings it is either the second one (after Quantile80) and even the first one (depending on training set configuration e.g. top public Pyboost 0.574 utlized LeaveOneOut and tricky preparation of the train set). PS Not enough time: PYBOOST predicting directly 18211 targets , i.e. not predicting TSVD-components followed by tsvd.inverse_transform - but just directly. We did not have enough time to tune  params, out-of-box we got 0.594 notebook - not enough score comparing to our other models, so not included in the final ensemble. On the other hand we checked it is quite diverse from the tsvd-based PYBOOST, so we think it is promising to combine these two approaches. We planned to try feature engineering by target encoding not only from TSVD , but from biologically motivated groups of genes, or from most important features  ( as grandmaster Silogram did in 2022 ) but did not have enough time for that. 3.2.2 Family of MLP-like Neural Networks employing target encoding We developed a Neural Network model which features are:  Target Encoding of PCA components. And then we developed a huge number of variations for that basic model. Key ensemble gained 0.566 and included 8 model variations. The main notebook with models: Notebook MLP with Target Encoding Highlights: Easy to diversify the basic model  and benefit from the ensemble of the variations - changing augmentation, noise levels, varying features, training subsets, activations etc. - one obtains models with similar performance, but diverse enough to boost the ensemble (blend) Raw single cell RNA-seq data employed in the same scheme, same can be done for ATAC-seq Model is very stable and easy to implement - various changes do not degrade the performance Genes clustering into groups is easily employed and boost the performance Magic (simple) train duplicating trick improved score significantly: 0.600+ -> 0.580+ Training on \"almost entire\" train subsets boosted 0.580+-> 0.570+; blend boosted to 0.566 Modeling organization and details: Feature creation: Target Encoding of cell-type and compounds by PCA-components, 100 components considered for both Architecture: Multi-layer perceptron with 2 layers (200,256,18211) ; activation: “relu” Prediction scheme: 18211 targets directly (PCA is used for feature creation, but we do not predict PCA components here - in contrast to Pyboost scheme) CV scheme: 5-fold cross-validation scheme - folds containing only  leaderboard  drugs are used, split randomly in 5 groups. Training/Tuning: loss: MAE; optimizer: AdamW; batch size: 256; max learning rate: 0.01, decayed with weight: wd = 0.5 - one-cycle learning rate strategy lr_one_cycle ; epoch number have been tuned and fixed to 20. (Fixed epoch number allows to retrain model on the (almost) entire train set, while early stopping methods forbid that way). Training/Submit: Retrain model on “almost entire” train subsets (i.e. entire train with exclusion 2-3-10 subsamples) Magic (simple) train duplicating trick improved score significantly: 0.600+ -> 0.580+ The strategy to create variations of the basic model employed the following techniques: Changing the training set methods: exclusions of the samples which originate from extremely low numbers (1 or 2) of single cells processed in pseudo-bulk procedure. Genes clustering into groups (e.g. 3 groups by K-means); processing each group separately and concatenating the predictions Augmentation techniques: varying number train duplicates; different noise levels for cell type and compounds; linear combinations of features + targets to create new samples; Params used during the challenge: notebook version 52 .  The precise description of  all 10 variations of the basic model entered in the final submission is here . The diversity analysis of the these variations is here - one can see - some models are quite diverse from the others - correlation score 0.9. We employed the same idea training on \"almost entire\" train subsets as described for PYBOOST above. It boosted scores approximately: 0.580+-> 0.570+. 3.2.3 Family of Neural Networks based on NLP-like SMILES embedding We developed several NN models employing direct encoding of SMILES by embedding layer (technique coming from NLP). The key single model achieved 0.574, another quite diverse model entering the final ensemble -  0.587 and the last hours combination achieved 0.571 score - incorporating  pseudo-labeling technique  (not included in the selected blend submit).   These solutions originate from the public one by Kishan Vavdara though substantially reworked from architectural and training points of view uplifting the score from 0.607 (original) to 0.574 and further. Highlights: Lion - new powerful optimizer - outperformed Adam Magic (simple) train duplicating trick improved score 0.582 -> 0.574 SMILES encoding by the embedding layer Modeling organization (key 0.574 model): The main notebook , the submission with 0.574 (0.766 pricate) score is version 11 . Feature Encoding: SMILES - by Embedding layer, Cell Types - One-hot; both concatenated Architecture: 5-Layer (1558,512,256, 128, 256, 18211) Perceptron with carefully chosen Batchnorm and Dropout layers positions, activation: “elu” Preprocessing: Standard Scaler for targets, Add Gaussian Noise for features Training: Lion optimizer; loss: competition loss - MRRMSE (custom);  5 almost random folds; best (by validation score)  epoch (out of 300)   is restored for each fold - that appears to be quite important Prediction scheme: 18211 targets directly, (TSVD -  not used at all) The trick with duplicating the train for each fold yields 0.582->0.574 uplift. Similar to our other NN models. Tuning: params were optimized by CV The model is defined in notebook section \"The model\" , the next cell contains a figure with the graphical description. So the model organized as follows: SMILES are encoded via the embedding layer and Cell Type via one-hot; both encodings concatenated; that followed by 5-dense-layers perceptron (1558,512,256, 128, 256, 18211) carefully  interchanged with batchnorm and dropout layers; activation is “elu”. We checked the stability of the model as follows. Rerun it with several times with similar params compare CV scores and submit. We stably observed similar CV scores and moreover LB scores at the range 0.581-0.583 - before adding train duplication trick and 0.574 after. That is quite in contrast to the original model - which has  larger score variance:  0.600 - 0.617 at least (see experiments here ). What did not worked well for that version of the NN: SMILES augmentation package LSTM/CNN architectures, other optimizers, pseudolabeling, dropping out noisy samples. The trick to retrain model on the entire train also was not successful for that NN (in contrast to other models) because the epoch number determined by early stopping was different from fold to fold and fixing it to some particular number - degraded the CV-scores and so we did not want to risk employing  the models not having good CV scores. Spent quite a lot efforts to resolve it, but unsuccessful. That family of models also included other variants: It yielded 0.587 score in initial version (included in final blend) notebook1 , notebook2 .  And last hours change yielded 0.571, but not giving significant boost to the entire blend construction (so not included in the chosen submits): Highlight: The 0.571 versions heavily employed pseudolabeling: https://www.kaggle.com/code/bejeweled/op2-u900-part-of-solution-pytorch-tf-nns The other findings are the following: With the LSTM layer after smile embeddings. With sigmoidal range activation as the model output. With multiplication of outputs by coefs. With pseudolabels from these models blends. PS What did not work: we spent quite efforts on Neural Network based on one-hot encoding of the compounds, even achieving local CV-uplift, but LB score still appeared to be 0.619 notebook , changing architecture, augmenting train, changing one-hot to similar encodings: Helmert, Backward difference, etc - nothing worked. We got same LB score as in the early version - which is just   average of many random seeds in the simple version of the net from the public: notebook . The original net seems to be quite unstable - public score varies with the seed 0.599 - 0.620, and not so good results on private. 3.2.4 Analysis of several cross-validation schemes and CV-LB correspondence Here we describe our approaches to cross-validation, analysis of the CV-LB correspondence. More details (tables, figures, etc) can be found in the separate post . CV - LB correspondence is quite problematic in the current challenge. Its better understanding would be important for research community future works. Even aftermath writeups analysis seems to reveal that good solution for CV-LB correspondence is not found yet.  During the challenge several logical CV-schemes were proposed - AmbrosM - discussion , notebook or MT's scheme: discussion , notebook .  MT proposes to put in validation SAME CELL-TYPES as on LB, while AmbrosM proposes SAME COMPOUNDS. However even early analysis showed far from perfect correspondence to LB for both schemes.  Note: we developed and openly shared the Python class which conveniently encapsulates these and other CV-schemes. Here are some our findings: Highlights Local (CV) row-wise correlation score is better  related (0.5)  to LB (mrrmse score) than other metrics Local (CV) mrrmse score is near zero correlated with the LB (mrrmse score)  for all CV schemes considered NK cells local mrrmse is better  correlated to LB ( 0.2+), while for T-cells CD8+ it is negative (-0.1+) NK-cells local mrrmse is well related with LB for Pyboost models, but not for other e.g. NN models Random folds are NOT worse than more logical and sophisticated CV-schemes; and seems preferable for NN models Public and private LB scores -  highly correlated:  0.98, despite poor CV-LB correspondence So, there seems to be many surprises: despite the LB metric is mrrmse - the best locally related to it - is the OTHER metric - row-wise correlation;  while local mrrmse performs near zero. Another surprise - CV-LB correspondence is poor - while public-private LB is very good - 0.98 correlation. And also it is surprising that random folds performs not worse than more logical schemes. Further notes/suggestions: Models of the same nature/features  - the CV-LB correspondence  somehow  working (not so good but still) for all CV schemes. So strategy can be - tune each particular model by CV, verifying by LB - that what we used. The main problem to compare different models - even close models Pyboost and Catboost,  with same public LB scores e.g. 0.584 may show quite different CV score like 0.92 vs 0.89, and even worse for boosting vs NN. So for final blend we decided to rely more on LB score, rather than on CV. Setup. Potential bias. The analysis is based on more than 50 quite diverse models, still it can be biased by models choice. We see very clearly that local metrics better corresponding to LB are quite dependent on the model/features/etc. See further analysis in the separate post . PS To complement: here is table ( from here ) showing correlation of CV and LB for different metrics for a set of SIMILAR models (NN based on target encoding) - we see it is quite high (better for public, less for private): metric corr_vs_public corr_vs_private MRRMSE 0.69 0.39 corr_rows -0.79 -0.53 corr_cols -0.74 -0.48 R2 -0.64 -0.42 Pay attention that for models of diverse nature - correlations are much lower, even for that family of models but with bigger modifications of  feature construction - correlations become much lower (see table ). (See post for more details). 3.2.5 Multi-stage blend scheme with diversity control and  weights equal to 0.5 at each stage Here we describe our approach to ensemble (blend). The more detailed explanations and working code are in the notebook . Highlights: Main problem - absence of good CV - LB correspondence - forbids the usual strategy to choose weights by CV Solution relies on: how to increase and control diversification; how to avoid overfit-proning choice of weights - a scheme of multi-step blend  with the only weight = 0.5 on each step. Measure of diversity - average target-wise correlation of predictions Check by various experiments: models with correlation score 0.8-0.9 - consistently give substantial uplift in blend (about +0.01 - 0.006) Avoid overfit-proning question: how to choose weights of the models with DIFFERENT scores, by the following scheme: Core scheme: sequential blend of the models with EQUAL (almost) scores, giving them EQUAL blend weight (=0.5): step1: LB score 0.575 = 0.5 Pyboost(0.584) + 0.5 Catboost(0.584) step2: LB score 0.566 = 0.5 step1 (0.575 ) + 0.5 NN-NLP (0.574) step3: LB score 0.559 = 0.5 step2 (0.566 ) + 0.5 MLP_TargetEncEnsemble (0.566) final polishing: 0.558 - blend more Pyboost (0.574,0.577) and NN (0.569,0.570,0.572,0.587) models: Experiments with other blend ideas: Different weights for B-cells and Myeloid cells (partially successful) Tried, but had not enough time to succeeded: Estimate variance and correlations for each target (or each row) of predictions and choose blend weights according to modifications of the classical statistical formula - weights are proportional to variance - bigger variance - less confidence - lower weight in blend 4. Robustness 4.1 How robust is your model to variability in the data? Here are some ideas for how you might explore this, but we’re interested in unique ideas too. The robustness of our models can be advocated e.g. as follows. Our public and private leaderboard ranking is approximately the same, moreover it corresponds to our ranking during last weeks of challenge (ignoring the effect of public-LB-probing notebooks appearing at the end). Aftermath: we computed the correlation between public and private scores of our submits and it is 0.98. So models well generalize on the unseen data. Additionally we performed the following tests for most of our models during the challenge - changed params a bit and made submissions - the variations was always around 0.001-0.002. All the models have been optimized by local cross-validation scores and only then submitted to LB, we accepted only those changes which improve both CV and LB. 4.2 Add small amounts of noise to the input data. What kinds of noise is your model invariant to? Bonus points if the noise is biologically motivated. Gaussian noise has been included at the feature generation stage for our neural network models. We tested several  values of noise magnitude and chose the optimal values of the noise level. 5. Documentation & code style The code is documented in the notebooks. The section 3.2 \"Model Design -  Details” here  provides quite detailed description of the solution. 6. Reproducibility Source code is here available in the notebooks: Pyboost: the basic baseline notebook ,  other version of the PYBOOST are in the notebook . Catboost Notebook , version 64, scores 0.584, 0.776 MLP-like Neural Networks employing target encoding Notebook , params used during the challenge: notebook version 52 . Neural Networks based on NLP-like SMILES embedding - the main notebook , the submission with 0.574 (0.766 private) score is version 11 . Blend: Notebook , selected final submit is version 15 - direct link . Most of our submissions can be found in the Kaggle datasets submits and out-of-fold predictions , Open Problems 2 Submits, etc Information on all submits with public and private scores is in the file Our initial PYBOOST notebook has been openly shared and forked about 100 times, being a component of top public solo models as well as  many medal winning solutions - probably the best indication of the reproducibility. Concluding remarks MRRMSE and log-p-values - may not be the perfect choice The metric mrrmse and preprocessing - log-p-values by Limma - seems to cause certain problems. It seems that combination - mrrmse and log-p-values is too much sensitive to outliers. During the competition - the leaderboard has been probed too easily.  It is also quite unusual appearance of the better than top1 late submits just 1-2 days after the end and medal zone solutions like “Nothing but just multiplied a factor of 1.2”. All that indicates: a) we (as a community) not fully understand the problem b) the metric was not chosen perfectly. We are not fully convinced that the argument that p-values allow to catch difference in distributions, while log-fold change will capture only the difference in averages between distributions - that would be the case for p-values of the concordance criteria like KS or Chi2, but it seems  p-values by Limma capture only the difference in averages. What should be the proper choice of the metric and processing ? - seems to an interesting and important question. small number of samples - but sill stable - how to anticipate ? It seems the small number of  samples was really frightening and prevented participation of many experienced Kagglers at the challenge. Small number of samples typically leads to high instability and shake-up at the end, thus people not willing to invest their time with big chances to be randomly ranked at the end. However, surprisingly,  that seems to have appeared to be  a mistake. There were only moderate changes in ranking for the leaders, aftermath shows quite high correlation 0.98 between public and private leaderboard scoring. So in some sense, a small number of samples was compensated by a large number of targets and overall ensured certain stability. Could be anticipated from the beginning i.e. despite small number of samples overall predictability is quite stable ? Overall “Open problems” team and Kaggle team are doing great job bringing cutting-edge datasets to community consideration and thus allowing to contribute the cutting-edge scientific research. We are happy to be a part of that activity. Please sign in to reply to this topic. comment 5 Comments Hotness Liuda Cheldieva Posted 2 years ago · 207th in this Competition arrow_drop_up 3 more_vert Would it be possible for you to provide me with a link for every submission in blend? Alexander Chervov Topic Author Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert Thanks for your question ! The main submits (in particular those in the final submit) are collected in the Kaggle dataset: https://www.kaggle.com/datasets/alexandervc/open-problems-2-submits-etc The blend (ensemble) notebook is here: https://www.kaggle.com/code/alexandervc/op2-u900-team-blend; the selected final submit is version 15 - direct link: https://www.kaggle.com/code/alexandervc/op2-u900-team-blend?scriptVersionId=153084243 Please advise me if you need something else. PS Some more submits  (partly interseсting with the above mentioned, mainly by the \"MLP-like Neural Networks employing target encoding\")   are collected in the datasets : (with oof predicts): https://www.kaggle.com/datasets/antoninadolgorukova/op2-submits-and-yoof And in: https://www.kaggle.com/datasets/antoninadolgorukova/op2-submissions PSPS Information on ALL submits of out team with public and private scores is in the file -  see also  the notebook . Liuda Cheldieva Posted 2 years ago · 207th in this Competition arrow_drop_up 1 more_vert Can you please send me the link to Pyboost 0.718 private LB? Alexander Chervov Topic Author Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert https://www.kaggle.com/code/alexandervc/op2-explore-4th-place-magic That is 4-th place Magic applied to AmbrosM Pyboost on t-scores Alexander Chervov Topic Author Posted 2 years ago · 13th in this Competition arrow_drop_up 2 more_vert To complement the main post: Here is PYBOOST/SKETCHBOOST Nips-poster attached (see under the post) and screenshotted: Here is demonstration of speed-up Pyboost achieves comparing to CatBoost, XGBoost. (All three for GPU). More details in the paper and webinar . Boosting_NeurIPS (8).pdf",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Jie Wu · 15th in this Competition  · Posted 2 years ago arrow_drop_up 7 more_vert Public 7th and Private 15th solution (Nothing but just multiplied a factor of 1.2) First many thanks go to those who created 0.574 and 0.577 public notebook. While I started this competition 2 months ago, I found the public LB is very variant, it’s very different from CV. Ensemble some worse LB results like everyone (or most people) found that ensemble results with 0.702 LB will boost the result. So I thought this would be a shake competition. Hence, I didn’t spend too much time on how to build a more diversity or robust model (I didn’t think I could), but I focused on some LB probing or tricks. For instance, multiplying a factor to any of my results (the best one is 1.2 on public LB), I could boost my results. This made me more confident that there would be some big shake up, but I also believed some of the gold medal teams will be quite stable, they will stay there. I used public 0.574 and 0.577 results + some of my own models (Public LB 0.578). For my own models: •    Conv1D NN •    LSTM •    MLP •    LGBM Features: •    Standard Scaler train label columns for NN models •    One-hot encoded cell_type,sm_name •    Split SMILES to character and use TFIDF to get embedding. Final results: Step1 •    sub_pub[:128] = 0.55 Public 0.574[:128] + 0.45 public 0.577[:128] •    sub_pub [128:] = 0.6 Public 0.574[128:] + 0.4 public 0.577[128:] •    then postprocess it using what’s done in https://www.kaggle.com/code/jeffreylihkust/op2-eda-lb Step2 •    final_sub = 1.2 (0.95 sub_pub + 0.05*my_0578) About the factor: I tried factors of 0.95, 1.05, 1.1, 1.15, 1.2, 1.25, 1.3, 1.4, 1.5, but 1.2 gives best public LB. A bit pity, I have a few results in the gold area, but their public LBs is 0.02 worse. Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Pablo Rodriguez-Mier · 16th in this Competition  · Posted 2 years ago arrow_drop_up 7 more_vert 16th Place Solution Writeup for the Open Problems – Single-Cell Perturbations (Los Rodriguez) We finally had some time to writeup our strategy for the OP2 challenge. It was a super engaging competition, and we're really thankful to both the organizers and fellow competitors for making it such a blast! The whole experience taught us a ton, and we're happy to share what we did/discover along the way. Can't wait for the next challenge! Context In this competition, the main objective was to predict the effect of drug perturbations on peripheral blood mononuclear cells (PBMCs) from several patient samples. For convenience, we have created a Python package with the model here https://github.com/scapeML/scape . Business context: https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview Data context: https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/data Overview of the approach Similar to most problems in biological research via omics data, we encountered a high-dimensional feature space (~18k genes) and a low-dimensional observation space (~614 cell/drug combinations) with a low signal-to-noise ratio, where most of the genes show random fluctuations after perturbation. The main data modality to be predicted consisted of signed and log-transformed P-values from differential expression (DE) analysis. In the DE analysis, pseudo-bulk expression profiles from drug-treated cells were compared against the profiles of cells treated with Dimethyl Sulfoxide (DMSO). In addition, challenge organizers also provided the raw data from the single-cell RNA-Seq experiment and from an accompanying ATAC-Seq experiment, conducted only in basal state. At the beginning of the challenge, we tested different models using the signed log-pvalues (“de_train” data) alone, such as simple linear models, ensembles of gradient boosting with drug and cell features, conditional variational autoencoders, etc. We soon realized that a simple Neural Network using only a small subset of genes to compute drug and cell features (median of the genes grouped by drug and cell) was enough to have a competitive model. The figure above shows the final architecture used for our submission. We used a Neural Network that takes as inputs drug and cell features and produces signed log-pvalues. Features were computed as the median of the signed log-pvalues grouped by drugs and cells, calculated from the de_train.parquet file. Additionally, we also estimated log fold-changes (LFCs) from pseudobulk expression, to produce a matrix of the same shape as the de_train data but containing LFCs instead. We also computed the median per cell/drug as features. Similar to a Conditional Variational Autoencoder (CVAE), we used cell features both in the encoding part and the decoding part of the NN. Initially, the model consisted of a CVAE that was trained using the cell features as the conditional features to learn an encoding/decoding function conditioned on the particular cell type. However, after testing different ways to train the CVAE (similar to a beta-VAE with different annealing strategies for the KL term), we finally considered a non probabilistic NN since we did not find any practical advantage in this case with respect to a simpler non-probabilistic NN. Neural Net We created a method to parametrize the architecture of the NN and the feature extraction from different data sources. This is the code needed to create the NN through the scAPE library specifically created for this challenge: https://github.com/scapeML/scape/blob/222b19f47a32afb8d157aecd5e46b23d90b73e9d/scape/_model.py#L678 We used n_genes=64 (top 64 genes sorted by variance across conditions).This generates a NN with 9637475 parameters (36.76 MB). The inputs are computed from de_train and from log-fold changes calculated from pseudobulk. Cell features are duplicated both in the encoder and decoder. We did some permutations to estimate the distribution of CV errors, permuting drug and cell features, also in the encoder and decoder part. Drug features have more impact on the final error (something to expect since there are 146 datapoints per cell type + B/Myeloid drugs). For the cell features, in general we observed that when used through the encoder and decoder, the NN places more importance in the cell features on the decoder rather than the encoder. This might suggest that cell features are more important for performing a conditional decoding of the drug features which is cell-type specific. Model selection Using the previous NN, we did a leave-one-drug-out for NK cells, which resulted in 146 models. We used the median of the predictions from the 146 models on the cell/drugs for the submission to generate what we call base predictions . The idea of using this strategy is motivated by the fact that NK cells are the most similar ones to B/Myeloid cells. The other advantage of adopting a leave-one-drug-out approach for NK-cells is that it allows us to estimate how well the model generalizes to unseen drugs, on a per-drug basis per cell type. We also observed that in general, the median was much better than the mean for aggregating the results of the 146 models, since it is more robust to outliers (some models did not generalize well on some drugs, and early stopping selected bad models in those situations). We also trained a second neural network with the same hyperparameters, but this time using only the top 256 most variable genes and focusing on the 60 most variable drugs. In this second set of predictions, instead of predicting the 18211 genes, the NN predicts only the top 256 genes used as inputs. We did this because we realized the NN was learning to decide if there was an effect on a given cell type from a small set of genes (essentially, determining where to place values close to 0 in the matrix). We reasoned that training again on only a subset of the data, where most of the changes were concentrated, would help increase performance for that subset of genes. We generated 60 models and computed the median of the predictions, which we referred to as enhanced predictions . We finally replaced the base predictions with the enhanced predictions (on the subset of genes/drugs). For the final submission, to be more conservative, we mixed the predictions in 0.80/0.20 proportions (0.80 given to the enhanced predictions). We tested this strategy with several different base predictions, and it always resulted in a boost of performance, which was also the case on the private leaderboard. A reproducible notebook for the submission is available at: https://github.com/scapeML/scape/blob/main/docs/notebooks/solution.ipynb . One limitation of this strategy is that most of the trained models are very similar, and the blending with the median is very conservative. We also tested different CV strategies, and we found that using blendings of models trained on a 4-CV setting with handpicked drugs on both B/Myeloid cells provided better results in the private leaderboard. However, we didn't trust this strategy that much since it was not very stable and it was hard to understand how well those models were performing in particular cell/drugs combinations. Baselines We think that having simple baselines is important to understand 1) if the model works, and 2) how good it is. We decide to use two simple baselines: predicting zeros (as the baseline used in the competition, which achieves a 0.666 error in the public LB), and the median of the genes grouped by drugs (computed on the training data). The second baseline is more informative. We combined those baselines with our leave-on-drug-out strategy to produce plots per drug, so we could have an upper bound estimation of the generalization for each drug. Here is an example for NK and Prednisolone: Specific questions 1. Use of prior knowledge We decided against using LINCS data in our model because it primarily focuses on cancer cell data, which tends to have a molecular state quite distinct from the PBMCs we were investigating. Despite our exploration of published work and datasets related to predicting drug-induced changes in single-cell states, none of them encompassed the vast array of drug perturbations examined in the challenge. Additionally, we chose not to integrate external data into our approach due to concerns about handling batch effects caused by differences in laboratory settings, protocols, and other related factors. We've also tried to use ATAC-seq with no success. We believe that this data would be useful in the case of not having any measurement for B/Myeloid. However, more informative than ATAC-seq are the actual perturbational profiles on the small subset of drugs on those cells. Here is a summary of different features we tested: Dummy binary variables for cell types and drugs. Basal omics features, including average expression in DMSO and average accessibility per the ATAC-Seq data. Summary statistics of the drug response after grouping by cell type and drug, including standard deviation, mean and median. A “raw” fold-change computed over the raw counts of the single-cell RNA-Seq data (this is, without the corrections applied by limma). Centroids of the principal component space of the drug response data, using cell-type and drug as grouping variables. And we obtained the best results using the median of the response after grouping by cell type and drug in combination with the raw fold changes, using only a subset with the most variable genes in the dataset. 2. Exploration of the problem We found that the error distribution for the drugs was more or less even except for the first four drugs, which accounted for 15% of the total error. As expected, we also found out that the response of drugs that were harder to predict was very different from training cell-types in comparison to the test cell-type. For instance, the drug that accounted for the maximum proportion of the error (IN1451), produced a strong response in NK cells, but seemed to have little effect in T cells CD4+, T cells CD8+ and T regulatory cells. Our approach was refined to better understand cell-type errors, aiming to identify the most challenging cell type for accurate prediction. We evaluated 15 drugs across all cell types, selecting 4 at random for testing. This test set was used for cell-type cross-validation, where the model was trained on data from all 15 drugs, excluding the 4 test drugs within a specific cell type. Our method facilitated evaluation of predictive performance for each cell type. Our findings, illustrated in the figure below, indicated that myeloid cells were more difficult to predict than others, corroborating RNA-Seq PCA analysis results. Regarding genes, we investigated if specific biological functions were harder to predict. An enrichment analysis of the top 5% genes with the highest average error in our local CV setup was conducted using MSigDB hallmarks and decoupleR . This revealed that certain hallmarks, such as epithelial mesenchymal transition and TNF alpha signaling, had a significant number of genes with high error rates: 3. Model design We wanted to check if simpler models could perform just as well. So, we cut down the input features in our models. Considering that our architecture was simple already, we aimed to find the fewest input features that could match the local CV performance of using the top 128 genes with the highest variance. Interestingly, we found that models with 8 to 64 input features would achieve similar performance that the model that employed 128 features. Regarding explainability, even though the model is not easily interpretable, we put some extra care in understanding better how the NN behaved through the leave-on-drug-out + baselines, and by doing permutations on the input data after training a model, to asses the impact on the validation loss. We observed that while both components had a direct impact on the performance of our model, the mean of the errors after drug features permutation was higher compared to the average error after cell features permutation. This is something to expect, as we have more data points of gene values per drug (146 data points per cell type except B/Myeloid), but we only have 6 data points grouping by cell type. We used this type of permutation tests to estimate the importance that different features had in the CV error. 4. Robustness Our model included a Gaussian Noise layer from Keras to perturb the input data. We used this to test CV errors for different noise levels. The following figure shows that a gaussian noise of std=0.01 w. This is the value we selected for training the final models: 5. Documentation & code style For convenience, we refactored the code and created a package called “scape” ( https://github.com/scapeML/scape ) using https://pdm-project.org/ , which contains the code that we finally used for the submission. The code is documented using numpydoc docstrings, and we included a series of notebooks using the scape package to learn how to use it and how to manually create the setup for generating our submission. We have put effort into developing a library that allows for the comfortable configuration and parameterization of neural networks, with an automatic mode for calculating diverse features from drugs and cell lines. 6. Reproducibility In order to improve reproducibility, we show how the tool package can be installed and used directly from Google Colab https://colab.research.google.com/drive/1-o_lT-ttoKS-nbozj2RQusGoi-vm0-XL?usp=sharing . We also included an environment.yml file to exactly recreate the environment we used for testing using conda. Sources https://github.com/scapeML/scape https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac016/6544613?login=false https://www.kaggle.com/code/pablormier/op2-biologically-aware-dimensionality-reduction Biology Deep Learning Regression Please sign in to reply to this topic. comment 4 Comments Hotness Nikolenko_Sergei Posted 2 years ago · 79th in this Competition arrow_drop_up 3 more_vert I read with great interest your detailed and in-depth analysis presented in your raitap. Your conclusions and research methodology, look interesting! In examining the data further, I have noticed certain groups of genes that appear to exhibit stronger responses to stressful conditions. These include genes associated with epithelial-mesenchymal transition (HALLMARK_EPITHELIAL_MESENCHYMAL_TRANSITION) and response to hypoxia (HALLMARK_HYPOXIA). In the graphs shown, these groups of genes show increased percentages of expression at various threshold levels compared to other groups, including non-specific \"not housekeeping\" genes. I would be extremely interested in your opinion on this observation. Do you think that the enhanced response of the genes in question may correlate with an error in the data, or could this reflect their actual increased sensitivity to stress? Perhaps there is some biological mechanism that accounts for this behavior of these genes, or it may be due to the peculiarities of the analysis methods? I would appreciate exchanges and additional comments on this issue. My Notebook Special thanks to Alexander Chervov: Notebook . His notebook served as the basis for the analysis. Martin Garrido-Rodriguez Posted a year ago · 16th in this Competition arrow_drop_up 1 more_vert Hi Nikolenko, Thanks for your comment! We mostly focused on exploring biological hallmarks contributing to our prediction error. We did not go further in the functional analysis of the differential profiles per se, but what you found out is definitely interesting. Answering your first question, we also found these two terms in the top of our hallmark analysis, indicating that genes belonging to these processes were harder to predict on average, probably because they have a larger variance. With regards to your second question, the truth probably lies somewhere in between of the two hypotheses. There are definitely molecular programs that allow the cell to respond to environmental stress (chaperones are a good example of this), and hence it would not be surprising to find groups of genes that prepare the cell or adapt the cell to stress conditions. On the other hand, the technology and preprocessing performed here can also influence what type of signal we observe (e.g. the pseudo-bulking step can remove some single-cell information but it is best practice at the moment). Further data would be needed to disentangle whether this signal is biological or technical (maybe using different single-cell technology or preprocessing pipeline). I hope this helps you!!! Los Rodriguez Frenio Redeker Posted 2 years ago · 30th in this Competition arrow_drop_up 1 more_vert Thank you for sharing your solution write-up and generously sharing all those resources! Your post is very well written – I enjoyed reading it. Congratulations on your 16th place! PS: Looks like you found a pretty unique and fairly robust cross-validation scheme. maxleverage Posted 2 years ago · 21st in this Competition arrow_drop_up 1 more_vert wow so 206 model outputs form your submission",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Ferenc Beres · 17th in this Competition  · Posted 2 years ago arrow_drop_up 4 more_vert 17th Place Solution for the Open Problems – Single-Cell Perturbations (ADEFR) We are thrilled to share our solution with the community. Thanks to Kaggle and the Organizers for the opportunity to participate in such an interesting challenge. 1 Context Competition Overview Competition Data 2  Overview Our solution comprises an ensemble of four gradient boosting based regression models , all trained on a shared feature set. Each model is endowed with distinctive hyperparameters and tailored training settings. Regression models are trained individually for each of the 129 compounds in the public/private test set, with the exception of instances where multi-regression is performed using SketchBoost . Local training and evaluation follow a \"one-vs-rest\" style cross-validation approach across four cell types with known target variables: ['NK cells', 'T cells CD4+', 'T cells CD8+', 'T regulatory cells'] . Important features used: Gene-wise PCA derived from the differential gene expression (DGE) table. Mean single-cell gene expression averaged over cell types and compounds, with particular emphasis on the control compound expression levels to enhance results. Gene-wise PCA derived from downsampled single-cell transcriptomics data. We averaged feature values of nearest neighbor genes (neighbors were calculated based on the DGE data). This potentially reduced noise and improved downstream modeling. Moreover, we realized the impact of the number of cells used for DGE per (cell type, compound) on the mean target value per (cell type, compound) . Consequently, we incorporated the number of cells listed in the training data per (cell type, compound) to define importance weights for each training record using the formula: w = 1-1/np.sqrt(1+num_cells['obs_id'][cell_type][compound]) Our final submission is a blend of our solution (more details in the 4 Models section) and a public one with 0.5-0.5 equal weights. 2.1 Things that did not  work Inclusion of multiome data based features Inclusion of external knowledge based on SMILES (we were experimenting with RDKit ) the STRING database sci-Plex , LINCS data 3 Feature engineering 3.1 Input data We extract features from the differential gene expression (DGE) data ( de_train.parquet ) and the single cell transcriptomics data ( adata_train.parquet , adata_obs_meta.csv ): import pandas as pd # differential gene expression (DGE) data tr = pd.read_parquet(data_path + 'de_train.parquet' , engine= 'pyarrow' )\ngenes = list (tr.columns[ 5 :])\nfeats = list (tr[tr[ 'cell_type' ]== 'B cells' ][ 'sm_name' ].unique()) # single cell data, pkl file includes normalized counts trx = pd.read_pickle(data_path + 'adata_train.pkl' )\ntro = pd.read_csv(data_path + 'adata_obs_meta.csv' )\ngenesx = list (trx.index)\nobs = pd.DataFrame({ 'obs_id' :trx.columns}).reset_index().set_index( 'obs_id' )\nobs = obs.join(tro.set_index( 'obs_id' )).sort_values( 'index' )\nX = trx.transpose()\nX = X.join(obs[[ 'cell_type' , 'sm_name' ]])\n... content_copy We exclude single-cell records based on the file provided by Kaggle excluded_ids = pd.read_csv(data_path + 'adata_excluded_ids.csv' )\nexcluded_ids[ 'val' ] = 1 excluded_ids = excluded_ids.pivot(index= 'gene' , columns= 'obs_id' , values= 'val' )\nexcluded_ids = excluded_ids.fillna( 0 )\nexcluded_ids = excluded_ids[trx.columns] filter = trx[[]] filter = filter .join(excluded_ids).fillna( 0 ) filter =  ( 1 - filter ) filter = np.array( filter .astype( bool ))\n\ntrx_ = np.array(trx)\ntrx_ = filter * trx_\ntrx_ = pd.DataFrame(trx_)\ntrx_.columns = trx.columns\ntrx_.index = trx.index\ntrx = trx_\n... content_copy Importantly, we downsample the single-cell data to get an equal number of cells per cell type and perturbation and we use this subsampled data for some of the downstream generated features D = []\nsample_num = X.groupby([ 'cell_type' , 'sm_name' ]).count(). min (). min () print (sample_num, end = ' ' ) for sm_name in X[ 'sm_name' ].unique(): print (sm_name,end = ' ' ) for ct in X[ 'cell_type' ].unique():\n    x = X[X[ 'sm_name' ]==sm_name]\n    x = x[x[ 'cell_type' ]==ct]\n    x = x[genesx]\n    x = x.head(sample_num)\n    D.append(x)  \nD = pd.concat(D,axis= 0 ) content_copy 3.2 Single-cell transcriptomics data based PCA We generate gene representations by performing PCA on this subsampled dataset from sklearn.decomposition import PCA\n\nn_components = 16 pca = PCA(n_components=n_components)\nd = D[genes].transpose() #d = (d - d.mean())/d.std()# in our experiments, normalization didn't improve performance features_pca_genes_sc = pca.fit_transform(d)\n... content_copy 3.3 Mean gene expression based on the single-cell transcriptomics data Mean gene expresison is calculated per cell type and compound X = trx.transpose()\nX = X.join(obs[[ 'control' , 'cell_type' , 'sm_name' ]])\nX = X[X[ 'control' ]] del X[ 'control' ]\nfeatures_mean_expression = {} for cell_type in X[ 'cell_type' ].unique(): print (cell_type, end = ' ' )\n    features_mean_expression_ = X[X[ 'cell_type' ]==cell_type]\n    features_mean_expression_ = features_mean_expression_.groupby( 'sm_name' )[genesx].mean()\n    ... content_copy 3.4 DGE based PCA cell_types_train = [ 'NK cells' , 'T cells CD4+' , 'T cells CD8+' , 'T regulatory cells' ]\nD = tr[tr.apply( lambda x: x[ 'cell_type' ] in cell_types_train, axis = 1 )][genes]\nD = D.transpose()\nD = (D - D.mean())/D.std()\nn_components = 32 pca = PCA(n_components=n_components)\nfeatures_pca_genes = pca.fit_transform(D) content_copy 3.5 Raw DGE values of the 17 compounds known for all six cell types features_dge = {} for cell_type in cell_types: print (cell_type, end = ' ' )\n    d = tr[tr[ 'cell_type' ]==cell_type]\n    d = d.set_index( 'sm_name' )\n    d = d[genes].transpose()\n    features_ = pd.DataFrame() for feat in feats: if feat in d.columns:\n            features_[feat] = d[feat] else :\n            features_[feat] = 0 features_dge[cell_type] = features_\n... content_copy 3.6 Nearest Neighbor based 'smoothing' over genes: from sklearn.neighbors import NearestNeighbors\n\nfeats = list (tr[tr[ 'cell_type' ]== 'B cells' ][ 'sm_name' ].unique())\ntr_genes = tr[tr.apply( lambda x: x[ 'sm_name' ] in feats, axis = 1 )][genes]\ntr_genes = tr_genes.transpose()\ntr_genes = (tr_genes - tr_genes.mean())/tr_genes.std()\ngene_nn = 8 nbrs = NearestNeighbors(n_neighbors=gene_nn+ 1 ).fit(tr_genes)\nnn_distances, nn_indices = nbrs.kneighbors(tr_genes) for cell_type in cell_types: print (cell_type, end = ' ' )\n    d = features_dge[cell_type]\n    d_ = np.array(d)\n    d_nn = np.zeros(d.shape) for ii in range ( len (d)):\n        n_neighbors = nn_indices.shape[ 1 ]- 1 jj= 1 for neighbor in nn_indices[ii, 1 :]:\n            jj+= 1 d_nn[ii,:] += d_[neighbor,:]\n        d_nn[ii,:] = d_nn[ii,:]/n_neighbors\n    ... content_copy 3.7 Smoothed DGE values: We use Ridge regression to reduce the noise for DGE values import numpy as np def get_smoothing_coefs ( data, eps= 0.01 , lambd= 10000 ):\n    eps = eps * np.var(data, axis= 0 )\n    nmat = data + np.random.normal(size=data.shape) * eps[ None , ...]\n    solvemat = np.linalg.inv(nmat.T.dot(nmat) + lambd * np.diag(np.ones(nmat.shape[ 1 ]))).dot(nmat.T)\n    coefs = solvemat.dot(data) return coefs\n\nfeatures_dge_smoothed = {} for cell_type in cell_types: print (cell_type, end = ' ' )\n    d = features_dge[cell_type]\n    d_ = np.array(d).transpose()\n    coefs = get_smoothing_coefs(d_)\n    d_smoothed = (d_.dot(coefs)).transpose()\n    d_smoothed = pd.DataFrame(d_smoothed)\n    d_smoothed.index = d.index\n    cols = [] for col in feats:\n        cols.append( 'smoothed_' +col)\n    d_smoothed.columns = cols\n    features_dge_smoothed[cell_type] = d_smoothed content_copy 4  Models 4.1 Blending different boosting models In our modeling framework, we use various boosting libraries with different settings. LGBM: We predict DGE values for each of the 129 target compounds with different LightGBM regressors . This model achieved a public LB score of 0.571. LGBM_LIN: We added the prediction of a simple Linear Regression as extra features to the previous setting. Public LB score is 0.573. SketchBoost: We performed multi-regression for the 129 target compounds with a single SketchBoost model. We trained this model with the following parameters: {\"ntrees\":4096, \"lr\"=0.01, \"subsample\"=0.5, \"colsample\"=0.5, \"min_data_in_leaf\"=5, \"max_depth\"=5} DART: We also use LightGBM with DART trained for 130 iterations with the following parameters: {\"feature_fraction\": 0.7, \"bagging_fraction\": 0.6, \"data_sample_strategy\":'goss', 'boosting_type': 'dart'} . Furthermore, we average out the predictions of five DART models trained with different random seeds. In our solution, we blend these models with equal (0.25) weights and then combine our prediction with a public one , as described above in the Overview section. 4.2. Feature importance Below, we show some of our feature importance measurements for the SketchBoost component. Observe that almost half of all importance is attributed to DGE-based gene PCA features (3.4). Ridge regression smoothing (3.7) proved to be slightly more effective than KNN-based smoothing (3.6). The remaining importance goes to raw compound DGE features (3.5) and mean expression values (3.3). Finally, we show the top 20 most important features for the SketchBoost component. Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules AmbrosM · 18th in this Competition  · Posted 2 years ago arrow_drop_up 54 more_vert #18: Py-boost predicting t-scores Did you notice that in this competition no real EDA notebook has few real EDA notebooks have been published? Besides explaining my machine learning model, I'd like to share some observations which help understand the data and the intricacies of Limma. Integration of biological knowledge Don't trust the cell types! Let's recapitulate the course of the experiment in a simplified form. We can imagine an experimenter who is in front of a large pot of human blood cells. The pot contains a mixture of six cell types in certain proportions. T cells CD4+ take the largest share (42 %), only 2 % are T regulatory cells: The experimenter now takes 145 droplets out of the large pot. Every droplet contains 1550 ± 240 cells (normally distributed). If we counted the cells per cell type in the droplets, we'd see a multinomial distribution. The 145 droplets might be composed like in the following bar chart (fictitious data, sorted from smallest to largest droplet): In the next step, the experimenter adds 145 substances to the 145 droplets and waits 24 hours. After 24 hours the cells are analyzed. If we count the cells again, we get the following picture, as taken from the competition's training data (cell counts for the test data are hidden): In this diagram we first see that some compounds are so toxic that in some droplets less than 100 cells survive. These droplets are represented by the leftmost bars in the bar chart. The second observation is much more important: The long red part in the bars for Oprozomib and IN1451 show that these droplets contain several hundred T regulatory cells — much more than at the start of the experiment. Other compounds (e.g., CGM-079) have too many T cells CD8+ (green bar). How can we interpret this observation? Does IN1451 incite the T regulatory cells to multiply so that we have five times more of them after 24 hours? No. Does IN1451 magically convert NK cells into T regulatory cells? No. Does IN1451 affect the cells in such a way that they are misclassified? Maybe. Discussing differential gene expression for specific cell types becomes pointless if the cells change their type during the experiment. For the Kaggle competition this means that we have to deal with many outliers: Beyond the at least five toxic compounds, there are at least seven compounds which change the cells' types. Differential expression for these outliers is hard to model. They make cross-validation unreliable, and the outliers in the private leaderboard can't even be predicted by probing the public leaderboard. Cell count shouldn't affect differential gene expression Does gene expression in a cell depend on how many cells are in the experiment? Theoretically, it doesn't. A cell behaves the same way whether there are 10 cells in the experiment or 10000. We'd expect, however, a difference in the significance of the experimental results: An experiment with 10000 cells should give more precise measurements than a 10-cell experiment: As the cell count grows, variance of the measurements should decrease, t-score should be farther away from zero, and pvalues should decrease. The competition data don't fulfill this expectation. If we plot the mean t-scores versus the cell count for the 602 cell type–compound combinations (excluding the control compounds), we see a linear relationship: For every cell type, compounds with lower cell counts have positive t-score means, and compounds with higher cell counts have negative t-score means. This correlation between cell counts and t-scores shouldn't exist. It is an artefact of Limma rather than a biological effect. You can plot the diagram with median or variance instead of mean — it will look similar. You can even compare the cell counts to the first principal component of the t-scores and see the same correlation. We can now put together a list of 20 compounds which are to be considered outliers because of low cell counts. Notice that we don't declare single rows of the dataset to be outliers, but all 86 rows related to the 20 compounds: Outliers -------- AT13387 only 7 T regulatory cells\nAlvocidib                         ≤ 10 for several cell types BAY 61 -3606 mean t-score for CD8+ cells > 2 BMS -387032 only 10 T cells CD8+, no Myeloid cells\nBelinostat                        control compound with too many cells\nCEP -18770 (Delanzomib)            ≤ 10 for several cell types CGM -097 too many T cells CD8+\nCGP 60474 ≤ 10 for several cell types Dabrafenib                        control compound with too many cells\nGanetespib (STA -9090 ) only 4 T regulatory cells, too many NK cells\nI-BET151                          too many T cells CD8+\nIN1451                            ≤ 10 for several cell types LY2090314 only 6 T cells CD8+\nMLN 2238 ≤ 10 for several cell types Oprozomib (ONX 0912 )              ≤ 10 for several cell types Proscillaridin A;Proscillaridin-A ≤ 10 for several cell types Resminostat no T cells CD8+\nScriptaid only 2 T regulatory cells\nUNII-BXU45ZH6LI only 6 T cells CD8+\nVorinostat only 1 T regulatory cell content_copy After removing the outliers, the diagram looks much cleaner. The variance of the cell counts remains. It is a source of noise which impedes the correct interpretation (and prediction) of differential expressions. Maybe we'd get cleaner data if we equalized the cell counts before library size normalization. This would amount to throwing away a part of the measurements, which isn't desirable either. After considering the small size of the dataset, the amount of noise and the Limma artefacts (more of them will be shown in the next section), I didn't try to integrate any external biological data into my model. Exploration of the problem A mixture of probability distributions A histogram of a single row of the training data (18211 t-scores for T cells CD8+ treated with Scriptaid) shows that the distribution is multimodal. The highest mode consists of 269 genes which all have an identical t-score of -3.769. It turns out that these are the 269 genes which are never expressed in T cells CD8+, neither with the negative control nor with any other compound. Isn't this strange? A gene which is never expressed in the whole experiment should have a log-fold change of zero and should not get a t-score at all (because t-score computation involves a division by the variance, and the variance of a never-expressed gene is zero). For Myeloid cells treated with Foretinib, 3856 genes are not expressed (RNA count of zero), yet most of them have a positive t-score. Their highest t-score is 6.228 (resulting in a pvalue of 4e-10 and a log10pvalue of 9.33). If an RNA count is zero, the corresponding log-fold-change (and t-score) should never be positive. We may say that the distribution of the values is a mixture of two distributions: The values for the genes which are expressed (blue) have a more or less bell-shaped distribution. The values for the genes which are not expressed (orange) have a distribution with an unusual shape, and it is strange that positive differential expressions are reported when not a single piece of RNA is counted. What we see here is an artefact of Limma, which affects every row of the datset. It suggests that Limma output can be biased and is not ideal for investigating cell-type translation of differential expressions. Genes expressed in T cells CD8+ Scriptaid: 13560 Genes not expressed in T cells CD8+ Scriptaid: 4651 Mode : - 3 . 804 for 269 genes not expressed at all in T cells CD8+ content_copy Genes expressed in Myeloid cells Foretinib: 14355 Genes not expressed in Myeloid cells Foretinib: 3856 Mode : 6 . 379 for 81 genes not expressed at all in Myeloid cells content_copy An ideal training set In the competition overview, the organizers ask: Do you have any evidence to suggest how you might develop an ideal training set for cell type translation beyond random sampling of compounds in cell types? What is the relationship between the number of compounds measured in the held-out cell types and model performance? I think we are not yet ready to answer these questions. We first need cleaner data (and more of it): Cell types must be classified correctly. This may imply that we limit the scope of the work to compounds which do not hamper cell type classification. Samples containing too few cells must be eliminated from the dataset. These samples just add hay to the haystack where we want to find the needle. Even if we have many cells, genes with low rna counts may need to be eliminated. Otherwise they add even more hay to the haystack. Second, modeling strange t-scores of genes which are never expressed is a waste of time. We need to define a machine-learning task and a metric which reward biological insight rather than forcing people into modeling the noise created by upstream processing steps: As t-scores are always affected by cell counts and variance estimates, a metric based on less highly-processed data (i.e., log-fold changes or rna counts rather than log10pvalues or t-scores) may lead research into a better direction. Even with log-fold changes, genes with low rna count make more noise than genes with high rna count. A suitable metric should account for this fact. Model design T-scores are better than log10pvalues Limma performs t-tests. t-scores are (almost) normally distributed, which is good for machine learning inputs. For this competition, the t-scores were nonlinearly transformed to log10pvalues. The transformation squeezes the nice bell shape into a distribution with a much higher kurtosis. My machine learning models perform better if I transform the log10pvalues into t-score in a preprocessing step, predict t-scores, and transform the predictions back afterwards. Perhaps working with log-fold changes or RNA counts would be even better. The models I developed four models: Py-boost A recommender system based on ridge regression A recommender system based on k nearest neighbors ExtraTrees I first implemented the Py-boost model, derived from @alexandervc 's public notebook. I then implemented the ExtraTrees model, which resembles @alexandervc 's Py-boost model. All the decision trees are fully grown (i.e., overfitted). The model gets its generalization capability from noise which is added to the target-encoded features deliberately. I then implemented the knn recommender system to have some diversity in the ensemble. Cell types and compounds are identified with users and items, respectively; gene expression is identified with item ratings by users. ExtraTrees and k-nearest-neighbors share the weakness that they cannot extrapolate. Even after dimensionality reduction, our training dataset essentially consists of 614 points in a high-dimensional space, so that most of the points will lie on the convex hull. Of the 255 test points, many will lie outside the convex hull of the training points, which means that the model must extrapolate. To bring the extrapolation capability into the game, I implemented the ridge regression model. The models have cv scores between 0.878 (ExtraTrees) and 0.906 (Py-boost). Py-boost, which was the worst in cross-validation, has the best public and private lb scores (0.572 and 0.748, respectively). Data augmentation One of the models (k nearest neighbors) is fed with data augmentation : If we know the differential expressions for two compounds, we may assume that a mixture of the two compounds will produce a differential expression which is the average of the two single-compound differential expressions. I experimented with another kind of data augmentation a well: Because there are more than twice as many T cells CD4+ as either Myeloid or B cells and I knew that the cell count biases the results of Limma, I reduced the cell count of the T cells CD4+, pseudobulked them, ran them through Limma and added the results to the training data as another cell type. This augmentation improved the scores of ExtraTrees, but not to the level of Py-boost. Perhaps I should have combined the additional cell type with Py-boost… Robustness The robustness of my models is demonstrated in two ways: (1) The models are fully cross-validated. The cross-validation strategy, first documented in SCP Quickstart , ensures that the model is validated on predicting cell_type–sm_name combinations so that it knows only 17 other compounds for the same cell type. This cross-validation strategy is more robust than the ordinary shuffled KFold, where the model knows 4/5 of all compounds for the same cell type. (And it is much more robust than a simple train-test-split.) I have to admit, though, that I'm not happy with the cv–lb correspondence. (2) For all models the performance was tested after adding Gaussian noise to the input t-scores. All models are robust against small noise. When the noise gets stronger, the knn and ExtraTrees models suffer more than Py-boost and the ridge recommender system. Documentation and code style The code is documented in the notebooks. Reproducibility Source code is here: EDA which makes sense ⭐️⭐️⭐️⭐️⭐️ SCP #26: Py-boost, recommender system and ET GitHub Conclusion Let me conclude by summarizing the four main messages of this post: Recommender systems are a promising starting point for developing models for cross-cell-type differential gene expression prediction. Because of commercial interests, recommender systems are a well-researched topic, and a lot of information is available. Data augmentation is useful, and mixtures of compounds are a natural approach to data augmentation. Although Kaggle competitions with data cleaning, outlier removal and unusual metrics are entertaining, the research objective would profit from another setting. Providing clean data and scoring with a well-understood metric would help participants focus on the real topic rather than the noise in the data. We have seen that Limma in certain situations produces biased outputs. I hope that professional Limma users are aware of these effects and account for them when interpreting results in their research. Please sign in to reply to this topic. comment 17 Comments 1 appreciation  comment Hotness Arbidos Posted 10 months ago arrow_drop_up 1 more_vert Do you use Py-boost now? Samuel Rutz Posted 2 years ago · 22nd in this Competition arrow_drop_up 8 more_vert I don't think the conclusion that limma is biased is evident. Limma is being used incorrectly/poorly in this competition. An important step of the usual limma application is to remove genes that \"consistently have zero or very low counts.\" using the filterByExpr function from edgeR (see limma userguide). To avoid having NaNs in the data the organizers skipped this step which probably interfered with the TMM normalization. Further the Empirical Bayes procedure in limma is susceptible to genes with very small (e.g. all zero) or very large variance. Phipson et al. 16 suggest a more robust EB method which can be activated using robust=TRUE in the eBayes function but this was not used by the competition organizers. You et al. 23 suggest a modification to voom which accounts for heteroscedasticity between groups in scRNA-seq data and suggest that not doing so leads to problems. This also was not used. Nevertheless I am also very suspect of DE results in general but especially for scRNA-seq data. DE analysis is very tricky and many intransparent assumptions are made which can often lead to problems (e.g. Squair et al. 21 show that many methods produce lots of false positives, Li et al 22 suggest high false discovery rates when there are large sample sizes) All in all I don't think DE results are good prediction targets and I believe that this lead to the competition being focused on accounting for DE computation quirks rather than on predicting biologically meaningful change. Robust hyperparameter estimation protects against hypervariable genes and improves power to detect differential expression Phipson et al 16 Modeling group heteroscedasticity in single-cell RNA-seq pseudo-bulk data You et al. 23 Confronting false discoveries in single-cell differential expression Squair et al. 21 Exaggerated false positives by popular differential expression methods when analyzing human population samples Li et al. 22 Marília Prata Posted 2 years ago arrow_drop_up 5 more_vert \"Did you notice that in this competition no real EDA notebook has been published? \" at the beginning of your topic. That question is remarkable, mostly for those that call every/anything EDA. Now, my PySmiles is laughing at mine Ridiculous Rdkit scPerturbations. Thank you for the topic and the Kaggle Notebook where we can read the snippets. Indescreet question: How long (years of study/formation) to arrive in your professional level? Very likely more than 20 years. In my previous careers, More than 20 years is what is required to achieve such level of knowledge, skills and mostly Experience, leaving the Ego behind to help those that are starting in any field. Thank you so much AmbrosM. Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 3 more_vert Thanks again for the great write-up ! To complement your table of suspicious samples. Here is another evidence that something is wrong with them. Let me look on count of genes which are strongly perturbed for each sample we can expect it should not be too much. But there are samples where we see about HALF(!) genes are perturbed strongly - that is suspicious. And that list quite corresponds to yours: (Also note: that these samples seems to be the WORST predicted - see https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/discussion/461663 ) https://www.kaggle.com/code/alexandervc/op2-eda-new?scriptVersionId=155628891&cellId=10 hoho Posted 2 years ago · 229th in this Competition arrow_drop_up 3 more_vert Congratulations! I like your idea about cell counts and data augmentations. Although Kaggle competitions with data cleaning, outlier removal and unusual metrics are entertaining, the research objective would profit from another setting. Providing clean data and scoring with a well-understood metric would help participants focus on the real topic rather than the noise in the data. I'm the one who focused on the noise in the data and then give up 😅. After reading your post, I deeply regret that I did not even try to find a solution for it. Beyond the tech part, I also learned a lot from your post, thank you for sharing. Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 3 more_vert Combined with magic-top4 trick - your solution gets 0.718 on private LB - which is better than current top1. See https://www.kaggle.com/alexandervc/op2-explore-4th-place-magic Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 3 more_vert Great writeup ! It is always pleasure to learn from your work  ! Very happy that you find my notebook on pyboost useful. Congratulations with the medal ! AmbrosM Topic Author Posted 2 years ago · 18th in this Competition arrow_drop_up 2 more_vert HI @alexandervc , thanks for bringing py-boost to my attention, and congratulations to you, too! Antonina Dolgorukova Posted 2 years ago · 13th in this Competition arrow_drop_up 3 more_vert Did you notice that in this competition no real EDA notebook has been publishedt two of mine are crying in the corner ashamed 😅 Thank you for the great description, I learned a lot, it's very interesting too) And congratulations with the silver! AmbrosM Topic Author Posted 2 years ago · 18th in this Competition arrow_drop_up 2 more_vert Sorry, @antoninadolgorukova , I overlooked them (maybe because I'm not fluent in R). Congratulations to you, too! Antonina Dolgorukova Posted 2 years ago · 13th in this Competition arrow_drop_up 3 more_vert Don't be sorry I just meant what I said) Ashamed I saw strange things in the data but did not dig deeper to find out the reasons. So thank you a lot for the explanations and sharing! C R Suthikshn Kumar Posted 2 years ago · 214th in this Competition arrow_drop_up 3 more_vert Congratulations. Thanks for sharing the nice writeup with colorful graphs and charts. I think if you had provided useful references with your article, it would be helpful. Barry Posted 2 years ago · 51st in this Competition arrow_drop_up 4 more_vert I sincerely thank you for perfect sharing, @ambrosm ! You have provided us with great help, and your method is very impressive! And I have learned a lot of knowledge from your sharing. Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert To complement your analysis on drugs which potentially lead to misclassification of cell types: We can see that: 'Oprozomib (ONX 0912)','MLN 2238','CEP-18770 (Delanzomib)' are concentrated in the same umap-seen cluster.  It is classified by T CD4/8 cells, but we see that cluster is so far-separate from the other cells type clusters, that we can easily imagine that it can be mis-classified (since it is hard to imagine classification algorithms seen such kind of data - they were trained on normal cells , not drug-treated): See: https://www.kaggle.com/code/alexandervc/op2-rna-seq-data-scanpy-adata-cell-cycle?scriptVersionId=156757371&cellId=45 Similar other drugs you found: 'CGM-097', 'LY2090314','Ganetespib (STA-9090)','IN1451' - generate stand-alone clusters, so again it is reasonable to suspect that cell type classification may fail: Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert And to add more details/confirmation about 'Ganetespib (STA-9090)' Your analysis suggest that we have too many NK-cells That indeed quite corresponds to misclassification: we see cluster - it is expected to be one cell-type, but we see two (and one of them - NK-cells - the one appeared in your analysis). Moreover - normally we can expect each cell type contains three donors - but that does not happen here. Here we see one cell type contains two donors another cell type just one donors. That is another  clear indication, that cell types were misclassified. Edward Posted 2 years ago · 43rd in this Competition arrow_drop_up 2 more_vert Thank you, it was interesting to read the detailed description of the data analysis. Your notebooks carried a lot of ideas  in this competition Abhas Malguri Posted 2 years ago arrow_drop_up 2 more_vert Great writeup! And great visualizations indeed! Appreciation (1) VIKRAM MISHRA Posted 2 years ago arrow_drop_up 2 more_vert Heads Up, and thanks for sharing this.",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules IraQbot · 19th in this Competition  · Posted 2 years ago arrow_drop_up 7 more_vert 27th Tabtransformer 🫡 to the shake up survivor Used ensemble of ft transformer of 12 models(same as tab because no numerical) trained on different seed, 5 folds by models all used config picked based on mae (dim range 48 to 64, depth 4 to 8, head 8) Very high dropout 0.4 No augmentation, no use of the blend ressources (i just checked one and i was lost) Hard time to understand how to start and the competition I did not take the competition seriously until i dropped from the top 10 of the public leaderboard because of the blend fog, but was too lazy to do/think anything fancy to get back on track. 1st tabular competition (i am not a fan of that!) bronze to silver next goal is gold ! Please sign in to reply to this topic. comment 2 Comments Hotness Alexander Chervov Posted a year ago · 13th in this Competition arrow_drop_up 2 more_vert Congratulations with medal on Kaggle Single Cell Perturbations Challenge ! I wonder would it be possible to share the code of the solutions ? C R Suthikshn Kumar Posted 2 years ago · 214th in this Competition arrow_drop_up 3 more_vert Congratulations. Thanks for sharing the details.",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules emoji_people Jalil Nourisa · 20th in this Competition  · Posted 2 years ago arrow_drop_up 11 more_vert 20th Place Solution Writeup For Open Problems - Single-cell Perturbations Competition Please find attached our detailed solution write-up, submitted for consideration for the Judge's reward. For your convenience, we have included all necessary citations within the attached PDF document. Additionally, the individual notebooks referenced in our write-up are accessible at the following GitHub repository: https://github.com/AntoinePassemiers/Open-Challenges-Single-Cell-Perturbations/tree/master OP2_Write_Up_JN_AP.pdf Please sign in to reply to this topic. comment 8 Comments Hotness Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert Your analysis of MRRMSE pitfalls is quite striking - in particular presented on Figure 7  ! (screenshotted below here). And related to the questions which bother me a lot: to what extent we are predicting biological effects, rather than various artefacts of Limma/whatever ? What can be good demonstrations that we are predicting biology ? (@malteluecken , @danielburkhardt )  What are the good metrics to measure biological part of prediction ? And what pre-processing is better ? Let me rephrase your analysis as follows - our targets contain - a) small number of big values - bio-signal b) large number of small values - noise (uniform at 0-1 for p-values). The question is:  what dominates the MRRMSE - (a) or (b), i.e. bio-signal or noise  ? Your analysis shows: that MRRMSE is dominated by small values (below say 2, i.e. p-value =0.01) - which are mostly noise, but not biological signal. (These values correspond to p-values above 0.01 - which means not a signal, but just random uniformly distributed noise which arise when null hypothesis is true. Uniform distribution is clearly seen). So it might seem that it strongly forbids use of MRRSME as a bio-meaningful metric, however I am not completely sure about that, because of the following: On the one hand - indeed -  your analysis demonstrates that MRRMSE is bad metric for local training the models, because it is dominated by noisy (not bio meaningful) values and thus forces models to learn the noise-part which is completely meaningless - noise is random - it will not generalize to unseen data. However, I am not so sure how bad it is for evaluating the models performance on the unseen data, because: most probably noise part of values is unpredictable and all models would have more or less random predictions on that part - and so law of large numbers makes all models in a sense equal on that part noisy part, and what makes one prediction better than the other - is prediction of  big biologically meaningful values.  Not sure that is correct argument. Would be happy to hear your opinion. PS May be  we should think how to split genes in \"pure noise\" and \"bio signal\" , choose different strategies to predict the two classes. Any way, let me complement your analysis by the following: Let me consider your  MRRMSE-corrected  metrics with different thresholds: 1,2,3 and  explore how well that LOCAL metric is correlated with the public LB MRRMSE-score. Outcomes - mostly confirm your insight: MRRMSE-corrected is BETTER correlated with LB, than standard MRRMSE . That is especially for @ambrosm CV-scheme, also for random scheme, strangely enough for MT-scheme is NOT the case. See the screenshot: https://www.kaggle.com/code/alexandervc/op2-cv-vs-lb-analysis-u900-team (The analysis might be biased - by choice of the models, on the other hand, my collection of models is quite diverse - but that was done before Pyboost and other strong models.) emoji_people Antoine Passemiers Posted 2 years ago · 20th in this Competition arrow_drop_up 1 more_vert Thanks for the feedback @alexandervc , and thank you for the follow-up investigations you made. The numbers you show are quite intriguing and promising, as they show that our revised version of MRRMSE could be even more relevant for internal CV than we originally thought. It also suggests that I might have been too stringent by using 3 as a threshold, and that a threshold of 1 might be more suitable. But let's remain cautious, because the CV-LB relationship is probably too complex to draw any conclusion here. I would have expected the correlation between CV and LB to increase if both were based on the corrected version of MRRMSE. But let me venture an explanation of why the LB-CV correlation is lower for the corrected MRRMSE when using the MT scheme. With the MT scheme, the positive controls (Dabrafenib and Belinostat) will end up in the validation set much more frequently than in other CV schemes, and these controls carry a lot of signal (many high DE values). Because of this, the CV should in principle penalize conservative models more, and encourage predictions that deviate from the null distribution. However, the test set does not contain positive controls, and therefore contains less significant values. Consequence is that they will penalize the less conservative models a bit more, hence the difference between CV and LB. This probably does not explain everything, but this is my best bet at the moment. Also, your analysis is based on only 47 data points (which I guess took a lot of efforts to collect), which makes me wonder how reliable these correlation coefficients are. Did you compute p-values or confidence intervals? Regarding your suggestion that MRRMSE is probably not as bad in practice thanks to the law of large numbers, I think the intuition makes sense since the sum of noise terms would converge to the same value and therefore appear as a simple \"bias term\" in the MRRMSE that would be the same for all models. However, this implicitly assumes that all models behave the same on the non-DE genes (for example, their predictions all follow a Gaussian distribution with same location and scale), and it is definitely not the case. For most models, the mode of predictions (the \"peak\" in the distribution) is strictly greater than 0, and its exact value depends on the model you choose. Also, the variance of the predictions depends on whether or not you used ensembling, and how many models you ensembled. Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert One more comment/question: Do not you think that these exceptional samples on Figure 3,4,5 of yours  - may be the same \"bad\" samples which appear in @ambrosm analysis. Which can also be characterized as those where  percent of strongly perturbed genes is exceptionally big (my complement to AmbrosM post - reproduced below here). Also these sample can be characterized as the biggest error for the prediction models - see Antonina Dolgorukova analysis: https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/discussion/461663 Let me reproduce - first - your Figures 3,4 and  later  from my post By the way those highlighted is it MLN2238 or Porcn Inhibitor ? (Color seems to be the same) (Post: https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/discussion/458661#2566894 ) https://www.kaggle.com/code/alexandervc/op2-eda-new?scriptVersionId=155628891&cellId=10 emoji_people Jalil Nourisa Topic Author Posted 2 years ago · 20th in this Competition arrow_drop_up 2 more_vert Thank you very much, @alexandervc for your deeper look. These different analyses all pointing to the same direction. Thanks for sharing your point of view. Re \"By the way those highlighted is it MLN2238 or Porcn Inhibitor ? (Color seems to be the same)\" Its MLN 2238 (the color is confusing, i agree). Arvind (Yetirajan) Narayanan Iyengar Posted 2 years ago arrow_drop_up 1 more_vert Fantastic work @jalilnourisa ! emoji_people Jalil Nourisa Topic Author Posted 2 years ago · 20th in this Competition arrow_drop_up 0 more_vert thank you Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert Great analysis ! Congratulations with the medal ! Would it be possible to share the notebooks for your analysis ? PS Concerning Figure 2 Not fully understand the \"x-axis\" - it is ln( p-value) ? but ln(0.001) is -6.90, on the plot - we see something around -18 PS To complement your Figure 1 and illustrate uniform distribution for p-values (under null hypothisis) for simply-minded folks , that may be useful: (Left jump-up seems to present biological signal) https://www.kaggle.com/code/alexandervc/op2-eda-new?scriptVersionId=155423176&cellId=6 emoji_people Jalil Nourisa Topic Author Posted 2 years ago · 20th in this Competition arrow_drop_up 1 more_vert thank you for the comment. the code for this section is here https://github.com/AntoinePassemiers/Open-Challenges-Single-Cell-Perturbations/blob/master/op2-de-dl.ipynb . np.log(.001/18206)=-16.7, as we correct for multiple testing. Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert Thank you very much for the reply  ! PS To complement your Figure 2 (great finding on housekeeping genes, appreciate it a lot) , one may also (may be also would be helpful for simply-minded folks): https://www.kaggle.com/code/alexandervc/op2-eda-housekeeping-genes?scriptVersionId=155511517&cellId=11 PSPS A small misprint page 15: \"Chirvov [13]\" -> \"ChErvov [13]\"    :))) emoji_people Jalil Nourisa Topic Author Posted 2 years ago · 20th in this Competition arrow_drop_up 1 more_vert thank you very much for the comment, and my bad in mistyping your name! i will certainly fix it. best",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules maxleverage · 21st in this Competition  · Posted 2 years ago arrow_drop_up 9 more_vert SCP 21st Solution Feature Engineering Broadly two different sets of features were used for different models used in the final ensemble: Feature Set 1 Cell type - one-hot encoded SMILES - converted to 2048 bit vectors using RDKit Morgan fingerprints Drug properties: for each SMILE, log P and log of Molar Refractivity, standard scaled Control - whether or not drug is a control (1) or not (0) Feature Set 2 Same features as in Feature Set 1 Average SVD embedding values for drug effects on all cell types AND average SVD embedding values for cell response to various drugs Exclusive of 2), or usage of average SVD embedding values for log fold-change for drug effects on all cell types AND average SVD emedding values for log fold-change for cell response to various drugs (either use 2 or 3) Note that the number of SVD singular values to keep was chosen according to the Gavish-Donohoe (GD) SVD hard threshold method: https://arxiv.org/abs/1305.5870 Target GD SVD criterion used for singular value cutoff for low dimensional modes to keep SVD applied to target matrix and the singular value cutoff determined according to the Gavish-Donohoe threshold All models were trained against the SVD embedding of the original target, and model predictions were transformed back using the transpose of the V matrix Model Architectures and Data Upsampling Model 1: Simple direct regression on SVD embedding targets 8 layers Dense feed-forward neural netwok (5128 neurons per layer), output layer 114 neurons SELU activation each layer except for output layer (no activation, linear regression output) Output is in the SVD embedding space (114 columns) Loss: MAE or Pseudo-huber Epochs: 800 Batch size: 16 Cosine training schedule with warm restart every 200 epochs (alpha = 0.01, t_mul = 1.0, m_mul = 0.9) Stochastic weight averaging (SWA): SWA start from epoch 2 Predictions for 18211 genes: Let output be the predicted SVD embedding Take predicted SVD embedding and multiply by transpose of V matrix from SVD to get back to original 18211 representation Number of singular values to keep chosen according to Gavish-Donohoe threshold (see above) Model 2: Same architecture as Model 1 however sample weights introduced to loss function. Sample weight scheme: From training set filter out drug-cell pairs where B cells / myeloid cells were exposed to same compounds From 1), exposure to same set of compounds but observed difference in target ( - log10(p_val) * sign(LFC)) should be attributable to cellular difference For each cell type not B cells / myeloid cells calculate a notion of \"distance\" from the filtered and observed targets for exposure under same drugs using a distance metric of choice, e.g. Frobenius norm of difference of target matrices For each cell type not B cells / myeloid cells average out this \"distance\" metric calculated in 3) and then subtract from 1 i.e. distance to B cell or myeloid cell would be 0 s.t. one minus this amount would give each cell type of prediction interest a score of 1, whilst cell types further away gets a lower score Divide each cell type by the minimum score of the 6 cell types as calculated in step 4), and use this number as a weight for each row in training based on cell type used in the experiment Model is trained on this weighted loss inclusive of each row's weight Model 3: Skip connections architecture: 8 or 9 Dense layers Skip connections: Input dimension: 2056 Concatenate layer: Input concatenated with layer 2 pre-activation output (3072 neurons) leading to 5128 output dimension (3072+2056) before feeding into SELU activation layer Additive skip connections: SELU output of concatenate layer (5128) + pre-activation output of layer 4 (5128), SELU output of layer 4 + pre-activation output of layer 6 (5128), SELU output of layer 6 + pre-activation output of layer 8 (5128 / used where network has 9 hidden layers) Other details similar to Model 1 Model 4: Model 1 architecture but using training error to identify hard to predict drug-cell pairs for upsampling. Upsampling was done by identifying index of training samples (rows) which were at or below at certain training error threshold and then amplified by making a new copies (integer multiples) of these rows to be concatenated to original training set. The thinking here was that since the problem for predicting interactions for B / myeloid cells is potentially underspecified and to be extrapolated from observed interactions of other cells, the drug-cell pairs that have high row-wise accuracy or low MAE (or other regression metric) are not as important and performance on these rows can be sacrificed for better performance on the rows in training which have low row-wise accuracy or low MAE (or other regression metric). The amplified set was also manually checked for inclusion of the small number of B / myeloid cell observations in training. Broadly three types of this upsampling procedure were used with various models Upsampling procedure 1: Regression based row-wise metric (MAE) for determining cut-off threshold Simpler smaller neural network trained for 200 epochs on original training set Row-wise MAE computed for each sample Take median of 614 row-wise MAE metrics Take a positive multiple of this median (e.g. 3x or 15x) to select the base set of training rows to be upsampled Make K times more (e.g. 7x) copies of the training subset in 4) and concatenate to original training set Re-train larger model (could be any model architecture) on this upsampled training set Upsampling procedure 2: Sign classification using logistic loss for determining cut-off threshold The thinking behind this approach is that sign may be important to get right as an individual prediction where the magnitude (-log10(p_val)) is correct but where sign is not is very consequential for RWRMSE metric. Same procedure as in prior upsampling procedure, except neural network with regression output is trained against the sign of the log fold-change (i.e. target matrix is composed of +1/-1) Logistic Loss = (1/n) * Sum(i from 1 to n) L(y, t) where L(y, t) = ln(1 + exp(-y * t)) With t being in {-1, +1} i.e. the sign of the log fold change Row-wise accuracy (%) is computed on the training set Choose a cutoff below which the training rows are to be upsampled. I used arbitrary cutoffs such as 75% or accuracy cutoffs 3 standard deviations below the mean row-wise accuracy Repeat upsampling procedure as in the previous procedure amplifying this subset an integer number of times and retrain a larger model on this exapnded training set Upsampling procedure 3: Sign classification but focussed on rows with bad sign classification for small p-values Small p-values (e.g. less than 0.1) leads to large magnitudes when -log10 transformed, so intuition is get sign more correct for these as a bad sign classification flips these magnitudes to other side of real number line. Similar procedure to upsampling procedure, however we calculate accuracy only on subset of genes for each drug-cell pair where p-values are below a chosen threshold. Once these row-wise accuracy figures are computed, the same process as in the prior sign upsampling procedure is used to upsample a subset for retraining. Model 5: Triple regression head model with upsampling procedure and contrastive loss. The idea behind this architecture is to have share layers (e.g. 5 layers) between 3 different regression outputs. A \"contrastive\" loss (see below) was used to incentivise each regression head to learn a different hypothesis to the other 2 heads. This model architecture was mostly trained with sign upsampling procedure 2 as described in Model 4. Architecture: Shared weight layers: 5 Dense layers Activation: SELU for shared layers and regression heads, linear activation for regression outputs 3 regression heads: [3072, 2048, 1024] neurons before output layer for training y_train SVD embeddings Contrastive loss: Sum(head 1 to 3) of regression loss for each head + contrast_weight * average_pairwise_dissimilarity K = n_head choose 2 Average_Pairwise_Dissimilarity = 1/K * Sum(i from 1 to K) (Average Row-wise Cosine Similarity + 1.) If two non-zero vectors are exactly opposite, row-wise cosine similarity evaluates to -1. If they are exactly the same, we get +1 and if they are orthogonal we get 0. Adding 1 to the average row-wise cosine similarity ensures the minimization objective goes to 0 (instead of -1). Contrastive loss essentially balances between each regression driving down bias but also learning distinctive hypotheses from the data. The amount of contrast between the heads is controlled by the contrast_weight Each regression heads' output is multiplied by the transpose of the V matrix from SVD to get back predictions for original 18211 genes. Some submissions used the best head's predictions as determined by training error. Other predictions ensembled the 3 heads' predictions by equal or training loss derived weights (lower loss -> higher weight) Final Submission The two final submissions were LB RWRMSE weighted ensembles of the 16 best and 80 best submissions. For each submission I took the LB RWRMSE error, cubed them and subtracted from 1. to derive a score. These scores were then normalized against each other for the final weighted addition of the submissions.\\ Code https://github.com/maxleverage/kaggle-scp Please sign in to reply to this topic. comment 2 Comments Hotness Barry Posted 2 years ago · 51st in this Competition arrow_drop_up 3 more_vert Congratulations! @maxleverage 🎉 🎉 🎉, thanks for your sharing! Your method is excellent! But I have a question, as you mentioned in the text, are there any difference between Row-wise MAE and MAE? maxleverage Topic Author Posted 2 years ago · 21st in this Competition arrow_drop_up 3 more_vert No actually, the overall reduction to a scalar is the same (sum of elementwise MAE, divide by mxn) Barry Posted 2 years ago · 51st in this Competition arrow_drop_up 2 more_vert Thank you! @maxleverage maxleverage Topic Author Posted 2 years ago · 21st in this Competition arrow_drop_up 1 more_vert But if ur using MAE to rows to upsample, then yes you would do the reduction across the columns (for each drug-cell pair, average the elementwise MAEs across all genes observed for that pair)",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules makio323 · 24th in this Competition  · Posted 2 years ago arrow_drop_up 7 more_vert A model used in the 24th solution - pure linear algebra! Intro I share a linear algebra method, an unbiased and reproducible approach, resulting in decent Private/Public scores of 0.768/0.582.  My final submission is an ensemble of this Linear Algebra approach along with AE NN mimicking the linear algebra approach by NN, whose joint weight is  0.70, and the combination of the public notebooks, Pyboost , NN , and Linear SVR , with the total weight of 0.3.   It turns out that the pure Linear Algebra model gets the best private leaderboard score. In the first half of this competition, I struggled to overcome the wall of a 0.600 public score.  Some lucky runs of some NN models got over the wall, but not always.  There was also the second formidable wall of 0.585, which blended the results of public models. After the first half, I came up with this linear algebra approach and overcame the walls; the prediction is deterministic and reproducible and helped me to move on. The code of this linear model is available at my code notebook . Biological Hypothesis It is a bit old, before the deep neural network, I had research experience using linear algebra to predict the missing values in a matrix - Missing Value Expectation of Matrix Data by Fixed Rank Approximation Algorithm , and it may inspire me. An assumption behind this method is that the differential expressions (DEs) of 18,211 genes at one cell line (e.g. NK cells) can be linearly transferable to those of another cell line (e.g. B Cells) on the same chemical perturbation. A chemical perturbation triggers a complex activity interaction among the 18,211 genes, and different chemical perturbations make different activity patterns, resulting in various DEs from the same baseline condition.  However, there would be an unseen “master rule” to govern these interactions on each cell line.  If the master rule of one cell line (e.g. NK cells) could be similar to another cell (e.g. B cells), DEs on the same chemical perturbation could be predictable from one to another.  Even not knowing the master rule of each cell line, their relationship among cell lines could be captured such that f (DE _i_c ) = DE _j_c where DE _i_x and DE _j_x are the differential expressions of cell line i (NK cells, T cells CD4+, T cells CD8+, T regulatory cells) and j ('B cells', 'Myeloid cells') of chemical c perturbation, f is some special function. My approach is to assume that the linear system can be the proxy function, and to solve a system may provide the \"transfer\" such that DE _i_core x T = DE _j_core where DE _i_core and DE _j_core are m x n matrix, m is the number of chemicals shared by cel line i (NK cells, T cells CD4+, T cells CD8+, T regulatory cells) and j ('B cells', 'Myeloid cells') as shown below (including positive controls) and n is the number of genes (18,211).  T is an n x n matrix, considered a transformer matrix from one cell line to another. NK cells B cells 17 NK cells Myeloid cells 17 T cells CD4+ B cells 17 T cells CD4+ Myeloid cells 17 T cells CD8+ B cells 15 T cells CD8+ Myeloid cells 15 T regulatory cells B cells 17 T regulatory cells Myeloid cells 17 Once the transformer T is solved on the core chemicals, it may be applied to predict DEs of the target cell (e.g. B cells) from the known cell (e.g. NK cells) Prediction of DE _i_target = DE _j_target x T where DE _i_target is the DEs of prediction cell line i (B cells and Myeloid cells) of the target chemicals, 128 for B cells and 127 for Myoloid cells, and DE _j_target is the DEs of known cell line j (NK cells, T cells CD4+, T cells CD8+, T regulatory cells) of the target chemicals. This approach may provide a robust and unbiased prediction. Observation Using SVD to get the 1st and 2nd projected expression of the shared chemicals across 6 cell lines, and visualize the disperse among them (17 including positive controls, missing chemicals in T8 is replaced with those of T4).  Well, it is a bit difficult to recognize a clear pattern. Here is a grid plot of the previous one by chemicals.  In most of the chemicals, there is not much difference among 6 cell cline.  However, NK cell seems to be similar to the B Cell and Myoloid cell on the chemical perturbations that create the larger difference among 6 cell lines such as Belinostat (one of the positive controls), MNL 2238, and Oprozomib.  In the end, accuracy in predicting such chemicals may be important. Model Simpler model as in the background section. Solve linear system, transformer, from a base cell line (NK cells, T cells CD4+, T cells CD8+, T regulatory cells) to a target cell line ('B cells', 'Myeloid cells') of the DE using the chemicals tested in the two cell lines (15 plus 2 positive control). The transformer can be computed by multiplying a pseudo-inverse of DE _i_core with DE _j_core from the left. DE _i_core x T = DE _j_core T = DE _i_core -t * DE _j_core where DE _i_core -t is a pseudo-inverse of DE _i_core Apply the transformer to the DE of the base cell line/target chemicals and get the DE of the target cell line/chemicals. The simpler model consumes a lot of memory > 20Gb, and it cannot be performed on the free version of the Saturn Cloud, which I had used for convenience, I also propose an alternative solution using SVD with a projection space. SVD projection It first reduces the gene dimension (18,211) to the full rank dimension of the entire data set (614) by SVD, create the transformer in the projected space, and applies the transformer in projected space from known cell to target cell on the target chemicals, reconstruct the original gene dimension of the prediction. Project the DE data by SVD with the whole dimension. Solve linear system from a base cell line (NK cells, T cells CD4+, T cells CD8+, T regulatory cells) to a target cell line ('B cells', 'Myeloid cells') of the projected DE using the chemicals tested in the two cell lines (15 plus 2 positive control). Apply the transformer to the projected DE of the base cell line/target chemicals and get the projected DE of the target cell line/chemicals. Inverse the projected DE of the target cell line/chemicals to the predicted DE. Robustness, Code, and Reproducibility Please refer to my notebook : the code can run on the notebook, and the result is deterministic, so reproducible. Key Findings from the results This method indeed can be used to diagnose the similarity of the \"master rule\" among the cell lines for scientific insight, and my result suggests that NK cells, along with T cells CD4+ can be stronger predictor cell lines for B Cell and Myeloid cells.   Interestingly, T cells CD4+ is a better predictor for B cell while NK cell is a better predictor for Myeloid cells. This finding may align with the biological findings - NK cells can be derived from the myeloid lineage, Blood - The Journal of the American Society of Hematology 2011 3548 and T follicular helper cells cognately guide differentiation of antigen primed B cells in secondary lymphoid tissues - Trends in immunology 42.8 2021 . Prediction by single base cell line base cell public private NK cells 0.784 0.596 T cells CD4+ 0.775 0.607 T cells CD8+ 0.959 0.706 T regulatory cells 0.834 0.680 Prediction by two base cell lines base cell/target cell public private Predict B Cell by NK, Myeloid cells by T4 0.786 0.616 Predict B Cell by T4, Myeloid cells by NK 0.773 0.587 Differences in DE among certain cell lines can be captured linearly very well, and the linear relation is well transferable across different chemical responses, 0.768 in private and 0.582 in public DE of NK cell and T cells CD4+ cells are quite predictable for that of B Cell and Myeloid cells T cells CD4+ is a better predictor for B cell, and NK cells is a better predictor for Myeloid cell Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Frenio Redeker · 30th in this Competition  · Posted 2 years ago arrow_drop_up 8 more_vert #30: \"Melting\" the Data Thank you to the organizers of the Open Problems – Single-Cell Perturbations competition for this incredible opportunity. My approach combines four model types: Tabular Models with embeddings and dense layers, fine-tuned Transformer Models, Random Forests, and XGBoosted Forests, all trained on a \"melted\" format of the training data. In the following sections, I provide a comprehensive breakdown of my approach and the specific models employed. Links to models and other resources are provided throughout the text and can also be found in Section 6. Please note that my solution write-up was too long to post in a single part. Therefore, I have added the second part, covering Sections 3 to 6, in a comment below. 1. Integration of biological knowledge My initial approach involved training Tabular Neural Networks and Transformers specifically on the categorical features available in the training data. These models excel in encoding complex relationships within their embedding layers. Concurrently, I planned to augment the training data with information sourced from biological databases and web searches, and I intended to incorporate calculated molecular descriptors derived from SMILES. This enriched dataset would then be used to train tree-based models. Molecular Descriptors I calculated 1600+ molecular descriptors using the mordred python library and trained a random forest using all molecular descriptors in order to select the most important features (importance > 0.5 %) based on the feature importance results of the training run, which yielded a set of 23 molecular descriptors. All molecular descriptors as well as the list of most important descriptors can be found in the OP2 Additional Features Dataset in the files mol_descriptors.parquet and important_mol_descriptors.csv , respectively. import rdkit, rdkit.Chem import mordred, mordred.descriptors\n\ncalc = mordred.Calculator(mordred.descriptors, ignore_3D= True )\nmolecules = [rdkit.Chem.MolFromSmiles(smi) for smi in compounds[ 'SMILES' ]]\nfeatures = calc.pandas(molecules) content_copy Here compounds['SMILES'] contained a list of unique SMILES strings from the training data. Gene Information Gene information was obtained from the ENSEMBL database through the pybiomart python package. Note that gene info could only be obtained for 13,423 of the 18,211 genes in this way. from pybiomart import Server\n\nserver = Server(host= 'http://www.ensembl.org' )\ndataset = server[ 'ENSEMBL_MART_ENSEMBL' ][ 'hsapiens_gene_ensembl' ]\nresults = dataset.query(attributes=[ 'ensembl_gene_id' , 'external_gene_name' , 'cdna' , 'start_position' , 'end_position' ], filters={ 'link_ensembl_gene_id' : id_filter, 'transcript_is_canonical' : True }) content_copy From there I added additional features. For example, the number of each nucleobase (ACGT) obtained from the sequence data seemed particularly useful, according to feature importance results evaluated after training of Random Forests. A data frame containing added gene info can be found in the OP2 Additional Features Dataset as geneinfo.parquet . Cell Features Finding useful features that distinguish the 6 different cell types in the training data was a challenge for me. I ended up browsing Wikipedia, PubMed, and the website of the British Society for Immunology for information that I could add as cell features. The resulting data frame can be found in the OP2 Additional Features Dataset as cellinfo.csv . Result of Direct Integration of Biological Knowledge The best result that I was able to obtain using a tree-based model after adding the features described above was a Random Forest that achieved scores of 0.628 and 0.836 on the public and private leaderboards, respectively. For comparison, my best Random Forest trained on embeddings learned by a Tabular Neural Network (TabMod NN) achieved scores of 0.588 and 0.780. Embeddings Due to the inadequacy of direct integration of molecular descriptors, gene, and cell information, I decided to train Random and XGBoosted Forests on embeddings learned by the TabMod Neural Network. The use of embeddings significantly improved my results and might reasonably be considered an indirect form of \"integration of biological knowledge,\" because neural networks like TabMod NN are able to uncover and encode complex, often non-linear relationships inherent in biological systems into these embedding vectors. The embeddings obtained from TabMod NN can be found in the OP2 TabMod NN Embeddings Dataset . Plots of the cell_type , sm_name , and gene embeddings after PCA using 2 components as well as the corresponding code can be found in the Look at Embeddings Notebook . The plot of the gene embeddings shows two large distinct clusters – one very dense, the other less dense. The red marks with labels show a random selection of gene names to be displayed. 2. Exploration of the problem At first glance, the training data seemed like a collaborative filtering problem, where cell types are the users, drugs are the items, and gene expression confidence values are the targets. However, thinking about it that way leads to 18211 collaborative filtering problems – one for every gene – and I was intimidated by the idea of training 18211 models to come up with a single submission. Then I realized that collaborative filtering is just a special case of a tabular modeling problem with two features and one target, which led to the idea to \"melt\" the training data and the test data (convert from wide to long format): train_df = df_de_train.melt(id_vars=[ 'cell_type' , 'sm_name' ], value_vars=df_de_train.iloc[:, 5 :].columns, var_name= 'gene' , value_name= 'value' ) content_copy Melting of the training data results in a data frame with 11,181,554 rows (11,181,554/614 = 18,211), three features ( cell_type , sm_name and gene ), and one target (signed -log(p-value) called value ). I used this data format for all my models, which then had to predict only one target for each cell_type / sm_name / gene combination. Inference using a model trained on data in that format yielded a list of 4,643,805 prediction values, which I just reshaped back into the submission format of 255 x 18,211 (see e.g. my Random Forest Notebook ). 3. Model design For final submission, I used an ensemble of 4 different model types: Tabular Model Neural Networks, fine-tuned Transformer Models, Random Forests, and XGBoosted Forests. These are described in more detail below. The ensemble that achieved the placement in position 30 of the private leader board (with scores of 0.553 and 0.753 in the public and private leaderboards, respectively) had the following structure: 5 TabMod NNs using 600 dimensions for gene embeddings and random seeds 42, 55, 120, 457, and 736 and 5 TabMod NNs using 1000 dimensions for gene embeddings and random seeds 42, 199, 550, 855, and 970: df1 = df1_1* 0.1 + df1_2* 0.1 + df1_3* 0.1 + df1_4* 0.1 + df1_5* 0.1 + df1_6* 0.1 + df1_7* 0.1 + df1_8* 0.1 + df1_9* 0.1 + df1_10* 0.1 content_copy 5 Random Forests using random seeds 209, 569, 739, 885, and 926: df2 = df2_1* 0.2 + df2_2* 0.2 + df2_3* 0.2 + df2_4* 0.2 + df2_5* 0.2 content_copy 5 XGBoosted Forests using random seeds 117, 150, 234, 624, and 804: df3 = df3_1* 0.2 + df3_2* 0.2 + df3_3* 0.2 + df3_4* 0.2 + df3_5* 0.2 content_copy 5 fine-tuned Transformer Models, TinyBioBert and DeBERTa-v3-small (see details below): df4 = df4_1* 0.15 + df4_2* 0.15 + df4_3* 0.15 + df4_4* 0.15 + df4_5* 0.4 content_copy The weights used for the final submission were: submission = df1* 0.6 + df2* 0.1 + df3* 0.1 + df4* 0.2 content_copy The following figure shows a correlation heat map of the predictions of the different model types (see the Prediction Correlation Notebook for the source code): In the subsequent sections I present detailed descriptions of each model type, along with links to the relevant Kaggle notebooks. These notebooks are designed to run on Kaggle. However, the fine-tuning of transformer models, utilizing the full training data, was conducted on SaturnCloud. To adapt these notebooks for Kaggle, the training data was substantially reduced. As a result, the transformer models showcased on Kaggle are for demonstrative purposes only and do not achieve scores indicative of their full potential in the competition. TabMod NN TabMod NN performance was best when the training data was denoised using PCA and 10 components prior to training (see the TabMod NN Notebook for details and code). The two kinds of tabular neural networks used for final submission were identical except for the dimensionality (600 and 1000) of gene embeddings used. The models were based on fast.ai’s tabular_learner with the following configuration: learn = tabular_learner(dls, y_range=(y_min, y_max), emb_szs=emb_szs, layers=[ 1000 , 500 , 250 ], n_out= 1 , loss_func=F.mse_loss) content_copy To achieve a gene embedding size of 1000 instead of the default 389, the emb_szs dictionary was customized. emb_szs = { 'cell_type' : 5 , 'sm_name' : 26 , 'gene' : 1000 } content_copy The range of the final sigmoid layer was set to y_min to y_max which were obtained by determining the minimum and maximum target values in the training data after PCA denoising. The three dense layers of size 1000, 500, and 250 were optimized for appropriate expressivity. Finally, the mean squared loss function was chosen to best reflect the evaluation in the leaderboard. Experiments using mean absolute error, huber loss, and log-cosh did not lead to improvements of the model and instead reduced model performance. Random Forest Random Forests (see Random Forest Notebook ) were trained after adding embeddings for cell_type , sm_name , and gene to the training data. These embeddings were obtained from a TabMod NN similar to the one described in the previous section with gene embeddings of dimensionality 1000 but without the use of PCA denoising on the training data (see Export Embeddings Notebook for the code). Of those the cell_type embeddings of dimensionality 5 were left unchanged, whereas sm_name and gene embeddings (of dimensionality 26 and 1000, respectively) were reduced to 10 components each using PCA. All Random Forest models used in the final submission were identical except for the random seeds used. They were based on scikitlearn’s RandomForestRegressor using 100 trees on 66 % of samples in order to achieve a meaningful out-of-bag error and score. The square root of feature number was used as the maximum number of features per tree which has been shown to be beneficial for the model’s ability to generalize. The minimum number of samples per leaf was set to 5 in order to achieve high expressivity. Such a Random Forest could be trained using the following example code: from sklearn.ensemble import RandomForestRegressor\n\nm = RandomForestRegressor(n_jobs=- 1 , n_estimators= 100 , max_samples= 0.66 , max_features= 'sqrt' , min_samples_leaf= 5 , oob_score= True )\nm.fit(xs, y) content_copy XGBoosted Forest The same embeddings used to train Random Forests were also used to train XGBoosted Forests using the same dimensionality reductions. The XGBoosted Forests used in the final submission were also identical to each other except for the random seed used. They were trained using the following code (see XGBoosted Forest Notebook for details): import xgboost as xgb\n\nm = xgb.XGBRegressor(device= 'cuda' , n_estimators= 185 , learning_rate= 0.01 , max_depth= 20 , min_child_weight= 1 , gamma= 0 , subsample= 1 , colsample_bytree= 0.55 )\nm.fit(xs, y) content_copy The model parameters were optimized using the cross validation scheme proposed by AmbrosM , where all but 10 % of the data of one of four cell types is used as a validation set. It should be noted that the performance of the model could potentially be enhanced by configuring the max_depth parameter to values exceeding 20. This approach was not extensively explored due to the higher computational costs associated with larger values. Transformer Models Two kinds of Transformer models from the Huggingface Transformers data base were fine-tuned to output a number when given a standardized input sentence. The general model configuration is shown in the following code snippet: from transformers import TrainingArguments, Trainer\n\nargs = TrainingArguments( 'outputs' , save_steps=steps, learning_rate= 8e-5 , warmup_ratio= 0.1 , lr_scheduler_type= 'cosine' , fp16= True ,\n    evaluation_strategy= \"epoch\" , per_device_train_batch_size=bs, per_device_eval_batch_size=bs* 2 ,\n    num_train_epochs=epochs, weight_decay= 0.01 , report_to= 'none' , seed=random_seed)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels= 1 ) content_copy The argument num_labels=1 configures the final linear layer of the model to have a single output neuron which is appropriate for a regression task, as it leads to prediction of a single continuous value. The first model was based on the deberta-v3-small model ( link to article ) with 44M parameters pre-trained on general text data. The \"sparse\" input used for the DeBERTa model was: trdf[ 'input' ] = 'CELL TYPE: ' + trdf.cell_type + '; TREATMENT: ' + trdf.sm_name + '; GENE: ' + trdf.gene content_copy Fine-tuning the DeBERTa for 20-25 epochs on an A10 GPU using an 80/20 train-valid-split and a batch size of 256 took about 48-60 hours (see Transformer DeBERTa (Demo) Notebook for a training demo that runs on Kaggle in ~2 hours). The second model was based on the tiny-biobert model ( link to article ) with 15M parameters pre-trained on citations and abstracts of biomedical literature using the PubMed dataset. The \"verbose\" input used for the TinyBioBert model was: trdf[ 'input' ] = \"Estimate the −log(p-value) confidence for change in gene expression of \" + trdf.gene + \" in \" + trdf.cell_type + \" when treated with \" + trdf.sm_name + \" compared to DMSO.\" content_copy Fine-tuning the TinyBioBert model for 20-30 epochs on an A10 GPU using an 80/20 train-valid-split and a batch size of 256 took about 24-30 hours (see Transformer TinyBioBert (Demo) Notebook for a training demo that runs on Kaggle in ~1:30 hours). Interestingly, the \"sparse\" input used for the DeBERTa model did not lead to very good performance in the TinyBioBert model, whereas the more \"verbose\" input used for the TinyBioBert model led to slower training and worse results in the DeBERTa model. This is reflected in the following submission scores (all trained for 5 epochs using the same random seed of 42): public LB     privateLB\nDeBERTa sparse: 0.630 0.807 DeBERTa verbose: 0.637 0.810 TinyBioBert sparse: 0.634 0.812 TinyBioBert verbose: 0.624 0.807 content_copy The transformer part of the final ensemble consisted of four TinyBioBert models. These were trained for 30 epochs using seed 42, 30 epochs using seed 546, 20 epochs using seed 419, and 5x5 epochs using different random seeds. Each model was assigned a weight of 0.15 in the Transformer sub-ensemble. The single DeBERTa model used was trained for a total of 22 epochs using different random seeds, and was assigned a weight of 0.4. 4. Robustness The robustness of each individual model type was evaluated using triplicate submissions with different random seeds by calculating the mean and standard deviation of the scores achieved on the public and private leaderboard (in the case of transformers, the models also varied slightly in the number of epochs trained). TabMod NN (600): 0.567 ± 0.003 public, 0.777 ± 0.007 private. TabMod NN (1000): 0.562 ± 0.002 public, 0.778 ± 0.003 private Random Forest: 0.592 ± 0.004 public, 0.781 ± 0.002 private. XGBoosted Forest: 0.588 ± 0.002 public, 0.780 ± 0.001 private. Transformer (DeBERTa) 22-25 epochs: 0.606 ± 0.013 public, 0.784 ± 0.007 private. Transformer (TinyBioBert) 25-30 epochs: 0.607 ± 0.003 public, 0.777 ± 0.004 private. These results show that the fine-tuned DeBERTa model is the least robust of the models. The fine-tuned TinyBioBert model consistently performed as well as the TabMod NNs on the private leaderboard, but consistently looked worse on the public leaderboard. 5. Documentation & code style The code is documented in the notebooks linked throughout the notebook. Additionally, a list of links to all notebooks and datasets can be found in Section 6. 6. Reproducibility The source code and further documentation for all models as well as additional datasets can be accessed through the following links: Model Notebooks: TabMod NN Notebook Random Forest Notebook XGBoosted Forest Notebook Transformer DeBERTa (Demo) Notebook Transformer TinyBioBert (Demo) Notebook Other Notebooks: Look at Embeddings Notebook Prediction Correlation Notebook Datasets: OP2 Additional Features Dataset OP2 TabMod NN Embeddings Dataset Please sign in to reply to this topic. comment 12 Comments Hotness Frenio Redeker Topic Author Posted 2 years ago · 30th in this Competition arrow_drop_up 5 more_vert This comment used to contain Sections 3 to 6 of my solution write-up because a bug kept me from posting it in whole. Now that the bug is fixed, I replaced the original comment with this notice and added Sections 3 to 6 to the main topic. Antonina Dolgorukova Posted a year ago · 13th in this Competition arrow_drop_up 2 more_vert Hi, interesting and well described solution with melting the data and learning the embedding representations, thanks for sharing! In the Random Forest model, you used embeddings from a TabMod NN with a dimensionality of 1000 for genes and without using denoising on the training data, and as I understand it, it was better, while the TabMod NN itself performed better with embeddings after denoising and with a dimensionality of 600. Maybe it's just my poor understanding, but as I understand it, the better the model performs, the better the quality of the embeddings it learns during training. So how would you explain that lower quality embeddings performed better in RF? Frenio Redeker Topic Author Posted a year ago · 30th in this Competition arrow_drop_up 1 more_vert Thank you for the feedback! I was definitely surprised when I found that the RF performs better with the embeddings learned from the plain data without denoising, but unfortunately haven't come up with a good explanation so far. I would be very interested to hear if you have any hypotheses. On the subject of embedding sizes, I think the differences in performance between the dimensionalities of 600 and 1000 weren't significant. Pablo Rodriguez-Mier Posted 2 years ago · 16th in this Competition arrow_drop_up 3 more_vert Thanks for sharing, Frenio! Your score of 0.553 on the leaderboard really gave us a lot of nightmares. You led this challenge for a very long time! Frenio Redeker Topic Author Posted 2 years ago · 30th in this Competition arrow_drop_up 3 more_vert Haha, until you came in with your 0.546 score and gave me nightmares! And towards the end, before overfitting public notebooks took over, @eliork caused nightmares for us all I suppose. Marília Prata Posted 2 years ago arrow_drop_up 3 more_vert Congratulations and thanks for sharing your precious insights (topic and comments above) Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 4 more_vert Thanks a lot for sharing ! Any ideas what can be interpretation of the two clusters for genes ? To my mind there is no two natural clusters in human genes. There can be be different approaches giving different results , several are in HPA like: https://www.proteinatlas.org/humanproteome/tissue/expression+cluster PS Would you be so kind to make your key submit open , if possible ? Frenio Redeker Topic Author Posted 2 years ago · 30th in this Competition arrow_drop_up 3 more_vert Thank you for reading and for providing so many useful public notebooks to get people started in this competition! I do not know how to interpret these two gene clusters, unfortunately, but do I recall correctly that you also found two clusters of genes in one of your earlier public EDA notebooks using a different clustering approach? (I wasn't able to find it just now.) Happy to discuss more if you have ideas. PS: I am not sure I understand what you mean by key submit, but if your interested in my final submission notebook, I just made it public and you can find it here . I only used that notebook to load and manually blend the individual model predictions, though. Please let me know if I can provide any other resources that might be helpful. Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 2 more_vert Thanks for kind words ! Indeed in here there are something like 2 clusters in genes (one much bigger than the other) here: https://www.kaggle.com/code/alexandervc/op2-eda-baseline-s?scriptVersionId=143061639&cellId=21 (it works quite long time, so in other version of the notebook I reduced number of genes and we do not see that figure, so it is only in version 19). I did not analyzed that much… Thanks for reminding me that.. Frenio Redeker Topic Author Posted 2 years ago · 30th in this Competition arrow_drop_up 2 more_vert Great, thank you for finding the version that I had in mind! Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 2 more_vert There is interesting picture from @zhijianli https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/discussion/459623 His interpretation - up vs down regulated genes.  Seems very reasonable. May be it is similar to yours. Frenio Redeker Topic Author Posted 2 years ago · 30th in this Competition arrow_drop_up 2 more_vert Thank you for pointing me to this! I'll look into his solution more closely. Alexander Chervov Posted 2 years ago · 13th in this Competition arrow_drop_up 2 more_vert Thanks again for the great write-up ! I wonder about the \"Gene Information\" section - you mention that you convert genes information into features. Would you be so kind to clarify it a bit - because genes -are our targets  - so any info on genes -  do not depend on our features: cell type/drug, so it is a kind constant across sample-wise direction. There are enormous amounts   of information on human genes, but typically there is no way to use it in such kind of tasks, because it is \"constant\" across cells/cell types/drugs direction, so not a feature. Frenio Redeker Topic Author Posted 2 years ago · 30th in this Competition arrow_drop_up 3 more_vert Thank you for keeping up the research on this problem! I am more than happy to try and clarify. As I wrote in \"Exploration of the Problem\" I converted all input data into long format such that genes actually turned into a feature. Here is a screenshot of the \"melted\" data frame to make it more concrete: You can find the additional gene features I extracted from the ENSEMBL database in the geneinfo.parquet file in the OP2 Additional Features Dataset that I've made public. I merged this geneinfo with the training data on gene like shown in the following code snippet: train_df_ginf = pd.merge(train_df, selected_geneinfo, on= 'gene' , how= 'left' ) content_copy Please note that the CPU RAM on Kaggle is too low for this step to work, so this needs to be done on a different platform. I just published a notebook that shows how I added all additional features to the data ( Random Forest with Additional Features ), but due to limited RAM it will not run on Kaggle, unfortunately. I hope this helps and I am happy to explain more, so let me know if you have further questions. This comment has been deleted.",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Octopus210 · 34th in this Competition  · Posted 2 years ago arrow_drop_up 4 more_vert 34th Place Solution Writeup for Open Problems – Single-Cell Perturbations First of all, I would like to express my gratitude to the organizers and kagglers of this competition. I learned a lot from the open notebooks and discussions. Especially, I would like to express my big thanks to the following people. @mehrankazeminia @somayyehgholami @alexandervc @antoninadolgorukova @kishanvavdara @pablormier 34th Solution We blended the results of three tasks to find one result. Being different approaches, blending these models was very effective. The three approaches Task1 : test set as categorical variable Task2 : test set as continuous variable ( genes as sampled) Task3 : test set as continuous variable ( compounds as sample) Basically, it is a simple solution that blends public notebook's score and sklearn's model. If you combine it with a better performing model (like PY-BOOST), the performance will be even better. integration of biological knowledge In Task 3, we incorporated the decoupler. A small improvement in public scores was obtained. 2.Exploration of the problem In Task 2 and Task 3, CD8 data were excluded. op2-about-positive-control 3.Model design I used sklearn's MLP, lightgbm, and Ridge. 4.Robustness Robustness is considered to be high, because I sought a single result from three different perspectives. 5.Documentation & code style In preparation 6.Reproducibility In preparation Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Katerina · 43rd in this Competition  · Posted 2 years ago arrow_drop_up 11 more_vert 43d Place Solution for the Open Problems – Single-Cell Perturbations Context https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/data Overview of the approach Our optimal model is blend of two models: weight_1: 0.5 Pyboost model weight_2: 0.5 NN model Data preprocessing and feature selection Features 1. Pyboost features Our main improvement from public pyboost  implementation was that we explored and added as a feature categories for each drug: drug_cls = { \"Antifungal\": [\"Clotrimazole\", \"Ketoconazole\"], \"Corticosteroid\": [\"Mometasone Furoate\"], \"Kinase Inhibitors\": [\"Idelalisib\", \"Vandetanib\", \"Bosutinib\", \"Ceritinib\", \"Crizotinib\", \"Cabozantinib\", \"Dasatinib\", \"Selumetinib\", \"Trametinib\", \"Lapatinib\", \"Canertinib\", \"Palbociclib\", \"Dabrafenib\", \"Ricolinostat\",\"Tamatinib\", \"Tivozanib\", \"Quizartinib\",\"Sunitinib\",\"Foretinib\",\"Imatinib\",\"R428\",\"BMS-387032\",\"CGP 60474\", \"TIE2 Kinase Inhibitor\",\"Masitinib\",\"Saracatinib\",\"CC-401\",\"RN-486\",\"GO-6976\", \"HMN-214\",\"BMS-777607\",\"Tivantinib\",\"CEP-37440\",\"TPCA-1\",\"AZ628\",\"PF-03814735\", \"PRT-062607\",\"AT 7867\", \"BI-D1870\", \"Mubritinib\", \"GLPG0634\",\"Ruxolitinib\", \"ABT-199 (GDC-0199)\", \"Nilotinib\"], \"Antiviral\": [\"Lamivudine\", \"AMD-070 (hydrochloride)\", \"BMS-265246\"], \"Sunscreen agent\" : [\"Oxybenzone\"], \"Antineoplastic\": [\"Vorinostat\", \"Flutamide\", \"Ixabepilone\", \"Topotecan\", \"CEP-18770 (Delanzomib)\", \"Resminostat\", \"Decitabine\", \"MGCD-265\", \"GSK-1070916\",\"BAY 61-3606\",\"Navitoclax\", \"Porcn Inhibitor III\",\"GW843682X\",\"Prednisolone\",\"Tosedostat\", \"Scriptaid\", \"AZD-8330\", \"Belinostat\",\"BMS-536924\",\"Pomalidomide\",\"Methotrexate\",\"HYDROXYUREA\", \"PD-0325901\",\"SB525334\",\"AVL-292\",\"AZD4547\",\"OSI-930\",\"AZD3514\",\"MLN 2238\",\"Dovitinib\",\"K-02288\", \"Midostaurin\",\"I-BET151\",\"FK 866\",\"Tipifarnib\",\"BX 912\",\"SCH-58261\",\"BAY 87-2243\", \"YK 4-279\",\"Ganetespib (STA-9090)\",\"Oprozomib (ONX 0912)\",\"AT13387\",\"Tipifarnib\",\"Flutamide\",\"Perhexiline\",\"Sgc-cbp30\",\"IMD-0354\", \"IKK Inhibitor VII\", \"UNII-BXU45ZH6LI\",\"ABT737\",\"Dactolisib\", \"CGM-097\", \"TGX 221\",\"Azacitidine\",\"Defactinib\", \"PF-04691502\", \"5-(9-Isopropyl-8-methyl-2-morpholino-9H-purin-6-yl)pyrimidin-2-amine\"], \"Selective Estrogen Receptor Modulator (SERM)\": [\"Raloxifene\"], \"Antidiabetic (DPP-4 Inhibitor)\": [\"Linagliptin\",\"Alogliptin\"], \"Antidepressant\": [\"Buspirone\", \"Clomipramine\", \"Protriptyline\", \"Nefazodone\",\"RG7090\"], \"Antibiotic\": [\"Isoniazid\",\"Doxorubicin\"], \"Antipsychotic\": [\"Penfluridol\"], \"Antiarrhythmic\": [\"Amiodarone\",\"Proscillaridin A\"], \"Alkaloid\": [\"Colchicine\"], \"Antiviral (HIV)\": [\"Tenofovir\",\"Efavirenz\"], \"Allergy\": [\"Desloratadine\",\"Chlorpheniramine\",\"Clemastine\",\"GSK256066\",\"SLx-2119\", \"TR-14035\", \"Tacrolimus\"], \"Anticoagulant\": [\"Rivaroxaban\"], \"Alcohol deterrent\":[\"Disulfiram\"], \"Cocaine addiction\":[\"Vanoxerine\"], \"Erectile dysfunction\":[\"Vardenafil\"], \"Calcium channel blocker\":[\"TL_HRAS26\"], \"Anti-endotoxemic\":[\"CGP 60474\"], \"Acne treatment\":[\"O-Demethylated Adapalene\"], \"Stroke\":[\"Pitavastatin Calcium\",\"Atorvastatin\"], \"Stem cell work\":[\"CHIR-99021\"], \"Hypertension\":[\"Riociguat\"], \"Heart failure\":[\"Proscillaridin A;Proscillaridin-A\", \"Colforsin\"], \"Regenerative\":[\"LDN 193189\"], \"Psoriasis\":[\"Tacalcitol\"], \"Unknown_1\": [\"STK219801\"], \"Unknown_2\": [\"IN1451\"] Another features were  'cell_type' and 'sm_name' encoded with QuantileEncoder(quantile =.8) TruncatedSVD(n_components=50) was applied to target. 2. NN features For neural network we used just SMILES and cell_type colums Modeling 1. Pyboost params = { n_components = 50, ntrees = 5000, lr = 0.01, max_depth = 10 , colsample =  0.35, subsample = 1} 2. NN Model Loss function We used Mean Absolute Error (MAE) for train and Mrrmse metric for validation Validation Strategy Our team used k-fold cross-validation strategy. Our goal was to get stable score on train and do not fit our scores to public leaderboard. We had a few submissions with very high public score but these submissions were blends of many models with different coefficient. We were not sure that their scores would be stable on private part. As a result, we chose a 0.5-0.5 blends from two our own models which didn't show great result on public but score very stable on cross validation. Sources https://www.kaggle.com/code/alexandervc/pyboost-secret-grandmaster-s-tool https://www.kaggle.com/code/asimandia/kfold-simple-nn-refactored Please sign in to reply to this topic. comment 2 Comments Hotness _student Posted 2 years ago arrow_drop_up 3 more_vert Thanks for sharing @abramova , blend of two models is indeed a unique approach , new insight 👍 Edward Posted 2 years ago · 43rd in this Competition arrow_drop_up 1 more_vert Thank you, Katerina, briefly and accurately. I would add that we have tried other approaches and more complex networks. But simple, understandable methods led to a good result.",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Prem Chotepanit · 44th in this Competition  · Posted 2 years ago arrow_drop_up 3 more_vert 58th solution: Single-Cell Perturbations (Blending only) Disclaimer: I didn't mean to disrespect anyone's intention, and I know you all have done your hard work for this competition and most of the competitors are genuinely more skilful than me. How I ended up with ensemble only: I joined this competition at an early stage, but I had forgotten until the day before the deadline. I got the reminder notification but I had nothing to submit. So, I tried ensemble the best version of top public score notebooks, which are: https://www.kaggle.com/code/liudacheldieva/submit-only?scriptVersionId=152457701 https://www.kaggle.com/code/nikolenkosergei/pyboost-secret-grandmaster-s-tool-106cac?scriptVersionId=152757460 Normally, I will be kicked out of the medal tier when I do it in other competitions. It seems there's luck on my side. I unexpectedly took the 58th place on the private leaderboard. Link to the solution https://www.kaggle.com/code/batprem/58th-private-lb-ensemble-only?scriptVersionId=153068055 Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Ivan Aleksandrov · 45th in this Competition  · Posted 2 years ago arrow_drop_up 10 more_vert 45 Place Solution for the Open Problems – Single-Cell Perturbations Competition Competition Pages https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview https://openproblems.bio/ Overview of the Approach We try CatBoost in multi target mode, but woking worse than constants in submission. PyBoost it's a great framework for multi target. Details of the submission Our blend: public kernels [: 128]  0.574 * 0.2 + 0.577 * 0.8 [128: ] 0.584 PyBoost + 0.574 public Not working: Ridge regression ChemBert CatBoost Please sign in to reply to this topic. comment 0 Comments Hotness"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict how small molecules change gene expression in different cell types For this competition, we designed and generated a novel single-cell perturbational dataset in human peripheral blood mononuclear cells (PBMCs). We selected 144 compounds from the Library of Integrated Network-Based Cellular Signatures (LINCS) Connectivity Map dataset ( PMID: 29195078 ) and measured single-cell gene expression profiles after 24 hours of treatment. The experiment was repeated in three healthy human donors, and the compounds were selected based on diverse transcriptional signatures observed in CD34+ hematopoietic stem cells (data not released). We performed this experiment in human PBMCs because the cells are commercially available with pre-obtained consent for public release and PBMCs are a primary, disease-relevant tissue that contains multiple mature cell types (including T-cells, B-cells, myeloid cells, and NK cells) with established markers for annotation of cell types. To supplement this dataset, we also measured cells from each donor at baseline with joint scRNA and single-cell chromatin accessibility measurements using the 10x Multiome assay. We hope that the addition of rich multi-omic data for each donor and cell type at baseline will help establish biological priors that explain the susceptibility of particular genes to exhibit perturbation responses in difference biological contexts. To understand the dataset, it is important to know the design of the plates used to measure the treamtment effect. PBMCs from donors were thawed and plated on 96-well plates. Two columns of the plates were dedicated to positive controls (dabrfenib and belinostat) and one column was dedicated to a negative control (DMSO). The positive controls were selected because they tend to have a large impact on transcription, and the negative control is used as a solvent for the compounds used in this study. The remaining wells on the plate are allocated to each of 72 compounds. The full dataset comprises 2 different compound plates per donor for a total of 6 plates.  Note, each well contains PBMCs, which are a collection of different cell types. These include T cells, B cells, NK cells, and Myeloid cells like Macrophages and Monocytes. Based on the gene expression data measured in scRNA, we can computationally assign each cell to a cell type. Note, because we only measure ~350 cells per well and because compounds may have a toxic effect on some cells types, we don't always observe every cell type in every well. Another technical variable that will impact the raw data in this experiment is the chemical tagging of each well in each row of the plate, and then pooling all samples in each row into a single pool for sequencing. This is called Cell Multiplexing, and you can read more about it on the 10x Genomics website What is Cell Multiplexing? . What you need to know is that this creates some technical bias linking all the wells in each row of a plate. One purpose of including two positive controls and one negative control in each row of the plate is to allow us to account for this source of noise when we calculate differential expression. In this competition setup, participants are tasked with modelling differential expression (DE), which enables us to estimate the impact of an experimental perturbation on the expression level of every gene in the transcription (18211 genes in this dataset). We estimate the impact of each compound by first averaging the raw gene expression counts in each cell of a specific type in each sample, which is called pseudobulking in the single-cell literature. We then fit a linear model to the pseudobulked counts data using Limma and include the library (row), plate, and donor as technical covariates and compound as the experimental covariate. Here, pseudobulked means we summed the raw counts for all cells of a given type for each well in the experiment. A diagram of this process is shown below:  What is differential expression From The Encyclopedia of Bioinformatics and Computational Biology : Differential gene expression, commonly abbreviated as DG or DGE analysis refers to the analysis and interpretation of differences in abundance of gene transcripts within a transcriptome (Conesa et al., 2016). Lists of genes that differ between 2 sample sets are often provided by RNA-seq data analysis tools, or can be generated manually by statistical testing of data sets. Due to the large number of genes to be tested, (e.g., >20,000 in the human genome), multiple testing correction such as Bonferroni correction is usually applied. To learn more, we suggest starting here: Single-cell best practices - Differential gene expression analysis . The output of this model is an estimated fold-change in gene expression and a multiple-testing corrected p-value that a given gene's expression is dependent on the compound experimental variable. There is a long rabbit hole to go down in the world of differential expression testing. We don't have a complete mechanistic model of the data generative process of collecting scRNA data, and many groups disagree on the best way to account for nuisance variables or technical noise. We picked limma because it performs well in our testing. Note, there is an opportunity for privacy leakage from the test set if we release the raw counts data and the differential expression analysis computed on all samples. To protect against this, we fit the differential expression model twice. To generate the training data, we fit the DE model on only the samples from the training set. To generate the private and public test data, we fit the DE model to all samples in the experiment. This keeps the test data private and ensures the test data is the most accurate. Your task is to predict differential expression values for Myeloid and B cells for a majority of compounds. You will train your model on data from all 144 compounds in T cells (CD4+, CD8+, regulatory) and NK cells and a 10% of compounds in the Myeloid and B cells. This mirrors a scientific context where you might want to make predictions into new cell types while taking only 1/10th of the measurements in that cell type. Train: Public Test: Private Test: So the distribution of train / test split across cell types looks like this (download the image for a full size figure): Note that there is no additional test data beyond the indicated cell_type / sm_name pairs. The input to your model will be a tuple of cell_type and sm_name and the output of your model will be predicted signed -log10(p-values) for all 18211 genes. We also provide the raw data for the training split, along with 10x Multiome data for the donors at baseline. This raw data is not necessary to compete for the leaderboard prize. The de_train.parquet file comprises the main competition data. It contains values for a number of cell_type / sm_name pairs. Your goal is to predict corresponding values for the cell_type / sm_name pairs given in id_map.csv . Note: there is no DE data for the DMSO sample, because it is the negative control. All DE output is calculated in reference to the DMSO, i.e. the DE analysis asks \"how confident am I that each gene increased or decreased relative to DMSO due to the compound treatment\". adata_train.parquet - Unaggregated count and normalized data in COO sparse-array format. A supplement to de_train . In addition to the fields in de_train , this data also has: adata_obs_meta.csv - Observation metadata for adata_train . multiome_train.parquet - This is optional additional 10x Multiome data for each sample at baseline. multiome_obs_meta.csv multiome_var_meta.csv id_map.csv - Identifies the cell_type / sm_name pair to be predicted for the given id . sample_submission.csv - A sample submission file in the correct format. See the Evaluation page for more details. 9 files 4.58 GB csv, parquet Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 4.58 GB adata_excluded_ids.csv adata_obs_meta.csv adata_train.parquet de_train.parquet id_map.csv multiome_obs_meta.csv multiome_train.parquet multiome_var_meta.csv sample_submission.csv 9 files 28 columns ",
    "data_description": "Open Problems – Single-Cell Perturbations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 2 years ago Late Submission more_horiz Open Problems – Single-Cell Perturbations Predict how small molecules change gene expression in different cell types Open Problems – Single-Cell Perturbations Overview Data Code Models Discussion Leaderboard Rules Overview The goal of this competition is to predict how small molecules change gene expression in different cell types. Your work will help develop methods to predict how cells respond to small molecule drug perturbations, which could have important applications in drug discovery and basic biology. Start Sep 12, 2023 Close Dec 1, 2023 Merger & Entry Description link keyboard_arrow_up Human biology can be complex, in part due to the function and interplay of the body's approximately 37 trillion cells, which are organized into tissues, organs, and systems. However, recent advances in single-cell technologies have provided unparalleled insight into the function of cells and tissues at the level of DNA, RNA, and proteins. Yet leveraging single-cell methods to develop medicines requires mapping causal links between chemical perturbations and the downstream impact on cell state. These experiments are costly and labor intensive, and not all cells and tissues are amenable to high-throughput transcriptomic screening. If data science could help accurately predict chemical perturbations in new cell types, it could accelerate and expand the development of new medicines. Several methods have been developed for drug perturbation prediction, most of which are variations on the autoencoder architecture (Dr.VAE, scGEN, and ChemCPA). However, these methods lack proper benchmarking datasets with diverse cell types to determine how well they generalize. The largest available training dataset is the NIH-funded Connectivity Map (CMap), which comprises over 1.3M small molecule perturbation measurements. However, the CMap includes observations of only 978 genes, less than 5% of all genes. Furthermore, the CMap data is comprised almost entirely of measurements in cancer cell lines, which may not accurately represent human biology. Competition host Open Problems in Single-Cell Analysis is a non-profit scientific collaboration aiming to drive innovation in single-cell data science. They are partnering to host this competition with Cellarity, a first-of-its-kind therapeutics company that develops medicines by studying and altering the cellular signatures of disease. Although it is impossible to measure all perturbations in all cells, we hypothesize that it is possible to measure a subset of combinations and infer the rest. Today, we are far from this goal, but we hope that this competition will serve as an important proof of concept. Your work in helping to accurately predict chemical perturbations in new cell types could accelerate the discovery and enable the creation of new medicines to treat or cure disease. Evaluation link keyboard_arrow_up We use the Mean Rowwise Root Mean Squared Error to score submissions, computed as follows: MRRMSE = 1 R R ∑ i = 1 ( 1 n n ∑ j = 1 ( y i j − ˆ y i j ) 2 ) 1 / 2 where R is the number of scored rows, and y i j and ˆ y i j are the actual and predicted values, respectively, for row i and column j , and n is the number of columns. Submission File For each id in the evaluation set, you should predict a value for each of the 18,211 genes named in the remaining columns. Each id corresponds to a cell_type / sm_name pair, which you may identify from the id_map.csv file. Your submission should contain a header and have the following format: id,A1BG,A1BG-AS1, ... ,ZZEF1\n0,0.0,0.0, ... ,0.0\n1,0.0,0.0, ... ,0.0\n2,0.0,0.0, ... ,0.0\n3,0.0,0.0, ... ,0.0 ... content_copy Timeline link keyboard_arrow_up September 12, 2023 - Start Date. November 23, 2023 - Entry Deadline. You must accept the competition rules before this date in order to compete. November 23, 2023 - Team Merger Deadline. This is the last day participants may join or merge teams. November 30, 2023 - Final Submission Deadline. December 12, 2023 - Judges Award Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. \"Judges\" Prizes Scoring Rubrics link keyboard_arrow_up The Judges Awards will be decided by a team of experts in single-cell biology grading write-ups according to the following criteria. Each category will be graded on a scale of 1-5 , and winners will be selected from the top scoring submissions. We’re excited about this part of the competition because it encourages competitors to focus on scientific impact in addition to performance. You do not need to be top on the leaderboard to be considered for a Judges Award! You can submit regardless of your final leaderboard score. We’re more interested in scientific impact than performance for these awards. We're not sure how many submissions we'll get to this track of the competition, and we only have the bandwidth to grade ~100 submissions. We'll accept submissions from anyone, and every submission will be looked at by a judge, but not all submissions will be graded (i.e. assigned scores). For those that are graded, each submission will graded by 2 Judges on the following criteria. The final selection of the Prize-Winning Submissions will be made by the Judges, who will meet to discuss the submissions in December after the competition closes. 1. Integration of Biological Knowledge How does your model integrate biological knowledge into predictions? We want to know what you tried, and how it worked! This may include, but is not limited to: How did you integrate the ATAC data? Which representation did you use? How did you integrate LINCS data? How did this improve your model? Did you use the chemical structures in your model? Did you use other data sources? Which ones, why? What representation of the single-cell data did you use? Did you reduce genes into modules? Did you learn a gene regulatory network? We want to know! If adding a particular biological prior didn’t work, how did you judge this and why do you think this failed? 2. Exploration of the problem We’re interested in understanding the problem of generalizing perturbation responses across cell lines. We hope this competition not only produces record-breaking models, but also helps us better understand the problem. Here are the kinds of questions we want you to help us answer: Are there some cell types it’s easier to predict across? What about sets of genes? Do you have any evidence to suggest how you might develop an ideal training set for cell type translation beyond random sampling of compounds in cell types? What is the relationship between the number of compounds measured in the held-out cell types and model performance? Is there a sweet spot? 3. Model design It’s no secret that the top models on the leaderboard are often complex ensembles super-tuned to the test data, and this is amazing for our application. That said, we’re also interested in knowing if there are specific classes of models that perform especially well. Is there certain technical innovation in your model that you believe represents a step-change in the field? Can you show that top performing methods can be well approximated by a simpler model? Is your model explainable? How well can you identify what is causing your model to respond to certain inputs? 4. Robustness How robust is your model to variability in the data? Here are some ideas for how you might explore this, but we’re interested in unique ideas too. Take subsets of the training data (e.g. 95%, 90%, …, 10%). How well does your model performs as a function of percentage of the training data? Add small amounts of noise to the input data. What kinds of noise is your model invariant to? Bonus points if the noise is biologically motivated. 5. Documentation & code style Here we want to make sure your model and analysis notebooks are well documented and follow a consistent code style. At a minimum, we want to see: Documentation describes the general methodology of the solution Documentation describes the required hardware and software dependencies, as well as how to install and run the software Functions and their arguments are documented Code follows basic good practices for the chosen programming language. For example: PEP8 for Python, tidyverse style guide for R Code does not contain duplicated code 6. Reproducibility Here we want to make sure your model and notebooks are reproducible by other scientists. Code is available on GitHub A list of required dependencies is available (e.g. dependencies.txt for Python) Repository contains a Dockerfile or Viash component which can be used to train and run the model Documentation contains an example of how to run the method using the Docker container or Viash component Prizes link keyboard_arrow_up The competition is part of the NeurIPS 2023 Competition Track. In addition to cash prizes, winners will be invited to present at the NeurIPS 2023 Competition Workshop, held virtually in December. Leaderboard Prizes $50,000 will be awarded to the Top 5 teams with the highest scores on the Kaggle Leaderboard at the conclusion of the competition: 1st Place - $12,000 2nd Place - $10,000 3rd Place - $10,000 4th Place - $10,000 5th Place - $8,000 As a condition to being awarded a Leaderboard Prize, a Prize winner must provide a detailed write-up on their solution in the competition forums within 14 days of the conclusion of the competition, i.e. December 12, 2023 11:59PM UTC. Note, if you are a Leaderboard Prize winner and want to compete in the Judges Award, we will accept a Judges Prize style write-up as sufficient to be awarded a Leaderboard Prize. “Judges” Prizes $50,000 will be awarded to 5 teams ($10,000 each) for the \"Judges\" prizes at the conclusion of the competition. Please see “Judges\" Prizes Scoring Rubrics for the details of the evaluation criteria. To be considered for a Judges Prize, teams will need to post a write-up in the Discussion page linked as an official writeup (see instructions ), with headers corresponding to the sections in the “Judges\" Prizes Scoring Rubrics AND fill out a short submission form to confirm you are making a submission to the Judges Prize. Details about that form will go out on November 1. All submissions will be reviewed, and the top ~100 will be assigned grades on the rubric. Judges' reviews and scores will be single-blind (you won't know which Judges reviewed your submission). Final decisions are up to the sole discretion of the Competition Host and may not be appealed. Citation link keyboard_arrow_up Daniel Burkhardt, Andrew Benz, Richard Lieberman, Scott Gigante, Ashley Chow, Ryan Holbrook, Robrecht Cannoodt, and Malte Luecken. Open Problems – Single-Cell Perturbations. https://kaggle.com/competitions/open-problems-single-cell-perturbations, 2023. Kaggle. Cite Competition Host Open Problems in Single-Cell Analysis Prizes & Awards $100,000 Awards Points & Medals Participation 6,641 Entrants 1,318 Participants 1,097 Teams 25,529 Submissions Tags Genetics Biotechnology Tabular Custom Metric Table of Contents collapse_all Overview Description Evaluation Timeline \"Judges\" Prizes Scoring Rubrics Prizes Citation"
  },
  {
    "competition_slug": "icr-identify-age-related-conditions",
    "discussion_links": [
      "/competitions/icr-identify-age-related-conditions/discussion/430843",
      "/competitions/icr-identify-age-related-conditions/discussion/430860",
      "/competitions/icr-identify-age-related-conditions/discussion/430978",
      "/competitions/icr-identify-age-related-conditions/discussion/431173",
      "/competitions/icr-identify-age-related-conditions/discussion/430907",
      "/competitions/icr-identify-age-related-conditions/discussion/431048",
      "/competitions/icr-identify-age-related-conditions/discussion/431028",
      "/competitions/icr-identify-age-related-conditions/discussion/430897",
      "/competitions/icr-identify-age-related-conditions/discussion/430906",
      "/competitions/icr-identify-age-related-conditions/discussion/431312",
      "/competitions/icr-identify-age-related-conditions/discussion/431415",
      "/competitions/icr-identify-age-related-conditions/discussion/430937",
      "/competitions/icr-identify-age-related-conditions/discussion/431099",
      "/competitions/icr-identify-age-related-conditions/discussion/433013",
      "/competitions/icr-identify-age-related-conditions/discussion/432084",
      "/competitions/icr-identify-age-related-conditions/discussion/432726"
    ],
    "discussion_texts": [
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules room722 · 1st in this Competition  · Posted 2 years ago arrow_drop_up 320 more_vert How on Earth did I win this competetion? Hello there! This was really unexpected. I hoped to be in top 10%, but never dreamed of something more. Thanks to everyone who participated in the competetion and especially to those who discussed various ideas and shared their code! And big thanks to SAMUEL, who introduced reweighting the probabilities in this notebook: https://www.kaggle.com/code/muelsamu/simple-tabpfn-approach-for-score-of-15-in-1-min My solution was purely based on some sophisticated DNN: https://www.kaggle.com/room722/icr-adv-model UPDATE: Training notebook attached. What did not work for me: Gradient boosting was obviously overfitting, although I spent just little time on it and didn't make much fine-tuning. The \"greeks\" were useless. I think, because we have no greeks for the test data. FE led to overfitting. What did work: DNN based on Variable Selection Network. [1] No \"casual\" normalization of data like MinMaxScaler or StandartScaler, but instead a linear projection with 8 neurons for each feature. Huge values of dropout: 0.75->0.5->0.25 for 3 main layers. Reweighting the probabilities in the end worked really good. 10 folds cv, repeat for each fold 10-30 times, select 2 best models for each fold based on cv (yes, cv somehow worked in this competition!).The training was so unstable, that the cv-scores could vary from 0.25 to 0.05 for single fold, partially due to large  dropout values, partially due to little amount of train data. That's why I picked 2 best models for each fold. The cv was some kind of Multi-label. At first I trained some baseline DNN, gathered all validation data and labeled it as follows: (y_true = 1 and y_pred < 0.2) or (y_true = 0 and y_pred > 0.8) -> label 1, otherwise label 0. So, this label was somthing like \"hardness to predict\". And the other label was, of course, the target itself. It will be honest to say, that I was lucky to win, but for me personally that means also that DNN wins, and probably not just by luck. And as a big fan of DNNs this makes me proud and happy)) [1] The idea of this network was taken from here https://arxiv.org/abs/1912.09363 adv_model_training.ipynb 2 Please sign in to reply to this topic. comment 162 Comments 9 appreciation  comments Hotness CPMP Posted 2 years ago · 901st in this Competition arrow_drop_up 16 more_vert Well done! I am so happy the winner did not win by sheer luck. That you used a deep learning model of your own is a great sign. Chris Deotte Posted 2 years ago · 806th in this Competition arrow_drop_up 11 more_vert Congratulations. Great work! I also like DNN and I am happy to see DNN win. What did this submission score on public LB? room722 Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 6 more_vert Thank you very much! It scored 0.14. Ravi Ramakrishnan Posted 2 years ago · 854th in this Competition arrow_drop_up 7 more_vert Hearty congratulations for the victory and solo gold medal too @room722 The solo gold will be a huge boost to you for your next steps as you embark on your Kaggle competitions GM journey. Chirag Desai Posted 2 years ago · 2662nd in this Competition arrow_drop_up 1 more_vert I agree with @ravi20076 .. I have heard from few  kaggle grandmasters that wining solo was difficult for them and took long time. . Congrats and best wishes for future.👍 Onkar Kota Posted 2 years ago arrow_drop_up 5 more_vert Congrats!!!🎉 @room722 ! Your achievement is truly inspiring. I am eager to join similar competitions myself. @kaggleqrdl Posted 2 years ago · 633rd in this Competition arrow_drop_up 5 more_vert @room722 Curious, why can't you make it opensource? afaict, it's just a variation of this code here https://github.com/keras-team/keras-io/blob/master/examples/structured_data/classification_with_grn_and_vsn.py which is apache license. fwiw, your code can't be encumbered to win the prize. grant to the Competition Sponsor the license to the winning Submission stated in the Competition Specific Rules above, and represent that you have the unrestricted right to grant that license; You don't lose the right to open source your code. room722 Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Thank you, now I understand this better) Tomás Luna Posted 2 years ago · 1399th in this Competition arrow_drop_up 3 more_vert Congratulations for the great job @room722 ! About the missings imputation, I saw that you used the median, do you think it could be useful to use other method like knn? room722 Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert Thanks! Maybe, but I didn't experimented with this. Michal Bogacz Posted 2 years ago · 930th in this Competition arrow_drop_up 3 more_vert A wonderful and informative solution! This makes me reflect that I looped in FE and gradient boosting, being convinced that too little effect is a matter of too little effort in both topics. Theoretically, I knew that if the approach did not give the intended results, it had to be changed, but I was still trying to prove to myself that I was right with FE and gradient boosting, because that was the result of previous experiences (only on other data with larger numbers). Lesson learned, thanks so much and congratulations on your win! Jake Posted 2 years ago · 419th in this Competition arrow_drop_up 3 more_vert Interesting, I used a CNN and it was my only model in which the public and private lb were the same! Stephen Bailey Posted 2 years ago · 516th in this Competition arrow_drop_up 3 more_vert Congrats! It sounds like you came up with a very good solution. TheStrugglingEngineer Posted 2 years ago arrow_drop_up 4 more_vert Congratulations! It takes a great deal of hubris on bad losers part to claim that winning in this competition is a matter of luck. Humility will do everyone a great deal of good. @kaggleqrdl Posted 2 years ago · 633rd in this Competition arrow_drop_up 0 more_vert Luck plays a huge factor in these comps .. being lucky not to be competing against skillfull and talented folks like @room722 I had expected superior models by newcomers, but nothing quite this superior.  Well deserved 1st place win. Tilii Posted 2 years ago · 2555th in this Competition arrow_drop_up 4 more_vert Congratulations on your top spot. This actually sounds like a very original model, and your score stands out for sure. Looking forward to hearing all the details. Vasily Sevostynov Posted 2 years ago arrow_drop_up 1 more_vert Congrats! Great job! Viji Posted 2 years ago · 1147th in this Competition arrow_drop_up 1 more_vert Congrats for a well deserved win! Thanks for sharing your notebook. Activity regularization, non_linearity handling methods along with gating mechanisms used are impressive. If you can share - did you have to resample or preprocess for noise reduction?  Thanks room722 Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Thank you very much! No, I didn't) Viji Posted 2 years ago · 1147th in this Competition arrow_drop_up 0 more_vert Thanks for your confirmation. Mayank 823 Posted 2 years ago arrow_drop_up 1 more_vert wow getting placed 1 st is surely a big feat cheers RazPa Posted 2 years ago · 3429th in this Competition arrow_drop_up 1 more_vert congratulation! ! ! charles chuang Posted 2 years ago · 3457th in this Competition arrow_drop_up 1 more_vert Congrats! I think greeks data is useful for ML methods, but not for deep learning, right? JonesSausage Posted 2 years ago arrow_drop_up 1 more_vert Nice job, congrats! Woosung Choi Posted 2 years ago · 282nd in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for sharing your idea. Arist Smith Posted 2 years ago · 4779th in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for attaching your code to learn from! CarloSgara Posted 2 years ago · 571st in this Competition arrow_drop_up 1 more_vert Congratulations! GWCTCHS Posted 2 years ago arrow_drop_up 1 more_vert Wow, great job! Ern711 Posted 2 years ago · 1221st in this Competition arrow_drop_up 1 more_vert Congrats, sounds like an interesting approach! Kaustav Chaudhury28 Posted 2 years ago · 3584th in this Competition arrow_drop_up 1 more_vert Nicely Done, you used CV did you separately tune each cv vasavibala Posted 2 years ago arrow_drop_up 1 more_vert Congratulations for your achievement. Wonderful job done by using DNN Ashutosh Bharti Posted 2 years ago arrow_drop_up 1 more_vert Amazing job done! All the best for your future competitions. @room722",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules opamusora (Ivan Viakhirev) · 2nd in this Competition  · Posted 2 years ago arrow_drop_up 80 more_vert Wow (and our solution) To put it mildly, I'm in complete shock, I woke up today with my teammate congrats and I got intrigued as to what place we got? I burst out laughing when I saw it was 2nd place, and the funniest thing is my solution is just CV - no probing, no nothing here's my solution: https://www.kaggle.com/code/opamusora/main-notebook What we did: just like many people, we used time and max(time)+1 for test removed rows where time is None, I noticed a weird cluster that was far away from all other data when playing with umap and it were rows with absent time filling Nan values with -100, probably doesn't matter if its median or low numbers. reducing dimensions with umap and then labeling clusters with kmeans, it didn't bring a lot of score, but it's there did feature permutation manually, dropped any cols that made score even slightly worse for a model I used Catboost, xgb with parameters from some public notebook and tabpfn. LGBM didn't seem to work for me as it always dropped my CV then we just averaged our predictions for test and that's it Also, I want to mention that we wanted to try edit tabpfn to get embeddings and we had an idea to try fine tune tabpfn, but it didn't work out. I also tried optuna for optimizing my models, but it didn't work out Stacking didn't help my score much as well edit: My last 2 sumbissions on this competitions are the highest scoring ones 😅 3 Please sign in to reply to this topic. comment 36 Comments Hotness raddar Posted 2 years ago · 2404th in this Competition arrow_drop_up 18 more_vert I wanted to remove rows, but did not have balls to do that. this might have been game changer. congrats! Tilii Posted 2 years ago · 2555th in this Competition arrow_drop_up 14 more_vert Made some brave choices, like removing rows, that I never would have done. Take a bow! kvis Posted a year ago · 2nd in this Competition arrow_drop_up 1 more_vert great work! Iqbal Syah Akbar Posted 2 years ago · 4331st in this Competition arrow_drop_up 5 more_vert Congrats on the 2nd place! It's really funny that the 1st and 2nd solutions are wildly different. Also wow, dropping all rows with no time value at all. That's one hell of a move there when the dataset is already small. opamusora (Ivan Viakhirev) Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 3 more_vert Yeah, it also dropped my cv score, but data points were really far from the main cluster so I decided to remove it anyway yukiya Posted 2 years ago · 661st in this Competition arrow_drop_up 6 more_vert Congratulations, how nice it must be to wake up laughing 😁 removed rows where time is None So @cdeotte was right, there's something about time Artem Fedorov Posted 2 years ago · 836th in this Competition arrow_drop_up 0 more_vert All the rows without time had no health condition, that can't be by accident. Wojciech \"Victor\" Fulmyk Posted 2 years ago · 153rd in this Competition arrow_drop_up 3 more_vert @opamusora congratulations on getting 2nd place and I really enjoyed your contribution to the discord discussion today. I really appreciated you sharing the fact that you used permutation feature importance to select features, and that your best model had around 20 features (if I remember correctly). Very insightful! opamusora (Ivan Viakhirev) Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 2 more_vert Thanks. Around 20 features removed if I'm not mistaken. Tilii Posted 2 years ago · 2555th in this Competition arrow_drop_up 3 more_vert @opamusora It was great to hear details about your solution. Again, well done! Areej Malkawi Posted 2 years ago · 6332nd in this Competition arrow_drop_up 1 more_vert congratulations! did you use the greeks data set? jianmin yu Posted 2 years ago · 309th in this Competition arrow_drop_up 0 more_vert yes, he used greeks data. Gyula Maloveczky4 Posted 2 years ago · 2203rd in this Competition arrow_drop_up 1 more_vert Can you share details of your hyperparameter tuning? opamusora (Ivan Viakhirev) Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 0 more_vert CatBoost has standart parameters and everything else I took from public notebooks, it worked the best like that anyway and didn't need change. Sam Andersson Posted 2 years ago · 254th in this Competition arrow_drop_up 1 more_vert Awesome congrats! Pardeep Singh Posted 2 years ago · 3307th in this Competition arrow_drop_up 1 more_vert Great work @opamusora Congrats! Last Dance Posted 2 years ago arrow_drop_up 1 more_vert How did TabPFN work out for you ? PS: Dropping the rows was a real baller move. opamusora (Ivan Viakhirev) Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 0 more_vert Tabpfn improved my ensemble and overall its a model I will add to my arsenal from now on, from what I understood it's still in development so I'm eager to find out what its capable of. NA Posted 2 years ago arrow_drop_up 1 more_vert Congratulations on the huge win! Greetings to everyone who wanted to delete rows but did not decide to do so Simon H Posted 2 years ago · 1339th in this Competition arrow_drop_up 1 more_vert Interesting! Thanks for the write-up. I tried to use tabpfn too and it didn't work out well, I need to spend more time with it. Congrats on your place jump! The score change was wild! Boyka Posted 2 years ago · 827th in this Competition arrow_drop_up 1 more_vert Deleting rows, very instructive! Vitaly Kudelya Posted 2 years ago · 805th in this Competition arrow_drop_up 2 more_vert You dropped 23% of train data (time is None), they all have Class_0 This data seemed strange to me in train.csv, but dropping 23% of data seemed to much for me ) opamusora (Ivan Viakhirev) Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Yeah, I was hesitant to do it and it obviously dropped some score (since we remove a lot of easy class 0 ), but eventually I decided to keep it cause it would be better for general cases. Artem Fedorov Posted 2 years ago · 836th in this Competition arrow_drop_up 2 more_vert Couldn't keep myself from tring to remove rows and see what happens. Tested both my undersample and oversample solutions. Undersample solution lost two points in public dataset (from 0.16 to 0.18) and improved 5 points on private test set (from 0.43 to 0.38!!). Oversample solution also lost two points in public dataset (from 0.16 to 0.18) but it only improved from 0.45 to 0.44 on private test set. Any way, congratulations! No way I would try something like that myself, given the fact that results for the public test set improved. Man of the year Posted 2 years ago · 188th in this Competition arrow_drop_up 2 more_vert How did you make your CV? opamusora (Ivan Viakhirev) Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 2 more_vert A simple 4 split SKfold ruiruidayoooo Posted 2 years ago · 3546th in this Competition arrow_drop_up 0 more_vert @opamusora congratulations on getting 2nd place!! Thank you for sharing your notebook. In your solution, k-means & UMAP are used for clustering data. When I make labels for clusters, always being trouble to determine the n_neighbors (or n_clusters) parameter. If you wouldn’t mind, could you share the process to decide the numbers. Sambit Barik Posted 2 years ago arrow_drop_up 0 more_vert Great work @opamusora and nice writeup. Can you please describe which parameters did you tune and what was the CV making process ? mumagi Posted 2 years ago arrow_drop_up 0 more_vert congratulations! how much time have you guys been coding before this competition? opamusora (Ivan Viakhirev) Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 0 more_vert 3 years. By the way, I freaked out a bit when I saw Kevin Levrone pfp, since I got one on my media accounts 😂 mumagi Posted 2 years ago arrow_drop_up 1 more_vert hahaha kevin levrone is ubiquitous Kerwin Posted 2 years ago · 6258th in this Competition arrow_drop_up 0 more_vert Congratulations! But why the new version drops the part of removing raws which date is null? sissiki Posted 2 years ago · 2228th in this Competition arrow_drop_up 0 more_vert congrats & awesome work! 🙌 Your notebook is very clear and easy to understand, even for people who are new to Kaggle challenges! jianmin yu Posted 2 years ago · 309th in this Competition arrow_drop_up 0 more_vert remove data is brave, it improve ? score in public and private data. ls Posted 2 years ago · 5644th in this Competition arrow_drop_up 0 more_vert Nice work. Congrats on 2nd place",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules siguo · 3rd in this Competition  · Posted 2 years ago arrow_drop_up 32 more_vert 3rd Place Solution for the \"ICR - Identifying Age-Related Conditions\" Competition First of all, I was really surprised to be able to achieve this result. In fact, after participating in the competition and simply implementing a baseline code, I rarely paid attention to this competition again because the company usually worked overtime😅. The final result was a baseline code based on the catboost model at that time, and its public score was 0.21. Due to anonymous data features and medical-related health features, my initial idea is to construct new features through the ratio between different features, just as some indicators in the medical examination report are also calculated by the ratio between other indicators. Before this, I planned to filter some anonymous features through corr, so as not to construct too many invalid features. However, without further attempts, the final code is still the cross calculation of all features. The more effective operations in this competition should be the following two points 1.One is the cross calculation of features 2.The other is the catboost model Because my lightgbm model with the same features got 0.22 on public score and 0.38 on private score. here's my solution: https://www.kaggle.com/code/junyang680/icr-lightgbmbaseline Regarding the parameter selection of the lightgbm model and the catboost model, it seems to be based on the contents of some notebooks, but sorry, some have forgotten Please sign in to reply to this topic. comment 5 Comments Hotness siguo Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 3 more_vert I am wondering if the shake in this competition is so serious, is it because everyone has done too many operations on the small data to cause overfitting, or submitted the post-processing operations for fitting public to private? Hoang Nguyen Posted 2 years ago · 439th in this Competition arrow_drop_up 2 more_vert Both I believe. Everyone within top 250 got shaken down to the bottom. I'm sure they all tried LB probing and postprocessing, but given that many of them are Masters/GMs, I believe they are experienced enough to choose a less risky approach as final submissions. However, given the small size of the dataset, investing that much time is already a risk of overfitting, which turns out to be true. Congratulations on your gold medal! jianmin yu Posted 2 years ago · 309th in this Competition arrow_drop_up 0 more_vert Yes, some one has the experience that lower LB score is better. So much of competitors try to get the lower LB by many methods. Pardeep Singh Posted 2 years ago · 3307th in this Competition arrow_drop_up 0 more_vert Congrats @junyang680 and Thanks for sharing your code + writeup! Michael Bryant Posted 2 years ago · 2095th in this Competition arrow_drop_up 0 more_vert 😭. Good job.",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules Andrej Vetrov · 4th in this Competition  · Posted 2 years ago arrow_drop_up 22 more_vert 4rd Place Solution for the \"ICR - Identifying Age-Related Conditions\" I'm pleasantly surprised to see such a strong perturbation on the leaderboard, resulting in me going to the very top of the leaderboard. No doubt, when I sent my 3rd attempt 2 months ago (0.16 on public), I certainly did not suspect that it would end up in 4th place on private (0.34). Since then, I've seen the score decrease rapidly on public and I've realized that all these solutions are heavily overfitted, which is something I've tried to avoid in every solution I've made. Now for the key features of my solution. 1) Recursive filling of gaps in features using regression on CatBoostRegressor (default hyperparameters), 2) greeks['Epsilon'] Unknown were filled with greeks['Epsilon'].min() 3) row_id - row number in train and in test when sorting by Epsilon 4) Creation features with CatBoostClassifier training for each value in 'Alpha', 'Beta', 'Gamma', 'Delta' - probabilities for corresponding values of these categories similarly https://www.kaggle.com/competitions/icr-identify-age-related-conditions/discussion/430907 . To avoid overtraining for prediction on test, I used 5 - fold cross validation and then simple averaging, 5) Final model - CatBoostClassifier without any tuning of hyperparameters and feature elimination In subsequent attempts I tried to expand the feature space by inventing various new features, complications of filling in the gaps, for example, I tried to predict epsilon and row_id, but I did not get any improvement in the results on cross validation, moreover, the results became more unstable, so I realized that these complications only lead to overfitting and I stopped these attempts. My solution is https://www.kaggle.com/code/andrejvetrov/third?scriptVersionId=131958512 Please sign in to reply to this topic. comment 5 Comments Hotness Murugesan Narayanaswamy Posted 2 years ago · 2359th in this Competition arrow_drop_up 1 more_vert row_id is one of the most important features?! It must have served as pseudo feature to reflect time dependent changes. How much does the Private Score changes if you don't use row_id? This is similar to Parkinson's disease competition but the objective there was to predict disease progression. Andrej Vetrov Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 0 more_vert Yes, you are right, I added this simple feature more intuitively for the reason that although the size of the dataset is small, it has a time component that must be taken into account somehow. According to feature importance in catboost classifier the feature is on the 4th position. I was getting 0.2(0.37) without it, compared to 0.16(0.34) with it. Unfortunately, some other features I tried to extract from epsilon didn't give any gain. Murugesan Narayanaswamy Posted 2 years ago · 2359th in this Competition arrow_drop_up 0 more_vert Congratulations for the achieving the fourth position! Also,I think you are on-track to become Kaggle GrandMaster! Andrej Vetrov Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thank you so much Murugesan Narayanaswamy Pardeep Singh Posted 2 years ago · 3307th in this Competition arrow_drop_up 0 more_vert Great Work @andrejvetrov Thanks for sharing your code!👏🚀 Arvind (Yetirajan) Narayanan Iyengar Posted 2 years ago arrow_drop_up 0 more_vert Congratulations @andrejvetrov ! very nice work! Andrej Vetrov Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thank you very much Arvind (Yetirajan) Narayanan Iyengar",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules Cihat Emre Çeliker · 5th in this Competition  · Posted 2 years ago arrow_drop_up 39 more_vert 5. place solution Knowing the inevitable shakeup, I submitted couple basic solutions months ago and forgot about the competition. I was surprised when my friends congratulated me in the morning. Here's the code Main points: Trained models for each Alpha, Beta, Gamma, Delta and stacked these probabilities to be used as features. ####Created lgbm imputer models for every feature even if it has no missing values on train data. Used RepeatedStratifiedKFold(n_splits=5, n_repeats=5) with a basic catboost model. note: Removing imputers didn't effect the score, so the main strength is stacking greeks. After this solution, I tried brute-force feature engineering and other modeling approaches, but they didn't help on public so I left it as final submission Please sign in to reply to this topic. comment 10 Comments 1 appreciation  comment Hotness Pranav Jadhav Posted 2 years ago arrow_drop_up 1 more_vert Congratulations @celiker . 5th place itself is a great achievement. Pardeep Singh Posted 2 years ago · 3307th in this Competition arrow_drop_up 1 more_vert Congrats @celiker Great Work! 👏 C R Suthikshn Kumar Posted 2 years ago · 6260th in this Competition arrow_drop_up 1 more_vert Congratulations on topping the LB. Thanks for sharing the code.   Can you comment on balanced log loss being used as metric is the main reason for shakeup. Mehdi Pedram Posted 2 years ago · 2190th in this Competition arrow_drop_up 2 more_vert Congratulations on your gold medal. I guess your strength was the way how you used Greeks. waiting to see your code. Thanks for sharing. Cihat Emre Çeliker Topic Author Posted 2 years ago · 5th in this Competition arrow_drop_up 0 more_vert I've just tried without stacking greeks and private score was 0.4. It definitely helped hengck23 Posted 2 years ago arrow_drop_up 0 more_vert Lesson learned: \"Knowing the inevitable shakeup, I submitted couple basic solutions months ago and forgot about the competition. \" shakeup should be discussed in early stages of the competition and not the end. congrads!😃 HLI111111 Posted 2 years ago · 3449th in this Competition arrow_drop_up 0 more_vert I guess this showed that if some data is structured with some locally smooth/bounded features, finding good transformations to describe those \"locality\" and throw in GBDT works (and, for smaller dataset, regulating depths and number of node is important). Maybe? anindyakg Posted 2 years ago · 1161st in this Competition arrow_drop_up 0 more_vert Congratulations and thanks for sharing the tips ! Biliarto Sastro Cemerson Posted 2 years ago arrow_drop_up 0 more_vert Congrats! Great job! Appreciation (1) PixelShooter Posted 2 years ago · 3317th in this Competition arrow_drop_up 0 more_vert Thanks for your sharing!",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules Diego Silva França · 6th in this Competition  · Posted 2 years ago arrow_drop_up 30 more_vert 6th Position winner solution for the : ICR - Identifying Age-Related Conditions My solution is divided into 7 big steps: (Code version 10) Interpolate the missing data using a linear method using the 'interpolate' instance from pandas. Use a random forest classifier to find the most important features in the dataset using 'gini-importance'. Use Bayesian optimization to find the optimal parameters of the XGBoost classifier. Repeat step 3 multiple times to gather many optimal parameters for the XGBoost classifier. Make an ensemble of XGBoost classifiers using the optimal parameters. Fine-tune the XGBoost classifiers again using GridSearchCV (because Bayesian optimization is just an estimation of the parameters). Use a voting classifier (the mean of the probabilities of each XGBoost) to classify the test set. Here is my code (I performed step 2 in my personal computer): https://www.kaggle.com/code/diegosilvadefrana/notebooke87ef51e7e/notebook Please sign in to reply to this topic. comment 10 Comments Hotness Pardeep Singh Posted 2 years ago · 3307th in this Competition arrow_drop_up 1 more_vert Congrats and Great Work @diegosilvadefrana ! Thanks for sharing your solution! charles chuang Posted 2 years ago · 3457th in this Competition arrow_drop_up 1 more_vert Nice solution, only XGBoost but exquisite processing. I learned from you is that useless features should be removed to decrease the noise affection. BTW, I am also wondering that performance of XGboost compared to CatBoost. Thomas Meißner Posted 2 years ago · 1283rd in this Competition arrow_drop_up 1 more_vert Great solution! How to you define the steps for grid search when Bayesian search only returned an estimation? Diego Silva França Topic Author Posted 2 years ago · 6th in this Competition arrow_drop_up 0 more_vert The issue is that for certain parameters, such as \"n_estimators,\" Bayesian optimization treats them as float numbers. So, I had to approximate this float number to an integer, which may reduce the accuracy of the model. So I conducted a new round of optimization using GridSearch for the learning_rate to try compensate this issue. Ravi Ramakrishnan Posted 2 years ago · 854th in this Competition arrow_drop_up 0 more_vert Hearty congratulations for the position and solo gold @diegosilvadefrana All the best!! @kaggleqrdl Posted 2 years ago · 633rd in this Competition arrow_drop_up 0 more_vert Man I'm never going to use LGBM again.  Sigh.  Congrats on the solo gold! Curious what the intuition behind using roc_auc_score for gridsearch scoring? Diego Silva França Topic Author Posted 2 years ago · 6th in this Competition arrow_drop_up 1 more_vert Since the data was a little imbalanced and it was a binary classification, I figured that ROC_AUC was robust for selecting the optimal parameters. Also, the dataset was too small, so any dev set (in case of using a neural network) would be too biased. Ravi Ramakrishnan Posted 2 years ago · 854th in this Competition arrow_drop_up 1 more_vert I don't think we can eliminate LGBM in this case generally. I think many of us use this well for most daily work across use cases. It perhaps did not work here but is not likely to be the case in other datasets @kaggleqrdl @kaggleqrdl Posted 2 years ago · 633rd in this Competition arrow_drop_up 1 more_vert I was, indeed, being facetious. Tilii Posted 2 years ago · 2555th in this Competition arrow_drop_up 1 more_vert Well done and a richly deserved solo gold! Glad to see that you showed feature elimination to be a productive approach. Diego Silva França Topic Author Posted 2 years ago · 6th in this Competition arrow_drop_up 1 more_vert Thank very much for your kind words. Your notebooks are beacons of knowledge for me. 😀 Elias Batista Posted 2 years ago arrow_drop_up 0 more_vert Congrats, awesome solution!",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules Manthan Bhagat · 7th in this Competition  · Posted 2 years ago arrow_drop_up 15 more_vert 7th Place Solution After doing few submission in initial part of the Competition, I didn't work on it, as heavy shakeup was expected. But its a positive shakeup for me now (with a solo gold) 😅. Here my submission details: Fill Nan data with 0 5 fold Multi Label Stratified using Greeks Values EJ was categorical so used Label Encoding Also Label Encoding for Beta, Gamma and Delta Used MultiClass CatBoost Classifier for all the models Saw that Beta, Gamma and Delta have very high predictive capability, but were given only for training data,  so used all the other features to predict encoded Beta, Gamma and Delta using 5fold strategy, and used those features along with other given features to predict Multi Class Alpha. Then converted the Alpha probs to binary probs by adding B, D and G prob to predict class 1 and using A prob to predict class 0 Thats it. Thank You PS here my submission code Please sign in to reply to this topic. comment 7 Comments Hotness Pardeep Singh Posted 2 years ago · 3307th in this Competition arrow_drop_up 1 more_vert Great Work @manthanbhagat Congrats on Solo Gold 🙌 Manthan Bhagat Topic Author Posted 2 years ago · 7th in this Competition arrow_drop_up 0 more_vert Thank you @pardeep19singh Random Draw Posted 2 years ago · 306th in this Competition arrow_drop_up 0 more_vert Thanks for sharing your solution. Congratulations on gold AND moving up to competition master! samson fha Posted 2 years ago · 5056th in this Competition arrow_drop_up 0 more_vert Thank you for sharing and congrats! I have a question here: You used all features to predict Beta, Gamma and Delta, then used (all features + Beta, Gamma and Delta) to predict final Alpha , did this method utilize more information than simply using all features to predict Alpha ? Manthan Bhagat Topic Author Posted 2 years ago · 7th in this Competition arrow_drop_up 0 more_vert Yes that's exactly what I did. Its not about how much more information we give to model, its about how we represent the information and also what is the predictive power of each feature, since Beta, Gamma and Delta had high predictive power, I use them. charles chuang Posted 2 years ago · 3457th in this Competition arrow_drop_up 0 more_vert Thanks for your nice sharing. I also predict the metadata (gamma, delta, etc. ), but not return good privacy score. Maybe something wrong with me. This comment has been deleted.",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules Clayton Kjos · 8th in this Competition  · Posted 2 years ago arrow_drop_up 16 more_vert 8th Place Solution for the \"ICR - Identifying Age-Related Conditions\" Competition Frankly, I was somewhat shocked by the result. I had a pretty solid idea that there would be a lot of shakeup - but certainly wasn't expecting to be in the top 10. The public/private scores for this submission are 0.19/0.34 respectively. Context: Business context: https://www.kaggle.com/competitions/icr-identify-age-related-conditions/overview Data context: https://www.kaggle.com/competitions/icr-identify-age-related-conditions/data Overview of the approach I focused first on predicting if someone had a specific Age-Related Condition rather than the class. This proved more effective than predicting class alone. Effectively, I create an Ensemble of Ensemble Predictors focused on specific conditions. The models in the primary ensembles were XGBoost & TabPFN. Details of Submission Imputing Strategy My primary imputing strategy utilized XGBoost to predict most of the missing values rather than dropping, filling with 0, mean, mode, etc. Compared to median imputing, this resulted in an improvement in the Public score of 0.04 (with no change in Private Score). In this submission, there were two fields that contained the majority of the nan values in the training set. As I couldn't be sure this would hold true in any other set, I created a function to impute these values in a brief loop while dropping additional columns from subsequent impute runs when multiple columns in the target were nan values. Context: Business context: https://www.kaggle.com/competitions/icr-identify-age-related-conditions/overview Data context: https://www.kaggle.com/competitions/icr-identify-age-related-conditions/data Overview of the approach I focused first on predicting if someone had a specific Age-Related Condition rather than the class. This proved more effective than predicting class alone. Effectively, I create an Ensemble of Ensemble Predictors focused on specific conditions. The models in the primary ensembles were XGBoost & TabPFN. Details of Submission Imputing Strategy My primary imputing strategy utilized XGBoost to predict most of the missing values rather than dropping, filling with 0, mean, mode, etc. Compared to median imputing, this resulted in an improvement in the Public score of 0.04 (with no change in Private Score). In this submission, there were two fields that contained the majority of the nan values in the training set. As I couldn't be sure this would hold true in any other set, I created a function to impute these values in a brief loop while dropping additional columns from subsequent impute runs when multiple columns in the target were nan values. # For Use with Float Fields def imputeRegressor ( df, field, dl=[] ): print ( f\"Impute Regressor w/ XGBoost for {field} \" ) print ( f\"NaN Values @ Start: {df[field].isna(). sum (). sum ()} \" ) # Multiple Fields May contain NA at the same time. drop_list = set (df.columns).intersection( set (dl))\n    imputeDF = df.copy()\n    imputeDF.drop(drop_list, axis= 1 , inplace= True )\n    impute_X = imputeDF.drop(field, axis= 1 ).dropna()\n    impute_y = imputeDF[field]\n    imputeDF = impute_X.join(impute_y)\n    trainDF = imputeDF[imputeDF[field].notna()].dropna()\n    train_X = pd.get_dummies(trainDF.drop(field, axis= 1 ))\n    train_y = trainDF[field] # We can only solve for the ones where other fields are not na. testDF = imputeDF[imputeDF[field].isna()]\n    test_X = testDF.drop(field, axis= 1 )\n    test_y = testDF[field]\n    test_X.dropna(inplace= True )\n    testDF = test_X.join(test_y)\n    test_y = testDF[field] print ( f\"Predicting for a training set of {train_X.shape} and test set {test_X.shape} \" ) if ( len (test_X)) == 0 : print ( \"Nothing to Predict\" ) return # Handle Different Feature Counts colsToUse = set (train_X.columns).intersection( set (test_X.columns))\n    train_X = train_X[colsToUse]\n    test_X = test_X[colsToUse]\n    modelOne = XGBRegressor(n_estimators= 200 ).fit(train_X, train_y)\n    preds = modelOne.predict(pd.get_dummies(test_X))\n    predictedDF = test_X\n    predictedDF[field] = preds.astype( float ) # Save out the Results to the base DF for i, row in predictedDF.iterrows():\n        df.loc[i, field] = row[field]\n    remainingToSolve = df[df[field].isna()].count() print ( f\"NaN Values @ End: {df[field].isna(). sum (). sum ()} \" ) content_copy Feature Engineering Aside from the imputing step above and converting categoricals into ints, I did no manipulation of the float data. There are quite a few fields with outliers that could perhaps be addressed, and at least one where the values appear to max out. In this case, I didn't have time to play around. Ensemble Models Used XGBoost's Classifier w/o fine tuning (fine tuning overfitted) TabPFN As a metric (for XGBoost) I used balanced log loss, with the binary logistic objective. Each model was fit using cross validation, scored by Balanced Log Loss, and the best performing model from each run was chosen. Greeks & Class The greeks file contains the Alpha field that differentiates the specific conditions that might result in the Class value being equal to 1. The remainder of the Greeks file was ignored. Ensemble of Ensemble Strategy Instead of a single ensemble predicting Class, we train 4. We train ensemble models to predict Class Alpha \"B\" Alpha \"D\" Alpha \"G\" For Positive predictions, we take the max of positive predictions for B,D,G & average the result with the positive prediction of class. For Negative predictions, we take the min of negative predictions for B,D,G & average the result with the negative prediction of class. Note that in the code below, I refer to the predictors for B, D, and G, as Beta, Delta, and Gamma. This is not to be confused with the fields of the same name (which are not used). def ensembler ( train_X, modelAlpha, modelBeta, modelDelta, modelGamma, postProcess= False ): \"\"\"\n    Creates an Ensemble Prediction for Train_X based on the respective features\n    Ensembles the Alpha, Beta, Delta, & Gamma Predictors.\n    Beta, Delta, and Gamma individually predict each disease while Alpha predicts overall disease. \n    We combine these together to generate an ensemble of the ensembles. \n    \"\"\" alphaPreds = modelAlpha.predict_proba(train_X, False )\n    betaPreds = modelBeta.predict_proba(train_X, False )\n    deltaPreds = modelDelta.predict_proba(train_X, False )\n    gammaPreds = modelGamma.predict_proba(train_X, False )\n    ensemblePreds = [] for i, pred in enumerate (betaPreds):\n        truthy = mean([ max ([betaPreds[i][ 1 ], deltaPreds[i][ 1 ], gammaPreds[i][ 1 ]]), alphaPreds[i][ 1 ]])\n        falsey = mean([ min ([betaPreds[i][ 0 ], deltaPreds[i][ 0 ], gammaPreds[i][ 0 ]]), alphaPreds[i][ 0 ]])\n        ensemblePreds.append([falsey, truthy]) # Post Process our Predictions if postProcess:\n        ensemblePreds = modelAlpha.post_process_proba(ensemblePreds) return np.array(ensemblePreds) content_copy Special Note on Balanced Log Loss & The Greek Strategy For this competition, we did not need to ensure that predicted probabilities sum to 1. There are quite a few public notebooks utilizing a version of this function that simply inverts the positive predictions. This would obviously be incorrect with my prediction strategy described above as I can have total probabilities both greater and less than 1. For this notebook, I chose to ensure the Balanced Log Loss function I used took the full prediction probabilities and dealt with them independently rather than ensuring the probabilities summed to 1. Both should presumably be equivalent. Caveats from a better performing notebook. A better performing notebook (Private 0.33) ignored the Class model altogether. Prediction Balancing I balanced the prediction probabilities based on the weight of the predictions by the ensembled' models on a per model basis during training, and in aggregate when making predictions. I found this performed better during limited testing than providing an up-front weighting to XGBoost for the classes based on the ratio in the training set. Picking Features For each of the Ensemble Models (Class, B, G, D), I ran it through a loop removing features with feature importances scored <=0 to check which had the best performance through training. I kept the feature set with the best scoring run. Caveats from a better performing notebook. A better performing notebook (Private 0.33) used all features. Hyperparameter Tuning I ran through 200 trials w/ Optuna on each model (Class, B, D, G) to tune hyper parameters. Ultimately, I found that tuning resulted in worse scores for both public & private sets (0.01 to 0.05 worse). However, given the time limitation, I did not re-perform this for the reduced feature set for each model. This leaves room for re-running hyperparameter tuning on the reduced feature sets. I still suspect that at this point we're already working on overfitting against my core solution. A note on Post Processing I tried simple prediction post processing strategies where I pushed high values towards 1, and low values towards 0. I also tried ratio oriented approaches. Both tended to improve the training result score significantly (~0.5 all the way down to ~0.04 range) - but this resulted in significantly worse scores on both the public and private data set. I observed this strategy in the highest scoring public notebook ( https://www.kaggle.com/code/vadimkamaev/postprocessin-ensemble ) that seemed to have been the source of many high scores. I'm curious now if this harmed those notebooks in a fashion as it did mine. Sources ( https://www.kaggle.com/code/vadimkamaev/postprocessin-ensemble ) Inspired my failed attempts at post processing & source of my swap to balancing predictions during prediction rather than via the xgboost class weighting parameter (tabPFN didn't seem to have this, and attempts to mix/match were unnecessarily complex). Please sign in to reply to this topic. comment 5 Comments Hotness Pardeep Singh Posted 2 years ago · 3307th in this Competition arrow_drop_up 0 more_vert Congrats @claytonkjos Thanks for sharing such a detailed writeup! C R Suthikshn Kumar Posted 2 years ago · 6260th in this Competition arrow_drop_up 0 more_vert Congratulations on Topping the LB. Very interesting  approach. This is indeed interesting. You have  ensemble with TabPFN and XGBoost , we can learn from your approach where things need improvement.  I think balanced log loss metric  may be the reason for many upsets in this competition.  Any small misclassification will have a very large impact on the score. Clayton Kjos Topic Author Posted 2 years ago · 8th in this Competition arrow_drop_up 0 more_vert Seem Many people tried combinations of XGBoost, TabPFN, LGBM, and CatBoost. I'm kind of keen to know if there's an alternative configuration that would have made a significant difference. Clayton Kjos Topic Author Posted 2 years ago · 8th in this Competition arrow_drop_up 0 more_vert I ran some submissions after the fact to try out CatBoost in the mix. +0.02 (worse) including it with XGBoost and TabPFN. Still testing out some other combinations. Mehdi Pedram Posted 2 years ago · 2190th in this Competition arrow_drop_up 0 more_vert Congratulations on your gold medal. Thanks for sharing. Clayton Kjos Topic Author Posted 2 years ago · 8th in this Competition arrow_drop_up 0 more_vert Thank you.",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules Anna · 9th in this Competition  · Posted 2 years ago arrow_drop_up 20 more_vert 9th Place Solution for the \"ICR - Identifying Age-Related Conditions\" Competition Hello! This was unexpected, I'm really happy! Context section Business context: https://www.kaggle.com/competitions/icr-identify-age-related-conditions Data context: https://www.kaggle.com/competitions/icr-identify-age-related-conditions/data Overview of the Approach I was very concerned about overfitting my model. With so many different opinions in the discussions and the difficulty of accurately gauging technique effectiveness due to the small dataset, I was unsure about the best approach to take. As a solution, I decided to integrate a mix of different models along with a variety of data preprocessing techniques. Details of the submission Part one: Data Preprocessing: Leveraged an over sampler to balance data distribution. Use of Greeks' Epsilon with Epsilon.max() + 1 for the test set. Employing the SimpleImputer with the strategy set to 'constant'. Models: Ensemble of two XGBClassifiers and two TabPFNClassifiers. Employed 5-fold cross-validation, picking the best model. Part two: Data Preprocessing: No Greek's Epsilon. Implemented feature scaling. Used feature selection, opting for a subset of 40. Models: XGBClassifier and LGBMClassifier. Employed 15-fold cross-validation, culminating in an ensemble mean. Conclusion I found that the best score was from computing the mean of the first ensemble assigning a weight of 3 to it, while each of the other two models held a weight of 1. What did not work for me: postprocessing (obviously! 😉). Other models such as Tree Classifiers and Neural Networks… Using other fields (apart from Epsilon) in Greeks Sources: I drew inspiration from a publicly shared notebook for the first part: https://www.kaggle.com/code/aikhmelnytskyy/public-krni-pdi-with-two-additional-models I'd also like to thank everyone who actively participated in the discussion forum, I believe that this allowed me to learn a lot! Please sign in to reply to this topic. comment 6 Comments Hotness Hoang Nguyen Posted 2 years ago · 439th in this Competition arrow_drop_up 3 more_vert In your part 2, may I ask Why did you decide to use feature scaling? I thought XGB and LGBM are robust against feature scale and don't require scaling before fitting? What method did you use to select the top 40 features, and why top 40 but not other numbers? Thanks and congratulations on your first gold! Anna Topic Author Posted 2 years ago · 9th in this Competition arrow_drop_up 0 more_vert Hello, I employed feature scaling primarily for consistency as I was anticipating the use of other models. I utilized a few feature selection methods to identify the most informative features. The choice of 40 features was somewhat arbitrary and it ended up like that because of other thresholds I used. Thank you for your congratulations! Pardeep Singh Posted 2 years ago · 3307th in this Competition arrow_drop_up 1 more_vert Congrats @annafabris Thanks for sharing this writeup! C R Suthikshn Kumar Posted 2 years ago · 6260th in this Competition arrow_drop_up 1 more_vert Congratulations on topping the LB. Thanks for sharing details about your approach. Mehdi Pedram Posted 2 years ago · 2190th in this Competition arrow_drop_up 1 more_vert Congratulations on your gold medal. My guess is that using epsilon the way you described helped you a lot. Thanks for sharing. This comment has been deleted.",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules ryo-Ichi · 13th in this Competition  · Posted 2 years ago arrow_drop_up 10 more_vert 13th Place Solution for the \"ICR - Identifying Age-Related Conditions\" Competition As per your comments, it was anticipated that this competition would be a shake-up, but honestly, I am surprised by this result. Just to make sure, I will provide my solution below. Context Business context: https://www.kaggle.com/competitions/icr-identify-age-related-conditions/overview Data context: https://www.kaggle.com/competitions/icr-identify-age-related-conditions/data Overview of the Approach section Data processing ・Missing Values: Create missing value flags and fill with the mean ・Column \"EJ\": label encoding ・Addition of Group Features: For variables other than the target, add ['min', 'max', 'mean', 'std'] of each feature grouped by the \"EJ\" column. Training Based on the discussion by Chris below, I created three models with downsampling and added class weights during training.Validation was performed using StratifiedKFold with a value of n_splits=10 for all models, and then the results were aggregated using seed averaging. ・Model1：LGBMClassifie（features=all） ・Model2：LGBMClassifier（training with the top 20 impactful features of Model 1） ・Model3：CatBoostClassifier(features=all) Details of the submission ・submission[\"class_1\"]=Model1 0.2 + Model2 0.2 + Modell3*0.6 ・result：Public=0.22, Private=0.36 Sources ・ https://www.kaggle.com/competitions/icr-identify-age-related-conditions/discussion/412507 Please sign in to reply to this topic. comment 4 Comments Hotness Chris Deotte Posted 2 years ago · 806th in this Competition arrow_drop_up 3 more_vert Great job! Congratulations on solo Gold Arvind (Yetirajan) Narayanan Iyengar Posted 2 years ago arrow_drop_up 2 more_vert Congratulations @ryotaichikawa ! wonderful! ks160050056 Posted 2 years ago · 701st in this Competition arrow_drop_up 0 more_vert First, congrats on a solo Gold @ryotaichikawa ! Can somebody please answer a few doubts? It will help me on my learning journey What group features were used during inference? My guess is that the same values were used from training based on EJ Downsampling with class weights during training - does this mean that the final ratio after downsampling wasn't 1:1? I think the positives to negatives ratio was partially increased but not all the way up to 1 because of the small dataset size. How were the model weights decided between Catboost and LGBM? ryo-Ichi Topic Author Posted 2 years ago · 13th in this Competition arrow_drop_up 0 more_vert Thank you for your questions. Below are the answers. 1.Cross-validation was performed using StratifiedKFold(n_splits=10) instead of GroupKFold. 2.The data ratio after downsampling (seed=42) was as follows, maintaining a 1:1 ratio: Original data -> Class0: 509, Class1: 108 Downsampled data -> Class0: 108, Class1: 108 3.The ensemble ratio was honestly determined intuitively; there might have been room for further exploration.",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules Hard Money Sniper · 15th in this Competition  · Posted 2 years ago arrow_drop_up 8 more_vert 15th Place Solution for the \"ICR - Identifying Age-Related Conditions\" I feel incredibly fortunate to have reached 15th place in the competition. I started by referencing the familiar baseline( https://www.kaggle.com/code/aikhmelnytskyy/public-krni-pdi-with-two-additional-models) . By delving into the comprehensive notebooks and discussions, I learned and adapted my approach. What did not work for me: Avoided using Epsilon from greeks due to potential data drift. De-anonymizing (But it's still a magic method for me who is not good at math, thanks!) and feature derivation. Feature selecture using target permutation, I found that the features filtered by \"gain\" and \"split\" are not in good agreement. Optuna, postprocessing, and oversampling, they weren't effective for me. What did work for me: Simulate a local verification process through Nested k-folds with StratifiedKFold. Not the best_model but full models of cv_outer. A diverse ensemble model with probability reweighting. Incorporating the greeks.Alpha into the training. I'm very lucky this time (contrary to real life…orz). My deepest gratitude goes out to the members of the Kaggle community who generously and selflessly share their knowledge. I also hold immense respect for those inquisitive minds who are never hesitant to ask questions and challenge the status quo. Kindly bear with any oversights or shortcomings!! Please sign in to reply to this topic. comment 2 Comments Hotness Ravi Ramakrishnan Posted 2 years ago · 854th in this Competition arrow_drop_up -1 more_vert Hearty congratulations for the solo gold @hardmoneysniper This solo gold will help you a lot in your journey to become a Kaggle GM. This comment has been deleted.",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules emoji_people MarkDjadchenko 590891 · 17th in this Competition  · Posted 2 years ago arrow_drop_up 6 more_vert 17th place!🏆🏆 At the beginning of the competition, I immediately set two goals without greek and without complex ensembles. My solutions are feature generation by the multiplication method and the SVC model https://www.kaggle.com/code/markdjadchenko/public-krni-pdi-with-pseudo-labelling Please sign in to reply to this topic. comment 5 Comments Hotness TAPAN AUTI Posted 2 years ago arrow_drop_up 0 more_vert Congratulations brother!! Thomas Meißner Posted 2 years ago · 1283rd in this Competition arrow_drop_up 0 more_vert Congratulations! How did you chose which features to drop? emoji_people MarkDjadchenko 590891 Topic Author Posted 2 years ago · 17th in this Competition arrow_drop_up 0 more_vert I just went through all the possible options and if the accuracy increased, then I went further, and if not, then I left it, and so on until the accuracy started to deteriorate This comment has been deleted. emoji_people MarkDjadchenko 590891 Topic Author Posted 2 years ago · 17th in this Competition arrow_drop_up 1 more_vert Private = 0,36 и Public = 0,46, CV = 0,1599",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules Sunil Gautam · 19th in this Competition  · Posted 2 years ago arrow_drop_up 9 more_vert 19th Place Solution for the \"ICR - Identifying Age-Related Conditions\" Competition Hey Kagglers, I guess mine is one of the simplest approach for this competition. Solution: https://github.com/SunilGolden/Kaggle-ICR Approach Under sampled training data Imputed null values with zero Encode categorical column using Ordinal Encoder Scaled other columns using Min Max Scaler Used k-fold cross validation to evaluate TabPFN, XGBoost, CatBoost, HGBoost, Light GBM, Random Forest, AdaBoost, GBM, SVM models and a few versions of their emsembles with balanced log loss Finally, I trained XGBoost, CatBoost, HGBoost, Light GBM, Random Forest, and GBM models and then ensembled them. Ensembling Tabular Health Beginner XGBoost Please sign in to reply to this topic. comment 5 Comments Hotness Tilii Posted 2 years ago · 2555th in this Competition arrow_drop_up 1 more_vert Finally, I trained XGBoost, CatBoost, HGBoost, Light GBM, Random Forest, and GBM models and then ensembled them. Do you mean a simple average of all models? Good job and congratulations on the medal. Sunil Gautam Topic Author Posted 2 years ago · 19th in this Competition arrow_drop_up 0 more_vert Yes, I simply averaged the probabilities given by each model. Thanks @tilii7 Sourabh Singh Posted 2 years ago arrow_drop_up 1 more_vert Nice Job @sunilgautam .Keep it up This comment has been deleted. NA Posted 2 years ago arrow_drop_up 1 more_vert Very interesting. Congratulations!",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules Kathy on the Way · 28th in this Competition  · Posted 2 years ago arrow_drop_up 5 more_vert 28th Solution To begin, I'd like to express my gratitude to Kaggle for hosting this competition, and extend my heartfelt congratulations to the winners of this competition! Here is the main takeaway from my solution. After reviewing discussions on the dangers of post-processing and data distribution on the forums and conducting several experiments, I've come to the realization that I should emphasize the prevention of overfitting due to the small dataset, rather than complicating my model. I allocated a significant amount of time to feature engineering while focusing on maintaining simplicity in model building. I used stratified cross-validation, and an XGBoost model was constructed with specific parameters obtained through RandomizedSearchCV. A lot of normal feature engineering methods were used. the public score for my submission is 0.18 Please sign in to reply to this topic. comment 1 Comment Hotness Mohneesh_Sreegirisetty Posted 2 years ago · 292nd in this Competition arrow_drop_up 0 more_vert Congratulations on the medal.",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules Daikon99 · 30th in this Competition  · Posted 2 years ago arrow_drop_up 9 more_vert 30th Place Solution for the ICR - Identifying Age-Related Conditions Competition On the competition's closing day, I was surprised to see a message from my friend and, upon checking the leaderboard, I found that I had come in 30th place. After submitting a few times early in the competition, I participated in HuBMAP, so I didn't expect to win a medal. Context Business context: https://www.kaggle.com/competitions/icr-identify-age-related-conditions/overview Data context: https://www.kaggle.com/competitions/icr-identify-age-related-conditions/data Solution I haven't done anything special. I used this excellent notebook: https://www.kaggle.com/code/datafan07/icr-simple-eda-baseline and I am grateful to https://www.kaggle.com/datafan07 . The private score of this notebook was 0.37816, which is equivalent to a silver medal-worthy score. I only made a modification in the stratified k-fold section of this notebook to perform a split that also considered EJ. After reading this discussion ( https://www.kaggle.com/competitions/icr-identify-age-related-conditions/discussion/411632) , I realized that gender should also be considered during fold splitting. greeks = pd.merge(train[[ 'Id' , 'EJ' ]], greeks, on= 'Id' ) for train_index,val_index in skf.split(train, greeks.iloc[:, 1 :- 1 ]): content_copy Due to this modification, the private score improved from 0.37816 to 0.37426. Please sign in to reply to this topic. comment 2 Comments Hotness Mohneesh_Sreegirisetty Posted 2 years ago · 292nd in this Competition arrow_drop_up 1 more_vert Spot on, Most of the people over engineered for the problem. I also left the competition 25 days ago, as I was not able to reduce the error further without the tricks the top kernels were mentioning, So, I just trusted what my CV was showing with what solution I was thinking. Using Gender to split K-Fold was the trick I missed, might have worked well for me. But congrats on the medal. Daikon99 Topic Author Posted 2 years ago · 30th in this Competition arrow_drop_up 1 more_vert Thank you!",
      "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules Davide Stenner · 40th in this Competition  · Posted 2 years ago arrow_drop_up 2 more_vert 40th Place Solution for the ICR - Identifying Age-Related Conditions Competition I'm really surprised by my position in the private leadrboard having gained over 3283 position :D Context section Business context: https://www.kaggle.com/competitions/icr-identify-age-related-conditions Data context: https://www.kaggle.com/competitions/icr-identify-age-related-conditions/data Overview of the approach The difficulty of this challenge arose from the very limited number of available rows. This also posed significant challenges in defining a suitable cross-validation scheme. As a cross-validation strategy, a stratified scheme was chosen based on the Alpha column. The main component of my solution was to employ a contrastive learning approach. The reason for opting for contrastive learning was that this way, the scarcity of available observations would be compensated by a large number of simulated rows. A substantial portion of the approach is derived from Setfit (source link at the end). The model utilized was an LGBM, which, starting from the absolute difference of all features, aimed to classify whether two features belonged to the same class or not. In addition to the initial features on which the absolute difference was computed, the following additional features were added: number_zero: % of features with zero difference mean_diff: mean of the absolute difference std_diff: standard deviation of the absolute difference median_diff: median of the absolute difference diff_mean: absolute difference between the means of all initial values diff_std: absolute difference between the standard deviations of all initial values diff_median: absolute difference between the medians of all initial values Details of the submission For training the model, pairs were sampled in a 1:5 ratio (class 0 vs. 1) to ensure a balanced dataset. For each observation, a certain number of random examples from the same class and from different classes were selected. Any duplicate combinations were removed. The metric used to determine the appropriate number of rounds was AUC. During the inference phase, the following post-processing steps were performed: Predict the probability that a new observation belongs to class 0 (by comparing it with all class 0 observations). Calculate the probability that it belongs to class 1 (by comparing it with all class 1 observations). With prob_0 and prob_1 (calculated as the mean of each previous calculated probabilities), calculate prob = prob_1 / (prob_0 + prob_1). This functions as a kind of ensemble. This way, for each individual observation to be predicted, 617 different predictions need to be made. The section to confront each new observation with the train observation is the given function: def get_retrieval_dataset(\n        test: pd.DataFrame, target_example: pd.DataFrame, \n        feature_list:list\n    ) -> Tuple[pd.DataFrame, list]:\n\n    test_shape = test.shape[0]\n    target_example_shape = target_example.shape[0]\n\n    test_x = test[feature_list].to_numpy( 'float32' )\n\n    target_example = np.concatenate(\n        [\n            target_example for _ in range(test_shape)\n        ], axis =0\n    )\n    test_x = np.repeat(test_x, target_example_shape, axis =0)\n    index_test = np.repeat(test.index.values, target_example_shape, axis =0)\n\n    retrieval_dataset = fe_pipeline( dataset_1 =target_example, dataset_2 =test_x, feature_list =feature_list,\n    )\n    retrieval_dataset[ 'rows' ] = index_test\n\n    return retrieval_dataset content_copy What didn't work: Metric Learning using Neural Networks, both DNN and TabNet, did not work regardless of the chosen metric (cosine similarity, mse, contrastive loss, etc.). Unfortunately, I couldn't achieve better CV results than the LGBM, likely due to the extremely limited number of available observations playing a key role. Contrastive learning on Alpha column Use a weight for each given training observation the observation Sources Inference Notebook SetFit Please sign in to reply to this topic. comment 0 Comments Hotness"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Use Machine Learning to detect conditions with measurements of anonymous characteristics The competition data comprises over fifty anonymized health characteristics linked to three age-related conditions. Your goal is to predict whether a subject has or has not been diagnosed with one of these conditions -- a binary classification problem. Note that this is a Code Competition , in which the actual test set is hidden. In this version, we give some sample data in the correct format to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. There are about 400 rows in the full test set. 4 files 356.88 kB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 356.88 kB greeks.csv sample_submission.csv test.csv train.csv 4 files 124 columns ",
    "data_description": "ICR - Identifying Age-Related Conditions | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. InVitro Cell Research · Featured Code Competition · 2 years ago Late Submission more_horiz ICR - Identifying Age-Related Conditions Use Machine Learning to detect conditions with measurements of anonymous characteristics ICR - Identifying Age-Related Conditions Overview Data Code Models Discussion Leaderboard Rules Overview Start May 11, 2023 Close Aug 11, 2023 Merger & Entry Description link keyboard_arrow_up Goal of the Competition The goal of this competition is to predict if a person has any of three medical conditions. You are being asked to predict if the person has one or more of any of the three medical conditions (Class 1), or none of the three medical conditions (Class 0). You will create a model trained on measurements of health characteristics. To determine if someone has these medical conditions requires a long and intrusive process to collect information from patients. With predictive models, we can shorten this process and keep patient details private by collecting key characteristics relative to the conditions, then encoding these characteristics. Your work will help researchers discover the relationship between measurements of certain characteristics and potential patient conditions. Context They say age is just a number but a whole host of health issues come with aging. From heart disease and dementia to hearing loss and arthritis, aging is a risk factor for numerous diseases and complications. The growing field of bioinformatics includes research into interventions that can help slow and reverse biological aging and prevent major age-related ailments. Data science could have a role to play in developing new methods to solve problems with diverse data, even if the number of samples is small. Currently, models like XGBoost and random forest are used to predict medical conditions yet the models' performance is not good enough. Dealing with critical problems where lives are on the line, models need to make correct predictions reliably and consistently between different cases. Founded in 2015, competition host InVitro Cell Research, LLC (ICR) is a privately funded company focused on regenerative and preventive personalized medicine. Their offices and labs in the greater New York City area offer state-of-the-art research space. InVitro Cell Research's Scientists are what set them apart, helping guide and defining their mission of researching how to repair aging people fast. In this competition, you’ll work with measurements of health characteristic data to solve critical problems in bioinformatics. Based on minimal training, you’ll create a model to predict if a person has any of three medical conditions, with an aim to improve on existing methods. You could help advance the growing field of bioinformatics and explore new methods to solve complex problems with diverse data. This is a Code Competition. Refer to Code Requirements for details. Evaluation link keyboard_arrow_up Submissions are evaluated using a balanced logarithmic loss . The overall effect is such that each class is roughly equally important for the final score. Each observation is either of class 0 or of class 1 . For each observation, you must submit a probability for each class. The formula is then: Log Loss = − 1 N 0 ∑ N 0 i = 1 y 0 i log p 0 i − 1 N 1 ∑ N 1 i = 1 y 1 i log p 1 i 2 where (N_{c}) is the number of observations of class (c), (\\log) is the natural logarithm, (y_{c i}) is 1 if observation (i) belongs to class (c) and 0 otherwise, (p_{c i}) is the predicted probability that observation (i) belongs to class (c). The submitted probabilities for a given row are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, each predicted probability p is replaced with max ( min ( p , 1 − 10 − 15 ) , 10 − 15 ) . Submission File For each id in the test set, you must predict a probability for each of the two classes. The file should contain a header and have the following format: Id,class_0,class_1 00 eed32682bb, 0.5,0.5 010 ebe 33f668,0 . 5 , 0 . 5 02 fa 521e1838 , 0.5,0.5 040 e 15f562a2 , 0.5,0.5 046 e85c7cc7f, 0.5,0.5 ... content_copy Timeline link keyboard_arrow_up May 11, 2023 - Start Date. August 3, 2023 - Entry Deadline. You must accept the competition rules before this date in order to compete. August 3, 2023 - Team Merger Deadline. This is the last day participants may join or merge teams. August 10, 2023 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - $18,000 2nd Place - $15,000 3rd Place - $10,000 4th Place - $7,000 5th Place - $5,000 6th Place - $5,000 Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time Internet access disabled Submission file must be named submission.csv Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors. Citation link keyboard_arrow_up Aaron Carman, Alexander Heifler, Ashley Chow, CGlenICR, and Ryan Holbrook. ICR - Identifying Age-Related Conditions. https://kaggle.com/competitions/icr-identify-age-related-conditions, 2023. Kaggle. Cite Competition Host InVitro Cell Research Prizes & Awards $60,000 Awards Points & Medals Participation 21,337 Entrants 7,327 Participants 6,430 Teams 57,333 Submissions Tags Tabular Binary Classification Health Weighted Multiclass Loss Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Citation"
  },
  {
    "competition_slug": "amp-parkinsons-disease-progression-prediction",
    "discussion_links": [
      "/competitions/amp-parkinsons-disease-progression-prediction/discussion/411505",
      "/competitions/amp-parkinsons-disease-progression-prediction/discussion/418143",
      "/competitions/amp-parkinsons-disease-progression-prediction/discussion/411546",
      "/competitions/amp-parkinsons-disease-progression-prediction/discussion/411398",
      "/competitions/amp-parkinsons-disease-progression-prediction/discussion/411388",
      "/competitions/amp-parkinsons-disease-progression-prediction/discussion/411395",
      "/competitions/amp-parkinsons-disease-progression-prediction/discussion/411380",
      "/competitions/amp-parkinsons-disease-progression-prediction/discussion/411394",
      "/competitions/amp-parkinsons-disease-progression-prediction/discussion/411436",
      "/competitions/amp-parkinsons-disease-progression-prediction/discussion/442273"
    ],
    "discussion_texts": [
      "AMP®-Parkinson's Disease Progression Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. AMP®-PD · Featured Code Competition · 2 years ago Late Submission more_horiz AMP®-Parkinson's Disease Progression Prediction Use protein and peptide data measurements from Parkinson's Disease patients to predict progression of the disease. AMP®-Parkinson's Disease Progression Prediction Overview Data Code Models Discussion Leaderboard Rules dott · 1st in this Competition  · Posted 2 years ago arrow_drop_up 96 more_vert 1st Place Solution First of all we would like to thank Kaggle and AMP PD for hosting this competition and providing a great dataset to dig into and rich enough to get lost in it for several months. It is always an extra motivation to work on problems that can bring value to the medical field or contribute to scientific research. Additional words of gratitude of course go to @kyakovlev for the amazing work he has done, I think we formed a great team for this competition and the results reflect that. We publish the full code of the winning notebook here Quick summary Our final solution is a simple average of two models: LGB and NN. Both models were trained on the same features (+ scaling/binarization for NN): Visit month Forecast horizon Target prediction month Indicator whether blood was taken during the visit Supplementary dataset indicator Indicators whether a patient visit occurred on 6th, 18th and 48th month Count of number of previous “non-annual” visits (6th or 18th) Index of the target (we pivot the dataset to have a single target column) The winning solution fully ignores the results of the blood tests. We’ve tried hard to find any signal in this crucial piece of the data, but unfortunately we came to the conclusion that none of our approaches or models can benefit from blood test features significant enough to distinguish it from random variations. The final models were trained only on the union of clinical and supplementary datasets. LGB For the entire duration of the competition LGB was our model to beat and only a NN trained with the competition metric as the loss function was able to achieve competitive performance on CV. At first, we tried running a regression LGB model with different hyperparameters and custom objective functions, but nothing was better than l1 regression, which does not optimize the desired metric SMAPE+1. We also noticed that on CV the performance of every model is always better when the regression outputs are rounded to integers. Then we switched to an alternative approach. Our LGB model is a classification model with 87 target classes (0 to maximum target value) and logloss objective. To produce the forecast we applied the following post-processing: given the predicted distribution of target classes, pick a value that minimizes SMAPE+1. Taking into account the observation that the optimal predictions are always integers, the task boils down to a trivial search among 87 possible values. Such an approach would have worked well for the original SMAPE metric also, because the approach treats cases with multiple local minimums naturally. We ran an optimization routine to tune LGB hyperparameters to minimize SMAPE+1 on CV using the described post-processing. NN The neural network has a simple multi-layer feed-forward architecture with a regression target, using the competition metric SMAPE+1 as the loss function. We fixed the number of epochs and scheduler, and then tuned the learning rate and hidden layer size. The only trick there was to add a leaky relu activation as the last layer to prevent NN from getting stuck at negative predictions. Of course there are alternative ways to solve this issue. Cross-Validation We’ve tried multiple cross-validation schemes due to the small training sample size, all of them were stratified by patient id. Once a sufficient number of folds is used, they all are quite well correlated to each other. Better than to the public leaderboard :) The final scheme we relied on was leave-one-(patient)-out or, in other words, a group k-fold cross validation with a fold for each patient. We used it because it doesn’t depend on random numbers. The cross-validation correlated well enough with the private leaderboard, and the submit we chose turned out to be our best private LB submission. What worked The most impactful feature was the indication of whether a patient visit happened on the 6th month or not. It correlates strongly with the UPDRS targets (especially 2 and 3) and with frequency of medications being taken. As we can observe only the data correlation, it is impossible to judge what is the core reason for that. During the competition our hypothesis was that the patients that had more severe symptoms during the first examination (UPDRS scores parts 2 and 3) were more likely to get invited for a visit after 6 months and more likely to get medications prescribed. But for model training it was important that the patients that made a visit on the 6th month, have higher UPDRS scores on average. The same is true for an 18th month visit as well, but these 2 features are correlated. I personally wonder if presence / absence of these variables in the models are the reason for the private LB cliff effect around 20th place. Another curious effect related to it is observed for the forecasts made at visit_month = 0. If you look at the model forecasts for 0, 12 and 24 months ahead, they are consistently lower than the forecasts 6 months ahead. It is very logical from the math point of view - if a patient will show up on the 6th month, they will have higher UPDRS scores on average, and if not - the forecast will be ignored. But such model behaviour is unreasonable from a clinical point of view of course. It was also important to pay attention to the differences between training and test datasets as e.g. nicely summarized here . That, for instance, explains well why adding a feature indicating the visit on the 30th month could improve the CV, but ruin LB. What didn’t work Blood test data. We’ve tried many approaches to add proteins and/or peptides data to our models, but none of them improved CV. We narrowed it down to a bag of logistic regressions that forecasted the visit on the 6th month based on the blood test result on the 0th month. We applied soft up/down scaling of model-based predictions for patients that were more/less likely to visit on the 6th month based on the logistic regression probabilities. It worked on public LB after tuning a couple of “magic” coefficients directly on public LB itself. That gave us a boost all the way up to the second place on public LB, but it was clearly an overfit. We chose a “mild” version of that approach as our second final submission. It scored worse on private LB than the other submission, but, interestingly enough, not by as much as one could have expected (60.0 vs 60.3). Thanks to everyone who participated in the competition, those who kept many interesting discussions going on the forum and those who suggested improvements! And congrats to all the winners! 3 Please sign in to reply to this topic. comment 38 Comments 1 appreciation  comment Hotness Konstantin Yakovlev Posted 2 years ago · 1st in this Competition arrow_drop_up 13 more_vert It was a huge huge pleasure to team up with @dott1718 (Dmitry) in this competition. I felt as a total junior DS - Dmitry's ideas, approach and dedication were astonishing - I've learned so much and so grateful for all the time we spent together. This competition was super unique for me with many new puzzles to solve. Thank you again and congrats to all the winners and participants - It was remarkable ride. Konstantin Yakovlev Posted 2 years ago · 1st in this Competition arrow_drop_up 16 more_vert A SMAPE1P thing: MAPE / SMAPE / SMAPE1P are very tricky metrics for GBT models: Need custom loss or target transform with l1 loss (l1 is never optimal choice for gbt) Multimodal Zero inflation …. Here is the \"hack\" that is boosting the SMAPE1P score (and works also on real data - confirmed few weeks ago): Step 1:  Multiclass Classification params = { 'boosting_type' : 'gbdt' , 'objective' : 'multiclass' , ... } content_copy Step 2: Train model (could be xgboost catboost lgbm) estimator.train( ... ) content_copy Step 3: Predict probabilities: probas = estimator.predict(...) content_copy Step 4: Make direct SMAPE1P optimization from probabilities def single_smape1p (preds, tgt):\n    x = np. tile (np. arange (preds.shape[ 1 ]), (preds.shape[ 0 ], 1 ))\n    x = np. abs (x - tgt) / ( 2 + x + tgt)\n    return (x * preds). sum (axis= 1 )\n\ndef opt_smape1p (preds):\n    x = np. hstack ([ single_smape1p (preds, i). reshape (- 1 , 1 ) for i in range (preds.shape[ 1 ])])\n    return x. argmin (axis= 1 )\n\nfinal_predictions = opt_smape1p (probas) content_copy If you are sure that \"SMAPE\" variation is the metric that reflects your business value -  do the \"hack\" (at least try). Chris Deotte Posted 2 years ago · 4th in this Competition arrow_drop_up 5 more_vert Amazing hack! Great discovery and thanks for sharing! 16 more replies arrow_drop_down Chris Deotte Posted 2 years ago · 4th in this Competition arrow_drop_up 3 more_vert Congratulations @dott1718 and @kyakovlev Fantastic job! The winning solution fully ignores the results of the blood tests. We’ve tried hard to find any signal in this crucial piece of the data, but unfortunately we came to the conclusion that none of our approaches or models can benefit from blood test features significant enough to distinguish it from random variations. The final models were trained only on the union of clinical and supplementary datasets. I just had a thought. Now that the competition ended and the signal was fully explained by AmbrosM here , it might be the case that there is signal in the protein peptide information. Perhaps the problem is that once our models detect the 3 groups of patients using visit_month then protein peptide does not provide more information. However if the host could hide the 3 groups of patients (i.e. remove the signal from visit month), I wonder whether the protein peptide information can help locate the 3 groups? In other words, i do not think that this competition demonstrated that the protein peptide does not contain signal (by the fact that top teams did not use it.). I think this competition demonstrated that the signal from visit_month contains whatever signal may exist in the protein peptide information. Do you think this might be the case? If so, we should let the host and biology community know this. Chris Deotte Posted 2 years ago · 4th in this Competition arrow_drop_up 1 more_vert Now i just read that you did try using blood without visit month. So perhaps you demonstrated that it does not have predictive power. Blood test data. We’ve tried many approaches to add proteins and/or peptides data to our models, but none of them improved CV. We narrowed it down to a bag of logistic regressions that forecasted the visit on the 6th month based on the blood test result on the 0th month. We applied soft up/down scaling of model-based predictions for patients that were more/less likely to visit on the 6th month based on the logistic regression probabilities. dott Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Yes, it is exactly why we tried to narrow it down to a single homogenous group, but without much success. At some point I managed to get a CV improvement with some proteins data of a 0.01 magnitude which was much smaller than expected random effect on LB of around 0.1. So a different competition setup with more data might have been successful in identifying proteins/peptides. The way I managed to \"force\" proteins into the LGB model was to select the top 5-10 ones with the highest absolute correlation with the targets and scale down their gain with feature_contri parameter dramatically. Vitaly Kudelya Posted 2 years ago · 12th in this Competition arrow_drop_up 4 more_vert Why is leave-one-(patient)-out cross-validation also called \"stratified cross validation with a fold for each patient\" in the post? I thought that for stratification we need our folds to be stratified for target,  for example ) This comment has been deleted. dott Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert You are right, thank you for pointing that out! It is indeed group k-fold and not stratified k-fold, I've corrected the text to prevent potential confusion. oka_haru Posted 2 years ago · 115th in this Competition arrow_drop_up 4 more_vert Congratulations!!! Thanks for sharing your solution!! Jindo Posted 2 years ago · 115th in this Competition arrow_drop_up 4 more_vert Congratulations🎉 I really learned a lot from your source code and explanations. Thank you very much. Samvel Kocharyan Posted 2 years ago · 262nd in this Competition arrow_drop_up 4 more_vert Congratulations Dmitry and Konstantin! What a brilliant skill to focus on one smaller part of data (visits) and have enough wisdom and experience to ignore the others (CSF). I would say it is one of my main outcomes from this competition. I hope that organizers will not be dissatisfied since that all the data they provided were not used in a winning solutions and the hypothesis about predictive power of proteins and peptides is not confirmed yet. Anyway this competition makes all of us a bit closer to the understanding of the Parkinson's desease. Congratulations to everyone who won in leaderbord and congratulations to all those who have beaten themselves in this competition. It was an addictive trip and interesting experience. Jai Chauhan Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! sir Rida Mahmood Posted 2 years ago · 833rd in this Competition arrow_drop_up 1 more_vert Many congratulations @dott1718 . Also I highly appreciate you for sharing your winning solution. Stay blessed! sameelie Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! Thanks for sharing your approach to a simple but effective solution. Konstantin Yakovlev Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert https://www.kaggle.com/code/kyakovlev/amp-blood-randomness/ Small example for \"fake\" correlations in proteins / peptides Gaurav Srivastava Posted 2 years ago · 378th in this Competition arrow_drop_up 2 more_vert Heartiest Congratulations @dott1718 . And thanks for sharing. Keep up the good work 😊 Vincent Schuler Posted 2 years ago · 14th in this Competition arrow_drop_up 2 more_vert Congratulations @dott1718 and @kyakovlev ! Best team name and best solution 💪 I tried multiclass classification as well but couldn't make it work better than a simple regression model. Thanks for sharing your method, it's instructive ! dott Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thank you Vincent! I was cheering for you to get to the gold zone on private LB, hopefully you are not too upset about the result. Vincent Schuler Posted 2 years ago · 14th in this Competition arrow_drop_up 0 more_vert Thanks 😄 Nothing too frustrating, hopefully I'll get it another time ! dott Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert I am sure you will! Aaron C Bastian Posted 2 years ago · 1775th in this Competition arrow_drop_up 2 more_vert It is unfortunate that no signal was found in the protein data, as that was the entire point of the challenge.  Parkinson's remains a clinical diagnosis it seems. Ion05 Posted 2 years ago · 115th in this Competition arrow_drop_up 2 more_vert Congratulations! Thanks for sharing your solution with us. It has been very helpful for me. Thank you!! windupbirdjdt Posted 2 years ago · 1323rd in this Competition arrow_drop_up 2 more_vert very interesting discussion and work, thank you for taking the time to update on this, its interesting that the blood test data was not directly an input Jayanth Sai Marreddi Posted 2 years ago · 528th in this Competition arrow_drop_up 2 more_vert Congratulations! Thanks for sharing your solution gives a lot insights to me to see how does top people in leaderboard approach a project. Viji Posted 2 years ago · 910th in this Competition arrow_drop_up 2 more_vert Congratulations! Thanks for sharing your approach to a simple but effective solution. Chris Deotte Posted 2 years ago · 4th in this Competition arrow_drop_up 2 more_vert Thanks for sharing your solution. I never achieved good results from GBT. (I think CV was 57 compared with my MLP's 55). And i did not achieve any significant boosts from SMAPE optimization (besides rounding). I also did not find a better loss for my MLP than MeanAbsoluteError loss. Now after reading your solution, i may go back and revisit these ideas and see if i can boost my MLP by ensembling it with GBT and adding some SMAPE optimization. My two top models were MLP (4th place) and SVR (9th place) but the CV did not boost when i ensembled the two. Konstantin Yakovlev Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Same with our ensembles - NN with lgbm showed just a tiiiny boost on CV. Chris Deotte Posted 2 years ago · 4th in this Competition arrow_drop_up 2 more_vert I like your trick of training multi-class then optimizing output to SMAPE. That's clever. Our LGB model is a classification model with 87 target classes (0 to maximum target value) and logloss objective. To produce the forecast we applied the following post-processing: given the predicted distribution of target classes, pick a value that minimizes SMAPE+1. Taking into account the observation that the optimal predictions are always integers, the task boils down to a trivial search among 87 possible values. Such an approach would have worked well for the original SMAPE metric also, because the approach treats cases with multiple local minimums naturally. MichaelDarling Posted 2 years ago arrow_drop_up 0 more_vert Hi @dott1718 , congratulations!  Would you be able to send along the testing data provided by the competition?  I have a student working with this data for his capstone project and would love to get this data.  We contacted the competition organizers but haven't heard back. Aleksey SCHUKIN Posted 2 years ago · 262nd in this Competition arrow_drop_up 0 more_vert Congratulations! Thanks for sharing your approach. Tomonori Sasaki Posted 2 years ago · 621st in this Competition arrow_drop_up 0 more_vert Congratulations! Thank you for publishing and sharing. The discussion around the metrics in this competition was difficult and confusing to me. I will learn from the published notes. Backyard Posted 2 years ago arrow_drop_up 0 more_vert Good job,thank you very much. Because I can learn from it . Jeong Taewoo Posted 2 years ago arrow_drop_up 0 more_vert I thought XGBoost almost always had good performance, but I guess it wasn't necessarily. Your writing and notebook helped me improve my skills a lot. Thank you!! Josef Švenda Posted 2 years ago · 769th in this Competition arrow_drop_up 0 more_vert As I understood the discussion: XGB would be a comparable method if it would have custom objective function aligned with SMAPE metric. Right @cdeotte , @dott1718 , @kyakovlev ? Anyway excellent learning opportunity! Thanks guys! GridSearchGK Posted 2 years ago arrow_drop_up 0 more_vert Congratulations !! Well done 🔥 3SE3_212011556_Aronad Brilliantio Evan Maheswara Posted 2 years ago arrow_drop_up 0 more_vert Congratulations! Thanks for sharing your approach.",
      "AMP®-Parkinson's Disease Progression Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. AMP®-PD · Featured Code Competition · 2 years ago Late Submission more_horiz AMP®-Parkinson's Disease Progression Prediction Use protein and peptide data measurements from Parkinson's Disease patients to predict progression of the disease. AMP®-Parkinson's Disease Progression Prediction Overview Data Code Models Discussion Leaderboard Rules Ethan · 2nd in this Competition  · Posted 2 years ago arrow_drop_up 10 more_vert 2nd Place Solution Sorry for late sharing. First of all, thanks to competition organizers for hosting this interesting competition and my great teammates(@hyd,@rib). And congrats my friend rib for becoming GM. Our solution is simple, we found information related to \"visit_month\" very useful and information related to proteins are useless. So we focus on the samples, structures and models. Samples We used samples with visit_month in \"[0, 6, 12, 18, 24 , 36, 48, 60, 72, 84]\" for training. And found that our cv is much better correlated to LB. Features All about “visit_month” 1) use visit_month as meta feature 2) gap between visit_month and last visit_month 3) times of visit for each patient Little about “protein” use NPX’s ratio of each patient, instead of the original values Structure 16 labels Predict each “updrs” for predicted_month_diff=0,6,12,24 4 labels Use predicted_month_diff as a feature, and predict each one’s 4 updrs Models Simple MLP with different structures and parameters.Finally we blends with: Models training in different numbers of labels Models training in different structure of network Models training in different parameters of network code: https://www.kaggle.com/code/dc5e964768ef56302a32/2nd-solution Please sign in to reply to this topic. comment 3 Comments Hotness klsharma22 Posted a year ago arrow_drop_up 0 more_vert Congratulations on your breakthrough. I would really like to see your code for learning purpose. could you please tell me where to find your code? Majid Ahmad Khan Posted 2 years ago arrow_drop_up 0 more_vert Congrats on getting 2nd position serangu Posted 2 years ago arrow_drop_up 0 more_vert @chenxin1991 thank you for sharing solution!",
      "AMP®-Parkinson's Disease Progression Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. AMP®-PD · Featured Code Competition · 2 years ago Late Submission more_horiz AMP®-Parkinson's Disease Progression Prediction Use protein and peptide data measurements from Parkinson's Disease patients to predict progression of the disease. AMP®-Parkinson's Disease Progression Prediction Overview Data Code Models Discussion Leaderboard Rules Hajime Tamura · 3rd in this Competition  · Posted 2 years ago arrow_drop_up 18 more_vert 3rd place solution First of all, I would like to thank Kaggle and AMP PD for organizing this great competition. I would also like to thank all the kagglers who participated in this competition. I am very happy to have won my first gold medal and also to have won a prize. My solution is simple and consists of three main functions. 1. Grouping As many have pointed out, I have divided the groups into two, each optimized for a different visit interval (6 or 12 months). One key point is that we need to focus not only on the 6th month, but also on the 18th month. There are patients who are missing the 6th month but have the 18th month present, and this patient is not healthy. By using the cumulative minimum function, I considered patients with either the 6th month or the 18th month present as unhealthy. [Group A : Healthy] Patients with a minimum visit interval of 12 months or more [Group B : Unhealthy] Patients with a minimum visit interval of 6 months or less 2. Labeling (mainly Group B) As I looked at the data for Group B, the unhealthy group, I found several patterns. The frequency of protein collection and protein information are linked to the severity of symptoms. I then generated several labels based on protein collection frequency and protein information and used them as features. The following 9 labels were finally adopted. [more severe symptoms] Protein was collected at 6 months Protein was collected at 6 months and again at 12 months Low number of unique \"UniPort\" (approximately the bottom 20%) Low number of unique \"UniPort\" (approximately the bottom 10%) [milder symptoms] Protein not collected at 6 months Protein was collected at 6 months but not at 12 months Protein not collected at 6 months, but collected at 18 months High number of unique \"UniPort\" and high change of \"Peptide\" from the previous measurement (approximately the top 20%) High number of unique \"UniPort\" and high change of \"Peptide\" from the previous measurement (approximately the top 10%) 3. Modeling Initially, I also tried LightGBM using these features, but LB worsened and I did not use them in my final model. In my final model, I used these features to obtain the coefficients (severity) by grid search. Due to the small sample size of the train data (248 patients), some labels improved for the train data (248 patients) but worsened for the LB (50 patients). In my various experiments, I decided to adopt only those that improved both train and LB (248 + 50 patients). I thought this would be a more robust model. As a result, the final scores were also stable. Please sign in to reply to this topic. comment 6 Comments Hotness Xiao-Su (Frank) Hu Posted 2 years ago · 729th in this Competition arrow_drop_up 1 more_vert Thank you very much for sharing! Congrats! The Devastator Posted 2 years ago arrow_drop_up 1 more_vert Extracting domain knowledge patterns by hand is VERY impressive. Congratulations, well deserved! The Devastator. SM Nuruzzaman Nobel Posted 2 years ago · 329th in this Competition arrow_drop_up 1 more_vert Congratulations! Chris Deotte Posted 2 years ago · 4th in this Competition arrow_drop_up 1 more_vert Congratulations great job solo cash gold! What type of model did you use as your final model? Hajime Tamura Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 4 more_vert Thank you very much. You were my goal during the competition. My final model was a simple linear multiple regression model with partial regression coefficients obtained by grid search. Hajime Tamura Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 0 more_vert Sorry for the delay. I have published my code. Please forgive me if some parts of the code may be hard to read. https://www.kaggle.com/code/thajime/amp-pdpp-3rd-place-solution/notebook",
      "AMP®-Parkinson's Disease Progression Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. AMP®-PD · Featured Code Competition · 2 years ago Late Submission more_horiz AMP®-Parkinson's Disease Progression Prediction Use protein and peptide data measurements from Parkinson's Disease patients to predict progression of the disease. AMP®-Parkinson's Disease Progression Prediction Overview Data Code Models Discussion Leaderboard Rules Chris Deotte · 4th in this Competition  · Posted 2 years ago arrow_drop_up 148 more_vert 4th Place Gold - Single Model RAPIDS cuML SVR! AMP®-Parkinson's Disease Progression Prediction Wow, this competition had a close finish at the top. I suspected this would be the case because I believe there was only 1 source of reliable signal (i.e. patient visit dates ) and all the tops teams were working hard to extract an extra SMAPE 0.1 from it. The metric SMAPE is a percentage so 0.1 is actually 0.001 . That's how close the finish was! The final leaderboard is separated into two groups. The top 18 teams have SMAPE <= 62.5 and the next teams have SMAPE >= 68.4 . I believe the top 18 teams used signal from patient visit dates and the next teams did not . Signal from Protein/Peptide data In this competition, Kaggle provided 227 Protein NXP features and 968 Peptide PeptideAbundance features. That is 1195 features for each of 17 possible visit dates. We only have 248 train patients. And the curse of dimensionality begins when number of features > train samples / 10 . In other words we only have enough train data to reasonably train 25 features not 1195 features! I did an experiment where i made 1000 columns of random numbers. Using forward feature selection, I found that columns of random numbers would boost GroupKFold CV score the same amount that protein/peptide features did . This means that there may be signal hiding in the protein peptide data but it is too weak to detect patterns with only 248 patients (because no protein nor peptide boost CV more than random numbers can). Signal from Patient Visit Dates Next I searched patient visit dates for signal. Many Kagglers overlooked that we can engineer features from patient visit dates. Here are some example features when was patient's first blood work measured? did patient get blood work at their first doctor visit, yes or no? how many times did a patient visit the doctor? how long ago was the patient's last visit? Etc etc. We can create 100s of features about when a patient visited the doctor and when a patient had blood work done. I quickly noticed the following trend. Patients who visit the doctor more often have larger UPDR scores . This is shown in the following 3 plots. The first are patients who visited the doctor a normal number of times. The second are patients who visited 1 standard deviation less than normal. And the last are patients who visited 1 standard deviation more. In each plot, we display the average target value per visit month for these 3 groups of patients: Feature Engineering The above plots show that there is signal in the patient visit dates . What is the best way to extract this signal? I generated 100s of features and used for-loops with RAPIDS cuML SVR to find which features extract the most signal. In the end, simple \"booleans\" worked best (and the model \"created its own features internally\"). For each visit month, i created a boolean variable. For example for visit month = 24, i created the following \"boolean\": v24 = 0 if we know that patient did not visit on visit month = 24 v24 = 1 if we know that patient did visit on visit month = 24 v24 = -1 if we do not know if patient visited on month = 24 The reason for the third category is because at each time step of Kaggle's API we are asked to predict 0, 6, 12, 24 months into the future. So if the current visit month = 12 and we are predicting visit month 36, we do not know if the patient visited during visit month = 24. Single Model RAPIDS cuML - 8th Place Gold A single RAPIDS cuML SVR model trained with 11 features which are visit_month and v0 , v6 , v12 , v18 , v24 , v36 , v48 , v60 , v72 , v84 where the v features are described above achieves CV = 55.5 and Public LB = 55.4 and Private LB = 60.5 . This is 8th place Gold. Using RAPIDS cuML was great because it allowed me to experiment dozens of models in minutes! Single Model TensorFlow MLP - 4th Place Gold After I found the above features, i tried different model types. I tried XGBoost with PseudoHuber loss . It's CV was not as good as RAPIDS cuML SVR . Next I tried TensorFlow MLP with MeanAbsoluteError . We built an MLP with 10 hidden layers where each hidden layer has 24 units and activation Relu. We used no Dropout and no BatchNormalization. We trained it for 15 epochs with Adam optimizer LR = 1e-3 and then 15 epochs LR = 1e-4 . This achieves CV = 55.0 and Public LB 54.9 and Private LB 60.1 . This is 4th place Gold. Creating Train Data Our model trained with train_clinical_data.csv only. Creating proper train data for the above features was not trivial. We needed to convert each row from the original train data into 4 new rows . If the original row was patient_id = 55 and visit_month = 24 . Then we needed to replace this row with 4 new rows: patient_id=55, visit_month=24, And v0=X1, v6=-1, v12=-1, v24=-1, v>24=-1 patient_id=55, visit_month=24, And v0=X1, v6=X6, v12=-1, v24=-1, v>24=-1 patient_id=55, visit_month=24, And v0=X1, v6=X6, v12=X12, v24=-1, v>24=-1 patient_id=55, visit_month=24, And v0=X1, v6=X6, v12=X12, v24=X24, v>24=-1 where X1 , X6 , X12 , X24 are the values 0 or 1 based on whether patient_id=55 visited on months 0, 6, 12, 24 in the train data. The 4 new rows are current visit month minus 0, 6, 12, 24. If any of these subtractions are not valid visit months then we don't make that row. 4th Place Solution Code I published my 4th place submission code using TensorFlow MLP here . Enjoy! Please sign in to reply to this topic. comment 30 Comments 5 appreciation  comments Hotness Ankur Limbashia Posted 2 years ago · 751st in this Competition arrow_drop_up 7 more_vert Very interesting to know how the top 18 teams have used the visit month feature! I wish I had thought the same. Congratulations !!! Cody_Null Posted 2 years ago · 804th in this Competition arrow_drop_up 7 more_vert Yes, you can bet I am going to spend more time doing EDA from here on out and that I will be plotting every variable against each other Vincent Schuler Posted 2 years ago · 14th in this Competition arrow_drop_up 3 more_vert Congratulations and thanks for your review ! The idea of the \"v-booleans\" is clever and better than what I used. I'm surprised by the boost given by the MLP compared to XGBoost and SVR ! Bonus question : how did you manage to get your 52.8 LB score ? (I think I'm not the only one you made feel bad during some weeks 😃) Chris Deotte Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 4 more_vert My best public LB score is actually LB = 51.3 but i did not submit it because i didn't want my public LB score to be too good 😃 From EDA, i saw that approximately 25% of train patients were control patients. I was curious if test data contained control patients or not (because this would affect our model approach). So I probed the first 165 patients out of test data's 384 patients. Among those 165 patients, about 24 were in public test (vs private), and about 8 are control patients. So yes, test data contains control patients too. If we submit linear trend public notebook and then multiply the 8 control patients (i discovered) targets by 0.2, we achieve LB = 51.3 . Chris Miles Posted 2 years ago · 748th in this Competition arrow_drop_up 1 more_vert Wow that is so cool. Is this somewhat how you went about probing? Track the visit patterns of all patients. Choose the 165, maybe by lowest patient_id. On the last iteration, loop through the 165 patients, and run if patient_id_visit_pattern == control_pattern: time.sleep(10 seconds) Then number_control_patients = run_time_seconds / 10 Also, can you please share some insight into how you keep track of submission runtime? I can't find anywhere on kaggle that tells me how long a submission took. So unless I keep watch on the submission page I don't know when it ended. I suppose I could make a program to watch the html and keep track, but I figured expert LB probers like yourself may have a simpler solution. Thanks for another crystal clear explanation of your solution. Chris Deotte Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 3 more_vert Chris, the trick to probe test patients is having a consistent list of patients that has the same ordering each time you run the submit notebook. So I would create a NumPy array and add each new patient id that the Kaggle API provides in the order that they arrive in the Kaggle API for-loop and test dataframe. Then each time i run the code, the 384 test patients will be in the same order with same array index. To probe a control patient, we just write if patient id has index X in the \"consistent array\" then multiply patient id targets by 0.2. If the public LB score improves then, this patient is both in public test and a control patient. And we try X=0, then X=1, then X=2, etc. ADAM. Posted 2 years ago · 8th in this Competition arrow_drop_up 3 more_vert Thanks for sharing. I also added a random column in my lgb models. I found that the CV of features with a random column is much worse. So I tuned parameters to make models to converge(i.e. no much difference between with a random columns and without a random column.) Muhammad Ammar Jamshed Posted 2 years ago arrow_drop_up 1 more_vert This is truly spectacular @cdeotte ! Keep up your amazing work liuyt49 Posted 2 years ago arrow_drop_up 1 more_vert congratulations, this is Great! Gaurav Srivastava Posted 2 years ago · 378th in this Competition arrow_drop_up 1 more_vert Amazing Work @cdeotte ! Thanks for sharing 😊 caleb Kemboi Posted 2 years ago arrow_drop_up 1 more_vert congratulations, this is Great! Siyun Yang Posted 2 years ago arrow_drop_up 1 more_vert Thank you for sharing and congratulations! Atuljo Posted 2 years ago arrow_drop_up 1 more_vert I tried the problem but had no idea how to approach this problem. Your solution is very helpful. Jayanth Sai Marreddi Posted 2 years ago · 528th in this Competition arrow_drop_up 1 more_vert Thank you for sharing learnt a lot, I also question myself why haven't I thought of this , Great Work Joel Sathiyendra Posted 2 years ago arrow_drop_up 1 more_vert I literally had no idea how to approach this problem. Your solution is very insightful. Thank you for sharing @cdeotte .🎉 Tisha Posted 2 years ago arrow_drop_up 1 more_vert Congratulations man!! Oscar Aguilar Posted 2 years ago · 573rd in this Competition arrow_drop_up 1 more_vert Congrats @cdeotte and thanks for sharing your approach👍 Nat Posted 2 years ago · 301st in this Competition arrow_drop_up 1 more_vert Congrats @cdeotte , thank you for sharing your solution with us! It was well written and an awesome learning tool for me. Catharis Posted 2 years ago arrow_drop_up 1 more_vert Congratulations and thanks for sharing your approach! @cdeotte steubk Posted 2 years ago · 257th in this Competition arrow_drop_up 1 more_vert Insightful solution as usual. Thank you for sharing @cdeotte ! Giorgi Goletiani Posted 2 years ago · 1332nd in this Competition arrow_drop_up 1 more_vert Congrats! I actually guessed you were only using visit_month because you took the lead quite early on when you had not quite figured out what was scored and what was not based on protein data availability, but I still did not want to believe that protein/peptide data was useless. Simple solution proves to be the best solution once again. emoji_people AC Posted 2 years ago · 617th in this Competition arrow_drop_up 1 more_vert Oh Wow!! Thank you for sharing! This approach is so good! Using random numbers, instead. Congratulations! :) 🙌 yuan Posted 2 years ago arrow_drop_up 1 more_vert good job good job qi7a xu Posted 2 years ago arrow_drop_up 2 more_vert congratulations！ moth Posted 2 years ago · 552nd in this Competition arrow_drop_up 2 more_vert Congrats @cdeotte Bojan must be sad that XGBoost was surpassed by RAPIDS SVR 😂 Cody_Null Posted 2 years ago · 804th in this Competition arrow_drop_up 1 more_vert I had thought the same thing haha Nooruddin Hyderabadwalla, CFA Posted 2 years ago arrow_drop_up 0 more_vert Very innovative approach here @cdeotte Appreciation (5) Bogdan Stamenov Posted 2 years ago arrow_drop_up 1 more_vert Thanks for sharing and congrats yang kaiyu36 Posted 2 years ago · 80th in this Competition arrow_drop_up 1 more_vert thank you for sharing Xiao-Su (Frank) Hu Posted 2 years ago · 729th in this Competition arrow_drop_up 1 more_vert Thank for sharing! Fnoa Posted 2 years ago · 13th in this Competition arrow_drop_up 1 more_vert Great explanation! Thanks @cdeotte",
      "AMP®-Parkinson's Disease Progression Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. AMP®-PD · Featured Code Competition · 2 years ago Late Submission more_horiz AMP®-Parkinson's Disease Progression Prediction Use protein and peptide data measurements from Parkinson's Disease patients to predict progression of the disease. AMP®-Parkinson's Disease Progression Prediction Overview Data Code Models Discussion Leaderboard Rules AmbrosM · 5th in this Competition  · Posted 2 years ago arrow_drop_up 75 more_vert #5: Find the control group When I selected my two final submissions a few hours ago, I decided for one model which uses only the month data for the predictions (no peptides and proteins) and has a public lb score of 54.7. another model, which additionally uses the peptides and proteins, has a better cv but a bad public lb score (55.3). It turned out that the public leaderboard was the better indicator than the cv, and the peptide/protein feature engineering was useless. Recognizing the control group If we plot the median updrs scores for every month, we see that the months which are multiples of 12 (the cyan markers on the gridlines) are usually lower than the non-multiples of 12 (the magenta markers between the gridlines). This cannot be a coincidence. A scatterplot of the 248 patients versus the months of their updrs assessments reveals that there are three groups of patients: The patients of the green group had their first visits in months 0 , 3 , 6 , 9 , 12 . The patients of the orange group had their first visits in months 0 , 6 , 12 , 18 , 24 and the last visit in month 60 . The patients of the red group had their first visits in months 0 , 12 , 24 . content_copy If we plot the updrs scores over time of every patient, we see differences among the groups. The red group in particular has the lowest updrs scores, which means that these are the healthiest people, and updrs_4 has rarely been measured for them. We can hypothesize that the red group is the control group (a group of people without Parkinson's disease), and the experimenters decided to test the control group only once a year and to skip the updrs_4 test for this group. The real patients (green and orange groups) were tested more often and with all four updrs tests. Conclusion: We can distinguish the control group from the real patients according to their first non-zero visit_month: If the first non-zero visit_month is <12, we have a real patient; if the first non-zero visit_month equals 12, the person belongs to the healthy control group. This distinction has high predictive value for the updrs scores. The model The model has only two features: the group to which a patient belongs the month of the prediction Depending on the group, it predicts a linear or isotonic regression: Lessons learned A thorough EDA is important, and the EDA must be adapted to the dataset. Automated EDA tools don't find the hidden information. Unusual metrics (smape plus one) require unusual methods. If the training dataset is small, simple models turn out best. Medical data is scarce and expensive. If we haven't been able to prove a connection between proteins and Parkinson symptoms, this doesn't mean there is none. It only means that another thousand patients must be convinced to participate in a five-year study, and we might see a follow-up competition in 2028… In biology and medicine, we usually search for very weak effects: Protein measurements are imprecise and updrs scores depend on the mood of the patient and the doctor. If anybody was expecting SMAPE scores far below 50, this expectation was unrealistic. Source code is here . Please sign in to reply to this topic. comment 23 Comments 4 appreciation  comments Hotness Gaurav Srivastava Posted 2 years ago · 378th in this Competition arrow_drop_up 1 more_vert Very Insightful 🔥 Congratulations 🎉 and Thanks for sharing @ambrosm 🤘 Oscar Aguilar Posted 2 years ago · 573rd in this Competition arrow_drop_up 1 more_vert Congrats @ambrosm and thanks for sharing your approach. Nice discovery👍 steubk Posted 2 years ago · 257th in this Competition arrow_drop_up 1 more_vert Congratulations and thank you for sharing @ambrosm ! A very clever solution ! Remek Kinas Posted 2 years ago arrow_drop_up 1 more_vert Congratulations @ambrosm . 👏👏👏 Quim Quadrada Posted 2 years ago · 1147th in this Competition arrow_drop_up 1 more_vert Hi, congratulations! May I ask you, how did you do the predictions? Did you check each patient's non-zero first month and the based on this you predicted the updrs values? AmbrosM Topic Author Posted 2 years ago · 5th in this Competition arrow_drop_up 0 more_vert Yes, @quimquadrada , in month zero I do not yet know the group of the patient, but starting from month 6 the group is known, and then the model predicts either red or green according to the bottommost diagram of the post. Yoobin Posted 2 years ago · 1469th in this Competition arrow_drop_up 1 more_vert Thank you for your notes. this is indeed great insight! I have to say this competition is really ruined by the organization. The aim should be finding useful info for patient, like protein stuff. But at the end, it becomes a data issue, lol. Cody_Null Posted 2 years ago · 804th in this Competition arrow_drop_up 1 more_vert Lesson learned. Plot all variables against the target. emoji_people AC Posted 2 years ago · 617th in this Competition arrow_drop_up 1 more_vert Hey! Congratulations! :) Thank you for sharing your findings. If it's okay, could you please tell what's the private LB score of your other notebook that uses protein/peptide. AmbrosM Topic Author Posted 2 years ago · 5th in this Competition arrow_drop_up 2 more_vert Hi @ahsuna123 I've submitted 7 notebooks based on groups and proteins with private scores from 60.0 to 60.8. The one I chose for the final submission has 60.4. emoji_people AC Posted 2 years ago · 617th in this Competition arrow_drop_up 1 more_vert Oh, yes! That's a huge difference. I had the same experience of notebooks using only visit_months performing much better than those using protein/peptide features on private LB. Thank you for sharing. Learnt a lot from your public notebooks and of course, this one! Congratulations once again. Well Deserved! :) This comment has been deleted. emoji_people AC Posted 2 years ago · 617th in this Competition arrow_drop_up 0 more_vert Likewise! After seeing some of the great solutions made public! It's definitely intriguing! Will go more in depth for EDA from next time! :) Makotu Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Congrats! Thank you for sharing. I learned a lot from your notebook and discussion in this competition. Hasan Patel Rodriguez Posted 2 years ago · 703rd in this Competition arrow_drop_up 1 more_vert Great work, excellent thoughts on EDA. Sergey Saharovskiy Posted 2 years ago arrow_drop_up 2 more_vert I played with the source code for a bit. It is a typical Kaggle dark magic… Though, @ambrosm it does not undermine your analytical skills. Congratulations! I could not find any signal and practical use of the possible solution, so I skipped the competition. Looking forward to crossing the swords with you on the leaderboard anytime soon. SOUMENDRA PRASAD MOHANTY Posted 2 years ago arrow_drop_up 2 more_vert @ambrosm , one best thing I learnt from you is how to make dig into the data & extract required info smartly . Great work . Congratulations on gold . could you please tell me how many hours did you spent in this comp !!(Out Of Blue ) AmbrosM Topic Author Posted 2 years ago · 5th in this Competition arrow_drop_up 4 more_vert Thanks, @soumendraprasad . As I started late into the competition, it's perhaps five weeks times three hours per evening = 100 hours. Vincent Schuler Posted 2 years ago · 14th in this Competition arrow_drop_up 2 more_vert Congrats @ambrosm ! And thanks for the graphs and the explanation. Ankur Limbashia Posted 2 years ago · 751st in this Competition arrow_drop_up 2 more_vert Wow, excellent explanation!! Congratulations!! Chris Deotte Posted 2 years ago · 4th in this Competition arrow_drop_up 2 more_vert Wow, great discovery! Congratulations @ambrosm !! Ankur Limbashia Posted 2 years ago · 751st in this Competition arrow_drop_up 1 more_vert Eagar to learn about your approach @cdeotte Appreciation (4) yuda lee Posted 2 years ago · 1067th in this Competition arrow_drop_up 1 more_vert Insightful! Thank you for sharing! Xiao-Su (Frank) Hu Posted 2 years ago · 729th in this Competition arrow_drop_up 1 more_vert Impressive! Thank you for sharing. Rocha Erik Posted 2 years ago · 1059th in this Competition arrow_drop_up 1 more_vert Great work and thank for sharing it!! emoji_people Woodpecker Posted 2 years ago · 928th in this Competition arrow_drop_up 1 more_vert I have learnt a lot! Thanks!",
      "AMP®-Parkinson's Disease Progression Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. AMP®-PD · Featured Code Competition · 2 years ago Late Submission more_horiz AMP®-Parkinson's Disease Progression Prediction Use protein and peptide data measurements from Parkinson's Disease patients to predict progression of the disease. AMP®-Parkinson's Disease Progression Prediction Overview Data Code Models Discussion Leaderboard Rules Joseph · 8th in this Competition  · Posted 2 years ago arrow_drop_up 28 more_vert 8th place solution: One trick to win the gold Thanks my teammates @hookman @zui0711 @max2020 @librauee , I have learnt a lot from them. A crucial trick found by them is there are two groups in the data: The true patients and the control group . We can find this by the minimum visit_month diff of each id: for df in [clinical, sup_clinical] :\n    df [ 'visit_month_diff' ] = df .groupby ( [ 'patient_id' ] ) [ 'visit_month' ] .diff ()\n    df [ 'visit_month_diff_min' ] = df .groupby ( 'patient_id' ) [ 'visit_month_diff' ] .transform ( 'min' ) content_copy There are 3, 5, 6, 12, 36 visit_month diff in clinical and sup_clinical, we choose only 3, 6, 12, 36 parts as training data, and transform 3, 36 to 6. We find 3, 6 and 36 has obviously higher updrs values and 12 has lower updrs values(5 part dropped as abnormal). We can draw a conclusion visit_month diff==6 are true patients and visit_month diff==12 are control group. We use piecewise function to optimize two groups smape separately: def calculate_predictions(pred_month, trend): if target == 'updrs_4' : \n        pred_month = pred_month. clip ( 60 , None ) \n    pred_month2 = (pred_month -60 ). clip ( 0 , None ) return np. round (trend[ 0 ] + pred_month * trend[ 1 ] + pred_month2 * trend[ 2 ]) content_copy In the first loop inference, we cannot get visit_month diff, so we train additional coefficients based on true patients + control group. In every loop, we record history data so that we could get the the minimum visit_month diff . Finding these two groups can help you reach 54.2~54.8 on public board and 60.1~60.7 on private board. Please sign in to reply to this topic. comment 0 Comments Hotness",
      "AMP®-Parkinson's Disease Progression Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. AMP®-PD · Featured Code Competition · 2 years ago Late Submission more_horiz AMP®-Parkinson's Disease Progression Prediction Use protein and peptide data measurements from Parkinson's Disease patients to predict progression of the disease. AMP®-Parkinson's Disease Progression Prediction Overview Data Code Models Discussion Leaderboard Rules Makotu · 9th in this Competition  · Posted 2 years ago arrow_drop_up 39 more_vert 9th Place Solution First of all, I would like to express my gratitude to everyone who organized this competition, thank you! Given the small amount of data, the competition was quite uncertain, but I'm glad that I was able to achieve good results. Now, I will describe my solution below. Solution Overview The important points of my solution are as follows. Rule-based Patient splitting (most important) I was looking at train data and noticed that healthy patients with very low updrs values are tested only every other year (0, 12, 24, 36, 48, 60 month…) and semi-annual data (6, 18, 30, 42, 54 month ..) were not present. This was also the case in the test data. Patients without data at either the 6th or 18th month would see a significant improvement in LB by lowering the UPDRs value. Modeling using only visit_month Modeling basically uses only visit_month feature.  However, the \"healthy patients with very low updrs values\" mentioned in (1) always have low updrs even after a month has passed, so these patients are removed from the train for modeling. Supplemental data is also used, but data for patients with month = 5 and patients with only month = 0 data are removed. The modeling used below. simple linear regression (referred AmbrosM notebook .) catboost regression with loss as huber loss catboost regression with loss as mae loss Three types of regression were created and weighted averaged to obtain a higher CV. For validation, group k fold was performed by aligning the distribution of target as much as possible in each fold. Submit Selection For the last submit, the following two sub were selected. A. LB and CV Best sub: applying approaches 1) to 2), both LB and CV is high and align (CV:54.41 LB:54.4). B. only CV Best sub: For patients for whom protein/peptido is available in the test data, using the results of the model with protein / peptido features without using the results of visit_month. In this case, CV improves by about 0.4, but LB decreases by the same amount.(CV:54.02 LB:54.9) As a result, sub A was better score(Private:60.5). sub B score is 61.2. In summary, it was a very uncertain competition, but by looking carefully at the data, and carefully adopting an approach that improves both CV and LB, I can achieve good results. Basically I did all the work in kaggle notebook, so I am publishing the code. However, it has not been refactored, so readability is poor. Code Please sign in to reply to this topic. comment 9 Comments 1 appreciation  comment Hotness sirius Posted 2 years ago arrow_drop_up 1 more_vert Congrats. GM on the way Cody_Null Posted 2 years ago · 804th in this Competition arrow_drop_up 1 more_vert Great work! What a good spot to notice the need for patient splitting. Very interesting approach! Oscar Aguilar Posted 2 years ago · 573rd in this Competition arrow_drop_up 0 more_vert Congrats🎉 Thanks for sharing your approach (this is the second solution that used visit_month , interesting!). Anan Posted 2 years ago arrow_drop_up 0 more_vert Congratulations! Thx for sharing! NANACHI Posted 2 years ago · 49th in this Competition arrow_drop_up 0 more_vert Congratulations & Thank you for sharing! I learned a lot from your solution! Barry Posted 2 years ago · 822nd in this Competition arrow_drop_up 0 more_vert Congratulations! It's a great idea! emoji_people DongYK Posted 2 years ago · 715th in this Competition arrow_drop_up 0 more_vert Would you explain how you captured the difference between healthy patients and not?? Ankur Limbashia Posted 2 years ago · 751st in this Competition arrow_drop_up 0 more_vert Congratulations! Very interesting thought process. Appreciation (1) emoji_people DongYK Posted 2 years ago · 715th in this Competition arrow_drop_up 0 more_vert Amazing! Thank you for sharing.",
      "AMP®-Parkinson's Disease Progression Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. AMP®-PD · Featured Code Competition · 2 years ago Late Submission more_horiz AMP®-Parkinson's Disease Progression Prediction Use protein and peptide data measurements from Parkinson's Disease patients to predict progression of the disease. AMP®-Parkinson's Disease Progression Prediction Overview Data Code Models Discussion Leaderboard Rules Maruichi01 · 12th in this Competition  · Posted 2 years ago arrow_drop_up 19 more_vert 12th Place Solution Before anything else, I want to express my deep gratitude to my team members @vitalykudelya @yukisawamura @yukisawamura @salaryman who contributed their time, efforts, and expertise in this competition.  Next, we'd like to extend our thanks to the Kaggle community and the hosts. Thank you for providing a challenging dataset and a great learning opportunity. Overview of our solution Our solution is based on user grouping, trend calculation for each group, user group prediction (classification), and trend assignment for each group. You can check our solution code here . It takes super long time to be scored beacuse my pipeline is not good😓 1. Grouping During our exploratory data analysis (EDA), we noticed significant differences in the target trends among users based on the presence or absence of Medication information (across all 'visit_month' data, regardless of being 'On' or 'Off').  It is basis of our pipeline. 2. Trend Calculation Based on the groups' information, we created three types of trends: those without Medication information, those with Medication information, and overall trends. These trends were adopted from the ' Only_Trend ' and ' Protein Shift ' notebooks. Particularly, the protein 'P05060' greatly contributed to improving the Public LB score. 3. Feature Engineering + Group Classification We implemented binary classification using LightGBM to predict whether a user belongs to a group with Medication information or not. The prediction was made based on 'visit_month' related information, and Protein and Peptide data. For our final submission, to increase the robustness of the model, we utilized 10-fold cross-validation (CV) and random seed averaging. 4. Using Binary Classification Results for Trend Mapping Based on the results of the binary classification, we mapped the trends of each group. For our final submission, if the prediction from binary classification was above 0.75, we assumed it belonged to the group with Medication information. If it was below 0.15, we assumed it belonged to the group without Medication information. For all other cases, we used the overall trend. As the dataset for this competition was small, both CV and LB scores were unstable. We struggled until the end to decide which submission to choose. If you have any questions or feedback, please feel free to comment. We look forward to learning with all of you in the community. Please sign in to reply to this topic. comment 4 Comments Hotness emoji_people IonDrive Posted 2 years ago · 709th in this Competition arrow_drop_up 3 more_vert Congratulations Vitaly and Stallone on becoming competition masters! And as well to Maruichi and Salaryman on becoming competition experts! Thanks for sharing your solution, as well as the public solutions you provided during the contest. emoji_people AC Posted 2 years ago · 617th in this Competition arrow_drop_up 0 more_vert Hey! Thank you for your notebook! And Congratulations! :) Do you know why this particular protein 'P05060' is giving better results. I submitted a notebook after reading this post of yours right now and my public lb score got a boost from 55.5 to 54.9 and even my private lb improved. Vitaly Kudelya Posted 2 years ago · 12th in this Competition arrow_drop_up 7 more_vert P05060 was chosen as the best protein improving the score on the train dataset (for NPX groups top5 quantile and low5 quantile) without any medical knowledge Protein P05060 imporoved cross-validation score, public score and private score over Trend. I'm not sure if this is pure luck or if we have a real signal in the P05060 protein ) Jayanth Sai Marreddi Posted 2 years ago · 528th in this Competition arrow_drop_up 2 more_vert Secretogranin-1 is a neuroendocrine secretory granule protein, which may be the precursor for other biologically active peptides",
      "AMP®-Parkinson's Disease Progression Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. AMP®-PD · Featured Code Competition · 2 years ago Late Submission more_horiz AMP®-Parkinson's Disease Progression Prediction Use protein and peptide data measurements from Parkinson's Disease patients to predict progression of the disease. AMP®-Parkinson's Disease Progression Prediction Overview Data Code Models Discussion Leaderboard Rules Fnoa · 13th in this Competition  · Posted 2 years ago arrow_drop_up 21 more_vert 13th place solution First of all, congratulations to the winners! My solution is based on LGB models, where I built a separate model for each forecast horizon (0, 6, 12, 24) and target (updrs_1, updrs_2, updrs_3, updrs_4). The key variables I utilized for all targets were: \"visit_month\" \"num_visits\": Number of visits the patient had before the visit_month \"relation\" : Relation between these two variables The objective function was MAE. With these features, I achieved: CV ~ 54.57 (public score: 54.5, private score: 60.3) It is worth to say that about 70% of my submissions outperformed the two I eventually selected. (Luckily I got the gold) FEATURE SELECTION I performed an analysis to identify proteins that enhanced my local validation, particularly for updrs_1, updrs_2 and updrs_3. This was done by running multiple fold divisions and taking the average of CV to mitigate randomness. OTHERS THINGS I introduced random noise to the protein NPX values to prevent overfitting. I randomly set 15% of the protein NPX values to Null, also as a precaution against overfitting. I did target transformation for updrs_3 and updrs_4 (np.log1p/np.expm1) With all this My best local validation score was 53.46 (public score: 54.8, private score: 60.9). THINGS THAT DIDNT WORK FOR ME Remove outliers for training Customize objective function Ensembling with different models Predict trend and then predict residuals Lessons learned I should have chosen one submission based on the public LB and another based on CV, instead of both relying solely on CV. Next time, I will dedicate more time to conducting a deeper Exploratory Data Analysis (EDA)./ Please sign in to reply to this topic. comment 4 Comments Hotness Jayanth Sai Marreddi Posted 2 years ago · 528th in this Competition arrow_drop_up 1 more_vert Thanks for sharing all of your submission scores, Congrats for a solo gold!! The Devastator Posted 2 years ago arrow_drop_up 1 more_vert Amazing that such a simple solution can get such a good score, congrats! The Devastator. cikir97733 Posted 2 years ago arrow_drop_up 1 more_vert Nice work! Javier Martín Posted 2 years ago arrow_drop_up 1 more_vert Congratulations on your solo gold medal @trasibulo !!",
      "AMP®-Parkinson's Disease Progression Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. AMP®-PD · Featured Code Competition · 2 years ago Late Submission more_horiz AMP®-Parkinson's Disease Progression Prediction Use protein and peptide data measurements from Parkinson's Disease patients to predict progression of the disease. AMP®-Parkinson's Disease Progression Prediction Overview Data Code Models Discussion Leaderboard Rules Wojciech \"Victor\" Fulmyk · 43rd in this Competition  · Posted 2 years ago arrow_drop_up 0 more_vert 43rd (top 3%) silver medal solution The following presents my solution, which got 43rd place on the private leaderboard and earned me a silver medal. You can see the code for my solution here . General remarks and sketch of my solution method I constructed my solution using protein data and visit_month only. I did not use the peptide data at all. I felt that all of the necessary information in the peptide data was summarized in the protein data as every peptide has a 1:1 relationship with a protein. Moreover, because a protein has many peptides, using the peptide data could lead to a situation where a particular protein was detected for 2 different observations using different peptides; thus, using peptide data could have led to a situation where the same information (the detection of a protein) could be obfuscated by seemingly different features (different peptides). This competition was very difficult because of the very large number of targets which competitors are forced to predict. In particular for all 4 updrs scores, we were asked to predict that score 0, 6, 12, and 24 months ahead. Moreover, we must make these predictions from every possible visit_month . In other words, if, for instance, visit_month == 36 , then we must predict all 4 updrs for the 36th month, the 42nd month, the 48th month and the 60th month. To make things even worse, no conclusive list of possible visit_month s was ever provided throughout the competition. I strongly believe that this extraordinary complexity made this competition extremely challenging and interesting. To tackle the complexity, for every combination of updrs (except for updrs_4 ), visit_month , and months ahead, I trained a separate CatBoost model. For each of these models, I optimized the hyperparameters using Optuna. For updrs_4 I used a linear model provided by https://www.kaggle.com/code/ambrosm/pdpp-almost-only-trends by Ambrosm which itself was a variation of code originally posted by https://www.kaggle.com/code/vitalykudelya/only-trends by Vitaly Kudelya . I used a clip month of 54 for updrs_4 . Moreover, because of the large number of targets, it was possible for some observations in the test set and for some targets the prediction ought to be outside the range of what was seen in training. Because tree-based models extrapolate very poorly, I felt that if the prediction updrs score provided by CatBoost was at the boundary of what was seen in training, then that prediction might be very poor. As such, in cases where CatBoost predicted a score at the boundary of the training data, that score was disregarded and the linear model described in the previous bullet point was used instead. What worked for me A combination of CatBoost and the linear model in instances where CatBoost predicts the boundary appeared to work best in GroupKFold cross-validation. Due to the large number of targets CatBoost did a worse job by itself without the linear component. Setting the objective to MAE for CatBoost appeared to provide better results than MSE. What did not work for me XGBoost LightGBM Random forest SVR Plain vanilla CatBoost without the linear model for updrs_4 and for CatBoost predictions outside the bounds of what was seen in training Models based on visit_month alone. I missed the trick all top notebooks used, but absent the trick, I found that including the protein data gave superior results. Concluding remarks I really enjoyed this competition and I hope the organizers will find our solutions useful. Congrats to all winners and good luck to all in future competitions!!! Please sign in to reply to this topic. comment 0 Comments Hotness"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Use protein and peptide data measurements from Parkinson's Disease patients to predict progression of the disease. The goal of this competition is to predict the course of Parkinson's disease (PD) using protein abundance data. The complete set of proteins involved in PD remains an open research question and any proteins that have predictive value are likely worth investigating further. The core of the dataset consists of protein abundance values derived from mass spectrometry readings of cerebrospinal fluid (CSF) samples gathered from several hundred patients. Each patient contributed several samples over the course of multiple years while they also took assessments of PD severity. This is a time-series code competition: you will receive test set data and make predictions with Kaggle's time-series API. train_peptides.csv Mass spectrometry data at the peptide level. Peptides are the component subunits of proteins. train_proteins.csv Protein expression frequencies aggregated from the peptide level data. train_clinical_data.csv supplemental_clinical_data.csv Clinical records without any associated CSF samples. This data is intended to provide additional context about the typical progression of Parkinsons. Uses the same columns as train_clinical_data.csv . example_test_files/ Data intended to illustrate how the API functions. Includes the same columns delivered by the API (ie no updrs columns). amp_pd_peptide/ Files that enable the API. Expect the API to deliver all of the data (less than 1,000 additional patients) in under five minutes and to reserve less than 0.5 GB of memory. A brief demonstration of what the API delivers is available here . public_timeseries_testing_util.py An optional file intended to make it easier to run custom offline API tests. See the script's docstring for details. 13 files 59.92 MB csv, py, so Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 59.92 MB amp_pd_peptide amp_pd_peptide_310 example_test_files public_timeseries_testing_util.py supplemental_clinical_data.csv train_clinical_data.csv train_peptides.csv train_proteins.csv 13 files 48 columns ",
    "data_description": "AMP®-Parkinson's Disease Progression Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. AMP®-PD · Featured Code Competition · 2 years ago Late Submission more_horiz AMP®-Parkinson's Disease Progression Prediction Use protein and peptide data measurements from Parkinson's Disease patients to predict progression of the disease. AMP®-Parkinson's Disease Progression Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Feb 16, 2023 Close May 19, 2023 Merger & Entry Description link keyboard_arrow_up Goal of the Competition The goal of this competition is to predict MDS-UPDR scores, which measure progression in patients with Parkinson's disease. The Movement Disorder Society-Sponsored Revision of the Unified Parkinson's Disease Rating Scale (MDS-UPDRS) is a comprehensive assessment of both motor and non-motor symptoms associated with Parkinson's. You will develop a model trained on data of protein and peptide levels over time in subjects with Parkinson’s disease versus normal age-matched control subjects. Your work could help provide important breakthrough information about which molecules change as Parkinson’s disease progresses. Context Parkinson’s disease (PD) is a disabling brain disorder that affects movements, cognition, sleep, and other normal functions. Unfortunately, there is no current cure—and the disease worsens over time. It's estimated that by 2037, 1.6 million people in the U.S. will have Parkinson’s disease, at an economic cost approaching $80 billion. Research indicates that protein or peptide abnormalities play a key role in the onset and worsening of this disease. Gaining a better understanding of this—with the help of data science—could provide important clues for the development of new pharmacotherapies to slow the progression or cure Parkinson’s disease. Current efforts have resulted in complex clinical and neurobiological data on over 10,000 subjects for broad sharing with the research community. A number of important findings have been published using this data, but clear biomarkers or cures are still lacking. Competition host, the Accelerating Medicines Partnership® Parkinson’s Disease (AMP®PD), is a public-private partnership between government, industry, and nonprofits that is managed through the Foundation of the National Institutes of Health (FNIH). The Partnership created the AMP PD Knowledge Platform, which includes a deep molecular characterization and longitudinal clinical profiling of Parkinson’s disease patients, with the goal of identifying and validating diagnostic, prognostic, and/or disease progression biomarkers for Parkinson’s disease. Your work could help in the search for a cure for Parkinson’s disease, which would alleviate the substantial suffering and medical care costs of patients with this disease. This is a Code Competition. Refer to Code Requirements for details. Evaluation link keyboard_arrow_up Submissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0. For each patient visit where a protein/peptide sample was taken you will need to estimate both their UPDRS scores for that visit and predict their scores for any potential visits 6, 12, and 24 months later. Predictions for any visits that didn't ultimately take place are ignored. You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow this template in Kaggle Notebooks: import amp_pd_peptide env = amp_pd_peptide.make_env() # initialize the environment iter_test = env .iter_test() # an iterator which loops over the test files for (test, test_peptides, test_proteins, sample_submission) in iter_test:\n    sample_prediction_df[ 'rating' ] = np.arange(len(sample_prediction)) # make your predictions here env .predict(sample_prediction_df) # register your predictions content_copy Timeline link keyboard_arrow_up February 16, 2023 - Start Date. May 11, 2023 - Entry Deadline. You must accept the competition rules before this date in order to compete. May 11, 2023 - Team Merger Deadline. This is the last day participants may join or merge teams. May 18, 2023 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - $ 25,000 2nd Place - $ 20,000 3rd Place - $ 15,000 Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time Internet access disabled Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv . The API will generate this submission file for you. Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors. Citation link keyboard_arrow_up Leslie Kirsch, Sohier Dane, Stacey Adam, and Victoria Dardov. AMP®-Parkinson's Disease Progression Prediction. https://kaggle.com/competitions/amp-parkinsons-disease-progression-prediction, 2023. Kaggle. Cite Competition Host AMP®-PD Prizes & Awards $60,000 Awards Points & Medals Participation 13,500 Entrants 2,197 Participants 1,805 Teams 40,764 Submissions Tags Tabular Biology Custom Metric Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Citation"
  },
  {
    "competition_slug": "predict-student-performance-from-game-play",
    "discussion_links": [
      "/competitions/predict-student-performance-from-game-play/discussion/420217",
      "/competitions/predict-student-performance-from-game-play/discussion/424329",
      "/competitions/predict-student-performance-from-game-play/discussion/420235",
      "/competitions/predict-student-performance-from-game-play/discussion/420349",
      "/competitions/predict-student-performance-from-game-play/discussion/420119",
      "/competitions/predict-student-performance-from-game-play/discussion/420528",
      "/competitions/predict-student-performance-from-game-play/discussion/420046",
      "/competitions/predict-student-performance-from-game-play/discussion/420132",
      "/competitions/predict-student-performance-from-game-play/discussion/420077",
      "/competitions/predict-student-performance-from-game-play/discussion/420041",
      "/competitions/predict-student-performance-from-game-play/discussion/420158",
      "/competitions/predict-student-performance-from-game-play/discussion/421098"
    ],
    "discussion_texts": [
      "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules Bertrand P · 1st in this Competition  · Posted 2 years ago arrow_drop_up 164 more_vert 1st Place Solution for the Predict Student Performance from Game Play Competition Unbelievable to write this! Thanks! As it is the usage, we first thank the host and Kaggle . These are special thanks because you and us have had a special link in this competition as we gave you more work by reporting data leaks. No doubt you tried to do your best. You are right to animate this community and to trust in it. You are part of it. Please take care of this community that is able to build so much together by sharing. As all of us you have made mistakes and we hope you will learn from them. We also want to thank all of you , Kagglers. We love and are grateful to be part of our group/community. Thanks for sharing and for the collective learning experience. Context Business context: https://www.kaggle.com/competitions/predict-student-performance-from-game-play/overview , Data context: https://www.kaggle.com/competitions/predict-student-performance-from-game-play/data . Overview of the Approach Our solution is essentially a blend of a XGBoost and a NN models. Both heavily rely on duration that appeared to be a powerful leverage. Time was aggregated in different ways and combined with counts for the GBDT while it is transformed via a custom TimeEmbedding block based on 1D convolutions that produce a representation combined with user event representations for the NN. Robustness and efficiency founded our work. XGBoost models were validated on 10 bags of 5 folds and features incorporated only if the mean of the CV of these 10 bags was greater than the level of noise we quantified while we opted for a majority/consensus strategy to build the NN, i.e. validate choices only if 4 of 5 folds were improved. The 3rd place of the efficiency LB was achieved with a lightweight NN accelerated via TF Lite. Details of the submission Code After publishing this write-up we decided to open our code: https://www.kaggle.com/competitions/predict-student-performance-from-game-play/discussion/420332 . It is composed by several parts: how to train the XGBoost models , how to pretrain and train the NN models and the inference notebook used to win this competition. Data Looking at the 1st data released showed that there aren't a lot of sessions so not a lot of sequences. Moreover these are long sequences. This is not ideal for a deep learning approach. Exploring the Field Day Lab research instructed that the Jo Wilder application was built to help learning to read and that way more than 11,500 learners had played this game. These 2 ideas led to search for a bigger dataset. In 1 Google search and 3 clicks we came up to the open data portal ( https://fielddaylab.wisc.edu/opengamedata/ ) which contains a lot of sessions. 1 hour and 3 bash commands latter we knew that the train set was in part in the open data. So we took a week to build a pipeline that extracts 98 % of the sessions of the train set perfectly and with minor errors for the last 2 % . Our data are even better than the comp data because we knew before the host confirmation that for the sessions with 2 games the target was skewed (0 if wrong in 1 of the 2 games when we aim at predicting the responses for the 1st game). It seems that fixing these targets can bring a significant boost up to +0.002. We took 1 more week to build a GBDT/XGBoost baseline that would have scored top 10 given the CV score, with the use of the supplemental data (~20,000 sessions) that gave +0.003/0.004 at that time. As we simulated the API locally (see after), we used some training sessions to infer and noticed that it scored 0.718. We were hoping that the LB sessions were not part of the open data portal but our 1st submission, LB 0.708, immediately showed to us that we had rebuilt about a half of the data and especially the targets in the public LB, because 0.708 = (0.698 + 0.718) / 2. The host and Kaggle have been immediately informed. You know what happened next ( https://www.kaggle.com/competitions/predict-student-performance-from-game-play/discussion/415820) . After the release of the LB data we measured that we perfectly rebuilt ~7000 sessions over the ~11,500 of the LB data. We spent the first month exploring the data until we understood/knew it pretty well. For example we even reconstituted sessions for what might be schools (several games on 1 IP session), extracted every single session with at least 1 answer, … After the update we made a first submission that scored 0.72. This was shocking because this meant that some leaked data were remaining. A few days later we noticed that the open data was not totally similar with the state we found it 1 month before. A file was missing. So we returned to the host and Kaggle to give them more work. This process/work led us to perfectly understand the data model (that changed since the 1st release of the game). This also allowed us to deeply understand the data itself. Note that we only used the sessions for which we had responses to all questions of the 2 1st level groups. 1) This is more consistant with the sessions we want to predict (game from the beginning to the end) and 2) this approach preserves performance (vs all data) while reducing training time. Our dataset is constituted by 37323 complete sessions (23562 comp + logs) in a total of 66376 sessions . The supplemental data (that we fully added 1 month ago) gave us consistently CV +0.002 . Model Our solution is mainly an ensemble of GBDT + NN models. Trust your validation We think that the main reason of the robustness of our solution is that we only relied on CV for decision making. No choice had been made on LB. Probing showed to us that the private test consists in the 1st 1450/1500 sessions served by the API. This is a small set. In our experiments 5,000 sessions is the minimum to guarantee a stable CV/LB alignment. A set less than 2,000 is very noisy so robustness was the way to go . We only added features that improved the CV for sure . This is not easy to delete features that you believe in but this is needed as science is not a matter of belief. There are several ways to do so: for example monitor all folds in a CV (and accept only on majority or consensus), monitor several bags (composition of CV to not overfit validation), … For the GBDT approach, we mainly validated on the mean of 10 bags (we defined a bag as a composition of the folds). As we estimated the noise to be ~0.0003, only improvements greater than the noise have been considered. For the NN as we needed to iterate quicker we only used a single bag and only incorporated > 0.0003 overall improvements with at least 3 or 4 (over 5) folds improved. Metric We experimented a lot on finding a threshold by question but found that this approach is less robust than a single threshold. We mainly used 0.625 as global threshold despite our highest LB scores that were obtained with a threshold per question. GBDT We prototyped a baseline with XGBoost because of the structured/tabular nature of the data . The feature engineering process is interesting to understand what is predictive and to understand the causation, i.e. how the features or decision criteria that enable to predict correctly. Generally speaking we followed 3 ways to build features: business knowledge , our intuition playing the game and a meticulous exploration of the data . Business knowledge refers to using expert knowledge. Reading the papers of the researchers that built this game allow to understand the game beyond usage. For example, Jo Wilder has been built to improve the players reading skills. So this means that the text duration should be important. These are like killer features. We exclusively made use of Polars because of the CPU constraints and to simply learn it. Our features (663, 1993, 3734 for each level_group) are mainly durations and counts for different aggregations : how much time in a level, in a room, reading a text, interacting in some way (event type), how many events in a level_group, how many events of each type, how many events of each type in a room or a level, … We also built a few notebook dedicated features: how many type of events on the notebook in a level, … Despite our efforts we weren't able to extract useful information from the coordinates, the only few features of this type had been mean and std for some events in the activities (journal interactions for example). We considered that injecting targets predicted in the previous level groups was a compression of the signal, meaning a loss of information, so we used, for each session, all interactions from the beginning of the game/session . This led to a +0.002 at the time of this choice. After the API needed to order the data, we noticed that models trained both on original order and on index order but validated on index order (inference order) improved our scores. This leads to more variety that was needed to improve stability and robustness . The same goes for the composition of the validation sets: usage of several bags (composition of validation sets) based on the comp data but also on the extracted data improved our scores. We detected late that increasing the number of folds from 5 to 10 could also be leveraged. The code for GBDT allows to switch from XGBoost to LightGBM and CatBoost with a simple variable parameter but despite the good scores (~0.001 less than XGBoost), this did not bring to ensemble so we sticked to only XGBoost. We experimented a lot around feature selection but were unable to build a stable strategy. So instead of a top-down approach consisting in deleting useless features, we adopted a bottom-up approach choosing carefully each group of features. Our XGBoost models score CV ~0.7025 +/-0.0003 and blending 5 of them (the only XGBoost we still have with correct score) scores LB 0.704 . NN After achieving a good score with gradient boosting and having understood well the data we focused on deep learning. The first attempt was with Transformers . The 1st results were disappointed: CV 0.685 with 2 hours / fold (as far as we can remember). Transformers are very computationally intensive. Resources: https://arxiv.org/pdf/1912.09363.pdf , https://arxiv.org/pdf/2001.08317.pdf , https://arxiv.org/pdf/1711.03905.pdf , https://arxiv.org/pdf/1907.00235.pdf , … We then gave a try to Conv1D . In one day we had a very simple model that scored as Transformers but 10x faster allowing to iterate quicker. So we pushed this approach and could seamlessly scaled it beyond our expectations. Difficult to share the tens or hundreds of experimentations needed to achieve the final solution which is both based on a simple architecture and a slightly complex training pipeline. Architecture roots We browsed the literature based on the question: how to model time in deep learning? This research made us come to the idea of time-aware events (i.e. https://proceedings.mlr.press/v126/zhang20c/zhang20c.pdf ) and back to WaveNet ( https://arxiv.org/pdf/1609.03499.pdf ) because it uses Conv1D to model long sequences with considerations on causation . Other papers also inspired us: https://arxiv.org/pdf/1703.04691.pdf build on top of WaveNet paper for time series, https://idus.us.es/bitstream/handle/11441/114701/Short-Term%20Load%20Forecasting%20Using%20Encoder-Decoder%20WaveNet.pdf?sequence=1&isAllowed=y also build on top of WaveNet. We also have to mention the excellent work that @abaojiang shared ( https://www.kaggle.com/competitions/predict-student-performance-from-game-play/discussion/398565 and https://www.kaggle.com/code/abaojiang/lb-0-694-tconv-with-4-features-training-part) . It inspired our research and maybe successfully biased it. Let's focus on the model of our efficiency submission that is also one of our final ensemble and which performance is nearly the same as models with a few more features. Feature representations 5 features as inputs: duration, text_fqid, room_fqid, fqid, event_name + name (this is the event type from the original data model as far as we remember). Each of these information is encoded/embedded into a vector representation (d_model = 24) to be the merged. The 4 categorical features feed a classical Embedding layer and the duration a TimeEmbedding which is a custom block. Developing the GBDT solution showed that the duration was crucial, so we put a crucial amount of time trying to model it greatly. The TimeEmbedding layer is a composition of 4x ConvBlock which is inspired by the Transformer main block: Conv1D -> skip connection -> layer norm -> dropout. class TimeEmbedding (tf.keras.layers. Layer ): def __init__ ( self , n_blocks, d_model, dropout_rate ): super ( TimeEmbedding , self ).__init__() self .conv_blocks = [ ConvBlock (d_model, dropout_rate=dropout_rate) for _ in range(n_blocks)] def call ( self , inputs ):\n        x = tf.expand_dims(inputs, axis=- 1 ) for conv_block in self . conv_blocks: x = conv_block(x) return x content_copy class ConvBlock (tf.keras.layers. Layer ): def __init__ ( self , d_model, dropout_rate ): super ( ConvBlock , self ).__init__() self .conv1d = tf.keras.layers. Conv1 D(d_model, kernel_size= 5 , padding= 'same' , activation= 'gelu' ) self .layer_norm = tf.keras.layers. LayerNormalization () self .dropout = tf.keras.layers. Dropout (rate=dropout_rate) def call ( self , inputs ):\n        x = self .conv1d(inputs)\n        x = x + inputs\n        x = self .layer_norm(x)\n        outputs = self .dropout(x) return outputs content_copy Time-aware events As said, the goal of building these representations was to model time-aware events. We considered the categorical features as events because they represent the user interactions with business entities of the game. We then tried to incorporate duration to make them time-awared. Our main intuition showed to be the best. It is a simple solution based on operation priority to represent that the duration should be associated to each event before associated them together : duration * event_1 + duration * event_2 + … which had been factorized to duration * (event_1 + event_2 + …). class ConvNet (tf.keras.Model): def __init__ ( self, input_dims, n_outputs, d_model, n_blocks= 4 , name= None ): super (ConvNet, self ).__init__(name=name) self .input_dims = input_dims self .n_outputs = n_outputs self .d_model = d_model self .n_blocks = n_blocks self .event_embedding = tf.keras.layers.Embedding(input_dims[ 'event_name_name' ], d_model, mask_zero= True ) self .room_embedding = tf.keras.layers.Embedding(input_dims[ 'room_fqid' ], d_model, mask_zero= True ) self .text_embedding = tf.keras.layers.Embedding(input_dims[ 'text' ], d_model, mask_zero= True ) self .fqid_embedding = tf.keras.layers.Embedding(input_dims[ 'fqid' ], d_model, mask_zero= True ) self .duration_embedding = TimeEmbedding(n_blocks=n_blocks, d_model=d_model, dropout_rate= 0.2 ) self .gap = tf.keras.layers.GlobalAveragePooling1D() def call ( self, inputs ):\n        event = self .event_embedding(inputs[ 'event_name_name' ])\n        room = self .room_embedding(inputs[ 'room_fqid' ])\n        text = self .text_embedding(inputs[ 'text' ])\n        fqid = self .fqid_embedding(inputs[ 'fqid' ])\n        duration = self .duration_embedding(inputs[ 'duration' ])\n        x = duration * (event + room + text + fqid)\n        outputs = self .gap(x) return outputs def get_config ( self ):\n        config = super ().get_config().copy()\n        config.update({ 'input_dims' : self .input_dims, 'n_outputs' : self .n_outputs, 'd_model' : self .d_model, 'n_blocks' : self .n_blocks, 'name' : self ._name,\n        }) return config @classmethod def from_config ( cls, config ): return cls(**config) content_copy The 2 representations are equivalent: either you can think time-aware events as a combination of time and sub-events or as a combination of sub-events and time. Training pipeline The training pipeline is not totally straight forward. @dongyk published great schematics that can be useful to illustrate what is explained bellow: https://www.kaggle.com/competitions/predict-student-performance-from-game-play/discussion/420217#2332166 . 1st step (pre-training?) The best approach for us consists in a kind of backbone that represents the events of a level_group . This backbone is trained on all the data available for this level_group (i.e. on complete + incomplete sessions). It is associated with a temporary SimpleHead optimizing BCE loss. class SimpleHead (tf.keras. Model ): def __init__ ( self , n_units, n_outputs, name= None ): super ( SimpleHead , self ).__init__(name=name) self .ffs = [tf.keras.layers. Dense (units, activation= 'gelu' ) for units in n_units] self .out = tf.keras.layers. Dense (n_outputs, activation= 'sigmoid' ) def call ( self , inputs ):\n        x = inputs for ff in self . ffs: x = ff(x)\n        outputs = self .out(x) return outputs content_copy This approach allows to score CV 0.70025 +/- 0.0005 . 2nd step (training?) The weights of each of the 3 backbones (1 by level_group) are freezed for the 2nd level of training to speedup training but also because it is more stable and efficient. These backbones can be thought as \"embedders\". During this 2nd step, all the submodels that composed the solution were trained on all complete sessions in an end-to-end setup . The input data are 3 sequences of the 5 features, 1 for each of the 3 level groups. Each \"embedders\" outputs a 24 dim-vector representation. These outputs are the inputs of a head in which enters the representation of level_group '0-4' to predict the 3 first questions and the concatenation of the previous and the current representations for level_groups '5-12' and '13-22' to make use of all information. Proceeding like this allows to optimize the overall performance and to monitor it based on the F1 score that is the score of the competition. This means we optimized BCE with F1 score as a metric. Our winning submission uses a simple MLP head but also a skip head (512 -> 512 -> 512 allow it for example). MMoE did not improve the simplest approaches. This approach allows to score CV 0.70175 +/- 0.0003 which is comparable to the GBDT solution . Inference Build a simulator Early in the competition we built a simulator of the API. Doing so we never experimented any submission error. Maybe trying to keep ideas and code as simple as possible was also key to debug easily. Efficiency We invested the efficiency part of the challenge for GBDT as well as NNs. Using Treelite for XGBoost allow us to divide by 2 the execution time. Our deep learning models were lights: 400,000 weights for the end-to-end model which combines every parts/sub-models. Having already used TF Lite we knew it could be a game changer. Converting our models led to a significant boost in inference time without any performance loss (we do not remember exactly but we think it is at least 6x faster on our local inference simulator). Beginning to explore pruning as well as hard quantization showed that the performance loss would be significant (which is OK in production but not in a competition) so we sticked to a simple TF Lite conversion. We have not leveraged what seems to be a problem in the efficiency metric. As we identified the private test sessions to be the 1450/1500 first served by the API we tried to just predict the others to check which time was used (public for public and private for private). Doing so we gain a place but choose to not use this. Our efficiency submission is a NN that scores public LB 0.702 and private LB 0.699 in less than 5 minutes . Ensemble We experimented a lot of ensembling alternatives. In the end we sticked to a simple average 50/50 GBDT/NN with: 2 kinds of GBDT: trained on original order + trained on index order (validated on index order that is the inference case), 3 kinds of NNs: trained on original order + trained on index order with 5 or all features. As our models are lightweight we were able to build a hugh ensemble: 2 x 4 x 10 folds XGBoost + 3 x 4 x 5 folds NNs . The bottleneck for us is the 8 Go RAM constraint. The winning submission scores CV 0.705, public LB 0.705 and private LB 0.705 . Conclusion The main achievement of our work is that it is a good solution for the researchers, learners and children that can benefit of it and we hope it will contributes to progresses for a better learning experience. Up to you guys! Thanks if you read until here! If you have any question do not hesitate to ask. We will do our best to respond. Presentation to the host A video presentation to the host has been recorded and can be available on demand. Feel free to ask via PM. Sources Below are the main sources that we used. More sources can be found in section Details of the submission above. https://www.kaggle.com/competitions/predict-student-performance-from-game-play/discussion/420332 , https://fielddaylab.wisc.edu/opengamedata/ , https://arxiv.org/pdf/1609.03499.pdf , https://www.tensorflow.org/lite/guide Please sign in to reply to this topic. comment 57 Comments 1 appreciation  comment Hotness Giba Posted 2 years ago · 346th in this Competition arrow_drop_up 7 more_vert Huge congrats @cpmpml and @pdnartreb ! Thanks for sharing such a great solution. Double congrats for @cpmpml for finally break the 2nd place curse and get your first winning! Rongchu Posted 2 years ago · 64th in this Competition arrow_drop_up 3 more_vert Congrats, impressive solution! Thank you for your detailed explanation. Will you share the kernel? I can't wait to learn. Bertrand P Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 3 more_vert Thanks @zrongchu ! We have decided to share our code: https://www.kaggle.com/competitions/predict-student-performance-from-game-play/discussion/420332 . Rongchu Posted 2 years ago · 64th in this Competition arrow_drop_up 3 more_vert Thanks! @pdnartreb , @cpmpml Vladimir Simões da Luz Junior Posted 2 years ago · 787th in this Competition arrow_drop_up 1 more_vert Greetings, @pdnartreb and @cpmpml ! I wanted to express my sincere appreciation and congratulations for your outstanding work in this recent competition. Your solution is truly impressive and it's evident that you put a tremendous amount of effort and expertise into developing it. Your focus on understanding the data model and extracting valuable insights is impressive. The combination of GBDT and NN models in your ensemble and search of external feature engineering demonstrates your deep understanding of the problem and your ability to leverage different techniques effectively. I have a question regarding your approach: In the training pipeline, you mentioned that the backbone, representing the events of a level group, is trained on all the available data for that group. Could you elaborate on how you handle the incomplete sessions during this training phase? How do you ensure that the backbone captures the most relevant information from both complete and incomplete sessions? Once again, congratulations on your remarkable achievement, and thank you for your contributions to the Kaggle community. Your solution is an inspiration to fellow data scientists and serves as a testament to your expertise. Best regards,keep up with the great work🦾🚀 CPMP Posted 2 years ago · 1st in this Competition arrow_drop_up 3 more_vert I am not sure I understand what you don't understand. Let me try still. Maybe this thread can help: https://www.kaggle.com/competitions/predict-student-performance-from-game-play/discussion/420217#2331165 For a given NN there are three NNs trained in the \"pretraining hase\", then they are assembled into a single NN in a second training phase. Incomplete sessions are only used in the pretraining phase. For instance, if a session has only data for the first level group (i.e. questions 1,2,3), then it is used to train the first of the three NNs, and it is not used for the other two. Vladimir Simões da Luz Junior Posted 2 years ago · 787th in this Competition arrow_drop_up 1 more_vert Thanks for the quick reply! Indeed the following thread has helped to interpret the model training phase. Specially the following schematics: Kudos to @dongyk , for that. It seems that you have leveraged the full dataset (complete + incomplete sessions), to create specialized models for each level_group, concatenating the inputs. Which enabled the model to capture specific information of all the level_groups independently and all together, by concatenating the model inputs. Therefore you could optimize BCE with F1 score as metric, since it turned out to be a classification problem…. Once again congratulations @pdnartreb and @cpmpml for the excellent work and thanks for contributing to the Kaggle Community🙏🚀 CPMP Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert yes, that's what we did: It seems that you have leveraged the full dataset (complete + incomplete sessions), to create specialized models for each level_group, concatenating the inputs. Hamed JOORATI Posted 2 years ago arrow_drop_up 1 more_vert Congratulations ! Very interesting ! Pasindu Sandakan Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! very helpful. Ansh Tanwar Posted 2 years ago · 554th in this Competition arrow_drop_up 1 more_vert Very insightful MG Posted 2 years ago · 168th in this Competition arrow_drop_up 1 more_vert Congratulations! Really interesting work emoji_people DongYK Posted 2 years ago · 675th in this Competition arrow_drop_up 1 more_vert Hi, I was wondering whether I understand it correctly. PSPFGP 1st Place - NN Training outputs = {}\noutputs [ '0-4' ] = heads [ '0-4' ] (convnet_outputs [ '0-4' ] )\noutputs [ '5-12' ] = heads [ '5-12' ] (\n        tf .keras .layers .Concatenate ()( [convnet_outputs[ '0-4' ] , convnet_outputs [ '5-12' ] ])\n    )\noutputs [ '13-22' ] = heads [ '13-22' ] (\n        tf .keras .layers .Concatenate ()( [convnet_outputs[ '0-4' ] , convnet_outputs [ '5-12' ] , convnet_outputs [ '13-22' ] ])\n    ) content_copy Does it describe like this?? Backpropagation is performed: from outputs['0-4'] to convnets['0-4'],    (I will call this as submodel A) from outputs['5-12'] to convnets['0-4'] and convnets['5-12'], and     (submodel B) from outputs['13-22'] to convnets['0-4'], convnets['5-12'], and convnets['13-22'].     (submodel C) However, this submodels are separated. So, Backpropagation which is performed from outputs['13-22'] to convnets['0-4'] (submodel C) does not affect convnets['0-4'] of submodel A. Did I understand correctly?? +) I was really surprised this approach to compute F1 score during training. Wow… Bertrand P Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Hi @dongyk ! You are working hard. Great! This is really satisfying to see that our work is useful for you. With this mindset you are going to learn a lot. Your understanding and your schematic seem nearly OK. Keep in mind that in step 2 of the training the weights of the ConvNets are freezed. This means backpropagation only changes the heads weights. In this 2nd step we have 3 sequences of data as inputs, 1 for each level_group. Each of these sequences feeds its own ConvNet/embedder that outputs a representation in the form of a 24 dim vector. The representations for level_group 0-4 feeds the SimpleHead for level_group 0-4 which outputs a vector of 3 values that are the probability of the 3 responses of this level_group. The representations for level_group 0-4 and 5-12 are concatenated in a 48 dim vector the feeds the SimpleHead for the 2nd level_group that outputs 10 values that correspond to the 10 questions of the level_group 5-12. Same for level_group 13-22. You correctly spotted that this had been made to train the whole ensemble to optimize the overall F1 score that is what we want. We tried every setup: training the whole ensemble from scratch, each part independently, … The setup we presented here corresponds to what worked best for us. Does this respond to your question(s)? If not feel free to ask. emoji_people DongYK Posted 2 years ago · 675th in this Competition arrow_drop_up 2 more_vert Thank you for your answering!! I'm learning a lot from your code. We tried every setup: training the whole ensemble from scratch, each part independently, … The setup we presented here corresponds to what worked best for us. I was thinking the same. Did they try the whole ensemble from scratch? Because it's more cumbersome to train seperately. Training separately can be helpful to train. Is the reason why you used only convnet['5-12'] for outputs['5-12'] not contained convnet['0-4']? And… I editted the figure. I found that you fed train_dataset only once and the same convnet['0-4'] is fed into simplehead['0-4'], simplehead['5-12'], and simplehead['13-22']. Is it more accurate? If I didn't freeze the convnets, can convnet['0-4'] backpropagated(affected) by not only outputs['0-4'] but outputs['5-12'] and outputs['13-22']? Bertrand P Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Wow! Thanks for your schematics that are 100% correct as I understand them! The write-up has been updated to link to your post. If the weights of the convnets are not freezed then they are updated by the backprop coming from each output they are linked to when training end-to-end. We first trained separatly which gave a baseline. Then we tried to train the end-to-end setup from scratch. As you might have noticed the output allow to monitor the loss which is composed by the losses of each level group that can also be monitored. We observed that the loss for 0-4 was the first to converge but also that, as the others go down, it tends to increase. We interpreted that as the optimization of previous level groups representation for next level groups. This is not what we wanted. We wanted a good representation for a level group that can be used by the next level groups. So we chose to first optimize each convnet separatly and to exploit its representation by heads in a second time. As F1 score cannot be optimized directy, the main goal of the end-2-end approach was to be able to monitor the F1 score to select the best weights. emoji_people DongYK Posted 2 years ago · 675th in this Competition arrow_drop_up 1 more_vert Cool. I feel good to understand it. We observed that the loss for 0-4 was the first to converge but also that, as the others go down, it tends to increase. We interpreted that as the optimization of previous level groups representation for next level groups. This is not what we wanted. We wanted a good representation for a level group that can be used by the next level groups. So we chose to first optimize each convnet separatly and to exploit its representation by heads in a second time. I also tried to come up with a way to change the architecture of the model, but I realized that it's the best idea considering the pros and cons of possible models. As F1 score cannot be optimized directy, the main goal of the end-2-end approach was to be able to monitor the F1 score to select the best weights. The way I see it, it's the most important approach in the NN code. Again, thank you so much. Mayur B. Ingole Posted 2 years ago arrow_drop_up 1 more_vert bravo 1St place yCarbon Posted 2 years ago · 317th in this Competition arrow_drop_up 1 more_vert Congratulations on your win🎉 And thanks you also for sharing your solution! If I may, may I ask a question about the third chapter, the beginning of the Data ? Due to the small number of game sessions, the length of the sequence (i.e. the length of the game to be put into the NN) is not long enough. This as not ideal for the Deep Learning approach, I interpreted. If so, what does it mean by \"Moreover these are long sequences.\"?　Does it relevant that in some cases the game is played many times and is actually a long sequence, but what we have as data is a short sequence? CPMP Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert There are too few sequences to enable complex NN model training. But sequence average length is rather long. yCarbon Posted 2 years ago · 317th in this Competition arrow_drop_up 1 more_vert Thank you.  I understood! Pranav Jadhav Posted 2 years ago arrow_drop_up 1 more_vert @pdnartreb 1st place bravo. Great Victory. Sercan Yeşilöz Posted 2 years ago · 152nd in this Competition arrow_drop_up 1 more_vert Congratulations on your first place! Minze Li Posted 2 years ago arrow_drop_up 1 more_vert very useful, thank you for your fanscinating work. PRIYANSHU YADAV Posted 2 years ago arrow_drop_up 1 more_vert Such an impressive combination of good ideas and hard hard work. emoji_people Dimas Mufid Posted 2 years ago · 1701st in this Competition arrow_drop_up 1 more_vert Congrats on winning first place! Thanks a lot for your detailed explanation. Impressive! 🔥🔥🔥🔥🔥🔥 Mohsin hasan Posted 2 years ago · 11th in this Competition arrow_drop_up 1 more_vert Congrats on the win! Really love the NN design here. I has a very similar NN design, except that I just one-hot encode everything due to low cardinality of categorical features. Robert Hatch Posted 2 years ago · 291st in this Competition arrow_drop_up 1 more_vert Wow, just wow! :D Such an impressive combination of good ideas and hard hard work. Very inspiring! Andriansyah1 Posted 2 years ago arrow_drop_up 1 more_vert This is very cool the understanding is very good and I like this lessonThis is very cool the understanding is very good and I like this lessonThis is very cool the understanding is very good and I like this lesson AbaoJiang Posted 2 years ago · 185th in this Competition arrow_drop_up 1 more_vert Congrats @cpmpml and @pdnartreb for winning the 1st place! Also, thanks a lot for the detailed explanation! May I ask how you decide to simply divide the numeric features ( e.g., duration ) by a constant (60000 in the code you open source), instead of  using other normalization methods. I suppose that shrinking the scale of the raw feature could make the training process converge faster and other normalization techniques don't show superiority over the way you eventually adopted. If there's misunderstanding, please put me right. Thanks a lot! Bertrand P Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert Thanks @abaojiang ! The writeup has been updated to mention your excellent notebook. We first saw the time-aware events and the Conv1D ideas in your work. Finding once more these ideas while exploring the litterature likely made us focus more on them. Continuous features are tricky to preprocess. We knew from our GBDT modeling work that time and especially duration was very important. We put a lot of careful efforts to try to model it correctly. Processing the duration seemed important so we tried a lot of things: scaling, normalization, standardization, clipping, … We explored the data to analyze the distribution of duration times and experimented with values around a baseline that seemed to us reasonnable because of the better distribution on the continuum. What worked best was clipping at 60 seconds + simple scaling. AbaoJiang Posted 2 years ago · 185th in this Competition arrow_drop_up 1 more_vert Hi @pdnartreb , Thanks very much for the mention. I'm so glad that my inconspicuous sharing can inspire you and other teams to come up with far better solutions. And, I think that's the reason why I learn to share and gradually enjoy sharing! During the first month of the competition, I tried techniques like normalization, standardization, quantile transformation, np.log1p , etc., all of which didn't help improve the CV score. Hence, I sticked to the original processing method that clipped duration at 3.6e6 ( i.e., one hour). However, I didn't try to clip it with smaller values. I'll try them out and verify more ideas with late submissions! I learn so so much from your sharing. (1) Instead of just using beautiful visualization to interpret the data, you try hard to understand the mechanism behind raw data generation and clean the data to improve the data quality. (2) In addition to the model architecture design, robust training process and techniques are also crucial ( e.g., the loss criterion to use, whether to share \"embedders\" across different level_group , how to choose checkpoints). (3) Create a robust CV strategy and resist the temptation to select submission based on LB. (4) FE plays an important role all the time, to name a few. Thanks a lot for the clarification, and good luck with your next competition!! ANSH TIWARI Student Posted 2 years ago arrow_drop_up 1 more_vert it is very useful content Joel Erikanders Posted 2 years ago · 4th in this Competition arrow_drop_up 1 more_vert Congrats and thank you for sharing 🎉 Impressive solution with many creative ideas! Fahmi Rizal Kurnia Posted 2 years ago arrow_drop_up 1 more_vert Congrats, mate! BELSONRAJA T Posted 2 years ago arrow_drop_up 1 more_vert Congratulations!  Keep it up! Piyush Sukhija Posted 2 years ago arrow_drop_up 1 more_vert Congratulations on winning! Patrick Gendotti Posted 2 years ago arrow_drop_up 1 more_vert Winner winner chicken dinner! :p",
      "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules mark4h · 2nd in this Competition  · Posted 2 years ago arrow_drop_up 20 more_vert 2nd Place Solution 2nd Place Solution First, I would like to take the opportunity to thank The Learning Agency Lab for hosting the competition and the Kaggle team for making it happen. Here are the details of the 2nd place solution. Summary A single ‭LightGBM ‬model was used to predict all the questions (i.e. not separate models per question or level group) 5 fold cross validation was used during development but for the final submission a single model was trained on all of the data The code was optimised to minimise the efficiency score For the final submission the vast majority of the execution time was spent on the LightGBM prediction stage There was extensive use of numba and C for the feature generation code The model contained ‬1296 features A Threshold value of 0.63 was used ‭ Features A lot of the most important features were based on the time taken to complete a task or react the an event. One of the most important features (after some of the basic features such as the question number and the total count of events for a level group) was the amount of time the user spent looking at the report in level 1 (feature name: L‬G0_L1_first_report_open_duration‭). A plot of the feature importance (LightGBM gain) of the top features can be seen below: Code ‭The code for each stage of the solution can be found here: preprocess data features code ( features code utility script ) generate features train model submission Please sign in to reply to this topic. comment 7 Comments Hotness Joel Erikanders Posted 2 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thank you for sharing! Very elegant solution. I also read your 1st place solution post to the VSB Power Line Fault Detection competition and am very impressed. I have a question about your choice to use a single LightGBM vs ensemble, including for example some NN. Do you find there is no benefit in adding more models to ensemble with LightGBM, or is it simply more that time is better spent understanding the data and improving the features than creating additional models? Would you have been more likely to include other models if the dataset was smaller? mark4h Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Thanks @erijoel and well done on your 4th place. I tend to find that it is better to focus on improving a single model than spread my efforts over multiple. That said, it can definitely be beneficial to add more models, particularly if they are different architectures. This is especially true if your are part of a team, when each member can focus on a different model. For small datasets, I think it still depends, I guess one advantage is that it typically requires less time/resources to train multiple models for small datasets which makes it more practical. emoji_people DongYK Posted 2 years ago · 675th in this Competition arrow_drop_up 0 more_vert It's very impressive to use a single model, which would make much lower the training time. Did using a single model with the question features improve your performance rather than using seperate models without them?? Congrats and thank you for sharing your solution. mark4h Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 2 more_vert On the question of single model vs separate models, my take was to use as simple a solution as possible (a single model), unless there was evidence that a more complex solution was needed (separate models). I never found any evidence that separate models were needed. emoji_people DongYK Posted 2 years ago · 675th in this Competition arrow_drop_up 0 more_vert Thanks. What do you think applying this technique to a NN? CPMP Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Awesome solution. I am glad you shared, I was real curious. Congrats for being 2nd on both LBs! In a way your solution looks similar to Jack (Japan)'s solution. No wonder the two of you trusted top of efficiency prize. Did you have a good CV LB correlation? We found that we had a great CV private LB correlation, but that public LB was more shaky, i.e. that some high CV models had a lower public LB but a great private LB. Fortunately filtering both on CV and public LB scores retained only good private LB scores for us. mark4h Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Thanks. Well done on 1st place. I can’t really add much to the topic of CV-LB correlation. I only made a handful of submissions and they were all of essentially the same model (tuned to improve execution time), so I don’t have much of a spread to correlate. emoji_people C R Suthikshn Kumar Posted 2 years ago · 1121st in this Competition arrow_drop_up 0 more_vert Congratulations on winning the second place. Thanks for sharing the detailed solution with useful notes.",
      "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules Patrick Yam · 3rd in this Competition  · Posted 2 years ago arrow_drop_up 44 more_vert 3rd Place Solution Thanks a lot to the hosts of the competition and my teammates ( @kingychiu , @tangtunyu , and @yyykrk ). I am thrilled that @kingychiu and I will become GM, @tangtunyu is one step closer to becoming a Master, and @yyykrk will get his second gold medal after this competition! Here We will explain our overall solution, @yyykrk also provided additional explanation of the parts he worked on: https://www.kaggle.com/competitions/predict-student-performance-from-game-play/discussion/420274 Classification Task Formulation In this competition, we are asked to predict 18 values for each session. Each session contains 3 level groups. There are multiple ways to model this. 18 binary classifiers 3 Level group classifiers, each one can be a. A multi-label classifier that predicts all values within a level group b. A binary classifier that takes “question index” as a feature within a level group 1 classifier that is a. A multi-label classifier that predicts 18 values within a session b. A binary classifier that takes “question index” as a feature within a session For Gradient boosted tree models, method 2b > method 3b > method 1. Method 2a and 3a are ignored because training the multi-label task is a lot slower with Gradient boosted tree models. For NN models, we focus on the method 2a and 3a, because These 2 methods are not well handled by tree models Multi-label learning makes more sense, because of the F1 score setting of this competition. (some posts discuss we should not optimize for 1 question). Multi-label NN models are faster to train and infer. Additional dataset generated from the raw data We create an additional dataset from the raw data, it contains 11343 complete sessions. This dataset boosts the CV scores for GBT models  by about +0.001~2, but there is not much effect on the public and private scores, and it has both positive and negative outcomes. However, it works very well for NN models, we see +0.002 improvement in both CV and public scores. Validation We are using 5-fold GroupKFold on session_id so that there won’t be any seen sessions in the validation set. Also we didn’t include additional data in our validation set. Gradient Boosted Tree Per question classifier is handled by @yyykrk , Per level, and All-in-1 classifier is handled by @tangtunyu @kingychiu . That’s why there are some inconsistencies in the data preprocessing steps, such as sort by index vs sort by time. Per Question Classifiers We create features for each level group and sorted by index. The features and the sorting methods differ from other models. # Code in polars df1 = df. filter (pl.col( \"level_group\" ) == \"0-4\" )\ndf2 = df. filter (pl.col( \"level_group\" ) == \"5-12\" )\ndf3 = df. filter (pl.col( \"level_group\" ) == \"13-22\" )\n\ndf1 = df1.sort(pl.col( \"session_id\" ), pl.col( \"index\" ))\ndf2 = df2.sort(pl.col( \"session_id\" ), pl.col( \"index\" ))\ndf3 = df3.sort(pl.col( \"session_id\" ), pl.col( \"index\" )) content_copy The number of features: Level group 0-4: 1,000 features Level group 5-12: 2,000 features Level group 13-22: 2,400 features Feature Selection We try feature selection with out-of-folds but the public scores tend to decrease, so we don’t select features about this model in the final submission. The typical features Elapsed time between the previous level group and the current level group. Elapsed time and index count between flag events. Prediction probabilities for previous questions. Sum of the most recent M (M=1,2,…) prediction probabilities. Flag events are events that must be passed during game progression. We extract them with reference to jo_wilder's source code, game playing, and the log data of users who have got perfect scores. Single Best Model(5folds XGBoost) CV: 0.702, Public LB: 0.700, Private LB: 0.701 Per Level Group Classifiers In order to allow the level group models to utilize information from previous level groups, we first split the training data by: # Code in polars df1 = df. filter (pl.col( \"level_group\" ) == \"0-4\" )\ndf2 = df. filter ((pl.col( \"level_group\" ) == \"0-4\" ) | (pl.col( \"level_group\" ) == \"5-12\" ))\ndf3 = df\n\ndf1 = df1.sort(pl.col( \"session_id\" ), pl.col( \"elapsed_time\" ))\ndf2 = df2.sort(pl.col( \"session_id\" ), pl.col( \"elapsed_time\" ))\ndf3 = df3.sort(pl.col( \"session_id\" ), pl.col( \"elapsed_time\" )) content_copy Feature selection is then applied after feature engineering. Features Engineering Room distance and screen distance (pl.col( \"room_coor_x\" ) - pl.col( \"room_coor_x\" ).shift( 1 )).over([ \"session_id\" ]). pow ( 2 ).alias( \"room_coor_x_dis\" ),\n    (pl.col( \"room_coor_y\" ) - pl.col( \"room_coor_y\" ).shift( 1 )).over([ \"session_id\" ]). pow ( 2 ).alias( \"room_coor_y_dis\" ),    \n    (pl.col( \"screen_coor_x\" ) - pl.col( \"screen_coor_x\" ).shift( 1 )).over([ \"session_id\" ]). pow ( 2 ).alias( \"screen_coor_x_dis\" ),\n    (pl.col( \"screen_coor_y\" ) - pl.col( \"screen_coor_y\" ).shift( 1 )).over([ \"session_id\" ]). pow ( 2 ).alias( \"screen_coor_y_dis\" ), content_copy Final scene, checkpoint and answer time By playing the game manually, we know that students are only taking the quiz at the end of each level. The shorter time they used to finish the session of answering questions, the higher probability that they answered those questions correctly. Captured by features like: pl.col( \"index\" ). filter ((pl.col( \"fqid\" ) == \"chap2_finale_c\" ) | (pl.col( \"event_name\" ) == \"checkpoint\" )).apply( lambda s: s. max () - s. min ()).alias( \"chap2_answer_indexCount\" ),\n                (pl.col( \"elapsed_time\" ). filter (pl.col( \"level_group\" ) == \"5-12\" ). min () - pl.col( \"elapsed_time\" ). filter (pl.col( \"level_group\" ) == \"0-4\" ). max ()).alias( \"chap1_answer_time\" ) content_copy Unnecessary moves Also from the experience of playing the game, we believe that there are many people who have played the game for more than one time. Would be great if we are have some feature to identify these players unnecessary_data_values = {} for q in range ( 23 ):\n    unnecessary_data_values[q] = {} for feature_type in [ 'text' , 'fqid' , 'text_fqid' ]:\n        unnecessary_data_values[q][feature_type] = []\n        unique_values = list (df. filter ((pl.col( \"level\" ) == q))[feature_type].unique()) for val in unique_values: if df. filter ((pl.col( \"level\" ) == q) & (pl.col(feature_type) == val))[ 'session_id' ].n_unique() < 23000 :\n                unused_data_values[q][feature_type].append(val) content_copy If they are playing for the first time, they likely have many unnecessary moves. Then we calculate the time / actions they have spent of these moves for col in [ 'elapsed_time_diff' ]:\n\n        aggs.extend([\n             *[pl.col(col). filter ((pl.col( \"level\" ) == level) & (pl.col( \"text\" ).is_in(unused_data_values[level][ \"text\" ]))).count().alias( f\"level_ {level} _unused_text_ {col} _counts\" ) for level in level_feature],\n             *[pl.col(col). filter ((pl.col( \"level\" ) == level) & (pl.col( \"fqid\" ).is_in(unused_data_values[level][ \"fqid\" ]))).count().alias( f\"level_ {level} _unused_fqid_ {col} _counts\" ) for level in level_feature],\n             *[pl.col(col). filter ((pl.col( \"level\" ) == level) & (pl.col( \"text_fqid\" ).is_in(unused_data_values[level][ \"text_fqid\" ]))).count().alias( f\"level_ {level} _unused_text_fqid_ {col} _counts\" ) for level in level_feature],\n             *[pl.col(col). filter ((pl.col( \"level\" ) == level) & (pl.col( \"text\" ).is_in(unused_data_values[level][ \"text\" ]))). sum ().alias( f\"level_ {level} _unused_text_ {col} _sum\" ) for level in level_feature],\n             *[pl.col(col). filter ((pl.col( \"level\" ) == level) & (pl.col( \"fqid\" ).is_in(unused_data_values[level][ \"fqid\" ]))). sum ().alias( f\"level_ {level} _unused_fqid_ {col} _sum\" ) for level in level_feature],\n             *[pl.col(col). filter ((pl.col( \"level\" ) == level) & (pl.col( \"text_fqid\" ).is_in(unused_data_values[level][ \"text_fqid\" ]))). sum ().alias( f\"level_ {level} _unused_text_fqid_ {col} _sum\" ) for level in level_feature],\n        ]) content_copy Time / actions spent on tasks Another class of feature to filter out experienced  players is to measure how fast they finish the tasks before the quiz in every level group. For example the first task of the game is to find the notebook, our hypothesis is that an experienced player would spend less time and actions to finish it. And they have a higher chance to answer the quiz questions correctly. Two examples for chapter 1 pl.col( \"elapsed_time\" ). filter ((pl.col( \"text\" ) == \"Now where did I put my notebook?\" ) | (pl.col( \"text\" ) == \"Found it!\" )).apply( lambda s: s. max () - s. min ()).alias( \"find_notebook_duration\" ),\n                pl.col( \"index\" ). filter ((pl.col( \"text\" ) == \"Now where did I put my notebook?\" ) | (pl.col( \"text\" ) == \"Found it!\" )).apply( lambda s: s. max () - s. min ()).alias( \"find_notebook_indexCount\" ),\n                pl.col( \"elapsed_time\" ). filter ((pl.col( \"text\" ) == \"Found it!\" ) | (pl.col( \"text\" ) == \"Let's get started. The Wisconsin Wonders exhibit opens tomorrow!\" )).apply( lambda s: s. max () - s. min ()).alias( \"go_upstairs_duration\" ),\n                pl.col( \"index\" ). filter ((pl.col( \"text\" ) == \"Found it!\" ) | (pl.col( \"text\" ) == \"Let's get started. The Wisconsin Wonders exhibit opens tomorrow!\" )).apply( lambda s: s. max () - s. min ()).alias( \"go_upstairs_events\" ) content_copy Feature Selection The selection is based on Catboost feature importance over the Catboost feature importance with shuffled labels. (Which is the idea of Null Importances https://www.kaggle.com/code/ogrellier/feature-selection-with-null-importances ) Compute Catboost feature importance with the entire training data. Shuffle the training data labels and obtain the importance again for N times. Compute the final importance by the base importance divided by mean random importance. We then use gp_minimize to search for the best feature size based on 5-fold cross-validation. In the end, we have 233, 647, 693 features respectively for each level group. With Catboost 5-fold CV out of fold F1: 0.7019 With Xgboost 5-fold CV out of fold F1: 0.7021 Then feature engineering is applied to each of the data frames above. And the transformed data frames are used to train our level group models. 18-in-1 Classifiers To train the 18-questions-in-1 classifier, we further concat the above 3 data frames together to form a large data frame. # Code in pandas all_df = pd.concat([\n    df1[FEATURES1 + [ \"q\" ]],\n    df2[FEATURES2 + [ \"q\" ]],\n    df3[FEATURES3 + [ \"q\" ]],\n], axis= 0 ) content_copy This mega concatenation creates many null values because some features only exist in a particular level group. That’s why when building the features for this 18-in-1 classifier: First, reuse the feature selection results from the Per Level Group case. Rerun feature selection again after the mega concatenation With Catboost 5-fold CV out of fold F1: 0.7002 With Xgboost 5-fold CV out of fold F1: 0.7007 Neural Network Model: Transformer + LSTM The pipeline of our NN is based on this public notebook: https://www.kaggle.com/code/abaojiang/lb-0-694-event-aware-tconv-with-only-4-features Numerical input: np.log1p( elapsed_time_diff ) Categorical inputs: event_comb, room_fqid, page, text_fqid, level Transformer part (3 variants): Type A: Conformer like transformer, with last query attention ( https://www.kaggle.com/competitions/riiid-test-answer-prediction/discussion/218318 ) Type B: Conformer like transformer, with last query attention Type C: Standard transformer Post Transformer LSTM: 1 Bidirectional LSTM + 1 LSTM layer Pooling method: Concat of sum, std, max, last Training method: As mentioned in the previous section, we train the model with multi-label, and there are two variants: a. One model per level group b. Same model for ALL level groups We find that combining models trained with different settings can improve both the CV and public LB. Additional data was used for training, it improves both CV and public LB for NN Best NN only ensemble (5 NN with different settings): CV: 0.7028, Public LB: 0.701, Private LB: 0.704 It turns out that NN doesn’t perform very well in Public LB, but does well in Private LB. Submission Selection We selected a submission with the highest LB, a submission with the highest CV, and a submission with a target on a reasonably high CV and a high variety of methods/models. Our best-selected sub is an ensemble of One level group Catboost, one 18-in1 Catboost, two 18-in1 Xgboost, and three NN. The NNs we selected are Type A per level group, Type B per level group, and Type C ALL level groups. This combination gives good diversity to the final ensemble. We ensemble GBT models and NN models on oof data separately with 2 standalone Logistic regression models, then combined them with GBT:NN = 6:4 ratio. The manual weighting in combining GBT and NN results is due to NN not performing well in public LB, so we didn't have enough confidence to give too much weight to our NN models as discussed below. Best selected ensemble: CV: 0.7046, Public LB: 0.706, Private LB: 0.704 0.705 subs that we haven’t picked We have three 705 private score submissions that are not selected. Our best-selected subs ranked 13th in all of our subs in terms of private score. Among these 705 private subs: Per-question GBT model + Group Level GBT model gives us the 705 private score, but not a high ensemble CV score. Per-level-group GBT + NN models with Logistic regression ensemble gives us the 705 private score, but not a high public score. Observations: NN models perform well in CV and private but very poorly in public, while GBT models fit the public so well, It is very strange… Single-question GBT models makes a lower CV ensemble but perform quite ok in both public and private Please sign in to reply to this topic. comment 11 Comments Hotness KhanhVD Posted 2 years ago · 193rd in this Competition arrow_drop_up 3 more_vert Congrats my friend @wimwim and @kingychiu on becoming GM. Very strong solution! Patrick Yam Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thank you my friend! CPMP Posted 2 years ago · 1st in this Competition arrow_drop_up 3 more_vert Great solution, thanks for sharing. Congrats on the result. There are many conclusions you made that we independently reached too. This is reasuusring in a way. I wonder how we can compare our submissions with your unselected 0.705. I hope we'll get more digits in score display soon. serangu Posted 2 years ago arrow_drop_up 2 more_vert @wimwim cool solution! Congrats with 3 place and gold medal! Akshay Vyas Posted 2 years ago arrow_drop_up 0 more_vert @wimwim Congratulations on your impressive performance! The detailed explanation of your solution and the different methods used for classification and feature engineering is quite insightful. Well done! Pooja Chauhan Posted 2 years ago arrow_drop_up 0 more_vert The 3rd place solution used gradient boosted tree (GBT) models and neural network (NN) models to predict student performance. They improved the models through feature engineering and achieved a high F1 score of 0.702 in cross-validation. Luthfi Holic Posted 2 years ago arrow_drop_up 0 more_vert congratulation for both new GM, its such a great achievement.. the solution is well written with detailed information Mohsin hasan Posted 2 years ago · 11th in this Competition arrow_drop_up 0 more_vert Congratulations @wimwim on both the strong finish and becoming GM. Interesting choice of NN architectures, more specifically last query attention. Also, for categorical inputs do you embed them? Any chance you would be able to make code public for these NN :) Patrick Yam Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 2 more_vert Hi @tezdhar , thank you for the question. You are right, I embed categorical inputs, here are the nodebooks to train these NN. Type A per level group : https://www.kaggle.com/wimwim/psp-transformer-type-a-grp-level Type B per level group: https://www.kaggle.com/wimwim/psp-transformer-type-b-grp-level Type C ALL level groups: https://www.kaggle.com/wimwim/psp-transformer-type-c-18in1 Akki Posted 2 years ago · 51st in this Competition arrow_drop_up 0 more_vert Thank your for your posting! \"We try feature selection with out-of-folds but the public scores tend to decrease, \" means that you performed feature selection for each fold? Patrick Yam Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Yes, and you can find more information about this section in @yyykrk 's solution write-up: https://www.kaggle.com/competitions/predict-student-performance-from-game-play/discussion/420274 Akki Posted 2 years ago · 51st in this Competition arrow_drop_up 0 more_vert Thank you so much!",
      "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules Joel Erikanders · 4th in this Competition  · Posted 2 years ago arrow_drop_up 48 more_vert 4th Place Solution Acknowledgement I'd like to thank the hosts for providing a very interesting and difficult project to work on for the past months. I am also grateful for all the public sharing on Kaggle, this has been an insane learning experience for me. Without all the public notebooks, discussion posts and old competition solutions available i would have had no chance in this competition. Overview Used most of the raw data for training, while validating only on the kaggle data. Ensemble of Transformer, XGBoost and Catboost, with 3 seeds and 5 folds each. Used a generic set of features based on time, index and screen_coor differences. Linear regression as a meta model. Thresholds have a big impact on LB score Data I used most of the raw data for training, including sessions that only completed level group 0-4 and 5-12. About ~38000 whole sessions and ~58000 sessions in total. Using the raw data increased CV by over 0.001. I validated only on the kaggle data. My initial data preprocessing is simply sorting by level group and index, same as what happens during inference. Also, my experiments indicated no benefit from using the hover durations, so after sorting i dropped the hover rows and re-indexed each session from 0 to len(session). Transformer I spent much of my time experimenting with transformers, which resulted in a light weight model that achieved 0.698 on the public and private LB, and 0.702 CV. class NN (nn.Module):\n    def __init__ (self, num_cont_cols, embed_dim, num_layers, num_heads, max_seq_len): super (NN, self). __init__ ()\n        self.emb_cont = nn. Sequential (\n            nn. Linear (num_cont_cols, embed_dim// 2 ),\n            nn. LayerNorm (embed_dim// 2 )\n        )\n        self.emb_cats = nn. Sequential (\n            nn. Embedding (max_seq_len + 1 , embed_dim// 2 ),\n            nn. LayerNorm (embed_dim// 2 )\n        )\n        encoder_layer = nn. TransformerEncoderLayer (\n            d_model=embed_dim,\n            nhead=num_heads,\n            dim_feedforward=embed_dim,\n            dropout= 0.1 ,\n            batch_first=True,\n            activation= \"relu\" ,\n        )\n        self.encoder = nn. TransformerEncoder (encoder_layer, num_layers=num_layers)\n        self.clf_heads = nn. ModuleList ([\n            nn. Linear (embed_dim, out_dim) for out_dim in [ 3 , 10 , 5 ]\n        ])\n\n    def forward (self, x, grp):\n        emb_conts = self. emb_cont (x[:, :, :- 1 ])\n        emb_cats = self. emb_cats (x[:, :, - 1 ]. type (torch.int32))\n        x = torch. cat ([emb_conts, emb_cats], dim= 2 )\n        x = self. encoder (x)\n        x = x. mean (dim= 1 )\n        x = self.clf_heads[[ \"0-4\" , \"5-12\" , \"13-22\" ]. index (grp)](x)\n        return x. unsqueeze ( 2 ) content_copy embed_dim: 64 num_layers: 1 num_heads: 8 max_seq_len: 452 (explained below), though the sequences are cropped to 256 I used the same single model for all questions I found the data easy to overfit with transformers, so in an attempt to improve the signal to noise ratio i did the following: Identify different points in the game by string concatenating event_name, level, name, page, fqid, room_fqid, text_fqid, in the dataframe. Some of these occur more than once in a session. Treat these as different points by enumerating them and adding the enumeration to their names. Filter out the rows with points that is present in over 0.999 of the sessions. This makes each session maximum 452 steps long. Create 6 feature columns: time difference, index difference, distance (cumulative distance moved, calculated from screen_coor's) difference, room_coor_x, room_coor_y and the categorical point column embedded. XGBoost This is my strongest single model with public LB 0.701, private 0.702, and 0.7029 CV. What stands out is that i flattened 5 of the transformer input columns (excluding the categorical column), and used all those values as individual features. The other features are mainly stats that can be found in public kernels, like mean and max time diff over the categoricals. The stats were calculated before applying the transformer input filtering. From early on I trained one model for each level group, inputing the question number as a feature. I found that CV increased by around 0.0002 compared to using a model for each question. This could be randomness, but i went with it since i thought 3 models instead of 18 would make my life easier during experimentation. Similar reasoning behind using one model for the transformer. Catboost Essentially looks the same as XGBoost. CV 0.7022. Ensemble I trained a linear regression meta model for each question, using the above models output probabilites as input, to produce the final predictions. I included probabilities of past questions and some future ones! For example the regression model trained to predict question 2 took probabilities on question 1-3 as input, to predict question 7 I used probabilities on question 1-13, and for question 16 i used probabilities on question 1-18. I took 3 seeds average before linear regression input to make it more robust. This finally results in public LB 0.702, private LB 0.703, CV 0.7044. On threshold and submission selection I tried to trust CV as much as possible, but the consistent gap between my CV and LB was suspicius until the last few days. Then I realized one reason could be my selected threshold was suboptimal on the test data.. I made some submissions with my highest CV solution, only changing the threshold, and noticed it was indeed suboptimal and caused more variation in LB score than most of my latest experiments. So in the end I selected 3 of the same solution, with different thresholds: 0.60 (best on LB), 0.62 (best during CV) and 0.64. Turned out 0.61 would have resulted in 0.704 private, but no regrets ;) Thank you for reading! Code Training code Submission notebook Please sign in to reply to this topic. comment 14 Comments Hotness Robert Hatch Posted 2 years ago · 291st in this Competition arrow_drop_up 1 more_vert I considered that thresholds could be impactful (either due to randomness or some 'real' reason) but didn't officially test CV vs public LB thresholds. If wanting to really deep dive on CV higher than LB, though, nested CV might've been the right approach? I.e outer fold 1 does nested training for the sole purpose of determining threshold (and ensemble weights? And hyperparams?), then outputs 0 and 1 outer CV predictions based on fold 1 specific threshold value. So on for all folds and logically it would be more accurate or slightly pessimistic CV instead of literally optimistic (aka optimized) CV. Joel Erikanders Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 0 more_vert I think you're right that nested CV would have lead to more accurate or slightly pessimistic CV. Though I'm not sure whether or not it would have been a better approach, or lead to better thresholds in the end, maybe. I suppose it's a tradeoff between fast iteration and accuracy of the experiments Robert Hatch Posted 2 years ago · 291st in this Competition arrow_drop_up 0 more_vert Yeah, it wouldn't change the threshold picked, still probably pick based on regular CV. It just might have given more insight and peace of mind about CV vs LB difference. Which might not even be worth the time and effort, lol. ;) Akshay Vyas Posted 2 years ago arrow_drop_up 1 more_vert @erijoel Congratulations on your impressive achievement and thank you for sharing your journey! Your ensemble approach with Transformers, XGBoost, and Catboost, along with the meta-modeling technique, is quite insightful. Pooja Chauhan Posted 2 years ago arrow_drop_up 1 more_vert The 4th place solution in the competition used an ensemble of Transformer, XGBoost, and Catboost models, along with linear regression as a meta-model. The participant applied feature engineering and experimented with different configurations to achieve high scores in both public and private leaderboards. emoji_people Priyanshu Chaudhary Posted 2 years ago · 8th in this Competition arrow_drop_up 1 more_vert Congratulations on the solo gold!! Pranav Jadhav Posted 2 years ago arrow_drop_up 1 more_vert @erijoel Congratulations. 4th place is quite a big achievement. Swapnil Chowdhury Posted 2 years ago arrow_drop_up 1 more_vert Congratulations on coming 4th @erijoel . Keep achieving new milestones and medals 👍 Joel Erikanders Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 1 more_vert Thank you 😄 emoji_people DongYK Posted 2 years ago · 675th in this Competition arrow_drop_up 1 more_vert Congrats your first gold medal !! Joel Erikanders Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thanks a lot! emoji_people C R Suthikshn Kumar Posted 2 years ago · 1121st in this Competition arrow_drop_up 2 more_vert Congratulations and thank you for sharing the solution. It is interesting to review the solution and serves as a good learning resource. CPMP Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert Thanks for sharing. I tried ensembling via linear regression per question, and while it improves Cv, LB and private is a bit lower (LB 0.704) than a single linear regression LB 0.705. I was surprised by that, and now I am surprised it worked for you. Joel Erikanders Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thanks for the comment! Surprising indeed, do you think the difference between question wise and single model  score is due to randomness or something else? The only time I tried linear regression I did it question wise with previous probabilities the way I described This comment has been deleted.",
      "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules Jack (Japan) · 7th in this Competition  · Posted 2 years ago arrow_drop_up 102 more_vert 7th Place Solution (Efficiency 1st) I am pleased to have fought the long and hard competition with all of you here. Here I would like to outline my solution. Overview To make predictions for 18 questions, I trained 3 LightGBM models, one for each level_group. The reason I did not build a model for each question was primarily to reduce inference time. Most of the features I have created are features based on the time difference between two consecutive actions. (More on this later.) The CV score was improved by about 0.002 by adding raw data published by the competition host. Unexpectedly, the submission for the Efficiency Prize had the best score in Private Leaderboard amoung the selected sumissions. The inference time of that is approximately 3 minutes. The notebooks reproducing my submission are as follows: https://www.kaggle.com/code/rsakata/psp-1-save-data https://www.kaggle.com/code/rsakata/psp-2-process-raw-data https://www.kaggle.com/code/rsakata/psp-3-fe-and-train-lgb https://www.kaggle.com/code/rsakata/psp-4-test-inference Feature Engineering The six variables (level, name, event_name, room_fqid, fqid, and text) were concatenated as aggregation keys, and the time difference from the previous or following record was summed for each key and used as the feature. If written in pandas-like code, df.groupby(['level', 'name', 'event_name', 'room_fqid', 'fqid', 'text'])['elapsed_time_diff'].sum() In addition to the time difference from the previous or following records, the number of occurrences of each key is also added as a feature. Since these features can be calculated by sequentially reading the user's session, they can be calculated very efficiently by treating the data as the Python list instead of using Pandas. Furthermore, the record whose event_name is 'notification_click' is considered as a important event, and the time difference between the two events is added to the feature. The procedures for calculating these features can be found by reading the third published notebook. Modeling Since the variety of keys (combinations of six variables) is very large, I reduced features before training by excluding in advance rare combinations that appear only in a small number of sessions. However, since the number of features still amounted to several thousand, I first trained LightGBM with a large learning rate (0.1) and performed feature selection based on gain feature importance. The training was then performed again with a smaller learning rate (0.02) using 500 to 700 features. In the second modeling, raw data published by the host ( https://fielddaylab.wisc.edu/opengamedata/ ) was included in the training. Although I was unable to reproduce the host's train.csv file completely, but I was able to reproduce it approximately using the second published notebook. Many of the sessions included in this data were different in nature from the competition data because they did not complete the game until the end. In fact, users who left the game midway through tended to have lower percentages of correct responses. To reflect this difference, the maximum level of each session was added as a feature. When training the model for the last level_group, I augmented the label of the second level_group, which contributed to the improvement in accuracy. I believe that the reason for this is that overfitting was suppressed by using more data to determine the split point when splitting nodes of decision trees. However, for the first and second level_groups, this data augmentation method did not contribute to improve accuracy in local validation. The CV/LB scores of my best submission is: CV: 0.7034 Public LB: 0.703 Private LB: 0.703 Other Remarks For stability of evaluation, 4-fold CV was repeated three times with different seeds. Based on the validation results, the threshold was set at 0.625. No adjustment was made for each question. To reduce inference time, models trained in CV were not used, but retrained models using all data were used for inference. Please sign in to reply to this topic. comment 28 Comments 2 appreciation  comments Hotness Ya Xu Posted 2 years ago · 62nd in this Competition arrow_drop_up 3 more_vert Unexpectedly, the submission for the Efficiency Prize had the best score in Private Leaderboard amoung the selected sumissions. The inference time of that is approximately 3 minutes. Wow, it's shocking to know you can build a model with such level of efficiency and competitiveness at the same time. I observed during my daily work that the list structure is far more efficient than dataframe when possible to use, but the grammar is very tricky and very difficult to master for large programs. It's very generous of you to make those notebooks public, I will learn a lot from them for sure. song xiaozhe Posted 2 years ago arrow_drop_up 1 more_vert Congratulations on your 7th place solution in the competition! @rsakata I notice in the second notebook when processing raw datasets, different data corresponding to different file_date is processed differently. For example: file_date == \"20220701\"; file_date <= \"20200901\";and else are processed in a different way. May I know how you realize the split point and the corresponding appropriate processing method for datasets in different stages? song xiaozhe Posted 2 years ago arrow_drop_up 0 more_vert Another question is that, in your 3rd notebook, in the Feature Engineering cell, I found several codes as follows: for i in range ( len (df_session)): if keys[i] in self .valid_keys: # count feature_idx = self .map_key[keys[i]] if level_group <= 1 :\n                features[feature_idx] += 1 # bdiff feature_idx += len ( self .map_key) if level_group <= 1 and i > 0 :\n                features[feature_idx] += values_time[i] - values_time[i- 1 ] # fdiff feature_idx += len ( self .map_key) if i < len (df_session) - 1 :\n                features[feature_idx] += values_time[i+ 1 ] - values_time[i] content_copy Does these code mean that the third level_group(level_group=2) is excluded during calculating the #cound,#bdiff,and #fdiff,  and why is that? Again thank you for your great job and share, hope you could answer. Jack (Japan) Topic Author Posted 2 years ago · 7th in this Competition arrow_drop_up 0 more_vert Regarding the first question, it is simply the result of a series of modifications to the program to eliminate errors that were output due to changes in the data format. Regarding the second question, note that for the third level_group, bdiff and count are skipped, but fdiff is calculated. The reason for this is that the validation results showed that the contribution was small for bdiff and count, and the validation score for the third level_group was almost the same without including it. song xiaozhe Posted 2 years ago arrow_drop_up 1 more_vert Thanks  a lot! That really helps me! Kota Posted 2 years ago arrow_drop_up 1 more_vert Congratulations!! i want to be like you,just study hard! Thank you for sharing tips. Muhammad Awn Posted 2 years ago arrow_drop_up 1 more_vert It's amazing just to be in the top 10! You'll most probably ace the next one 👍 Hoping Posted 2 years ago arrow_drop_up 1 more_vert Congratulations great job solo cash gold! Great work hope to learn from your excellent work. Andriansyah1 Posted 2 years ago arrow_drop_up 1 more_vert Good experience takeshi miura Posted 2 years ago · 1103rd in this Competition arrow_drop_up 1 more_vert Thank you for sharing your solution. This competition was my first experience participating in Kaggle. Seeing your solution has further motivated me to continue participating in Kaggle. TAK Posted 2 years ago · 395th in this Competition arrow_drop_up 1 more_vert Congratulations, Jack! And I really thank you so much for sharing your great value solution. I am amazed at the model's score, efficiency, and stability all in one. I will make good use of it for my improvements. konumaru Posted 2 years ago · 43rd in this Competition arrow_drop_up 1 more_vert Thank you for publishing the solution. I have one question. I am wondering what process you used to identify \"notification_click\" as important. Jack (Japan) Topic Author Posted 2 years ago · 7th in this Competition arrow_drop_up 3 more_vert Thanks for your comment. In looking at the data with my own eyes, \"notification_click\" seemed to be some sort of checkpoint event. konumaru Posted 2 years ago · 43rd in this Competition arrow_drop_up 2 more_vert Thank you for your response. I was in the silver zone this time, but thanks to you I learned that EDA is also important to get into the gold zone. Thank you. Akshay Vyas Posted 2 years ago arrow_drop_up 1 more_vert @rsakata Congratulations on your impressive 7th place finish!. Your feature engineering technique based on time differences between consecutive actions is intriguing. ino way Posted 2 years ago · 121st in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for sharing your solution so quickly. I am amazed at this solution that takes about 3 minutes of inference to get a high score! emoji_people Hoang Nguyen Posted 2 years ago · 8th in this Competition arrow_drop_up 1 more_vert Amazing work! And congratulations on your gold medal! I have some questions about your pipeline - hope you could answer How do you train one LGBM to predict multiple questions? Is LGBM not similar to other tree boosting methods (XGB, Catboos, etc.) which have only one output head? I have not had a lot of experience with LGBM. How did you augment the second level_group label? Did you use early stopping when doing CV? If so, since each fold will have a different number of trees, how did you determine the num_iterations for the model trained on the entire dataset? Thanks! Jack (Japan) Topic Author Posted 2 years ago · 7th in this Competition arrow_drop_up 4 more_vert Thank you for your comment and questions. Here are the answers. After computing the features, duplicate them for the number of questions and add the question numbers to the features. Same as 1. Yes. I used the simple average of best iterations when retrain models. emoji_people Hoang Nguyen Posted 2 years ago · 8th in this Competition arrow_drop_up 2 more_vert Thank you! KhanhVD Posted 2 years ago · 193rd in this Competition arrow_drop_up 2 more_vert Very solid score between CV, public LB and private LB. Congrats on results and thanks for sharing solution @rsakata Pooja Chauhan Posted 2 years ago arrow_drop_up 2 more_vert Congratulations on your 7th place solution in the competition! Your feature engineering technique using time differences between consecutive actions is interesting. Including the host's raw data and adjusting for users who left the game midway were smart choices. Your CV and leaderboard scores show a strong performance. Well done! Xiao-Su (Frank) Hu Posted 2 years ago · 76th in this Competition arrow_drop_up 2 more_vert Thanks for sharing, this is really impressive. I learned a lot! MICADEE Posted 2 years ago · 239th in this Competition arrow_drop_up 2 more_vert Though, i was outside without my machine with me and trying to beat traffic on my way home, having it in mind to ensemble my best two models (Catboost, GradientBoost) and with  just 6 hours to go in this great competition as at that time. It's quite unfortunate, i couldn't implement this ensemble method due to this time constraint, but still to me i still consider @rsakata solution as the best ever, talk of its neatness, easy to understand, implemented ideas, creativity, and simplicity of this solution, i must say. Awesome !!! A big congratulations 🎉🎊🍾㊗️ to @rsakata for this lovely solution. Thanks for sharing. CPMP Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert Well done! Thanks for sharing your solution. We too found that 0.625 threshold for all questions was best. Andriansyah1 Posted 2 years ago arrow_drop_up 0 more_vert Yes he is the best empty Posted 2 years ago · 40th in this Competition arrow_drop_up 2 more_vert congrats! that's impressive serangu Posted 2 years ago arrow_drop_up 2 more_vert @rsakata congrats with 7 place and gold medal! It is very cool! Swapnil Chowdhury Posted 2 years ago arrow_drop_up 2 more_vert Congratulations @rsakata for securing 7th place 🎉 This comment has been deleted. This comment has been deleted. Appreciation (2) ls Posted 2 years ago · 840th in this Competition arrow_drop_up 1 more_vert thank you and congrats Johny Aldean Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! Thank you for sharing!",
      "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules emoji_people Priyanshu Chaudhary · 8th in this Competition  · Posted 2 years ago arrow_drop_up 29 more_vert 8th Place Solution and Code The competition was really exciting and it gave us a chance to practice feature engineering. I'm very thankful for the support and help from my team @shinomoriaoshi @hoangnguyen719 and @martasprg . They were always there for me and together we made a big difference. I would like to thank the hosts, and special thanks to @cpmpml and @pdnartreb for identifying the issue of data leak, which made the competition right back on track. Special thanks to @cdeotte for his great starter notebooks and insights that helped me  in the early phase of the competition. Overview Here's an overview of what each of us worked on: ·         My main focus was on improving the XGBoost model and handling feature engineering. ·         Minh Tri Phan worked on a Transformer model with a CV (cross-validation) score of 0.699 and a public leaderboard (LB) score of 0.7. ·         Hoang processed the external data. ·         Martin worked on selecting the most relevant features. In our final submissions, we ensembled the XGBoost and Transformer models, which helped us achieve the gold position. Our ensemble submission had a public LB score of 0.705 and a private LB score of approximately 0.7025 . Additionally, we had two other submissions with single XGBoost models, where one had a public LB score of 0.705 and a private LB score of 0.700 . My Part Code: The code is a bit uncleaned, apologies for that. For any queries, contact me on LinkedIn FE code: https://www.kaggle.com/code/chaudharypriyanshu/mb-fb5-train-xgb-25-11-external-data/notebook Inference code: https://www.kaggle.com/code/chaudharypriyanshu/inference-xgb-25-11-17/notebook Training code: https://www.kaggle.com/code/chaudharypriyanshu/mb-fb5-train-xgb-25-9-training/notebook Overview I created a 5-fold XGBoost model for each question (a  total of 90 models). I used Kaggle kernels only to train XGBoost since it took only 45 mins on Kaggle’s P100 GPU to train all 90 models. The single XGBoost model achieved a Public leaderboard (LB) score of 0.705 and took 45-50 mins for inference, but it didn't perform as well on the private LB. When we included Hoang's external data, the model's score improved to 0.704 on the private LB. However, we decided not to use it because the public LB score was unusually low at 0.702 . Feature engineering Session length: simply accounts for the total length of the session per level group. Instance features: I created Object click-based features (first object click, room coordinates of that click, I called them Instance features)that were most important and gave an improvement of 0.0007, when I added them with standard features. I created a total of 36 features since there were 12 instances where object clicks were present. Magic bingo features: Inspired from the public notebooks. I created more such features for all 3 level groups and it improved the CV by 0.0003 . Standard features: a) Count features: I created count features based on Fqid, text_Fqid, room_fqid, level, and event_comb . These features capture the frequency of specific events or combinations. b) Binning of indexes: I performed binning on indexes with bin sizes of approximately 30 or 50 in sorted order. Raw indexes worked better on the private LB, while binned features yielded better results on the public LB. c) First and Sum features: I generated first and sum of elapsed_time_diff  for all categorical columns. I found that min, max, and std did not work well in my case. d) Aggregations based on hover duration. Top Level Group Features: Used top 15-25 features (according to feature importance), Duration and instance features across different level groups. Meta features: Using past questions predictions to predict the current question. i.e. for question t I used all predictions for questions (1 to t-1) . Using them gave an improvement of around 0.001 . Feature Selection (Martin's Part): I eliminated features that had zero importance based on their Gain and Shapley feature importance scores. After performing feature selection, I made adjustments to the learning rate by reducing it from 0.05 to 0.03 and adding more features. Additionally, I removed duplicate features and features with more than 95% values as null. External data: We used publicly available data. It had about 7500 sessions where all 18 questions were answered. Adding this external data improved our model's performance by 0.0005 in cross-validation and 0.002 on the leaderboard. Hoang also created processed external data that worked well on the private leaderboard (score of 0.704). If we had included it, our single XGBoost model could have reached a top 5. position. However, we decided not to use it because of its lower Public leaderboard score (a bad decision). Inference: We made improvements to retain the original order of the sequence during inference. We found that there are approx. 250 sessions with abnormal indexing(interestingly all of them are from the 5th and 6th December 2020) Created a function to preserve the original sequence for 99.5% of sequences, with only a small portion (0.5%) having misplaced events not more than 4-5 positions of the actual index. Reindexed these abnormal sessions which improved or scored on LB slightly. Things not worked: Ensemble with LGBM, Catboost didn’t work. Created a custom eval metric that uses benchmark true positives and negatives a model should have. It increased the CV by 0.001 but LB  decreased probably due to overfitting. Different thresholds for each question. (increased CV decreased LB). The below tables list our experiments with the best results. External Data Used CV Public LB Private LB final Sub No 0.6996 0.701 0.698 No No 0.7001 0.702 0.700 No No 0.6996 0.701 0.698 No Yes((Public ED) 0.7015 0.705 0.700 Yes Yes(Hoang's ED ) 0.7019 0.702 0.704 No Yes (Hoang's ED) 0.7022 0.703 0.703 No Tri's Part: The model is shown in the following figure: Particularly, it consists of 2 parts, (i) Training a neural network, then extracting the embedding. (ii) Concatenating the embedding from the neural network to a set of aggregated features, then training a gradient boosting model (XGBoost, CatBoost, LightGBM). Neural network I was inspired by the RIIID competition and @letranduckinh ’s solution, in which he customized the multi-head attention mechanism to adopt the time gap between 2 actions. In my opinion, if we have to relate the problem to an NLP problem, RIIID competition is like a token classification task (e.g., NER), meanwhile, this competition is like a document classification task. Therefore, I decided to use a transformer and some other recurrent network types. I used the encoder-only structure as I didn’t see any motivation to have the decoder. However, the transformer encoder alone didn’t work so well, so I decided to add some more (3) GRU layers in front of the encoder. The detailed architecture (Pytorch code) is given here ( https://github.com/minhtriphan/Kaggle-competition---Predicting-Student-Performance/blob/main/Transformer/model.py ). Some remarks about training: I used 3 models for 3 level groups. At each level, I used the sequence of previous levels (e.g. The model for the 0-4 level uses the 0-4 sequence, the model for the 5-12 level uses the 0-4 and 5-12 sequences, and so on.) I used all the given features to train the model, NUM_COLS = [ 'index' , 'time_diff' , 'room_coor_x' , 'room_coor_y' , 'screen_coor_x' , 'screen_coor_y' , 'hover_duration' ]\nTXT_COLS = [ 'level' , 'event_name' , 'name' , 'text' , 'fqid' , 'room_fqid' , 'text_fqid' ] content_copy I think the performance of a student, for example, in level 13-22 could carry some information to predict his/her performance in level 0-4. This is what I call the “global knowledge” of a student, and I want the network to capture that. Therefore, the neural network is trained in a multi-tasking manner, in which in the main output is the set of questions in the corresponding level (e.g., for level 0-4, the main output is 3-dimensional for questions 1, 2, and 3), the auxiliary head is used to predict all other questions. This trick helps to gain +0.002 in CV. Overall, the NN gets 0.695/0.700 in CV and public LB (before the API crisis, after that I never check how the NN works in the public LB anymore as it was combined always with XGBoost) Gradient Boosting However, the NN in my case was not super satisfactory. I then decided to extract the embedding from the trained NN, concatenate them into a set of aggregated features, then use XGBoost to train the model. This helped me to get a huge boost in both CV and LB. Overall, the scores of this approach are shown below, External Data Used CV Public LB Private LB No 0.6993 0.702 0.697 Yes 0.6989 0.701 0.699 Unfortunately, as I didn’t observe any gain in CV and public LB with external data, I decided not to choose that model to add to our model pool. Links: Training code: https://github.com/minhtriphan/Kaggle-competition---Predicting-Student-Performance----part-of--8th-solution.git Inference code: Without external data: https://www.kaggle.com/code/shinomoriaoshi/psp-v7b-infer With external data: https://www.kaggle.com/code/shinomoriaoshi/psp-v9a-infer Hoang's Part: Hoang has described his work in a separate thread that describes the preprocessing of external data, experimental results and why to trust CV over LB. Link to Hoang's Part: https://www.kaggle.com/competitions/predict-student-performance-from-game-play/discussion/420315 Please sign in to reply to this topic. comment 4 Comments Hotness ino way Posted 2 years ago · 121st in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for sharing your solution so quickly. I am currently reviewing and looking back at the Discussions of the gold medalists in this competition. I am not sure I understand the Magic bingo features that improved the CV by 0.0003. I know you are busy and don't have much time, but I would appreciate it if you could tell me about the Magic bingo features . emoji_people Priyanshu Chaudhary Topic Author Posted 2 years ago · 8th in this Competition arrow_drop_up 1 more_vert Hey Ino, In the dataset, there are fqids, that have the word bingo in them. They signify that a user was asked to find something on the screen and press it, when the user presses the correct item bingo fqid pops up. Therefore these features indicate the time and number of clicks it took for the player to get the correct item so that bingo-based fqid appeared. I'd advise going through one session and observing the fqid for level group 5-12, you will see some fqids that have word bingo in them. ino way Posted 2 years ago · 121st in this Competition arrow_drop_up 0 more_vert Thank you for your quick and detailed response. I understood that the word \"bingo\" was in the fqid, but did not know it would pop up when I pressed the correct item. As you said, I understand that features based on \"bingo\" words can help improve scores. Thank you so much for your help. Gaurav Rawat Posted 2 years ago arrow_drop_up 2 more_vert Congrats; nice correlation, Meta features on priors are the common theme , think that was the killer other than other smart features. And off course not being tempted by the lb 😀 But nice nn approach as well by your team emoji_people Hoang Nguyen Posted 2 years ago · 8th in this Competition arrow_drop_up 2 more_vert Thanks a lot for your work on this guys! So grateful I had a chance to collaborate and learn from you all.",
      "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules Makotu · 9th in this Competition  · Posted 2 years ago arrow_drop_up 81 more_vert 9th Place Solution First of all, I would first like to thank all the participants who dedicated so much time and effort to this competition, as well as the hosts and the management. While I still believe there are areas in which the administration of the competition could be improved, in this post, I will focus solely on discussing my solution. Overview I didn't do anything particularly special. Mainly, I just kept adding features to improve the accuracy of the single model. For each question, I built models using LightGBM and Catboost, and took the simple average of the two models. I used the models with the highest CV scores as the final candidates. The high CV submit also got almost the best score for private. LightGBM CV：0.7018 LB(Public)：0.703 LB(Private)：0.702 Catboost CV:0.7011 LB(Public)：0.7 LB(Private)：0.701 Merge(final submit) CV:0.7024 LB(Public)：0.7 LB(Private)：0.702 Features Rather than introducing each of the numerous features I created, I'll share my overall approach and discuss a few specific features that particularly contributed to the accuracy. As already demonstrated in public notebooks, an important element in this competition was \"how much time one spends playing.\" To delve deeper, I felt that \"how much time it took from a certain point to another\" was crucial, so I created many features related to this. Checkpoint feature (as I named it) In this game, there are events that almost every player will inevitably experience. For instance, every user will find a notebook and see the message \"found it!\" I identified these \"events that almost every user goes through,\" and used the time taken between these events (i.e., the elapsed time from event A to event B) and the number of clicks as features. This seemed to significantly contribute to the accuracy. I created such features in various patterns, like the elapsed time from viewing text A to text B, the elapsed time from one fqid to the next, the elapsed time from one room to the next, and so on. Other than this, I obviously included features like the time elapsed for each level and the average coordinates, as introduced in the public notebooks. The number of features increases with the level group. Ultimately, the feature counts were as follows: Level group 0-4: 3009 features Level group 5-12: 9747 features Level group 13-22: 18610 features Modeling Approach I chose to use 10-fold rather than 5-fold as it gave slightly higher CV scores (around +0.0005). I used straified Kfold with the number of correct answers for 18 questions of the user. So, the model is made with the distribution of the total number of correct answers of the users almost aligned. (However, I don't think it would be much different with a simple K-fold) For feature selection, I simply used the top 500 features based on their importance. To prevent leakage, I selected the feature importance for each fold, and retrained the model for each fold. For example, when training fold1, I first train with all features, then select the features using the fold1 model, and retrain the fold1 model with the top 500 features. I used the prediction probabilities of previous questions as features. For example, when predicting question 3, I used the prediction probabilities for questions 1 and 2. When predicting question 15, I used the prediction probabilities for questions 1 through 14, etc. (this improved CV by around +0.001) Inference took the following amounts of time: LightGBM: 90min Catboost: 120min Merge: 150min LightGBM's inference became significantly faster by compiling the model with a library called \"lleaves.\" https://github.com/siboehm/lleaves I'm sharing my inference code. Features not mentioned here can be somewhat understood by looking at it. https://www.kaggle.com/code/mhyodo/restart-model-merge-v1 What Didn't Work NN models (I tried several types, such as LSTM and MLP, but none contributed to the CV) Lastly, I've seen posts from others where the LB score was higher than the CV score, but in my case, they were pretty much the same. I was expecting some sort of shakeup, but I didn't anticipate making it into the top 10. I'm curious as to how those with higher LB scores achieved this, as I was unable to significantly increase my LB score. Anyway, thank you! If the ranking is confirmed, I can become a new GrandMaster! Please sign in to reply to this topic. comment 25 Comments 2 appreciation  comments Hotness Chris Deotte Posted 2 years ago · 88th in this Competition arrow_drop_up 5 more_vert Congratulations Makotu achieving solo gold 9th and Congratulations for become Kaggle competition Grandmaster! I also observed that my best CV models had CV score = Public LB score = Private LB score Takoi Posted 2 years ago · 13th in this Competition arrow_drop_up 3 more_vert Congratulations on achieving solo gold and GM! In my case, what significantly contributed to higher LB scores were the ensemble of LightGBM and NN models, as well as adjusting the threshold. The following are the scores when the threshold was changed: Threshold: 0.617 Public Score: 0.706 Private Score: 0.702 Threshold: 0.623 Public Score: 0.705 Private Score: 0.702 Threshold: 0.629 Public Score: 0.703 Private Score: 0.701 Makotu Topic Author Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert I see, thank you for your info! I had not tried either. Gaurav Rawat Posted 2 years ago arrow_drop_up 2 more_vert We also observed threshold played a part, best for us was 0.605 刘锦奥 Posted 2 years ago · 52nd in this Competition arrow_drop_up 3 more_vert Congratulations on becoming a GrandMaster! Thank you so much for sharing!👍👍👍 Hoping Posted 2 years ago arrow_drop_up 1 more_vert Great solution！Thanks for sharing and congratulations on becoming a GrandMaster! Robert Hatch Posted 2 years ago · 291st in this Competition arrow_drop_up 1 more_vert I also came up with checkpoint features. (But didn't do all the great work to build a robust solution, of course :) ) Did you notice that there are four different text options, each session_id only getting one of them? If you spend enough effort, (I did NOT, lol) you can have more checkpoints through matching up equivalent texts. But not sure if having more checkpoints would really even improve score. Makotu Topic Author Posted 2 years ago · 9th in this Competition arrow_drop_up 0 more_vert Yes, I noticed. However, in my case, even if I made the text consistent, the CV did not increase that much. sirius Posted 2 years ago arrow_drop_up 1 more_vert 👍👍Very glad to see your circle turn yellow Makotu Topic Author Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Thanks Sirius! Congrats to you too on your high placement in the KDDcup! Eugeniy Osetrov Posted 2 years ago arrow_drop_up 1 more_vert @mhyodo Thank you for sharing and congratulations! Next time I wish to get the first place! Pooja Chauhan Posted 2 years ago arrow_drop_up 1 more_vert Congratulations on your 9th place finish in the competition! Your approach of combining LightGBM and Catboost models through averaging was effective. The checkpoint feature and utilization of prediction probabilities of previous questions were smart additions. Well done! Akshay Vyas Posted 2 years ago arrow_drop_up 1 more_vert @mhyodo Congratulations on your achievement ! The checkpoint feature you developed, based on the time taken between specific events, seems particularly insightful. carloszone Posted 2 years ago · 1859th in this Competition arrow_drop_up 1 more_vert congratulations. Great work! I must say I agree with you that don't try NN models. Based on my experience, it isn't easy to get a great score from NN models in a run, even if the task seems to be designed for NN models💥. serangu Posted 2 years ago arrow_drop_up 1 more_vert @mhyodo congrats with 9 place and gold medal! emoji_people Hoang Nguyen Posted 2 years ago · 8th in this Competition arrow_drop_up 1 more_vert Did you use the extra 7000 sessions from the leaked data? I think many teams used it to get top public/private LB. Makotu Topic Author Posted 2 years ago · 9th in this Competition arrow_drop_up 4 more_vert I didn't use any extra data. emoji_people Hoang Nguyen Posted 2 years ago · 8th in this Competition arrow_drop_up 1 more_vert That leaked data seems to be the secret recipe for some top public LB positions. However I think some of us utilized it too much and caused overfitting/shake-downs. Kudos to you for building a gold-medal solution without the need of that data! Gaurav Rawat Posted 2 years ago arrow_drop_up 3 more_vert Though we didn’t select unfortunately 🙁 (due to low public lb)but with extra data we saw better private score around 0.703 0.704 , will post soon Maruichi01 Posted 2 years ago · 31st in this Competition arrow_drop_up 2 more_vert Congratulations on your solo gold and earning the title of Competitions Grandmaster! I've tried a similar approach, but I haven't been able to achieve a high score with LightGBM (always worse than Catboost.) Would you mind sharing how you set your hyperparameters for the models? And could you also explain your tuning process? Makotu Topic Author Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Here are my parameters. Since the features are very large and prone to overfitting, making min_data_in_leaf large enough and taking feature_fraction small enough improved the CV in my case. param = { 'boosting' : 'gbdt' , 'objective' : 'binary' , 'metric' : 'binary_logloss' , 'learning_rate' : 0.02 , 'max_depth' : 7 , 'min_data_in_leaf' : 300 , 'bagging_fraction' : 0.5 , 'feature_fraction' : 0.1 , # Level group 0-4: 0.3  Level group 5-12: 0.1  Level group 13-22: 0.05 'verbose' : - 1 , 'seed' : 1208 } content_copy I didn't spend much time on tuning, I tried several patterns by hand, guessing the parameters that seemed important. (Ex. max_depth, min_data_in_leaf, feature_fraction Maruichi01 Posted 2 years ago · 31st in this Competition arrow_drop_up 1 more_vert Thank you for your prompt response! Your advice is very helpful. Once again, congratulations on becoming a Grandmaster! Informhunter Posted 2 years ago · 97th in this Competition arrow_drop_up 2 more_vert Congrats on the gold medal and the incoming GM title! Can I ask, what was your CV strategy? (stratified multi-label/time-series window/something else?) Makotu Topic Author Posted 2 years ago · 9th in this Competition arrow_drop_up 2 more_vert Oh, yes, I forgot to mention that. Thank you very much. I will add it later. I used straified Kfold with the number of correct answers for 18 questions of the USER. So, the model is made with the distribution of the total number of correct answers of the users almost aligned. (However, I don't think it would be much different with a simple K-fold) ADAM. Posted 2 years ago · 14th in this Competition arrow_drop_up 2 more_vert Thanks for sharing and big congrats on your GM title!! emoji_people Hoang Nguyen Posted 2 years ago · 8th in this Competition arrow_drop_up 2 more_vert Congratulations on your soon-coming GM title! Great work! Anthony Chiu Posted 2 years ago · 3rd in this Competition arrow_drop_up 0 more_vert Congrats on being a GM! Appreciation (2) Joey-Yi Posted 2 years ago · 188th in this Competition arrow_drop_up 1 more_vert Thanks for sharing and congratulations! ls Posted 2 years ago · 840th in this Competition arrow_drop_up 0 more_vert thanks for sharing and congrats on gold",
      "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules shu421 · 10th in this Competition  · Posted 2 years ago arrow_drop_up 36 more_vert 10th Place Solution I respect all of you for your tough and long term fight and I am glad that we were able to fight together. I also want to thank my teammates ( @tereka , @deepkun1995 , @ryotak12 , @yurimaeda ) for their hard work. I'm happy because this is the first time I got a gold medal. Overview We're not doing anything special in our solution. We used 1 NN, 1 LightGBM and 4 XGBoost with various features for the Stage 1, and MLP and Logistic Regression stacking for the Stage 2. We used average and threshold optimization for the Stage 3. CV: 0.70573 Public LB: 0.706 Private LB: 0.702 Code Inference: https://www.kaggle.com/code/shu421/psp-10thsolution-public0706-private0702/notebook shu421 XGBoost and Stacking Training: https://github.com/shu421/Kaggle_PSP_10thSolution Models Stage 1: XGBoost (shu421 part) I created XGBoost for each level_group. The base features are not so different from those in the public code. It is an aggregate feature of elapsed_time_diff and hover_duration, and other numerical features. However, in addition to these, I used previous level_group features and predicted probability as current level_group features. I used numpy and numba to create them. Initially, I had used polars, but I switched to numba which is my teammate @yurimaeda 's approach. The submission time was significantly reduced from 2 hours with polars to just 13 minutes with numpy and numba. I used 5-StratifiedGroupKFold as cross-validation strategy. CV: 0.70111 Public LB: 0.702 Private LB: 0.699 Stage 2: Stacking We created MLP and Logistic Regression for each question. Thus, there are 18 models each, and the output dimension of each model is (n_samples, 1). Since stacking was very easy to overfit, we kept the model architecture simple. Here is the code for MLP. class MLP (nn.Module): def __init__ ( self, input_size, hidden_size, output_size ): super ().__init__() self .fc1 = nn.Linear(input_size, hidden_size) self .head = nn.Linear(hidden_size, output_size) self .dropout = nn.Dropout( 0.2 ) self .relu = nn.ReLU() def forward ( self, x ):\n        x = self .relu( self .fc1(x))\n        x = self .dropout(x)\n        x = self .head(x) return x content_copy Stage 3: Threshold Optimization We take the average of the predictions of the 2 models in the Stage 2 and optimize the threshold for each question. import numpy as np from sklearn.metrics import f1_score from scipy.optimize import minimize def f1_score_macro_for_thresholds ( y_true, y_pred_prob, thresholds ):\n    y_pred_binary = (y_pred_prob > thresholds).astype( int )\n    score = f1_score(y_true.flatten(), y_pred_binary.flatten(), average= \"macro\" ) return score def optimize_thresholds ( y_true, y_pred_prob, method= \"Powell\" ):\n    n_labels = y_pred_prob.shape[ 1 ]\n    init_thresholds = np.full(n_labels, 0.6 )\n\n    objective = lambda thresholds: -f1_score_macro_for_thresholds(\n        y_true, y_pred_prob, thresholds\n    )\n    result = minimize(\n        objective, init_thresholds, bounds=[( 0 , 1 )] * n_labels, method=method\n    ) return result.x content_copy We tried some optimization methods, but Powell worked best. This method improved CV by 0.008. What worked feature engineering elapsed_time_diff and hover_duration agg features was important threshold optimization ensemble lstm + transformer(ryota part) sort_frame What didn't work 1D CNN stacking(below methods seemed to be overfitting) CNN(1D/2D) RNN level_group preds datetime agg features use NN embedding as gbdt features TimeSeriesClustering (elapsed_time_diff) additional data Please sign in to reply to this topic. comment 2 Comments Hotness serangu Posted 2 years ago arrow_drop_up 1 more_vert @shu421 , @tereka , @deepkun1995 , @ryotak12 , @yurimaeda congrats with 10 place! You are winning fight and get medals! Swapnil Chowdhury Posted 2 years ago arrow_drop_up 0 more_vert Congrats on coming 10th place @shu421 🎉🎉🎉",
      "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules Takoi · 13th in this Competition  · Posted 2 years ago arrow_drop_up 52 more_vert 13th place solution 13th place solution First of all, I would like to thank the Kaggle community for sharing great ideas and engaging discussions. I would also like to thank the hosts for organizing this interesting task competition. Summary Ensemble of LightGBM and NN Cross validation: Nested cross validation Training data: Data for which the first four digits of the session_id are 2200 or less. Validation data: Data for which the first four digits of the session_id are 2201 or more. I trained the model on the training data using a 5-fold cross validation strategy, and evaluated it on the validation data using predictions from all 5 trained models. For the final submission, I trained the model on the entire dataset using a 5-fold cross validation approach. LightGBM I trained one model for level group 0-4 and another for level group 5-12. For these, I included features representing the target and trained a single model. For level group 13-22, I trained a distinct model for each target. Main features: The count of categorical data for each session_id The statistical measures of numerical data for each session_id An aggregate of the next action taken Scores CV : 0.7032 Public Score : 0.704 Private Score : 0.701 NN Model: Transformer + GRU The standalone Transformer didn't perform very well. The addition of GRU improved the score. Trained with fewer features I trained a separate model for each level group. Scores CV : 0.7010 Public Score: 0.700 Private Score: 0.700 Ensemble LightGBM * 0.66 + NN * 0.34 CV : 0.7053 Public Score : 0.706 Private Score : 0.702 Please sign in to reply to this topic. comment 12 Comments Hotness Pooja Chauhan Posted 2 years ago arrow_drop_up 1 more_vert Congratulations on your 13th place solution! It's impressive that you combined LightGBM and neural networks (NN) in an ensemble approach. Your use of nested cross-validation and careful splitting of training and validation data demonstrates a thoughtful approach. Well done! Akshay Vyas Posted 2 years ago arrow_drop_up 1 more_vert @takoihiraokazu Congratulations on your 13th place finish! Your strategy of training separate models for different level groups and incorporating features like categorical and numerical data counts demonstrates a thoughtful approach. emoji_people Hoang Nguyen Posted 2 years ago · 8th in this Competition arrow_drop_up 1 more_vert Congratulations on your gold medal! May I ask why you split the training and validation data this way? Cross validation: Nested cross validation Training data: Data for which the first four digits of the session_id are 2200 or less. Validation data: Data for which the first four digits of the session_id are 2201 or more. Takoi Topic Author Posted 2 years ago · 13th in this Competition arrow_drop_up 0 more_vert Thank you for your comment! I used nested cross-validation to evaluate for the following two reasons: The evaluation metric for this time fluctuates significantly based on the threshold, and just changing the threshold also significantly changed the Public Score. I wanted to create the same situation locally as the Public Score (including the ensemble of 5-fold) and check the change in CV due to changing the threshold. For instance, in situations like the one described in the link below, I believed that nested cross-validation would be more reliable than standard cross-validation. https://www.kaggle.com/competitions/predict-student-performance-from-game-play/discussion/388682 serangu Posted 2 years ago arrow_drop_up 2 more_vert @takoihiraokazu congrats on solo gold medal! ls Posted 2 years ago · 840th in this Competition arrow_drop_up 1 more_vert +1 congrats on gold AbaoJiang Posted 2 years ago · 185th in this Competition arrow_drop_up 2 more_vert Hi @takoihiraokazu , Congratulations on your solo gold medal! I have some questions about your solutions described as follows: For LightGBM, how many features are considered for each level_group model? For NN, how did you choose the sequence length of the event records? Thanks a lot for the sharing! Takoi Topic Author Posted 2 years ago · 13th in this Competition arrow_drop_up 2 more_vert Thanks! For the LightGBM model, the number of features considered for each level_group model is as follows: For level_group 0-4, I consider 1570 features. For level_group 5-12, I consider 3992 features. And for level_group 13-22, I consider 5290 features. As for the Neural Network, the sequence length of the event records was determined based on cross-validation results. Specifically, for level_group 0-4, I chose a sequence length of 250. For level_group 5-12, the sequence length is 500. And for level_group 13-22, the sequence length is 800. AbaoJiang Posted 2 years ago · 185th in this Competition arrow_drop_up 1 more_vert Thanks for the quick reply, what a robust DL-based model! One more question if you don't mind sharing, did you use early stopping to choose the best checkpoints or just let the training process converge to some satisfactory position? 4 more replies arrow_drop_down Ya Xu Posted 2 years ago · 62nd in this Competition arrow_drop_up 2 more_vert Congratulations on gold medal. I trained one model for level group 0-4 and another for level group 5-12. For these, I included features representing the target and trained a single model.For level group 13-22, I trained a distinct model for each target. May I ask the reason for this strategy? I find training model for each level_group/level has a very different cv/lb correlation. Takoi Topic Author Posted 2 years ago · 13th in this Competition arrow_drop_up 0 more_vert Thank you for your comment! Regarding level groups 0-4 and 5-12, I adopted the above method because creating a single model for them all, rather than creating a model for each target, resulted in better CV and public score. emoji_people Minh Tri Phan Posted 2 years ago · 8th in this Competition arrow_drop_up 2 more_vert Congratulations on your solo gold! Can you share your NN architecture? For me, NN alone doesn't work that well, but extracting embedding from NN and add them into Tabular features works pretty help. So I am curious how NN looks like in your case. And it would be greater that you could share the training details, some remarks, etc. Takoi Topic Author Posted 2 years ago · 13th in this Competition arrow_drop_up 8 more_vert Thank you for your comment! Also, congratulations on your gold medal! For instance, I created the following model for level group 5-12. # Numerical features used for NN num_cols_level2 = [ \"elapsed_time_log1p\" , \"elapsed_time_diff_log1p\" , \"room_coor_x\" , \"room_coor_y\" ] # Categorical features used for NN cat_cols = [ \"event_name\" , \"name\" , \"fqid\" , \"room_fqid\" , \"level\" ]\n\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min =1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\nclass PspTransformerLevel2Model(nn.Module):\n    def __init__(\n        self, dropout =0.2, input_numerical_size =4, input_name_nunique =7, name_embedding_size = 12, input_event_nunique =12, event_embedding_size = 12, input_fqid_nunique =130, fqid_embedding_size = 24, input_room_fqid_nunique =20, room_fqid_embedding_size = 12, input_level_nunique =24,level_embedding_size=12,\n        categorical_linear_size = 120,\n        numeraical_linear_size = 24,\n        model_size = 160,\n        nhead = 16, dim_feedforward =480, out_size =10):\n        super(PspTransformerLevel2Model, self).__init__()\n        self.name_embedding = nn.Embedding( num_embeddings =input_name_nunique, embedding_dim =name_embedding_size)\n        self.event_embedding = nn.Embedding( num_embeddings =input_event_nunique, embedding_dim =event_embedding_size)\n        self.fqid_embedding = nn.Embedding( num_embeddings =input_fqid_nunique, embedding_dim =fqid_embedding_size)\n        self.room_fqid_embedding = nn.Embedding( num_embeddings =input_room_fqid_nunique, embedding_dim =room_fqid_embedding_size)\n        self.level_embedding = nn.Embedding( num_embeddings =input_level_nunique, embedding_dim =level_embedding_size)\n        self.categorical_linear = nn.Sequential(\n                nn.Linear(name_embedding_size + event_embedding_size + \n                          fqid_embedding_size + room_fqid_embedding_size + \n                          level_embedding_size, categorical_linear_size),\n                nn.LayerNorm(categorical_linear_size)\n            )\n        self.numerical_linear  = nn.Sequential(\n                nn.Linear(input_numerical_size, numeraical_linear_size),\n                nn.LayerNorm(numeraical_linear_size)\n            )\n\n        self.linear1  = nn.Sequential(\n                nn.Linear(categorical_linear_size + numeraical_linear_size, \n                          model_size),\n                nn.LayerNorm(model_size),\n            )\n        self.transformer_encoder = TransformerEncoder(\n            encoder_layer = nn.TransformerEncoderLayer( d_model =model_size, nhead =nhead, dim_feedforward =dim_feedforward , dropout =dropout), num_layers =1)\n        self.gru = nn.GRU(model_size, model_size,\n                            num_layers = 1, batch_first = True , bidirectional = True )\n        self.linear_out  = nn.Sequential(\n                nn.Linear(model_size *2 , \n                          out_size)\n            )\n        self.pool = MeanPooling()\n\n    def forward(self, numerical_array, name_array,event_array,\n                fqid_array, room_fqid_array, level_array, \n                mask,mask_for_pooling):\n\n        name_embedding = self.name_embedding(name_array)\n        event_embedding = self.event_embedding(event_array)\n        fqid_embedding = self.fqid_embedding(fqid_array)\n        room_fqid_embedding = self.room_fqid_embedding(room_fqid_array)\n        level_embedding = self.level_embedding(level_array)\n        categorical_emedding = torch.cat([name_embedding,\n                                          event_embedding,\n                                          fqid_embedding, \n                                          room_fqid_embedding,\n                                          level_embedding\n                                          ], axis =2)\n        categorical_emedding = self.categorical_linear(categorical_emedding)\n        numerical_embedding = self.numerical_linear(numerical_array)\n        concat_embedding = torch.cat([categorical_emedding,\n                                      numerical_embedding], axis =2)\n        concat_embedding = self.linear1(concat_embedding)\n        concat_embedding  = concat_embedding.permute(1,0,2).contiguous()\n        output = self.transformer_encoder(concat_embedding, src_key_padding_mask =mask)\n        output = output.permute(1,0,2).contiguous()\n        output,_ = self.gru(output)\n        output = self.pool(output,mask_for_pooling)\n        output = self.linear_out(output)\n        return output content_copy The main improvements I made are listed below. Through a series of experiments, I found that the following practices were advantageous: Keeping the model size small: For instance, I kept the number of layers for both the Transformer and GRU at 1. I didn't enlarge the encoder layer or hidden size. I also maintained a relatively small embedding size for categorical features. Using fewer features: For instance, I didn't use features such as screen_coor_x, screen_coor_y, hover_duration, and text. emoji_people Minh Tri Phan Posted 2 years ago · 8th in this Competition arrow_drop_up 1 more_vert Thank you! Your practices are interesting. My NN was not super good, maybe it's because I kept the architecture and the input features complicated.",
      "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules Joseph · 14th in this Competition  · Posted 2 years ago arrow_drop_up 33 more_vert 14th Place Solution Joseph Part Thank my teammates for their efforts, I have learned a lot from them. Luckily we don't shake-down too much. Now I would like to introduce my solution to you. Modeling My modeling method is like a 'cumulative' one: using 0-4 part data to generate the train set of q1-q3, using 0-4 and 5-12 part data to generate the train set of q4-q13, using 0-4, 5-12 and 13-22 part data to generate the train set of q14-q18. Question is also a feature . It has merit that I don't need to save 'historical' data. And this one is time-saving and costs about 40min for inference. Feature Engineering There are my FE ideas: basic agg features: eclipse_time_diff sum, count and max of each group, each level, each event_name, …, each text; eclipse_time_diff sum, count under a particular room_fqid and an event_name, etc. behavior-change features: the number of room change, and the number of room change under each level; the number of text_fqid change, and the number of text_fqid change under each level, etc. In this picture, we can see a room change behavior, we calculate the change times and average to characterize one's ability to understand and reason. Some of them have pretty high feature importance. Meta features: Besides basic groupby feature engineering, I add the meta feature for 5-12 and 13-22 groups. There are two way to use them: each question's predict_proba as a feature, 5-12's model includes features q1_proba , q2_proba , and q3_proba , 13-22's model includes features q1_proba , q2_proba , … q13_proba . mean of all question in one group as a feature,  for instance, 5-12's model includes a feature mean_of_q1-q3_proba , 13-22's model includes features mean_of_q1-q3_proba , mean_of_q4-q13_proba . Models For my part, I use 9 models for my ensemble. They are 4 xgboost, 1 lightgbm, 2 dart, 2 catboost. The private-best single model is a dart, which achieved Public 0.704 and Private 0.704 . The public-best model is a xgboost, which achieved Public 0.705 and Private 0.698 . My dart notebook Game-Play-LGBDart[INFER] Private LB 0.704 The difficulty I think the most difficult part of this comp is to establish CV and choose the threshold and the submissions. As you can see, my dart model and xgboost model in the same CV strategy vary wildly. It's beyond my expectation. I even have no confidence to give my dart models a bigger weight. I believe many teams didn't choose their best results. Finally, I would like to pay tribute to all kagglers who share their ideas. See you next game. Please sign in to reply to this topic. comment 22 Comments Hotness serguei.ivanov Posted 2 years ago arrow_drop_up 1 more_vert @takanashihumbert congrats with the achievementl! ls Posted 2 years ago · 840th in this Competition arrow_drop_up 0 more_vert thank you and congrats on gold Akshay Vyas Posted 2 years ago arrow_drop_up 1 more_vert @takanashihumbert Congratulations on your 14th place finish! Your approach of using a cumulative modeling method for different question sets shows creativity and efficiency. The ensemble of different models further enhances your solution. minhtu.mt.mt Posted 2 years ago · 1878th in this Competition arrow_drop_up 1 more_vert Congrat on your gold medal. Did your team use the external data (raw data) from the fielddaylab site? I think that external data could change this game but it turns out you do not need it to get gold place Joseph Topic Author Posted 2 years ago · 14th in this Competition arrow_drop_up 1 more_vert The external data, as Adam mentions above, helps my local cv. The F1-score improved by nearly 0.001, but public LB didn't change. What confuses me the most is that xgboost models are far lower than dart on private LB. minhtu.mt.mt Posted 2 years ago · 1878th in this Competition arrow_drop_up 0 more_vert Thank you for your information. I did crawl and parse the external data and get the same result as you (slightly increase CV, LB does not change). Thought something wrong with my pipeline Chris Deotte Posted 2 years ago · 88th in this Competition arrow_drop_up 2 more_vert Congratulations Joseph and team! Well done achieving 14th place Gold !! Joseph Topic Author Posted 2 years ago · 14th in this Competition arrow_drop_up 2 more_vert Thank you, Chris. I have learned a lot from you in many competitions! ADAM. Posted 2 years ago · 14th in this Competition arrow_drop_up 2 more_vert Very very very glad to team with you guys !! @takanashihumbert @zui0711 @max2020 By the way, I want to share some thoughts on leaked data here. From this data, we can get about 7000+ extra session_id with complete logs(i.e. the user answers all the 18 question). By using this data, private score and local score can boost by 0.001. But it didn't work on the public LB. It really confused me during this competition. I tried many ways to use this extra data and all failed in public score. It's glad to see it work on private data although we didn't select those subs with private data. We have almost 20 subs can get 0.703 in private score but we didn't select it. Next time I would trust more on local cv😂 Joseph Topic Author Posted 2 years ago · 14th in this Competition arrow_drop_up 1 more_vert It's also my pleasure, Professor A. emoji_people Hoang Nguyen Posted 2 years ago · 8th in this Competition arrow_drop_up 1 more_vert On the leak data and private score, same here. I spent weeks pre-processing those external data trying to make it align with original training data only to receive public LB < CV. We didn't choose the submissions with external data, and then it turns out the 7000 sessions could have pumped us to money prize ranks. What a roller coaster of feelings. Hey You!! Posted 2 years ago arrow_drop_up 4 more_vert we also got extra 7000 sessions, but the lb score drop and didn't keep going emoji_people Hoang Nguyen Posted 2 years ago · 8th in this Competition arrow_drop_up 2 more_vert I observed the same thing. After many validation rounds, I believed my preprocessing had aligned with the original training data's preprocessing, and started to suspect a distribution shift between test set and train set, and potentially huge shake-ups/downs. Fortunately we were the lucky ones to get pumped up. emoji_people Hoang Nguyen Posted 2 years ago · 8th in this Competition arrow_drop_up 2 more_vert Btw congratulations on your 3rd gold medal in a row! You're well on your track to GM title! ADAM. Posted 2 years ago · 14th in this Competition arrow_drop_up 2 more_vert haha. Thanks. Still long way to get a solo medal.😂 Pooja Chauhan Posted 2 years ago arrow_drop_up 0 more_vert Congratulations on your 14th place solution! It's impressive how you employed cumulative modeling and various feature engineering techniques to create a successful solution. Ensembling 9 models, including XGBoost, LightGBM, Dart, and Catboost, shows your dedication to finding the best combination. The challenge of establishing CV, choosing thresholds, and making submissions was indeed difficult. Congratulations once again, and best of luck in future competitions! JInuk Haggs Posted 2 years ago arrow_drop_up 0 more_vert Congratulations on your achievement Gaurav Rawat Posted 2 years ago arrow_drop_up 0 more_vert Congrats 👏 we had used exactly same meta features 😊 just final selection faltered Joseph Topic Author Posted 2 years ago · 14th in this Competition arrow_drop_up 0 more_vert Thanks! We indeed need some luck in this comp. serangu Posted 2 years ago arrow_drop_up 0 more_vert @takanashihumbert congrats with gold medal! Joseph Topic Author Posted 2 years ago · 14th in this Competition arrow_drop_up 0 more_vert Thanks @serangu ! emoji_people Priyanshu Chaudhary Posted 2 years ago · 8th in this Competition arrow_drop_up 0 more_vert Congratulations on the 14th place finish and also for posting such a clean solution. I have a couple of questions regarding your approach if you don't mind: Could you please share the CV score of the LGBM dart and CatBoost models? I'm curious to know which parameters had the most significant impact on the performance of these two models (e.g., colsample_by_tree, etc.). Joseph Topic Author Posted 2 years ago · 14th in this Competition arrow_drop_up 0 more_vert dart cv: one is 0.70199, the other is 0.70203 catboost cv: one is 0.70122, the other is 0.70095 The second question is difficult for me to answer; because I didn't tweak the parameters too much.🤕 Hey You!! Posted 2 years ago arrow_drop_up 0 more_vert Congratulations on the gold medal our ensemble brings very low Private score, and our best submission is a single model with meta features Public 0.702 and Private 0.704 . did your team have success in feature selection? Joseph Topic Author Posted 2 years ago · 14th in this Competition arrow_drop_up 0 more_vert Thanks! And sadly I didn't do any feature selection.🤕 ADAM. Posted 2 years ago · 14th in this Competition arrow_drop_up 0 more_vert I did some feature selection by holdout cv in very early stage. Because of several reruns, I cannot compare the effects of feature selection on private score. But for public lb, they are almost same and feature selection can save a lot of time for training and inference.",
      "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules konumaru · 43rd in this Competition  · Posted 2 years ago arrow_drop_up 13 more_vert 43th Place Solution Solution Create features for each level_group. In addition, using the previous level_group features. LGBM and XGB model for each level. Optimize hyperparameters for each level. (Only XGB) I think the amount of features is almost the same as what is in the public. Not work for me Catboost model level_group probability as feature fo stacking model. sample weight for each level. optimize threshold of f1-score for each level. As a feature of gbdt, using event seqence vectorize with w2v. Not try yet Ensenble knoledge tracing model with transformer or 1dcnn Optimize hyperparameters of LGBM for each level. repo: https://github.com/konumaru/predict_student_performance Please sign in to reply to this topic. comment 4 Comments Hotness 刘锦奥 Posted 2 years ago · 52nd in this Competition arrow_drop_up 1 more_vert Congratulations on your 44th place, well done! One question: Can you tell me some of the results of your experiment? Especially the cv, public score, private score of the previous level group. Thank you! konumaru Topic Author Posted 2 years ago · 43rd in this Competition arrow_drop_up 0 more_vert Thank you. Sorry, but the results of the experiment are not neatly summarized and difficult to share. I was looking at the git commit log and SubmissionScore during the experiment, so I will only share the last few submission results so if you have time you can compare them by date, etc. commit log: https://github.com/konumaru/predict_student_performance/commits/main [update] cv=hogehoge 刘锦奥 Posted 2 years ago · 52nd in this Competition arrow_drop_up 0 more_vert Thank you🎉 Swapnil Chowdhury Posted 2 years ago arrow_drop_up 1 more_vert Congratulations for coming 44th @konumaru konumaru Topic Author Posted 2 years ago · 43rd in this Competition arrow_drop_up 0 more_vert Thank you",
      "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules tonic · 49th in this Competition  · Posted 2 years ago arrow_drop_up 10 more_vert 49th Place Solution First and foremost, I would like to express my gratitude to the hosts who made significant efforts in organizing the competition, despite numerous challenges. Congratulations to all the winners! While my model does not come close to the top performers, I am sharing my solution, hoping that it can be of use to someone, as it is relatively simple. Overview The central idea of my model revolves around the ensemble of raw log data processing using 1D-CNN and aggregated feature processing using GBDT. Individually, these models achieved CV=0.696 and Public LB=0.697, respectively. However, by ensembling them, I was able to improve the scores to CV=0.700, Public LB=0.700, and Private LB=0.700. Models The competition data provided consisted of gameplay logs, with several hundred to several thousand logs per session. Hence, I employed two modeling approaches: 1D-CNN, which directly extracts features from the temporal log sequences, and LightGBM, which utilizes aggregated features obtained through feature engineering. For 1D-CNN, I based my implementation on the public notebook by ABAOJIANG . As for feature engineering and LightGBM, I referred to the public notebook by ONELUX . I extend my gratitude to them for sharing their excellent notebooks. Regarding 1D-CNN, I used the encoder part of the public notebook as the base. After performing feature extraction using 1D-CNN, I applied the Multi-Head Attention structure before conducting temporal aggregation. I utilized five input features: numerical features such as diff(elapsed_time) and log(elapsed_time), and categorical features such as event_name + name, room_fqid, and fqid + text_fqid. For LightGBM, I added several features to the ones presented in the public notebook, resulting in inputting over 2000 features. Most of the additional features were related to text_fqid, including total time spent displaying text for each fqid and the reading speed per word. Furthermore, I combined a subset of these features (around 6) with 1D-CNN to create a new neural network model, which also had a positive effect (CV+0.001 approximately). I integrated these three models using linear regression-based stacking to generate the final predictions. However, for the simplest questions (2, 3, 18), I did not perform any modeling and predicted all of them as 1. The inference time was cutting it close at around 9 hours (528 minutes), and I was quite nervous during the final submission lol. What Didn't Work Here is a list summarizing the experiments I conducted that did not yield successful results: Building the encoder solely using Transformers Utilizing state-of-the-art time series neural networks like Patch TST or SCINet Including CatBoost in the ensemble Handling all the problems with a single model TabNet Merging additional data DAE (Denoising Autoencoder) Applying an anomaly detection model to the simplest questions (2, 3, 18) Using LightGBM with the latent features of the neural network Please sign in to reply to this topic. comment 4 Comments Hotness serangu Posted 2 years ago arrow_drop_up 1 more_vert @jinmiyashita congrats with 54 place! Thanks for sharing this! tonic Topic Author Posted 2 years ago · 49th in this Competition arrow_drop_up 0 more_vert Thank you! Swapnil Chowdhury Posted 2 years ago arrow_drop_up 1 more_vert Congratulations @jinmiyashita on coming 54th 🎉🎉. Thanks for sharing ur approach and experience tonic Topic Author Posted 2 years ago · 49th in this Competition arrow_drop_up 0 more_vert Thank you so much!"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Trace student learning from Jo Wilder online educational game This competition uses the Kaggle's time series API. Test data will be delivered in groupings that do not allow access to future data. The objective of this competition is to use time series data generated by an online educational game to determine whether players will answer questions correctly. There are three question checkpoints (level 4, level 12, and level 22), each with a number of questions. At each checkpoint, you will have access to all previous test data for that section. You have access to the training data and labels. There are 18 questions for each sessions - you are not given the answers, but are simply told whether the user for a particular session answered each question correctly. When you are ready to predict, use the sample notebook to iterate over the test data, which is split as described above and served up as Pandas dataframes. Make your predictions for each group of questions - at the end of this process a submission.csv file will have been created for you. Simply submit your notebook. The training columns are as listed below. The label rows are identified with <session_id>_<question #> . Each session will have 18 rows, representing 18 questions. For each <session_id>_<question #> , you are predicting the correct column, identifying whether you believe the user for this particular session will answer this question correctly, using only the previous information for the session. The timeseries API presents the questions and data to you in order of levels - level segments 0-4, 5-12, and 13-22 are each provided in sequence, and you will be predicting the correctness of each segment's questions as they are presented. Note that the hidden test set is roughly as large as the training set; you should expect it will take much longer to run on than the three test samples provided. This competition also includes an efficiency prize, which will be assessed at intervals throughout the competition. Check out the Efficiency Prize Evaluation page for more details. Note: this competition is aimed at producing models that are small and lightweight. We have introduced compute constraints to match - your VMs will have only 2 CPUs, 8GB of RAM, and no GPU available. You will still have a maximum of 9 hours to complete the task, but between the constraints and the efficiency prize there will be some interesting sub-problems to solve. Good luck! 8 files 4.74 GB csv, so, py Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 4.74 GB jo_wilder jo_wilder_310 sample_submission.csv test.csv train.csv train_labels.csv 8 files 46 columns ",
    "data_description": "Predict Student Performance from Game Play | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Predict Student Performance from Game Play Trace student learning from Jo Wilder online educational game Predict Student Performance from Game Play Overview Data Code Models Discussion Leaderboard Rules Overview Start Feb 6, 2023 Close Jun 29, 2023 Merger & Entry Description link keyboard_arrow_up Goal of the Competition The goal of this competition is to predict student performance during game-based learning in real-time. You'll develop a model trained on one of the largest open datasets of game logs. Your work will help advance research into knowledge-tracing methods for game-based learning. You'll be supporting developers of educational games to create more effective learning experiences for students. Context Learning is meant to be fun, which is where game-based learning comes in. This educational approach allows students to engage with educational content inside a game framework, making it enjoyable and dynamic. Although game-based learning is being used in a growing number of educational settings, there are still a limited number of open datasets available to apply data science and learning analytic principles to improve game-based learning. Most game-based learning platforms do not sufficiently make use of knowledge tracing to support individual students. Knowledge tracing methods have been developed and studied in the context of online learning environments and intelligent tutoring systems. But there has been less focus on knowledge tracing in educational games. Competition host Field Day Lab is a publicly-funded research lab at the Wisconsin Center for Educational Research. They design games for many subjects and age groups that bring contemporary research to the public, making use of the game data to understand how people learn. Field Day Lab's commitment to accessibility ensures all of its games are free and available to anyone. The lab also partners with ​​​nonprofits like The Learning Agency Lab, which is focused on developing science of learning-based tools and programs for the social good. If successful, you'll enable game developers to improve educational games and further support the educators who use these games with dashboards and analytic tools. In turn, we might see broader support for game-based learning platforms. Acknowledgments Field Day Lab and the Learning Agency Lab would like to thank the Walton Family Foundation and Schmidt Futures for making this work possible. This is a Code Competition. Refer to Code Requirements for details. Evaluation link keyboard_arrow_up Submissions will be evaluated based on their F1 score . Submission File For each session_id / question number pair in the test set, you must predict a binary label for the correct variable as described in the data page . Note that the sample_submission.csv provided for your usage also includes a grouping variable, session_level , that groups the questions by session and level. This is handled automatically by the timeseries API, so when making predictions you will not have access to this column. The timeseries API presents the questions and data to you in order of levels - level segments 0-4, 5-12, and 13-22 are each provided in sequence, and you will be predicting the correctness of each segment's questions as they are presented. The file should contain a header and have the following format: session_id ,correct 20090109393214576 _ q1 , 0 20090312143683264 _ q1 , 0 20090312331414616 _ q1 , 0 20090109393214576 _ q2 , 0 20090312143683264 _ q2 , 0 20090312331414616 _ q2 , 0 20090109393214576 _ q3 , 0 20090312143683264 _ q3 , 0 20090312331414616 _ q3 , 0 ... content_copy Timeline link keyboard_arrow_up DATES UPDATED February 6, 2023 - Start Date. June 21, 2023 - Entry Deadline. You must accept the competition rules before this date in order to compete. June 21, 2023 - Team Merger Deadline. This is the last day participants may join or merge teams. June 28, 2023 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up Leaderboard Prizes 1st Place - $ 12,000 2nd Place - $ 8,000 3rd Place - $ 5,000 4th Place - $ 5,000 Efficiency Prizes 1st Place - $ 12,000 2nd Place - $ 8,000 3rd Place - $ 5,000 Please see Efficiency Prize Evaluation for details on how the Efficiency Prize will be awarded. Winning a Leaderboard Prize does not preclude you from winning an Efficiency Prize. Code Requirements link keyboard_arrow_up Note: this competition is aimed at producing models that are small and lightweight. We have introduced compute constraints to match - your VMs will have only 2 CPUs, 8GB of RAM, and no GPU available. You will still have a maximum of 9 hours to complete the task, but between the constraints and the efficiency prize there will be some interesting sub-problems to solve. Good luck! This is a Code Competition Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= Disabled Internet access disabled Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv . The API will generate this submission file for you. Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors. Efficiency Prize Evaluation link keyboard_arrow_up Efficiency Prize We are hosting a second track that focuses on model efficiency, because highly accurate models are often computationally heavy. Such models have a stronger carbon footprint and frequently prove difficult to utilize in real-world educational contexts. We hope to use these models to help educational organizations, which have limited computational capabilities. For the Efficiency Prize, we will evaluate submissions on both runtime and predictive performance. To be eligible for an Efficiency Prize, a submission: Must be among the submissions selected by a team for the Leaderboard Prize, or else among those submissions automatically selected under the conditions described in the My Submissions tab. Must be ranked on the Private Leaderboard higher than the sample_submission.csv benchmark. Must not have a GPU enabled. The Efficiency Prize is CPU Only. All submissions meeting these conditions will be considered for the Efficiency Prize. A submission may be eligible for both the Leaderboard Prize and the Efficiency Prize. An Efficiency Prize will be awarded to eligible submissions according to how they are ranked by the following evaluation metric on the private test data. See the Prizes tab for the prize awarded to each rank. More details may be posted via discussion forum updates. Evaluation Metric We compute a submission's efficiency score by: Efficiency = 1 Benchmark − max F1 F1 + 1 32400 RuntimeSeconds where F1 is the submission's score on the main competition metric , Benchmark is the score of the benchmark sample_submission.csv , max F1 is the maximum F1 of all submissions on the Private Leaderboard, and RuntimeSeconds is the number of seconds it takes for the submission to be evaluated. The objective is to minimize the efficiency score. During the training period of the competition, you may see a leaderboard for the public test data in the following notebook, updated daily: Efficiency\nLeaderboard . After the competition ends, we will update this leaderboard with efficiency scores on the private data. During the training period, this leaderboard will show only the rank of each team, but not the complete score. Note: this competition is aimed at producing models that are small and lightweight. We have introduced compute constraints to match - your VMs will have only 2 CPUs, 8GB of RAM, and no GPU available. You will still have a maximum of 9 hours to complete the task, but between the constraints and the efficiency prize there will be some interesting sub-problems to solve. Good luck! Citation link keyboard_arrow_up David Gagnon, Maggie, Meg Benner, Perpetual Baffour, Phil Culliton, Scott Crossley, and ulrichboser. Predict Student Performance from Game Play. https://kaggle.com/competitions/predict-student-performance-from-game-play, 2023. Kaggle. Cite Competition Host The Learning Agency Lab Prizes & Awards $55,000 Awards Points & Medals Participation 19,097 Entrants 2,677 Participants 2,051 Teams 54,868 Submissions Tags Tabular Binary Classification F-Score (Macro) Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Efficiency Prize Evaluation Citation"
  },
  {
    "competition_slug": "godaddy-microbusiness-density-forecasting",
    "discussion_links": [
      "/competitions/godaddy-microbusiness-density-forecasting/discussion/395131",
      "/competitions/godaddy-microbusiness-density-forecasting/discussion/395264",
      "/competitions/godaddy-microbusiness-density-forecasting/discussion/418287",
      "/competitions/godaddy-microbusiness-density-forecasting/discussion/394821",
      "/competitions/godaddy-microbusiness-density-forecasting/discussion/417821",
      "/competitions/godaddy-microbusiness-density-forecasting/discussion/418770",
      "/competitions/godaddy-microbusiness-density-forecasting/discussion/417803",
      "/competitions/godaddy-microbusiness-density-forecasting/discussion/395011",
      "/competitions/godaddy-microbusiness-density-forecasting/discussion/418657",
      "/competitions/godaddy-microbusiness-density-forecasting/discussion/418355",
      "/competitions/godaddy-microbusiness-density-forecasting/discussion/417946"
    ],
    "discussion_texts": [
      "GoDaddy - Microbusiness Density Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. GoDaddy · Featured Prediction Competition · 2 years ago Late Submission more_horiz GoDaddy - Microbusiness Density Forecasting Forecast Next Month’s Microbusiness Density GoDaddy - Microbusiness Density Forecasting Overview Data Code Models Discussion Leaderboard Rules @kaggleqrdl · 1st in this Competition  · Posted 2 years ago arrow_drop_up 48 more_vert #1 solution - generalization with linear regression New:  I've uploaded the formal model submit file that was required to receive the prize distribution: https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/425000 Code here - https://www.kaggle.com/kaggleqrdl/first-place-code , see below for an explanation. Acknowledgements Let me first acknowledge Kaggle, the Venture Forward folks, and GoDaddy.   Not just for creating Kaggle itself, hosting the contest and providing the microbiz data - incredible feats of faith, truly -  but also for being patient with my fixation on transparency.  I am an egalitarian thru and thru, and firmly believe transparency is what gets us there. Introduction The contest was to predict microbusiness density 3,4,5 months lookahead for mar/apr/may 2023 per each of the 3135 US counties in the dataset.  Microbusiness density here is roughly equal to the # of GoDaddy internet domains registered / adult population in a county as measured by a 2 year lagging US census. Eg: if a total of 10 GoDaddy domains were registered in De Kalb, MO with 200 adult population according to the 2021 census in 2023 February, than microbusiness density for that county would increase by 10/200 or 0.05 in that month. We were given historical microbiz data for 2019 August -> 2022 December to train with, and a not-so-secret 'public LB' of data for January to validate against.  Scores on the public LB were provided by Kaggle, and we were given 5 submissions per day. Method To win this, I took a baseline prediction for the january public leaderboard based on overfitting it, and used linear regression to project into mar/apr/may 'sklearn.linear_model.LinearRegression' ,  [ 'pct_college_2021' , 'l11_uemp' , 'shiftf1_l1_active' ,]  - 4 month lookahead 'sklearn.linear_model.LinearRegression' ,  [ 'shiftf1_l1_active_logit' , 'pct_college_2019' , 'l4_active' , 'l10_lf' ] - 3 month lookahead 'sklearn.linear_model.LinearRegression' ,  [ 'l8_uemp' , 'l1_ur' , 'l11_uemp' , 'l10_ur' , 'pct_bb_2017' ] - 2 month lookahead content_copy l# here means lag # of months, shift is the actual value, just l#_ is pct_change shiftf1_l1_active_logit is whether there was a change in active over previous month (l1_active != 1) pct_college_2021 / pct_bb_2017 is from the census data the hosters shared uemp / lf / ur are from the dataset I shared here - https://www.kaggle.com/datasets/kaggleqrdl/gd2022datasets I selected these features by cross validating on the last possible window (2,3,4 month respectively) without overlap. Insights The key ideas that seemed to help: LB overfitting / probing.  I did this by using the top public LB notebook plus focusing on the 10 or so counties that had recent significant volatility in the dataset.  This was done because in non stationary time series data, the last value is your most powerful and compelling signal.  It's worth noting that due to the dec22/jan23 snafu (we were given 60% incorrect data, as measured by SMAPE), this technique was severely impaired and my lead in the public LB went from ~0.2 to ~0.08 over the top 20.  Overfitting the LB doesn't work very well when the LB has wrong data. More explanation here on this approach - https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/413109 I also explained these ideas early on in the contest here and here . The notebook has code used for SMAPE root solving and probing techniques LinearRegression - I tried practically every(*) other model and techniques, even dl/nn.  LR consistently resulted in more reliable CV scores.  Overfitting is always a concern with time series data and it's so easy to tune other models and convince yourself you found something superior. Early stopping on greedy feature selection.   This was done mostly on an intuition basis, but after running 1000s of experiments I developed a good feel for this I believe.  I noticed overfitting occurring when I did more than 4 or 5 features, plus the decreased rate of improvement was another signal that overfitting had occurred.  Certain features as well seemed to have more signal than others (eg, L10/11 employment, bb census) so I would keep going until they made it into the list but not much more. Last window CV for feature selection.  My hypothesis is that as a non stationary time series the relevant predictive features change and it's best to use the latest CV window (last few months of 2022).   This played out in the results I saw over the experiments I did, but I will say std deviation on the error term was quite high, even if the mean was superior to longer CV periods.  The risk was that so little CV time made it a bit of a gamble. Use as much original training data as possible - Generalizing through data sufficiency was a reoccurring theme in this comp, and any technique that didn't appreciate it was degraded respectively.  I used everything up to the lag window as per above.  I didn't modify any of the data (beyond removing 3 problematic counties), even left the break in 2021 alone.  I tried doing all sorts of things to clean up that data, but they just seemed to degrade results. rounding based on active/population of course was a must.  I found clipping, in the end, didn't help. *every regression model in sklearn, various arima libraries (a lot of time here, wasted I feel), hierarchical, xgb, catboost, lgbm, pytorch/dll.  I tried both manually tuning the previous as well as various auto approaches like grid search/optuna.  Lots of very exciting results, but eventually it occurred to me I was probably just overfitting. It's worth observing that my lead against the #2 and #3 spots increased significantly from the public LB versus the private LB.  I believe this was due to superior predictions for apr/may as my march results were somewhat lackluster.   My lead over the top 20 spots also increased, though it is less clear how much of the public LB influenced their models. Ideas that didn't seem as important I also tried a lot of different datasets (including all the ones I shared publicly) up to and including google keyword trends for 'godaddy'.  :)  The above worked best when combined with early stopping.  There would be sparks of good CV scores when using other features, but adding them frequently felt like overfitting. I briefly looked into using zone files and whois databases, but these were not openly available so of course I didn't use them. Crawling/Scraping would have in theory been public data, but I had committed to myself not to use any data that I didn't share openly, so I figured it wouldn't be worth the effort required. Everything I used I shared in the dataset I made publically available at the beginning of the comp. I did a lot of initial work around per county models, work I eventually decided was wasted.  There was barely enough data to validate the models and features above.  There might be a way of segregate the counties, the question is - Would you have to segregate your training data as well? Would the increased complexity be worth what might potentially be only incremental improvements?   Hard to say, especially since observing incremental improvements is mostly impossible when your CV scores are so volatile. Future directions One thing I spent time looking at was correlation between the counties.  There is an argument to be made I think that different counties may follow similar trends at potential lag times.  There might be more to do here, but it would need to be done globally rather than individually, and your models would require appropriate data sufficiency. Another feature I didn't plumb carefully enough was 1/2/3/etc annual renewal on GoDaddy domains, an idea I got by chatting with some domain resellers recently on a reseller domain community forum.  I suspect they would provide a well of ideas around this that could be leveraged more. Final thoughts As I mentioned above, and others have mentioned, the data was non stationary except for some global linear growth coefficients.  It's also been said that the smaller counties added a lot of noise, but I believe it was the right choice to try to see if we could do something there.  There is a paucity of stats and analysis around more rural areas and emergent issues are hard to detect because of it. Another way of looking at the data might be volatility instead of direction.  The wild fluctuations in active domains could just be due to data errors but they could also be due to underlying social and economic factors. In the latter case, we need to consider: could the volatility be an opportunity, like the first sparks of a fire? Could these counties benefit from more targeted and opportune investment? If volatile counties have increased potential, what are the triggers for volatility?  These are questions and experiments (signal + intervention) worth considering If the VF folks would like to contact me (heh), feel free to reach out.  As you may have noticed, I could talk about this stuff all day long with folks who are equally interested. .. There was a bug in the team update that didn't let me change the LB pointer which I had originally pointed to the message below, so you're stuck with both.  I originally pointed it here as I was more interested in what other folks did as I wasn't really that optimistic about my simplistic approach above.  In retrospect, I probably should have trusted the process of elimination that I went through. previous thread title: What is your mean active change over december for mar/apr/may? Edit - to make clear up front, you can share your results from your submission by just executing this code.  It uses microbusiness density and not active, but that should be fine for comparison. dfc = pd.read_csv( \"submission.csv\" )\nrt = pd.read_csv( \"revealed_test.csv\" ).set_index( \"row_id\" )\ndfc[ 'first_day_of_month' ] = dfc[ 'row_id' ].apply( lambda x:x[- 10 :])\ndfc[ 'cfips' ] = dfc[ 'row_id' ].apply( lambda x: int (x[:x.index( \"_\" )]))\ndfc = dfc.reset_index().set_index( \"row_id\" )\ndfc[ 'mbd_chg' ] = dfc.apply( lambda r:(r[ 'microbusiness_density' ] - rt.loc[ f\" {r[ 'cfips' ]} _2022-12-01\" , 'microbusiness_density' ])/rt.loc[ f\" {r[ 'cfips' ]} _2022-12-01\" , 'microbusiness_density' ], axis = 1 )\ndisplay(dfc.groupby( \"first_day_of_month\" ).mean()[ 'mbd_chg' ]) content_copy …. It's going to be awhile before we get final results, and I think May results might play a large role in determining the eventual winners.  While we wait, it might fun to share what we actually predicted.  We could compare our actual results, which would be good and I've uploaded my two here -  but for those who don't want to and perhaps simpler might just be sharing mean % change over active. Here's the code I used.  It requires having accurate values for december in your dataframe and also have calculated 'active' as one of your columns. If you don't want to use active, you can also share microbusiness density as well, but I found it less interesting because of the pop shift.  There were some outliers in the pop shift that made it harder to ensure that nothing went awry, so I liked having active available.   I've shared both below. You can calculate cfips/first day with this code: dfc[ 'first_day_of_month' ] = dfc[ 'row_id' ].apply( lambda x:x[- 10 :])\ndfc[ 'cfips' ] = dfc[ 'row_id' ].apply( lambda x: int (x[:x.index( \"_\" )])) content_copy Calculate active_chg values with this code: dfc = dfc.reset_index().set_index( \"row_id\" )\ndfc[ 'active_chg' ] = dfc.apply( lambda r:(r[ 'active' ] - dfc.loc[ f\" {r[ 'cfips' ]} _2022-12-01\" , 'active' ])/dfc.loc[ f\" {r[ 'cfips' ]} _2022-12-01\" , 'active' ], axis = 1 )\ndfc.groupby( \"first_day_of_month\" ).mean()[ 'active_chg' ] content_copy This is my seasonal model first_day_of_month 2022 - 11 -01   - 0.00635502 2022 - 12 -01 0.00000000 2023 -01-01 0.00328686 2023 -02-01 0.00328686 2023 -03-01 0.01505438 2023 -04-01 0.02265933 2023 -05-01 0.02476927 Name: active_chg, dtype: float64 content_copy This is my long trend first_day_of_month 2022 - 11 -01   - 0.00635502 2022 - 12 -01 0.00000000 2023 -01-01 0.00328686 2023 -02-01 0.00328686 2023 -03-01 0.00881651 2023 -04-01 0.01215006 2023 -05-01 0.01580625 Name: active_chg, dtype: float64 content_copy Using mbd code if you didn't have an active column: dfc = dfc.reset_index().set_index( \"row_id\" )\ndfc[ 'mbd_chg' ] = dfc.apply( lambda r:(r[ 'microbusiness_density' ] - dfc.loc[ f\" {r[ 'cfips' ]} _2022-12-01\" , 'microbusiness_density' ])/dfc.loc[ f\" {r[ 'cfips' ]} _2022-12-01\" , 'microbusiness_density' ], axis = 1 )\ndfc.groupby( \"first_day_of_month\" ).mean()[ 'mbd_chg' ] content_copy Seasonal sub first_day_of_month 2022 - 11 -01   - 0.00635502 2022 - 12 -01 0.00000000 2023 -01-01 0.01208731 2023 -02-01 0.01208731 2023 -03-01 0.02388208 2023 -04-01 0.03161783 2023 -05-01 0.03375395 Name: mbd_chg, dtype: float64 content_copy Long trend sub first_day_of_month 2022 - 11 -01   - 0.00635502 2022 - 12 -01 0.00000000 2023 -01-01 0.01208731 2023 -02-01 0.01208731 2023 -03-01 0.01764007 2023 -04-01 0.02099663 2023 -05-01 0.02467693 Name: mbd_chg, dtype: float64 content_copy Note for february I just used the january values. Sharing january isn't really that helpful of course, I just do it for fun here. Please sign in to reply to this topic. comment 28 Comments Hotness Chris Deotte Posted 2 years ago · 3rd in this Competition arrow_drop_up 3 more_vert Here are the 3 history sequences for \"Dec, Jan, Feb, Mar, Apr, May\" in the train data. After we adjust for census population changes. It appears that May decreases in 2022 and 2021. And May increases in 2020. What will May do in 2023?? Iurii Uspangaliev Posted 2 years ago · 7th in this Competition arrow_drop_up 1 more_vert In 2020 it was short term big drop and then huge economic recovery from COVID, so imo it was not general trend, but anomaly. I didn't apply and seasonality/trend , just used one value for all 3 month. Values from two models: @kaggleqrdl Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 3 more_vert For folks looking for a description of my model, I posted it here - https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/394810#2182222 However, my experience was similar to most folks - simple linear models were best, perhaps informed a bit by economic/census features, and there wasn't much to write about.  The CV studies I did showed that using last split CV was frequently superior (though more volatile for sure) to using more extensive validation. tarick.morty Posted 2 years ago · 721st in this Competition arrow_drop_up 1 more_vert Hi @kaggleqrdl since you worked on the trend and seasonal model, any reason why you went for a far more aggressive growth? Do you feel that we will get to see such a significant uptick in the coming months? From your mbd_chg data above, it seems your month over month growth is a factor of ~1.0048x (long trend sub). According to Chris calculations here , the m-o-m growth observed is around ~1.0032x . @kaggleqrdl Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Well, I let it run a little wild because of the layoffs in the tech sector.  I sort of felt those folks might try to start up microbiz.   Historically the tech sector has seen full employment, but lately less so, so I think using historical values we might be seeing some underestimations. It's all guesses and fun though.  I suspect we'll see a lot of new folk getting gold here SireeshLimbu Posted 2 years ago · 743rd in this Competition arrow_drop_up 0 more_vert I also thought the same last minute haha inspired from @uspangaliev 's post here Shiner Posted 2 years ago · 297th in this Competition arrow_drop_up 1 more_vert Share my 2 submissions: first_day_of_month 2023 - 03 - 01 0 . 010246 2023 - 04 - 01 0 . 012392 2023 - 05 - 01 0 . 017704 content_copy first_day_of_month 2023 - 03 - 01 0 . 009916 2023 - 04 - 01 0 . 011304 2023 - 05 - 01 0 . 015852 content_copy SireeshLimbu Posted 2 years ago · 743rd in this Competition arrow_drop_up 1 more_vert Hi! Nice that you came up with this! Fun! Following are mine: The top one is with some multipliers for Mar, Apr and May along with a small functionality for increasing/decreasing trend for the last 5 months. The bottom one is the plain models with LastValue Blacklisting. tarick.morty Posted 2 years ago · 721st in this Competition arrow_drop_up 1 more_vert mbd_chg with respect to December: first_day_of_month 2022 - 11 - 01 - 0 . 006355 2022 - 12 - 01 0 . 000000 2023 - 01 - 01 0 . 008735 2023 - 02 - 01 0 . 007559 2023 - 03 - 01 0 . 011393 2023 - 04 - 01 0 . 013531 2023 - 05 - 01 0 . 014565 2023 - 06 - 01 - 0 . 537519 Name : mbd_chg, dtype: float64 content_copy CPMP Posted 2 years ago · 490th in this Competition arrow_drop_up 1 more_vert Using @cdeotte mbd from his last value submission and your code, I get this. Jan, Feb, and June predictions are from a single model while the others are from my blend.  I did not tune January, February and June. first_day_of_month 2022 - 11 -01 0.000000 2022 - 12 -01 0.000000 2023 -01-01 0.001225 2023 -02-01 0.003946 2023 -03-01 0.009043 2023 -04-01 0.011110 2023 -05-01 0.010103 2023 -06-01 0.010566 Name: mbd_chg, dtype: float64 content_copy When I evaluated my models I used the median and not the  mean, as the mean is dominated by large counties, while score is dominated by the more numerous small counties. Median is: first_day_of_month 2022 - 11 -01 0.000000 2022 - 12 -01 0.000000 2023 -01-01 0.001539 2023 -02-01 0.003431 2023 -03-01 0.007743 2023 -04-01 0.009492 2023 -05-01 0.008602 2023 -06-01 0.009322 Name: mbd_chg, dtype: float64 content_copy chen Posted 2 years ago · 3135th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for sharing your thoughts!! @kaggleqrdl Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Thanks Chen.  I was hoping some folks might share their results, but so far everyone seems a bit shy.  Ah well. CPMP Posted 2 years ago · 490th in this Competition arrow_drop_up 1 more_vert Not shy, but it requires some coding for me for instance. And there is little incentive to do so.  I will try to find time. 6 more replies arrow_drop_down Luca Sharp Posted 2 years ago · 2666th in this Competition arrow_drop_up 1 more_vert Hi! thanks for sharing your thoughts and good luck 🎉 @kaggleqrdl Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Thanks Luca, good luck to you as well. Shiner Posted 2 years ago · 297th in this Competition arrow_drop_up 1 more_vert An interesting question, I just checked your submission files. Why you omit the big changes of some counties? Any reason, I feel a little confusing, because it seems a little bit dangerous. For instance: train.loc[cfips==39127, time==2022-12-01]: microbusiness_density=2.856936 your_sub.loc[cfips==39127, time==[03,04,05]]: microbusiness_density=1.293628,1.297313,1.300999 。 Do you use moving average? Or just assume they will drop back? @kaggleqrdl Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert I'll answer if you share your averages ;p @judith007 Krisztián Boros Posted 2 years ago · 2502nd in this Competition arrow_drop_up 1 more_vert Wow, congrats! @kaggleqrdl Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert Just to be clear, the public leaderboard here is nonsense as it frequently is in most Kaggle contests.  I enjoy overfitting them to show why that is the case.  That said, there may be some advantage to leveraging january last values as I tried to do, but that remains to be seen. There will very likely be fairly massive shakeup (shake down) here and I fully expect to be among the latter group. Nic Vicol Posted 2 years ago · 82nd in this Competition arrow_drop_up 2 more_vert Given that the naive model was hard to beat, I was conservative about growth and damped further horizons. These are my average rates for active with respect to the actuals as of 2022-12-01. ┌────────────┬──────────┐\n│ date       ┆ mean     │\n╞════════════╪══════════╡\n│ 2023-01-01 ┆ 0.003256 │\n│ 2023-02-01 ┆ 0.005212 │\n│ 2023-03-01 ┆ 0.006449 │\n│ 2023-04-01 ┆ 0.007289 │\n│ 2023-05-01 ┆ 0.00794 │\n│ 2023-06-01 ┆ 0.008466 │\n└────────────┴──────────┘ content_copy @kaggleqrdl Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Thanks Nic!  Good luck on your results. CoreyJamesLevinson Posted 2 years ago · 97th in this Competition arrow_drop_up 0 more_vert My results: @kaggleqrdl Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert :(  Not very kaggly, I think. Did you execute this code?  Getting the mbd chg over data from revealed test should be fairly straightforward. dfc = pd.read_csv( \"submission.csv\" )\nrt = pd.read_csv( \"revealed_test.csv\" ).set_index( \"row_id\" )\ndfc[ 'first_day_of_month' ] = dfc[ 'row_id' ].apply( lambda x:x[- 10 :])\ndfc[ 'cfips' ] = dfc[ 'row_id' ].apply( lambda x: int (x[:x.index( \"_\" )]))\ndfc = dfc.reset_index().set_index( \"row_id\" )\ndfc[ 'mbd_chg' ] = dfc.apply( lambda r:(r[ 'microbusiness_density' ] - rt.loc[ f\" {r[ 'cfips' ]} _2022-12-01\" , 'microbusiness_density' ])/rt.loc[ f\" {r[ 'cfips' ]} _2022-12-01\" , 'microbusiness_density' ], axis = 1 )\ndisplay(dfc.groupby( \"first_day_of_month\" ).mean()[ 'mbd_chg' ]) content_copy I admit to being a bit surprised that most folks hadn't already done this sort of EDA.   The contest will likely be won by whoever called the macro trend correct, which isn't always in a straight line. This comment has been deleted. 5 more replies arrow_drop_down Vitaly Kudelya Posted 2 years ago · 4th in this Competition arrow_drop_up 0 more_vert My Positive scenario submission 2023 -01-01 0.012127 2023 -02-01 0.014700 2023 -03-01 0.017712 2023 -04-01 0.020500 2023 -05-01 0.018016 content_copy My Negative scenario submission 2023 -01-01 0.012127 2023 -02-01 0.014171 2023 -03-01 0.016220 2023 -04-01 0.018874 2023 -05-01 0.015608 content_copy SireeshLimbu Posted 2 years ago · 743rd in this Competition arrow_drop_up 1 more_vert Ah. There's the famous May drop. Looks great! Good luck! CPMP Posted 2 years ago · 490th in this Competition arrow_drop_up 0 more_vert I have a drop for May too, see my numbers. It surprises me. Why would the drop be good? SireeshLimbu Posted 2 years ago · 743rd in this Competition arrow_drop_up 0 more_vert Hi again. I think a lot of people with good CV scores were getting this drop, I was also a bit worried, but in the end I just went with what I got (And I don't have this drop). Here is the mention of the future trends. I think it has to do with seasonality that your models could grasp on. I am not 100% sure though. Vitaly Kudelya Posted 2 years ago · 4th in this Competition arrow_drop_up 0 more_vert As I remember the drop in May happened in 2022 and 2021, may be it somehow related to tax declaration in April in USA or any other seasonal reason (or just coincidence). For me it seems that in May 2023 we will see drop for more than 50% probability ) Hugo Posted 2 years ago arrow_drop_up 0 more_vert Do you have a last_value group and a model prediction group? the result above is the average? Or you predict all cfips? @kaggleqrdl Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert I predict all cfips.  Manual blacklisting didn't seem to improve my scores, but I also had features which were similar in purpose, such as whether there was a change over some threshold in the last month(s).  Rounding on active of course helps as well. Hugo Posted 2 years ago arrow_drop_up 1 more_vert thanks. For the XGB model I built, if I only calculate the prediction part, March: 1.01157 April: 1.01702 May: 1.02149 But if average over the prediction part(60%) and the last_value part(40%), then March: ~1.007 April: ~1.009 May: ~1.011 I use the adjusted mbd. i think it is equal to use active. @kaggleqrdl Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert I pulled this from another contestants submission that they shared publicly.  I won't give any names, but I found it interesting 2023 -03-01 0.01776264 2023 -04-01 0.01917771 2023 -05-01 0.02003907 content_copy This comment has been deleted. This comment has been deleted.",
      "GoDaddy - Microbusiness Density Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. GoDaddy · Featured Prediction Competition · 2 years ago Late Submission more_horiz GoDaddy - Microbusiness Density Forecasting Forecast Next Month’s Microbusiness Density GoDaddy - Microbusiness Density Forecasting Overview Data Code Models Discussion Leaderboard Rules Daniel Phalen · 2nd in this Competition  · Posted 2 years ago arrow_drop_up 19 more_vert 2nd Place solution: The Godaddy Microbusiness Data Cleaning Challenge Interesting problem, and it was also interesting to see the solutions came up with.   I'm pretty new to these competitions.  The biggest uncertainty to me is how much probing of the hidden data is allowed, especially in a case like this where it matters a lot for time series prediction. I also though SMAPE was a weird choice of metric that didn't make sense in a business sense.  A change in a small county would have a large influence in SMAPE, where if you are trying to capture revenue streams, wouldn't you care a lot more about a change in the number of active forecasts in Los Angeles?  I can only imagine this would be useful for allocation of advertising dollars. The code: Godaddy Microbusiness Final The biggest issue - Data quality As has been pointed out by many people, SMAPE is a relative metric.  With the kind of forecast values we are looking at (~1.2-1.5 for 1 month, ~3.2 for months 3-5 together), we can look at the contribution of an individual CFIPS. Δ S M A P E , i = 200 n c o u n t i e s | F i − A i | F i + A i Which, in the case of something going from zero to non-zero can be ~0.0638 per month difference.  At the same time, a prediction for most of these models seems to be on the order of 0.5% change per month, which is far below the smallest change in active entries for many 25% of the counties. Second, there are many CFIPS where the data is terrible.  The hosts acknowledged that they had a methodology change in Jan 2021, leading to a number of jumps, and I suspect there was another change after the first month. The one which always bothered me was CFIPS 56033, Sheridan Co, WY, where there are 2.36 microbusinesses/working age person.  I can only guess there is some bug where if there is a misclassification they dump it in that CFIPS.  I also wondered if there was some fraud happening during COVID times as people chased PPP loans given the large jumps.  However, many of these issues would eventually revert, like the below: So the question I think that make or breaks this, if we identify a large jump, will it revert?  In this I believe we are helped by the gap between December and the first forecast month, March.  I also hope GoDaddy gets better in their data collection procedures, which would help.  In then end, if they want to use this in business, it would probably not be that useful to have a large machine learning error correction model for data collection issues. My solution is a mix of public leaderboard probing for individual CFIPS changes, reversion for outlier CFIPS, and a forecast of the smooth changes. Public leaderboard - still very useful information Given the equation above and as @petersorenson360 suggested, and that the leaderboard was active to 4 decimal places, you are sensitive to changes of less than 1% in the value an individual value.  Given you had about 20 days from revealed test to final submission deadline, you could probe about 100 CFIPS for final values.  It was actually easier if you made the value worse since the equation above would allow you to work out the exact number of active entries. Many large jumps would the revert, which we tried to work out how long that might take.  This will probably be the difference in the end of who wins and loses, and I think the couple month gap between the test and private data will help ensure they revert.   This was by far the top influence. Continuous Model After seeing GiBa's @titericz notebook, his data cleaning method reminded me of something used in futures algorithmic trading, called a continuous contract .  Basically, you need to take a discontinuous series of prices and make it smooth.  So for what I will call the continuous forecast, I used a data cleaning method where I looked for large jumps in the number of active entries, then did a shift to smooth them out.  So the month where there was a large active jump became zero active jump.  This smoothing method gave the best CV score of the number that I tried. I then setup the CV environment.  For the model used XGBoost, added extra indicators, and prevented peeking in the future: lagged density changes and lagged active values pct_bb, pct_college, pct_it, pct_foreign_born, median_hh_inc of the last year (Many public books used an implicit forward bias in their features by looking at 2021 census data when training on 2019 data) Labor Force and Labor force participation for the county 10 year average pct population change for the county latitude and longitude engagement, participation, and MAI_composite from the Godaddy website. The difference between the microbusiness density and the average of the neighboring counties, weighted by population. I found a population cut of about 5,000 was helpful.  Rounding the model to an integer number of active helped a bit.   A lot of these external indicators proved more helpful for the longer term forecasts, where there is some reversion to an average. Other notes: I actually found training on the 1 month forward change then feeding that prediction back into the model and recalculating all indicators gave the best CV, as opposed to directly forecasting the 3 month ahead forecast.  It turned out to be less biased. The scale variable used seemed to work in forecasting the public leaderboard, but did terribly in my CV environment. Again, noting that the last value is pretty good and I was using a roll forward model, I used the tuned public models to patch in a January forecast as an estimate for the ground truth, then rolled forward the XGBoost model from that. Please sign in to reply to this topic. comment 5 Comments 1 appreciation  comment Hotness ryan Posted 2 years ago · 112th in this Competition arrow_drop_up 1 more_vert Nice work! Given the data quality, smoothing was something I also spent quite some time trying to figure out at the beginning of this challenge. Especially given the large changepoint in the data on 2021-01-01 because of the methodology change in how GoDaddy computed the microbusiness density of counties. The smoothing method you described sounds rather similar to my method described here: Outliers, Changepoints, and Smoothing Essentially my strategy was to go from left to right and shift the data on the left anytime there was a changepoints (I computed changepoints via differencing and identifying points outside 4 standard deviations of them mean). I’de iteratively repeat this until no more changepoints were detected. I'de really like to hear more about the specifics of your smoothing algorithm and what your CV/LB were with and without smoothing. For me, my CV would increase with smoothing, but public LB score dropped. Daniel Phalen Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 0 more_vert You probably would have gotten a similar result based on your technique. For the cross validation, I know in some public notebooks there was an explicit violation of proper CV technique since the smoothing was done before the cross validation.  I think to properly setup your environment given the challenge and the data quality, all data cleaning, etc should be a part of the model itself and all cross validation should be done in a walk forward mode since it is a time series challenge. For example, the GRU models used with Group K fold would in this case use the the group CV for early stopping and validation of the training phase, but to test the overall model one would need to use the walk forward method. Not sure of your setup, so this may not affect you ryan Posted 2 years ago · 112th in this Competition arrow_drop_up 1 more_vert Thanks for your reply. Now that you mentioned this, I realize I performed smoothing before computing my CV… Whoops! I also played around with GRU's a litle bit, but did realize this look ahead bias that public notebook had (they were reporting CV scores with a SMAPE of ~1, which is obviously not representative of LB scores) and changed the code to perform a sliding window approach during training (which is your description of \"walking forward\" I presume). Best of luck in the coming months! tarick.morty Posted 2 years ago · 721st in this Competition arrow_drop_up 0 more_vert Thanks for sharing. How was your CV result month over month with and without using the Jan predictions? Daniel Phalen Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 2 more_vert For the Jan predictions, not sure I can answer that until the end of the competition.  Much of that is just a hope that the interaction of those predictions with my model will not cause some bias that I could not detect. Given my CV setup, a last value model gave these results on the original training data: Train up to and including data from  2022-01-01 and evaluate forecasts from  2022-04-01 to 2022-06-01 = 3.1956774734784967 Train up to and including data from  2022-02-01 and evaluate forecasts from  2022-05-01 to 2022-07-01 = 3.4152913611520037 Train up to and including data from  2022-03-01 and evaluate forecasts from  2022-06-01 to 2022-08-01 = 3.3317878979931765 Train up to and including data from  2022-04-01 and evaluate forecasts from  2022-07-01 to 2022-09-01 = 3.3842283789667764 Train up to and including data from  2022-05-01 and evaluate forecasts from  2022-08-01 to 2022-10-01 = 3.7841915766675602 Average SMAPE = 3.4222353376516024 And the final model I used: Train up to and including data from  2022-01-01 and evaluate forecasts from  2022-04-01 to 2022-06-01 = 2.962060956397758 Train up to and including data from  2022-02-01 and evaluate forecasts from  2022-05-01 to 2022-07-01 = 3.097867393456829 Train up to and including data from  2022-03-01 and evaluate forecasts from  2022-06-01 to 2022-08-01 = 3.0168450673149243 Train up to and including data from  2022-04-01 and evaluate forecasts from  2022-07-01 to 2022-09-01 = 2.9881429431194393 Train up to and including data from  2022-05-01 and evaluate forecasts from  2022-08-01 to 2022-10-01 = 3.3630555199321344 Average SMAPE = 3.085594376044217 Appreciation (1) liuxueyi Posted 2 years ago · 48th in this Competition arrow_drop_up 0 more_vert Very useful！Thanks a lot.",
      "GoDaddy - Microbusiness Density Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. GoDaddy · Featured Prediction Competition · 2 years ago Late Submission more_horiz GoDaddy - Microbusiness Density Forecasting Forecast Next Month’s Microbusiness Density GoDaddy - Microbusiness Density Forecasting Overview Data Code Models Discussion Leaderboard Rules Chris Deotte · 3rd in this Competition  · Posted 2 years ago arrow_drop_up 84 more_vert 3rd Place - Predict Multipliers with GRU Thank you Kaggle and GoDaddy for an exciting forecasting competition! Predict Multipliers with TensorFlow GRU Our forecasting models need to predict 3135 county microbusiness density predictions per future month. The public LB is one future month and the private LB is three future months (with 2 month gap between train and test). From Kaggle's M5 competition here we learned that predicting multipliers (between the test time period and train time period) was most important. For example in M5, we could take an average public notebook and multiply all predictions by 0.95 and win Gold medal !! Similarly in Kaggle's GoDaddy competition, a simple last value baseline multiplied by the correct January/December multiplier of 1.0045 as shown in Vitaly's notebook here achieved Gold medal public LB for many months! To predict multipliers, we convert train data into multipliers and then train a TensorFlow GRU. This simple model achieves 15th place Gold. Then if we post process the multipliers based on ratios learned from LB probing, we boost our solution to 3rd place Gold !! Original Train Data Kaggle gave us 41 historical months of data for each of 3135 USA counties. Step 1 - Adjust Train Data The definition of microbusiness density is micro businesses per 100 people over age 18 . The value of microbusiness density changes when county population changes. So the first step is to convert all microbusiness density to use the same 2021 census. We use the formula adjusted_microbusiness_density = microbusiness_density * (population / population_year_2021) content_copy Step 2 - Create 56,000 Time Series! For each county, we create 18 time series. We use 13 consecutive months to train our GRU and predict the next 5 months. (Pictured below is only 3 time series per county for illustration purposes). Afterward, we have 56,000 time series to train with! Step 3 - Use Only Top 90% Largest Counties We notice that the smallest 10% counties have nearly the same microbusiness density from month to month, so we only train our GRU using the largest 90% counties. During inference, we will use the last known value as prediction for small counties. Step 4 - Convert to Multipliers Each county has 13 months of train and 5 months of valid. We will convert the raw micro business data into ratios by replacing each value with the ratio of current month divided by previous month. Afterward we will have 12 months of train ratios and 5 months of valid ratios # CONVERT TO RATIOS for k in range( 17 ): new_data [:,k+ 1 ] = old_data[:,k+ 1 ] / old_data[:,k] content_copy Step 5 - Train GRU with GroupKFold When creating KFold we need to use GroupKFold and keep all time series for each county (from the possible 18 time series per each county) within the same fold. Otherwise CV score will be inaccurate and we cannot optimize our models hyperparameters correctly. Below is our TensorFlow GRU model which takes an input of 12 ratios and predicts 5 ratios: def build_model():\n\n    inp = tf.keras.Input(shape=(12,1)) # INPUT SHAPE IS 12\n    x = tf.keras.layers.GRU( units =8, return_sequences = True )(inp)\n    x = tf.keras.layers.GRU( units =8, return_sequences = True )(x)\n    x = tf.keras.layers.GRU( units =8, return_sequences = False )(x)\n    x = tf.keras.layers.Dense(5, activation = 'linear' )(x) # OUTPUT SHAPE IS 5\n    model = tf.keras.Model( inputs =inp, outputs =x)\n\n    opt = tf.keras.optimizers.Adam( learning_rate =1e-4)\n    loss = tf.keras.losses.MeanSquaredError()\n    model.compile( loss =loss, optimizer = opt)\n\n    return model content_copy Step 6 - Infer and Post Process We predict each county individually. To make predictions for January 2023. We begin with the last known value (of the county we are predicting) from December 2022 and multiply by the first ratio predicted by our model. To predict Feb 2023, we take our Jan 2023 prediction and multiply by the second ratio predicted by our model. To predict Mar 2023, we multiply Feb 2023 by third ratio. To predict Apr 2023, we multiply by fourth ratio. And finally to predict May 2023, we multiply by fifth ratio. We can improve our predictions by modifying the first ratio (which is the ratio of January 2023 divided by December 2022) by information from probing the public LB.  Probing informs us that the best January/December average ratio for the largest 90% counties is 1.0045 . Similarly, we can improve our predictions for small counties by probing the public LB to find ratios for different sized small counties. Post process boosts our GRU solution from 12th place Gold to 3rd place Gold !! GRU Solution Code Published Full solution code with preprocess, train, and infer is published here . More Solution Models My final two submissions were GRU with PP - 3rd place Linear Model with PP - 10th place More information about my linear model is here . Enjoy! Please sign in to reply to this topic. comment 26 Comments 4 appreciation  comments Hotness Deniz Posted 2 years ago · 704th in this Competition arrow_drop_up 1 more_vert Congrats Chris and thanks for sharing. I've also tried GRU over all dataset, rather than 90 %,  also my GRU was wider it seems. And it seems my solution is over complicated compare to yours. In my final solution, i selected another solution, so now i cannot see  final result for my GRU solution. Thanks again for sharing. By the way, why do we have to explicitly multiply the result? should not GRU find out this relation and apply? rather than multiplying, If we had used stacking (where final model is a linear regression), would that automatically add multiplying factor? Chris Deotte Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert The competition metric is SMAPE which is about ratios of error divided by mean of prediction and ground truth. When training a model, using different losses can have different effects on model accuracy with relation to competition metric. Another way to affect model accuracy without changing the loss function is to transform the targets. Using MSE loss with ratios does a better job optimizing SMAPE than trying to predict the raw microbusiness values with another loss. Also transforming targets make more targets similar. For example when using raw microbusiness values, maybe the values do not repeat themselves often, but when converting everything into ratios, the ratios do repeat themselves often which reinforce the model's learning. Pooja Chauhan Posted 2 years ago arrow_drop_up 1 more_vert Impressive 3rd place solution using TensorFlow GRU to predict multipliers in forecasting microbusiness density. Adjusted train data using population ratios. Created 56,000 time series for training. Focused on top 90% largest counties and used last known value for small counties. Trained GRU model with GroupKFold and post-processed predictions. Achieved 3rd place Gold. Well done! janedangrj Posted 2 years ago · 66th in this Competition arrow_drop_up 1 more_vert Thanks for sharing. Congratulations Chris! Yu Wu Posted 2 years ago · 11th in this Competition arrow_drop_up 1 more_vert Ty Chris, your post is always impressive and insightful Akshit Sharma Posted 2 years ago arrow_drop_up 1 more_vert Congratulations! & Thanks a lot for sharing this @cdeotte Darek Kłeczek Posted 2 years ago arrow_drop_up 1 more_vert Thanks for sharing Chris, great lesson in how to approach time series forecasting! Raheem Nasirudeen Posted 2 years ago · 2277th in this Competition arrow_drop_up 1 more_vert A big congratulations, I am so happy you win this ❤️ Serigne Posted 2 years ago arrow_drop_up 1 more_vert Congrats Chris Always coming with very insightful solutions and write-ups FL94 Posted 2 years ago · 174th in this Competition arrow_drop_up 1 more_vert Thanks for sharing, very instructive and concise :) Yijie Xu Posted 2 years ago · 321st in this Competition arrow_drop_up 1 more_vert Great explanation, and a very interesting approach - 56K time series wasn't something that came to mind at all! CoreyJamesLevinson Posted 2 years ago · 97th in this Competition arrow_drop_up 1 more_vert Great job Chris, thank you for sharing. Congrats on 3rd place! @kaggleqrdl Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert You know you want to - https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/418308 :) @kaggleqrdl Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Great job chris.  q:  Did you delete your previous leaderboard post on this?  I couldn't find it and was curious about the diff. Chris Deotte Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert No, my previous post is here Chris Deotte Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Congratulations @kaggleqrdl winning first place! TAK Posted 2 years ago · 2971st in this Competition arrow_drop_up 2 more_vert Congratulations, Chris! Thank you so much for sharing. I have learned some great eye-opening points from this. It helps guide me in looking back at my code again. Jaewook Kim Posted 2 years ago · 38th in this Competition arrow_drop_up 2 more_vert Congratulations @cdeotte and @kaggleqrdl 🎉 Thank for your kind explanation. I have some questions. How is the 1.0045 value obtained through probing used? When predicting 5 values, is it fixed as the first ratio value? Will the remaining 10% counties initially have 5 multipliers becoming 1? After probing for the remaining 10% counties, will only the first multiplier change to the explored value? Chris Deotte Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 3 more_vert How is the 1.0045 value obtained through probing used? When predicting 5 values, is it fixed as the first ratio value? After the GRU makes prediction for January, we compute the ratio of \"average January prediction\" divided by \"average December prediction\" for large counties. The result is 1.0032 . This is not the optimal ratio. So we multiply every January prediction for large counties by 1.0012 . Afterward, the the ratio of \"average January prediction\" divided by \"average December prediction\" is 1.0045 and our public LB and private LB are boosted. Regarding the small counties, each begins with multiplier of 1 which means that each small county prediction uses the December 2022 value. We then try different multipliers for different small counties and keep ones that improve LB. Basically we are finding what the small counties ground truth is for January 2023. Then our final submission uses the January values for small counties instead of December values for small counties. This boosts public and private LB. Both post processes combined improve CV and LB by about +0.05 Alexander Suen Posted 2 years ago · 297th in this Competition arrow_drop_up 0 more_vert Congrats! May I ask what is \"average December prediction\"? Is it predict by the 1st multiplier multiply Nov's MD? Abin Singh Posted 2 years ago · 346th in this Competition arrow_drop_up 0 more_vert Congratulations Chris. Thank you so much for sharing. LIXishere Posted 2 years ago · 34th in this Competition arrow_drop_up 0 more_vert hi Chris！Most people (including me) predict the rate of change (i.e. multiplier) or growth rate, while few people directly predict the density of enterprises. However, I am only because the former performs better in various verifications and results. Can there be some explanation from a theoretical perspective? Looking forward to your reply! justin1357 Posted 2 years ago arrow_drop_up 0 more_vert multiplier ,Simple but wonderful!!! Appreciation (4) ls Posted 2 years ago · 337th in this Competition arrow_drop_up 1 more_vert Congrats and thanks for sharing. MhatGPT Posted 2 years ago · 153rd in this Competition arrow_drop_up 2 more_vert Thanks for posting! dragon zhang Posted 2 years ago · 2910th in this Competition arrow_drop_up 2 more_vert thanks for sharing! This comment has been deleted.",
      "GoDaddy - Microbusiness Density Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. GoDaddy · Featured Prediction Competition · 2 years ago Late Submission more_horiz GoDaddy - Microbusiness Density Forecasting Forecast Next Month’s Microbusiness Density GoDaddy - Microbusiness Density Forecasting Overview Data Code Models Discussion Leaderboard Rules Vitaly Kudelya · 4th in this Competition  · Posted 2 years ago arrow_drop_up 19 more_vert My Forecasting Strategy Trend My cross-validation didn't show that complicated gradient boostings showed significantly better results than simple approaches. Also for some months  gradient boostings performed worse than the Last Value baseline and simple approaches showed more stable performance. The following formula shows how I estimated trend for cfips. active_quantile_group_21_last_6m_trend means the best multiplication constant (clipped from 1 to 1.006) for the last value of microbusiness_density for the next month on a previous 6 month for a group of cfips (total 21 groups by quantile of active). The same logic for state and concrete cfips group. forward_2 means the best multiplication constant for the next 2 month (clipped from 1 to 1.012). df_features_data[ 'trend' ] = (\n    df_features_data[ 'active_quantile_group_21_last_6m_trend' ] * 0.16 + df_features_data[ 'active_quantile_group_21_last_3m_trend' ] * 0.16 + df_features_data[ 'state_last_6m_trend' ] * 0.14 + df_features_data[ 'state_last_3m_trend' ] * 0.14 + df_features_data[ 'cfips_last_6m_trend' ] * 0.20 + df_features_data[ 'cfips_last_3m_trend' ] * 0.20 ) * 0.5 + ((\n    df_features_data[ 'active_quantile_group_21_last_6m_trend_forward_2' ] * 0.4 + df_features_data[ 'state_last_6m_trend_forward_2' ] * 0.4 + df_features_data[ 'cfips_last_6m_trend_forward_2' ] * 0.2 )** 0.5 ) * 0.5 content_copy Using the best public submission as a starting point for predictions Public Leaderboard data wasn't published and for time-series data it should be beneficial to use the best public submission. Select 2 Submissions. Positive and Negative It's a good idea to cover different scenarios selecting 2 submissions. I call it Positive submission (we under-forecasting trend) and negative (we over-forecasting trend). For example for Positive submission * (trend + 0.0005)**(month_number) May Seasonality I just used forecast for April * (1 - 0.0025) for Positive submission, because May showed a dropdown of values the last two years. Cfips with active < 150 My cross-validation showed that using the last value baseline for such cfips (as many public baselines do) is good only for one month forward forecast, for March, April, May i used some multiplication constants. The following plot shows that for active < 75 using the last value baseline for 3 month forecast is the best, for 75-150 it's better to use trend. Hope that random helps 🤞 Please sign in to reply to this topic. comment 3 Comments Hotness Shiner Posted 2 years ago · 297th in this Competition arrow_drop_up 0 more_vert I used a similar method , differentiate trend and somehow other information. Also I manually revised some months in my submission. revised = [(over_high_0005, -0 .005),(over_high_001, -0 .01),(over_high_002, -0 .02),(over_high_003, -0 .03),(over_high_004, -0 .04),(over_low_001,0.01),(over_low_002,0.02),(over_low_003,0.03),(over_low_004,0.04)]\nremain_last = remain_last\nremain_idx = (submission.cfips.isin(remain_last))&(submission.time.isin(['2023 -03 -01 ','2023 -04 -01 ','2023 -05 -01 '])) content_copy Kevin Morgado Posted 2 years ago · 2062nd in this Competition arrow_drop_up 0 more_vert Thanks for your insights @vitalykudelya . It is very interesting to see how the seasonality approaches affect the submission score. This comment has been deleted.",
      "GoDaddy - Microbusiness Density Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. GoDaddy · Featured Prediction Competition · 2 years ago Late Submission more_horiz GoDaddy - Microbusiness Density Forecasting Forecast Next Month’s Microbusiness Density GoDaddy - Microbusiness Density Forecasting Overview Data Code Models Discussion Leaderboard Rules 此般浅薄 · 6th in this Competition  · Posted 2 years ago arrow_drop_up 24 more_vert 6th Place Solution - Lightgbm with Target flattened Great Thanks to kaggle and Godaddy to hold this competition. Congratulations to all the winners. And many thanks go to people who created some mazing public notebooks. Here is my Solution: Validation Last 12 month data for validation. To evaluate any solution is validate, I additionally compute the number of improve months of the 12 valid month instead of simple average cv improve.Because some month have abnormal high/low raising rate: mbd(curr_month+1)/mbd(curr_month)-1 ,average cv may lead to overfit Outlier smooth Mean smoothing for 2022-06->2022-08 to handle the general sharp rise-fall phenomenon There are also other data points have the phenomenon like \"origin->Sharp rise-> Sharp fall->origin\".But I haven't found the best way to deal with them yet. Basic constant smoothing by fraction from giba Stable blacklist for those nearly unchanged cfips [28055, 13101, 13265, 31009, 31115, 31149, 38047, 38087, 48033, 48301] features： Action diff, Target shift, Target diff Target diff window Sum Std Target window Sum Quantile 0.2 Quantile 0.8 Lowwer bound： only have value when sign(Quantile 0.2)=sign(Quantile 0.8)，0 for other situations Quantile 0.8 for negative value Quantile 0.2 for positive value State cluster Enhance modeling framework construct the Enhance model frame, input 1)feature and 2) n_cross: number of future month to predict, output the raising rate of mbd in future n_cross month. In this way we can directly predict the public/private mbd by changing n_cross from 1-5 Detail structure: Enhance modeling frame features=features+n_corss n_corss=num of month to predict future mbd output target：[mbd(curr_month+n_corss)/mbd(curr_month)]**(1/n_corss)-1 Model：Xgboost Target_flatten：Train with flattened target, and do reverse transform in prediction The distribution of target variables changes with the change of active base. The lower the active base is, the higher the absolute value of the target is. Do a simulate to transform all active base into same target distribution. We assume the distribution will be flatttened after the transformation, in this way we can construct the transform formula by testing: coef=(1/(0.007*(active_series/10+105))+1) target_flatten=target_series/coef Train with flattened target, and do reverse transform in prediction Post process I noticed that the trend of public leaderboard month is abnormal higher than normal months.This phenomenon also happends on other month like 2022-07, In my opnion, such whole month trend is unpredictable, our model should not overfit it. Considering that the mbd for the public month is the basis for the subsequent private month, I post-processed the forecast results like: prediction=prediction*(1+0.001) Please sign in to reply to this topic. comment 4 Comments Hotness @kaggleqrdl Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Well done, 此般浅薄. 此般浅薄 Topic Author Posted 2 years ago · 6th in this Competition arrow_drop_up 1 more_vert Congratulations winning the 1st place! @kaggleqrdl Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Cheers.  Solo gold is a tough one!  Congrats to you as well. Any chance I can entice you to take a look at https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/discussion/418308 :)   You have an intriguing approach here CoreyJamesLevinson Posted 2 years ago · 97th in this Competition arrow_drop_up 1 more_vert coef=(1/(0.007*(active_series/10+105))+1) Why? I don't understand this part 此般浅薄 Topic Author Posted 2 years ago · 6th in this Competition arrow_drop_up 0 more_vert This formula is construct by testing without theoretical basis. Here is the procedure: get the initial target: raw['target'] = raw.groupby('cfips')['microbusiness_density'].shift(-1) raw['target'] = raw['target']/raw['microbusiness_density'] - 1 plot the target distribution for different active base.The distribution of target variables changes with the change of active base. The lower the active base is, the higher the absolute value of the target is. 3.Assume that after transforming all active base into same target distribution,the distribution will be flattened like a straight line.Then find a suitable transform formula just like this:",
      "GoDaddy - Microbusiness Density Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. GoDaddy · Featured Prediction Competition · 2 years ago Late Submission more_horiz GoDaddy - Microbusiness Density Forecasting Forecast Next Month’s Microbusiness Density GoDaddy - Microbusiness Density Forecasting Overview Data Code Models Discussion Leaderboard Rules https://www.linkedin.com/in/terence-siu/ · 10th in this Competition  · Posted 2 years ago arrow_drop_up 8 more_vert Public 531st -> Private 10th: The Complete Solution Code GoDaddy - Microbusiness Density Forecasting The solution is 3 months ago, so if there is anything missing, please let me know. The objective of the competition is to predict monthly microbusiness density in 3315 cfips (County FIPS) in the United States. The code implementation incorporates several special features and techniques to improve the accuracy of the predictions. Approach 1. Predicting 3315 Time Series This approach predicts the microbusiness density for each of the 3315 cfips individually. This allows for more granular and accurate predictions tailored to specific regions. 2. Technical Indicators The code utilizes popular technical indicators such as Exponential Moving Average (EMA) , Momentum (MOM) , and Relative Strength Index (RSI) . These indicators capture underlying trends, momentum, and market conditions, enhancing the predictive power of the model. 3. External Datasets In addition to the microbusiness density data, the model incorporates several external datasets. These datasets include information such as unemployment data , earnings , rent , DSG10 , tax rate , housing price , and population estimates . By incorporating these relevant external factors, the model can capture the influence of broader economic and demographic factors on microbusiness density. 4. Optuna for Hyperparameter Optimization Optuna is used to minimize the SMAPE in the Catboost model. 5. Catboost Model with Cross Validation The code employs the Catboost model . Also used Cross Validation (CV) to generalize well to unseen data. 6. Multiple Model Training To predict multiple future time periods, five separate models are trained. Each model is designed to predict a specific time horizon, such as t+1 month , t+2 months , and so on. 7. External Dataset Addition The code \" 6-external-datasets.ipynb \" is found on Kaggle and is provided to incorporate additional external datasets into the prediction model. Limitations Limited Historical Data : The current approach utilizes only the past three months of data to predict future microbusiness density. Exploring longer intervals of historical data could provide additional context and potentially improve the forecasting accuracy. Code Files 6-external-datasets.ipynb : add additional external datasets to the prediction model. It can be found on Kaggle and is intended to enhance the feature set. kaggle_competition_microbusiness.ipynb : contains the full model training code for microbusiness density prediction. It encompasses data preprocessing, model training using Catboost and Optuna , and evaluation using SMAPE . Github link : https://github.com/ttterence927/kaggle_competition_microbusiness/ Show Your Support If you find this code implementation valuable or interesting, please consider giving it a star on GitHub . Please sign in to reply to this topic. comment 2 Comments Hotness @kaggleqrdl Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Well done on your solo gold.  You can link to this from the leaderboard by updating the team page. https://www.kaggle.com/competitions/godaddy-microbusiness-density-forecasting/team https://www.linkedin.com/in/terence-siu/ Topic Author Posted 2 years ago · 10th in this Competition arrow_drop_up 2 more_vert Thank you :)) Learned a lot from you and all the people here. And congrats on winning 1st place!! ls Posted 2 years ago · 337th in this Competition arrow_drop_up 0 more_vert impressive and congrats",
      "GoDaddy - Microbusiness Density Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. GoDaddy · Featured Prediction Competition · 2 years ago Late Submission more_horiz GoDaddy - Microbusiness Density Forecasting Forecast Next Month’s Microbusiness Density GoDaddy - Microbusiness Density Forecasting Overview Data Code Models Discussion Leaderboard Rules Yu Wu · 11th in this Competition  · Posted 2 years ago arrow_drop_up 10 more_vert 11th Place Solution - Simplicity (Luck) Is All You Need First, my thanks to Kagglers for very informative and helpful discussions and notebooks. Also, thank you to Kaggle and GoDaddy for hosting the competition. My code is available at github , which is based on VADIM KAMAEV's public notebook. In this competition, my idea is to keep improving my CV and PB score without letting my model grow too complex. Validation (CV) I used the last 5 month data for validation. The model with the best validation score is used as one of my final submissions. Model Same with the public notebook, I used a Catboost Regressor to stack a LightGBM, an XGBoost, and a CatBoost model. Tuning a lot hyperparameters. Feature Engineering I didn't do too much novel feature engineering, only used techniques from public notebooks. But I change the hyperparameters a lot. Blending Submissions I used different hyperparameters of the model and different features (i.e., with different diff, max, mean, etc. settings) to conduct a large number of submissions. I select the best 4 (or maybe 3) submissions and use a weighted average to create a blended submission. The best (according to the public leaderboard) blended result is selected as one of my final submission. In the end, the private leaderboard score of blended submission is slightly better than the best CV model. Please sign in to reply to this topic. comment 5 Comments Hotness CoreyJamesLevinson Posted 2 years ago · 97th in this Competition arrow_drop_up 1 more_vert Congrats, well done! @kaggleqrdl Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Hey Yu, congrats on your solo gold!   Did you ever get a chance to eval  your model against the march data separately? Yu Wu Topic Author Posted 2 years ago · 11th in this Competition arrow_drop_up 1 more_vert Ty, I hope it would be a gold for me😂. Do you mean using the data from here ? I got 3 . 4473405812966584 for best single submission 3 . 4440930640476974 for blended submission content_copy Machine Lying Posted 2 years ago · 757th in this Competition arrow_drop_up 0 more_vert Congratulations! But this github link is not working…. Yu Wu Topic Author Posted 2 years ago · 11th in this Competition arrow_drop_up 1 more_vert thanks for your reminder, I added an extra . at the end of my URL 😭. Now, It should work. Machine Lying Posted 2 years ago · 757th in this Competition arrow_drop_up 0 more_vert Thanks! Very detailed steps",
      "GoDaddy - Microbusiness Density Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. GoDaddy · Featured Prediction Competition · 2 years ago Late Submission more_horiz GoDaddy - Microbusiness Density Forecasting Forecast Next Month’s Microbusiness Density GoDaddy - Microbusiness Density Forecasting Overview Data Code Models Discussion Leaderboard Rules Giba · 32nd in this Competition  · Posted 2 years ago arrow_drop_up 36 more_vert Giba RAPIDS SVR Solution For my first solution I used a global linear multiplier were I pick the coefficient optimizing SMAPE for each split month and forecast span separately. The second solution is based in SVR. You can check my notebook here: Super Fast RAPIDS SVR Once microbusiness_density can be calculated using active and county population , I used active as the target labels in my models. For validation I used last 12 months to calculate the SMAPE for each forecast range. To ease development I tracked only the gain each algorithm gives compared with last value benchmark. For example, predicting 3 months ahead with last value gives SMAPE 2.717, if my algorithm scores 2.617, then the gain will be 0.10 over the baseline. The SVR gain for each forecast period is: Forecast SMAPE Gain(12 months avg) 1 0.0171 2 0.0611 3 0.1555 4 0.2621 5 0.3524 6 0.4750 As you can see the gain is not much. But all other algorithms I tried (including GBDTs) scored worse than SVR in my approach. Other than that I wish good luck to everyone that dedicated time to this competition! 😉 obs. notebook V1 using sklearn SVR in 4607s. notebook V2 using RAPIDS SVR 136s. (34x faster 💪) Please sign in to reply to this topic. comment 6 Comments 1 appreciation  comment Hotness Siddharth Sah Posted 2 years ago arrow_drop_up 3 more_vert It's great to see your innovative approach using RAPIDS SVR, and the gains you achieved compared to the last value benchmark. The significant improvement in computational time from using RAPIDS SVR over sklearn SVR is impressive as well. Thank you for sharing your solution and notebook. Good luck on the private leaderboard, and I hope your solution continues to perform well! Chris Deotte Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Great job Giba. This is a strong SVR model with good CV score. I'm eager to see whether global linear multiplier or SVR will do better on final private LB! YaGana Sheriff-Hussaini Posted 2 years ago · 714th in this Competition arrow_drop_up 1 more_vert Thanks for sharing @titericz . Your solution is interesting as usual and will find time to look carefully later. Might have questions for you then. Good luck. CPMP Posted 2 years ago · 490th in this Competition arrow_drop_up 2 more_vert Interesting. How much better is SVR compared to your other models? Giba Topic Author Posted 2 years ago · 32nd in this Competition arrow_drop_up 2 more_vert SVR around ~20-30% better than XGB/LGB. Appreciation (1) jr3 Posted 2 years ago arrow_drop_up 1 more_vert Thanks for sharing. Good luck.",
      "GoDaddy - Microbusiness Density Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. GoDaddy · Featured Prediction Competition · 2 years ago Late Submission more_horiz GoDaddy - Microbusiness Density Forecasting Forecast Next Month’s Microbusiness Density GoDaddy - Microbusiness Density Forecasting Overview Data Code Models Discussion Leaderboard Rules therealmathisk · 35th in this Competition  · Posted 2 years ago arrow_drop_up 7 more_vert 35th Place Solution - GRU+XGB Goal of this competition was to predict monthly microbusines density domain registrations of small businesses divided by the active population for all counties in the US. The biggest difference to other timeseries competitions is in my opinion that the public leaderboard tracked the score of the one month forecast (January) while our final score would be determined on 3-5 months in the future (March, April, May). My expectation for this competition was to deal with various macroeconomic explanatory variables and maybe even include some capital markets data. As it turned out this was not the case😄. Things that did not work I spent more time than I'm willing to admit on scrolling through files from statsamerica ( www.statsamerica.org/downloads ) trying to find anything helpful to enhance the data we had at hand. Unfortunately I can't present any evidence for it's usefulness in this context. Anyway they have cool stuff ( https://www.statsamerica.org/innovation ) over there. Capital markets indicators also had little effect. The decision to register a domain is surprisingly not driven by the VIX or FOMC dots (or more precise it does not help on county level forecasting). Data Quality and Preprocessing Instead, outlier detection was the name of the game. I didn't probe the leaderboard but nevertheless spent much time comparing small counties and their erratic timeseries. Additionally, we had to deal with a structural change in the data in January 2021 and the underlying population numbers were also shifting. Therefore the target variable was adjusted for the latest census data and I removed suspect jumps for all counties in this month if they exceeded a threshold of +-7%: for o in tqdm(raw.cfips.unique()):\n        indices = (raw[ 'cfips' ]==o)\n        tmp = raw.loc[indices].copy().reset_index(drop= True )\n        var = tmp.microbusiness_density.values.copy()\n        var_pct = tmp.microbusiness_density.pct_change().clip(- 0.07 , 0.07 ) for j in range ( 40 , 0 , - 1 ): if j== 18 and (var_pct[j]== - 0.07 or var_pct[j]== 0.07 ):\n                var[j- 1 ] = var[j] else :\n                var[j- 1 ] = var[j]/( 1 +var_pct[j])\n        raw.loc[indices, 'microbusiness_density' ] = var content_copy Afterwards the timeseries was further denoised with an autoencoder: def create_autoencoder ( noise= 0.05 ):\n    i = tf.keras.Input(shape=( 24 ,))\n    encoded = tf.keras.layers.BatchNormalization()(i)\n    encoded = tf.keras.layers.GaussianNoise(noise)(encoded)\n    encoded = tf.keras.layers.Dense( 64 ,activation= 'relu' )(encoded)\n    decoded = tf.keras.layers.Dropout( 0.2 )(encoded)\n    decoded = tf.keras.layers.Dense( 24 )(decoded)\n    x = tf.keras.layers.Dense( 32 ,activation= 'relu' )(decoded)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout( 0.2 )(x)\n    x = tf.keras.layers.Dense( 24 ,activation= 'linear' )(x)\n\n    encoder = tf.keras.Model(inputs=i,outputs=x)\n    loss = tf.keras.losses.MeanSquaredError()\n    encoder. compile (optimizer=Adam( 0.001 ),loss=loss) return encoder content_copy I'm unsure if this could add any kind of leakage as it was applied before/outside of the CV fold structure. But it stabilized my CVs and had no negative impact on the public leaderboard score. Anyway I kept using it only for the GRU model and trained XGB on the 'raw' data. Models and Target Only counties with more than 150 microbusinesses were considered as input for the model. For counties below this threshold the last value was used. Building on top Chris proposal I used GRU with 24 months sequence of np.log1p(microbusiness density) input combining 3 folds (GroupKFold grouped by date) and forecasting 1, 3, 4, 5 months into the future. As a second model I used XGB derived from what GIBA shared , defining the 1, 3, 4, 5 months growth difference as a target. Most of the county level features were included here but I think they added little explanatory value. For postprocessing I clipped the XGB predictions of the percentage changes with the corresponding 10% and 90% quantile values observed after preprocessing. Both models are weighted with 50% in the final submission. Second Submission This was a simple linear model inspired by the success of 21 lines of code . It calculated the average monthly growth per county on the 21 months after the structural break in January 2021. The mean was clipped at -0.5% and +1% and for counties with less then 150 active domains this value was scaled by 0.3 (all of this was expert judgement based on very crude statistics, average growth, 5%,25%,75%,95% quantiles of growth). For counties with stale growth in November and December the last value was forecasted perpetually. This scored 4.0731 which would have been place 175 (still a silver medal with 10% of the effort). 🙀 Please sign in to reply to this topic. comment 0 Comments Hotness",
      "GoDaddy - Microbusiness Density Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. GoDaddy · Featured Prediction Competition · 2 years ago Late Submission more_horiz GoDaddy - Microbusiness Density Forecasting Forecast Next Month’s Microbusiness Density GoDaddy - Microbusiness Density Forecasting Overview Data Code Models Discussion Leaderboard Rules Hajime Tamura · 44th in this Competition  · Posted 2 years ago arrow_drop_up 8 more_vert 44th Place Solution - Simple LGBM Thank you for the competition on a very interesting topic. It was a time-series forecasting problem that predicts the true future, and was a fun theme to solve. I understood that the United States is a very large and diverse country. This is a simple LightGBM, 44th Place Solution. Solution Overview Only LightGBM with 32 features Multiplier Prediction I chose target as the multiplier between the previous month's data and the month to be predicted. Target smoothing Because there were so many outliers in the data, I smoothed the target by taking the medians of the three cases before and after the target. Model to forecast 1-6 months later I created several models to forecast 1-6 months later separately. Average of 3 months Average the multipliers for 3 months, including the months before and after the month of forecast. This is more accurate. Conversion of population from 2020 to 2021 Round(0) since it is a discrete value My Notebook I have published my notebook below. https://www.kaggle.com/code/thajime/godaady-44th-solution-lgbm Please sign in to reply to this topic. comment 0 Comments Hotness",
      "GoDaddy - Microbusiness Density Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. GoDaddy · Featured Prediction Competition · 2 years ago Late Submission more_horiz GoDaddy - Microbusiness Density Forecasting Forecast Next Month’s Microbusiness Density GoDaddy - Microbusiness Density Forecasting Overview Data Code Models Discussion Leaderboard Rules Yue Sun · 48th in this Competition  · Posted 2 years ago arrow_drop_up 11 more_vert Public 1125 - > Private 48 solution Methods that are beneficial for prediction: Outlier smoothing Coefficient adjustment + approximation active: The raw count of microbusinesses in the county The target to be predicted is population density = active / population. The population statistics change every January. The competition requires predicting the population density in 2023, which has changed in the denominator of population density. Based on the existing data, use (2023 predicted value * 2020 population) / 2021 population to estimate the coefficient of population trend change, and then correct the predicted value with this coefficient. Because the numerator part active is originally an integer, the predicted value multiplied by the population is usually a continuous value, such as 1.3, while the actual value may be 1, and rounding can help increase accuracy. The predicted value after coefficient correction should be multiplied by the 2021 population, then rounded, and then divided by the 2021 population for further correction. Optimal shift time = 12 Create lagged features for the past 12 months, if it is 24 or 6, it will make the effect worse, especially 24. The state and county fields are label encoded, plus the cfips field (equivalent to state + county), which are beneficial for the prediction results. Rate of change effect is better For predicting n + gap tasks, change the original density continuous value to be predicted to the growth value between the current value and the previous gap months. Make some lagged features for the original density continuous value, and then add some lagged features for this growth value, which improves the prediction accuracy After converting the density target value into a growth value, LightGbm and Xgboost models need to change the default objective parameter and use pseudo huber loss as the optimization indicator. ======== 对预测有增益的方法： 异常值平滑 系数调整 + 求近似值 active：The raw count of microbusinesses in the county 要预测的目标为人口密度 = active / 人口。 人口统计值每年一月会发生变化。竞赛要预测2023年的人口密度，在人口密度分母上已经发生了变化。基于已有的数据，使用（2023年预测值 * 2020年人口）/ 2021年人口 估算出人口趋势变化的系数，再将该系数修正预测值。 因为分子部分active本来是整数，预测值乘以人口通常是连续值，譬如1.3，而实际值可能是1，取整后可以帮助精度增加。系数修正后的预测值要乘以2021年人口，然后取整，再除以2021年人口，做进一步修正。 最优shift time = 12 创建过去12个月的滞后特征，如果是24或者6，都会让效果变差，尤其是24. state和county两个字段做label encoding，加上cfips（等同于state + county）字段，都对预测结果有增益。 rate of change 效果变好 1.预测n + gap的任务，将原本要预测的密度连续值， 改成当前值与上gap个月的增长值。 2.对原本对密度连续值做一些滞后特征，再加上对该增长值做滞后特征，对预测准度有提升 3.将密度目标值转换成增长值后，LightGbm和Xgboost模型需要更改默认的objective参数，使用pseudo huber loss作为优化指标. Please sign in to reply to this topic. comment 2 Comments Hotness RogerOcean Posted 2 years ago · 169th in this Competition arrow_drop_up 1 more_vert thanks for sharing, seeing Chinese is very friendly😁 Hongrui Posted 2 years ago · 183rd in this Competition arrow_drop_up 1 more_vert 感谢你的贡献，十分高兴能看见附有中文的解答"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Forecast Next Month’s Microbusiness Density Your challenge in this competition is to forecast microbusiness activity across the United States, as measured by the density of microbusinesses in US counties. Microbusinesses are often too small or too new to show up in traditional economic data sources, but microbusiness activity may be correlated with other economic indicators of general interest. As historic economic data are widely available, this is a forecasting competition. The forecasting phase public leaderboard and final private leaderboard will be determined using data gathered after the submission period closes. You will make static forecasts that can only incorporate information available before the end of the submission period. This means that while we will rescore submissions during the forecasting period we will not rerun any notebooks. A great deal of data is publicly available about counties and we have not attempted to gather it all here. You are strongly encouraged to use external data sources for features. train.csv sample_submission.csv A valid sample submission. This file will remain unchanged throughout the competition. test.csv Metadata for the submission rows. This file will remain unchanged throughout the competition. revealed_test.csv During the submission period, only the most recent month of data will be used for the public leaderboard. Any test set data older than that will be published in revealed_test.csv , closely following the usual data release cycle for the microbusiness report. We expect to publish one copy of revealed_test.csv in mid February. This file's schema will match train.csv . census_starter.csv Examples of useful columns from the Census Bureau's American Community Survey (ACS) at data.census.gov . The percentage fields were derived from the raw counts provided by the ACS. All fields have a two year lag to match what information was avaiable at the time a given microbusiness data update was published. 5 files 11.39 MB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 11.39 MB census_starter.csv revealed_test.csv sample_submission.csv test.csv train.csv 5 files 45 columns ",
    "data_description": "GoDaddy - Microbusiness Density Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. GoDaddy · Featured Prediction Competition · 2 years ago Late Submission more_horiz GoDaddy - Microbusiness Density Forecasting Forecast Next Month’s Microbusiness Density GoDaddy - Microbusiness Density Forecasting Overview Data Code Models Discussion Leaderboard Rules Overview Start Dec 16, 2022 Close Jun 17, 2023 Merger & Entry Description link keyboard_arrow_up Goal of the Competition The goal of this competition is to predict monthly microbusiness density in a given area. You will develop an accurate model trained on U.S. county-level data. Your work will help policymakers gain visibility into microbusinesses, a growing trend of very small entities. Additional information will enable new policies and programs to improve the success and impact of these smallest of businesses. Context American policy leaders strive to develop economies that are more inclusive and resilient to downturns. They're also aware that with advances in technology, entrepreneurship has never been more accessible than it is today. Whether to create a more appropriate work/life balance, to follow a passion, or due to loss of employment, studies have demonstrated that Americans increasingly choose to create businesses of their own to meet their financial goals. The challenge is that these \"microbusinesses\" are often too small or too new to show up in traditional economic data sources, making it nearly impossible for policymakers to study them. But data science could help fill in the gaps and provide insights into the factors associated these businesses. Over the past few years the Venture Forward team at GoDaddy has worked hard to produce data assets about the tens of millions of microbusinesses in the United States. Microbusinesses are generally defined as businesses with an online presence and ten or fewer employees. GoDaddy has visibility into more than 20 million of them, owned by more than 10 million entrepreneurs. We've surveyed this universe of microbusiness owners for several years and have collected a great deal of information on them that you can access via our survey data here . Current models leverage available internal and census data, use econometric approaches, and focus on understanding primary determinants. While these methods are adequate, there's potential to include additional data and using more advanced approaches to improve predictions and to better inform decision-making. Competition host GoDaddy is the world’s largest services platform for entrepreneurs around the globe. They're on a mission to empower their worldwide community of 20+ million customers—and entrepreneurs everywhere—by giving them all the help and tools they need to grow online. Your work will help better inform policymakers as they strive to make the world a better place for microbusiness entrepreneurs. This will have a real and substantial impact on communities across the country and will help our broader economy adapt to a constantly evolving world. Evaluation link keyboard_arrow_up Submissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0. Submission File For each row_id you must predict the microbusiness_density . The file should contain a header and have the following format: row_id,microbusiness_density 1001_2022-11-01,1.2 1002_2022-11-01,2.3 1003_2022-11-01,3.4 etc. The submission file will remain unchanged throughout the competition. However, the actively scored dates will be updated as new data becomes available. During the active phase of the competition only the most recent month of data will be used for the public leaderboard. Timeline link keyboard_arrow_up December 15, 2022 - Start Date. March 7, 2023 - Entry Deadline. You must accept the competition rules before this date in order to compete. March 7, 2023 - Team Merger Deadline. This is the last day participants may join or merge teams. March 14, 2023 - Final Submission Deadline. Forecasting Timeline: Starting after the final submission deadline there will be periodic updates to the leaderboard to as new data becomes available for inclusion in solution file. Updates will take place once a month. June 14, 2023 - Competition End Date - Winner's announcement. Note - your notebooks will be used to predict future data not currently included in the test or train sets. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary Prizes link keyboard_arrow_up First Prize: $20,000 Second Prize: $15,000 Third Prize: $10,000 Fourth Prize: $5,000 Fifth Prize: $5,000 Sixth Prize: $5,000 Citation link keyboard_arrow_up Addison Howard, Archit Agarwal, Ashley Chow, HCL-Rantig, Kellen J Gracey, Robert JC Brown, and Sohier Dane. GoDaddy - Microbusiness Density Forecasting. https://kaggle.com/competitions/godaddy-microbusiness-density-forecasting, 2022. Kaggle. Cite Competition Host GoDaddy Prizes & Awards $60,000 Awards Points & Medals Participation 14,495 Entrants 3,834 Participants 3,547 Teams 6,088 Submissions Tags Tabular Business SMAPE Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "learning-equality-curriculum-recommendations",
    "discussion_links": [
      "/competitions/learning-equality-curriculum-recommendations/discussion/394812",
      "/competitions/learning-equality-curriculum-recommendations/discussion/395110",
      "/competitions/learning-equality-curriculum-recommendations/discussion/394838",
      "/competitions/learning-equality-curriculum-recommendations/discussion/394984",
      "/competitions/learning-equality-curriculum-recommendations/discussion/394827",
      "/competitions/learning-equality-curriculum-recommendations/discussion/394813",
      "/competitions/learning-equality-curriculum-recommendations/discussion/395263",
      "/competitions/learning-equality-curriculum-recommendations/discussion/395190",
      "/competitions/learning-equality-curriculum-recommendations/discussion/394886",
      "/competitions/learning-equality-curriculum-recommendations/discussion/394910",
      "/competitions/learning-equality-curriculum-recommendations/discussion/394811",
      "/competitions/learning-equality-curriculum-recommendations/discussion/394955",
      "/competitions/learning-equality-curriculum-recommendations/discussion/394807",
      "/competitions/learning-equality-curriculum-recommendations/discussion/395018",
      "/competitions/learning-equality-curriculum-recommendations/discussion/394896"
    ],
    "discussion_texts": [
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules Ahmet Erdem · 1st in this Competition  · Posted 2 years ago arrow_drop_up 208 more_vert 1st Place Solution I had given a long break on Kaggle and came back with this competition because I thought it was very relevant to the app I am building, namely Epicurus. This break made me miss Kaggle and kept me motivated in this competition My actual solution and efficiency solution are very similar. So I will describe both of them at the same time. I hope I can refactor my code and share on Github soon. Pipeline: Candidate Selection (Retriever methods) -> Feature Engineering -> Lightgbm -> Postprocessing Validation Scheme: 1 fold validation All source topics and random 67% of the other topics are selected for the training set. The rest are validation topics. The contents which only match with validation topics are excluded from the training set. For evaluation, validation topics are matched with all the contents and competition metric is calculated. While training lightgbm model on the candidates, group4fold on topic_id is used on the validation set. Evaluation is done on the whole validation set afterwards. At the end of the competition, I had 0.764 validation score and 0.727 LB. While it is a big gap, improvements in my validation score were almost always correlated with LB. And I got my validation score as my Private LB score, which I didnt expect. Edit: Efficiency model got 0.718 validation, 0.688 Public LB, 0.740 Private LB and around 20 minutes CPU run-time. Topic/Content Representation Each topic is represented as a text using its title, its description and its ancestor titles up to 3 parents above in the tree. Example: 'Triangles and polygons @ Space, shape and measurement @ Form 1 @ Malawi Mathematics Syllabus | Learning outcomes: students must be able to solve problems involving angles, triangles and polygons including: types of triangles, calculate the interior and exterior angles of a triangle, different types of polygons, interior angles and sides of a convex polygon, the size and exterior angle of any convex polygon.' Each content is represented as a text using its title, its kind and its description (its text if it doesn’t have a description). Example: 'Compare multi-digit numbers | exercise | Use your place value skills to practice comparing whole numbers.' Candidate Selection TFIDF Char 4gram TFIDF sparse vectors are created for each language and matched with sparse_dot_topn, which is a package I co-authered ( https://github.com/ing-bank/sparse_dot_topn ) It works very fast and memory efficient. For each topic, top 20 matches above 1% cosine similarity are retrieved. Transformer Models I used paraphrase-multilingual-MiniLM-L12-v2 for efficiency track and ensemble of bert-base-multilingual-uncased, paraphrase-multilingual-mpnet-base-v2 (it is actually a xlm-roberta-base) and xlm-roberta-large for the actual competition. Sequence length: 64. But only the first half of the output is mean pooled for the representation vector. Last half is only fed for context. This worked the best for me. Arcface training: Training contents are used as classes. Therefore topics have multiple classes and l1-normalized target vectors. The margin starts with 0.1 and increases linearly to 0.5 at the end of 22 epochs. First 2 and last 2 epochs have significantly lower LR. Arcface class centers are initialized with content vectors extracted from pretrained models. Ensemble method: Concatenation after l2 normalization Edit: Models are re-trained with whole data for submission at the end. Top 20 matches within the same language contents are retrieved. In addition, for each topic, its closest train set topic is found and its content matches are retrieved as second degree matches. Matches from Same Title Topics For each topic, train set topics with the same title are found and their matched contents are retrieved. Matches from Same Representation Text Topics For each topic, train set topics with the same representation text are found and their matched contents are retrieved. Matches from Same Parent Topics For each topic, train set topics with the same parent are found and their matched contents are retrieved. All retrieved topic-content pairs are outer joined. Feature Engineering tfidf match score tfidf match score max by topic id tfidf match score min by topic id vector cosine distance vector cosine distance max by topic id vector cosine distance min by topic id topic title length topic description length content title length content description length content text length content same title match count content same title match count mean over topic id content same representation text match count content same representation text match count mean over topic id content same parent match count content same parent match count mean over topic id topic language topic category topic level content kind same chapter (number extracted from the text) starts same is content train content max train score topic max train score is content second degree match Lightgbm Model Hit or miss classification problem Overweight hit (minority) class Monotonic constraint and 2x feature contribution on most important feature: vector cosine distance 2 diverse lightgbms: Excluded features which will potentially have different distribution on real test set in one of the models, vector cosine distance min by topic id. Also used slightly different parameters and kfold seed. Postprocess Postprocessing was very important. Using relative probabilities (gaps with highest matches) and using different conditions for train and test set contents were the key. While matching train set contents was like a classification problem, matching test set contents was like an assignment problem. Topic-content pairs are included if they have one of the conditions below: Content has the best matching probability among other contents for the given topic. Content is among the train set contents and has above 5% probability and has less than 25% gap with the highest matching probability in the given topic. Content is among the test set contents and has less than 5% gap with the highest matching probability in the given topic. Content is among the test set contents and the topic is its best match and its total gap* is less than 55%. Code All training notebooks: https://github.com/aerdem4/curriculum-recommendations My actual inference notebook (v15 selected): https://www.kaggle.com/code/aerdem4/lecr-ensemble-v03 My best Efficiency notebook: https://www.kaggle.com/code/aerdem4/lecr-efficiency-minilm An alternative Efficiency solution: https://www.kaggle.com/code/aerdem4/lecr-efficiency-nobert Please sign in to reply to this topic. comment 75 Comments 3 appreciation  comments Hotness CPMP Posted 2 years ago arrow_drop_up 5 more_vert Congrats Ahmet! I knew you would come back to kaggle! Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert Thank you @cpmpml I couldn't stay away when the problem was very similar to the app I am building:) Giba Posted 2 years ago · 369th in this Competition arrow_drop_up 5 more_vert Big congratulations @aerdem4 ! This is a well deserved solo win. Thanks for sharing your solution. Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 3 more_vert Thank you @titericz ! Meeting you in real life helped me to stay motivated on Kaggle:) chumajin Posted 2 years ago · 12th in this Competition arrow_drop_up 5 more_vert Congratulations @aerdem4 You are too strong. I also used Arcface and LGBM, but I couldn't raise the score as much as you did. I'll try a little more based on this article! Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thanks @chumajin and congrats with your solo gold! Chris Deotte Posted 2 years ago arrow_drop_up 6 more_vert Great job @aerdem4 ! Awesome solo 1st place cash gold finish! Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thank you @cdeotte ! Trushant Kalyanpur Posted 2 years ago · 9th in this Competition arrow_drop_up 3 more_vert Congratulations on the solo gold 1st @aerdem4 ! We tried GBM as well but it wasnt as good as our 2nd stage rerankers. Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Congrats to you too! I guess there were some key features to include. Psi Posted 2 years ago · 4th in this Competition arrow_drop_up 4 more_vert Big congrats amazing job! We were very sure that your score can be achieved with a second stage LGB model but we couldn't get it working properly in the short time - for efficiency this is gold of course :) Thanks for the writeup! Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thank you @philippsinger Congrats to you guys with your very successful sprint. Your momentum scared me:) ‎ Srihari Posted 2 years ago arrow_drop_up 1 more_vert Congratulations bhai Kenneth Lo Posted 2 years ago arrow_drop_up 1 more_vert congrats, going to try our this exercise soon! reboot Posted 2 years ago · 413th in this Competition arrow_drop_up 1 more_vert Congratulations @aerdem4 ! Thank you for sharing Darek Kłeczek Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Congratulations Ahmet, what a way to come back! We were missing you on Kaggle, hope you stick around! Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thank you @thedrcat I will Kaggle time to time, especially in winters:) Maxim K. Surkov Posted 2 years ago · 86th in this Competition arrow_drop_up 0 more_vert Oh, that makes sense, approximate number of topic and content ids is 2e5, the total dimension of your embedddings is about 2k (am I right think?) in ensemble setup and it should fit in the memory limit, thanks. Priyanshu1235 Posted 2 years ago arrow_drop_up 1 more_vert congratulations ahmet!! Maxim K. Surkov Posted 2 years ago · 86th in this Competition arrow_drop_up 1 more_vert Congratz! Awesome lightweight solution! Could you please explain how did cope with the \"out of memory\" problem with your ensemble method: \"Ensemble method: Concatenation after l2 normalization\" (I tried this did not find solution without using so much memory) ? Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Thanks. It was keeping 2GB max. Did you keep them in float32 or 64? Junrui Wang Posted 2 years ago · 222nd in this Competition arrow_drop_up 1 more_vert Congratulation! dragon zhang Posted 2 years ago · 290th in this Competition arrow_drop_up 1 more_vert thanks for sharing.  A lot to learn from. Aditi Khare Posted 2 years ago arrow_drop_up 1 more_vert Congratulations AHMET ERDEM on the solo win. Keep Going !! Thank you for the writeup. Fatih Ozturk Posted 2 years ago arrow_drop_up 1 more_vert Big congrats Ahmet! A solo win suits you well! Btw I've already signed up for your app (; https://epicurusapp.com/ Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thank you very much for your support:) @fatihozturk ToYou2U Posted 2 years ago · 17th in this Competition arrow_drop_up 1 more_vert Congrats on solo 1st place! What is the recall rate of the top 20? If with second degree matches, what is the recall rate and average recall number? Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert Thanks. I need to double check the recall on top20. Ravi Shah Posted 2 years ago · 409th in this Competition arrow_drop_up 1 more_vert Congrats on first place @aerdem4 ! Fantastic solution! Thanks for the write up! Meilism Posted 2 years ago arrow_drop_up 1 more_vert Congratulation! Ori Hanegby Posted 2 years ago · 10th in this Competition arrow_drop_up 1 more_vert Congrats on the solo win and for the great writeup about your solution! Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thanks @ohanegby . Congrats with your solo gold and upcoming title! Shujun Posted 2 years ago · 40th in this Competition arrow_drop_up 1 more_vert Congrats on this incredible solution and solo win! I'm amazed Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 3 more_vert Thank you @shujun717 I just checked your profile, you seem to continue rocking when I was away. Congrats with your recent results! Tevfik Erkut Posted 2 years ago · 188th in this Competition arrow_drop_up 1 more_vert Congrats @aerdem4 !! I felt bad after not receiving any medals, so I think I should download the Epicurus app and chat with people like me. 👀 Ahmet Erdem Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thanks. I believe you will get a medal next time. You may still use Epicurus for teaming up with people:) heng Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert beautiful solution, congrats! AI Bushi Posted 2 years ago · 82nd in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for the very informative writeup. Anil Ozturk Posted 2 years ago · 31st in this Competition arrow_drop_up 1 more_vert Congrats! I've also tried GBDTs at the second stage but couldn't think of these comparison features and it performed almost the same, great job!",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules Konni · 2nd in this Competition  · Posted 2 years ago arrow_drop_up 79 more_vert 2nd Place Solution Thanks to Kaggle and the hosts of this competition, in particular to Jamie Alexandre for all of his helpful comments to all asked questions in the discussions and his great Data Exploration Notebook. Especially the code for traversing the topic tree was very helpful in the beginning. I spend quite some time during Christmas vacations on this competition and then did a longer break of over one month. So, in the end the finish was quite intense and submissions for Leaderboard and Efficiency Prize at the same time felled like participating in two different Competitions and quite to less Submissions for only one week left. Solution: Single Stage Retrieval based on Cosine Similarity My solution is based on a simple one stage approach to create embeddings for Topic and Content and calculation the cosine similarity for the retrieval task, without any further post-processing or second stage reranking. I will describe all the pieces of the puzzle to achieve high scores with such a relatively simple model. Validation Split: 10 Fold CV split based on minimizing the overlap of content relations between different folds. In this competition we have to split the topics into folds and all topics can have multiple attached content to it. Throughout this is a n x m relation of Topic x Content , it is difficult to create perfect splits that are aligned with the leaderboard. During CV-Split creation I tried to minimize the overlap of having the same content in different Folds, by creating 10 buckets and add topics to the bucket were all attached content of that topic creates the least overlap having the same attached content in different buckets. At least for fold 0 of my 10 Folds the alignment to the public leaderboard was quiet well. Whereas my Fold 2 is closer to the private leaderboard as it seems. I only used Fold 0, 1, and 2 for offline validation and results were always correlated to the leaderboard so I don’t care if they are perfectly aligned towards the score. Here is one example trained for only 32 Epochs with nearly perfect alignment of fold 0 towards public lb. Selected means here the average count of selected content per language over all topics in that fold and the F2 , Precision and Recall of that single model are displayed for all languages. -------------------------[Model: sentence-transformers/LaBSE]--------------------------\n\n---------------------------------------[Epoch: 32 ]-------------------------------------\nEpoch: 32 , Train Loss = 1 . 064 , Lr = 0.000050 ----------------------------------[margin: th 0 . 160 ]-----------------------------------\nCalculate Scores\nen  Score: 0.65911 - Precision: 0.59954 - Recall: 0 . 739 ( 2806x65939 ) - selected: 7 es  Score: 0.71289 - Precision: 0.61102 - Recall: 0 . 838 ( 1177x30844 ) - selected: 6 pt  Score: 0.78237 - Precision: 0.69215 - Recall: 0 . 862 ( 343x10435 ) - selected: 7 ar  Score: 0.51809 - Precision: 0.46233 - Recall: 0 . 663 ( 318x7418 ) - selected: 7 fr  Score: 0.59613 - Precision: 0.59448 - Recall: 0 . 650 ( 304x10682 ) - selected: 9 bg  Score: 0.68063 - Precision: 0.60765 - Recall: 0 . 751 ( 242x6050 ) - selected: 8 bn  Score: 0.15228 - Precision: 0.09667 - Recall: 0 . 211 ( 237x2513 ) - selected: 9 sw  Score: 0.69321 - Precision: 0.64327 - Recall: 0 . 763 ( 209x1447 ) - selected: 6 gu  Score: 0.76149 - Precision: 0.66632 - Recall: 0 . 834 ( 181x3677 ) - selected: 6 hi  Score: 0.63803 - Precision: 0.58325 - Recall: 0 . 744 ( 138x4042 ) - selected: 9 it  Score: 0.87791 - Precision: 0.85495 - Recall: 0 . 906 ( 73 x1300) - selected: 4 zh  Score: 0.63350 - Precision: 0.54224 - Recall: 0 . 740 ( 68 x3849) - selected: 10 mr  Score: 0.69542 - Precision: 0.57128 - Recall: 0 . 898 ( 24 x999) - selected: 12 fil Score: 0.72123 - Precision: 0.68860 - Recall: 0 . 778 ( 23 x516) - selected: 7 as  Score: 0.58904 - Precision: 0.53932 - Recall: 0 . 644 ( 13 x641) - selected: 5 my  Score: 0.71483 - Precision: 0.71825 - Recall: 0 . 842 ( 12 x206) - selected: 4 km  Score: 0.91160 - Precision: 0.88671 - Recall: 0 . 942 ( 11 x505) - selected: 5 kn  Score: 0.63651 - Precision: 0.55926 - Recall: 0 . 722 ( 9 x501) - selected: 9 te  Score: 0.86664 - Precision: 0.73492 - Recall: 0 . 968 ( 7 x285) - selected: 13 or  Score: 0.81583 - Precision: 0.69889 - Recall: 0 . 900 ( 5 x326) - selected: 11 ta  Score: 0.76419 - Precision: 0.52095 - Recall: 0 . 967 ( 5 x216) - selected: 6 ur  Score: 0.40010 - Precision: 0.31326 - Recall: 0 . 586 ( 5 x245) - selected: 17 pnb Score: 0.87594 - Precision: 0.83333 - Recall: 0 . 938 ( 4 x184) - selected: 8 ru  Score: 0.66330 - Precision: 0.63704 - Recall: 0 . 725 ( 3 x188) - selected: 12 pl  Score: 0.82159 - Precision: 0.99061 - Recall: 0 . 792 ( 3 x319) - selected: 30 swa Score: 0.08696 - Precision: 0.08696 - Recall: 0 . 087 ( 3 x495) - selected: 23 tr  Score: 0.54843 - Precision: 0.87778 - Recall: 0 . 518 ( 3 x225) - selected: 9 ---------------------------------------------------------------------------------------\nCV Score: 0.65414 - Precision: 0.58710 - Recall: 0 . 743 ---------------------------------------------------------------------------------------\nPublic Score: 0.65069 Privat Score: 0.69607 --------------------------------------------------------------------------------------- content_copy Input Data: No use of special Token for separation instead, I use the # as seperator. Topic: Title # Topic-Tree # Description The Topic Tree is reverse ordered and the same separator # is used so we end up with: Title # Parent # Grandparent # … # Description Content: Title # Description # Text (cut to 32 based on white space splitting) If for example Description is empty the model will see as input: Title # # Text Training: I used only Transformer Base Models with a max. sequence length of 96 tokens for both topic and content. The first and maybe also the most important part of the puzzle is the use of the InfoNCE Loss as symmetric contrastive loss-function. Cause we have here a n x m matching problem it is tricky when using this loss function due to high intersection of topics and content. As can be seen in the above visualization we want to have matches only on the diagonal of the similarity  matrix during loss calculation, cause when using Cross-Entropy in both directions Topic->Content and Content->Topic the labels are just: labels = torch.arange(len(logits)) But if we have two topics in the same batch that might share the same content we end up with high similarities besides the diagonal of the matrix whereas the label only says the high similarity on the diagonal are a correct match. On possibility to circumvent this problem is using label smoothing for the cross-entropy loss. But, having topics with related content in the same batch is simply noise for the model and should be avoided. The simplest way in Pytorch when using DP and not DDP is to write your own custom shuffle function and set shuffling in the data-loader to false, what means noting else than using a sequential sampler. My own custom shuffle function simply calculates before each epoch the composition of the batches and avoids sampling topics with related content in the same batch. Additionally after each epoch predictions for the whole training data are calculated to detect for each topic content that we miss and content that would be incorrectly assigned to that topic. Missing Content is stored in a list and the specific pair (topic, content) gets oversampled during shuffling. Wrong Content retrieved for a topic is more difficult to solve. Let’s say we have topic t1 with the wrong content c1 for that topic with a currently high similarity to that content. Now what we need is another pair (t2, c1) of our ground truth in the same batch, to push away c1 from t1 cause when using the InfoNCE loss all other N-1 contents in the batch are negatives for that sample. This sampling strategy is highly effective cause we increase the margin to all negative picks during the next epoch. Or at least we try, because if adding t2 would lead to a conflict based on related content (noise) it is rejected for adding in the same batch. I do this for a specific topic up to a max. number of 128 hard negatives for that sample, by a training batchsize > 768. With that shuffle/sampling strategy we end up with batches without conflicts in related content or ambiguities, so we have only a correct match on the diagonal line and lots of near but incorrect content (hard samples) in the batch. Language Switching: I translated the most common languages into each other for additional training data. But just adding this data to our training would be a disaster when using InfoNCE, cause if we translate for example an topic and content item from English into French and a quite similar topic and content already exists in the original French content we will end up again with noise during our loss calculation.  So what I did is a simple switching strategy after each epoch, shown in the next picture. So we can use the correlation.csv without having any trouble of ambiguities of translated to original content during loss calculation. Of course translation is never perfect and this alone creates some noise and further more we change the distribution of the training data. That’s the reason a switch is only used every second epoch and only between the languages en, es, pt and fr. Using language switching seems to bring a score boost of up to 0.01 – 0.02 what is not that much as expected maybe due to the noise this introduces during training. Knowledge Distillation (Efficiency Prize): Nothing special just used my pre-trained models as teacher, drop half of the transformer layers and train a second time using just the MSE-Loss. Weights of the student are initialized with the weights of the pre-trained teacher model. Distillation leads only to a slightly drop in performance when using 6 Layers, so the sweet spot for me was not dropping more layers, but of course this is always a trade of speed vs. accuracy. Quantization + JIT-Trace (Efficiency Prize): I used Torch Post Training Dynamic Quantization what is far from optimal. Unfortunately, there is no pre-installed huggingface optimum in the CPU kernel so using Intel Neural Compressor or OpenVINO needs offline installation, and if every second counts it makes not much sense to waste time with installation of additional packages. Maybe should have tested the ONNX runtime but run out of time. Unfortunately FX Graph Mode Quantization of Pytorch did not worked out for me with the huggingface models, otherwise I would have tested quantization aware training for better results. I ended up with just using Eager Mode Quantization -> Post Training Dynamic Quantization and compiling into a jit-traced model. This leads to a performance drop of around 0.01 – 0.02 on my cv-scores but increases the throughput and lowers the execution time. If using qint8 also on the Feed Forward part of the transformer on the intermediate up sample and output layer, the score drop is even higher so I ended up in only using qint8 on the attention layer. Optimal Threshold: Last but not least but the last part of the puzzle is using an dynamic threshold instead of a static one. I can not say if this would work also for other models but I use this calculation after each epoch to find the hard negatives for the next epoch and also during inference and it gives me a boost of 0.02 in Score or even higher. The basic idea is described in the following picture. The biggest advantage was, that this is much more stable than finding an optimal static threshold. Every model I trained before leads to a different optimal static threshold. With using this dynamic calculation the results was always the same and values for margin were always between [0.14 : 0.18] on different folds. Using a higher margin leads to better recall by losing precision and because of using the F2 Score, perhaps I should have tried to submit higher values as well. During training I use 0.16 as margin, for my best submission 0.18 is slightly better, but the difference is not that high. Results: Cause the contrastive training seems to be rock stable and showing absolutely no signs toward overfitting I trained all those models for 40 epochs and just used the final checkpoint. My experiments on fold 0-2 shows always, that no matter what checkpoint the difference in the last epochs to the best epoch is on the third digit, so it is save to train on the whole training data without any validation holdout. For Efficiency Prize I used an ensemble of only two models, cause a second model leads to the biggest jump in scores by only increasing the runtime to 23 minutes . For the Leaderboard I am using an ensemble of 5 models with a runtime of only 9 minutes on the P100 Instance , what is also quite fast. What did not work out: Using different margins for each language seems to lead to overfitting with worse results on the leaderboard. Cross-Encoder as second stage. As others also discovered, if the retrieval model becomes better, it is more likely that the second stage becomes useless or even leads to errors and overfitting. Using T4 instance. Don’t get me wrong, it is great that Kaggle increased for CPU Instance the RAM limit and introduced the 2xT4 option but I struggled with the RAM limitations on that Instance if using Pytorch DP so I switch back to P100 were I had not such problems. Training monolingual only on English. Even with my increased training samples due to translation of all Spanish, Portuguese and French Topics and Content into English, it does not show better results when using for example an all-mpnet-base-v2 , what is monolingual,  instead of using all data and train a multilingual model. Code: Training (GitHub) Inference: Leaderboard Prize (Kaggle) Inference: Efficiency Prize (Kaggle) Data: Translated Data Checkpoints: Checkpoints trained on all training data are available in the inference notebooks [Link] , [Link] Checkpoints trained on fold 0 for offline eval [Link] Please sign in to reply to this topic. comment 16 Comments Hotness gezi Posted 2 years ago · 26th in this Competition arrow_drop_up 5 more_vert Great work. I also found stage2 not improve much, but I did not expect stage1 alone can achive such high score with excellent trainning strategy. For DP and DDP I think your method still work using DDP with batch sampler. For anyone who intersted in the in batch neg + infonce loss method you could also refer to this one. https://github.com/nreimers/se-pytorch-xla/blob/master/train_many_data_files_v2.py Personally I think topic concat content mode still could perform better, but for stage1 model we have in batch neg trainning(see 200+ negatives per batch) for stage2 we could not use this, that made stage2 model converge much slower. Konni Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 0 more_vert Thanks to you, I will have look at. Ori Hanegby Posted 2 years ago · 10th in this Competition arrow_drop_up 5 more_vert Congrats for winning the 2nd place and for becoming a Kaggle Master! I'm working on my own note but I had a very similar approach for training the retriever model, including a similar thought process around diagonalization of the topic-content matrix. I ended up coding a custom training data building process, with painstaking logic to sample content with mutually exclusive topics for each batch. I loaded it in the data loader without shuffling and carefully choosing the batch size. I did however used Multiple Negative Ranking loss that hit a performance ceiling. At the end I added another training cycle with contrastive loss on the already well-refined training set which gave me a big boost for the retriever model, that as you described as well messed up my 2nd stage model and I had to retrain the whole thing :) What did you use for creating the translations for training? Thank you for the detailed writeup! gezi Posted 2 years ago · 26th in this Competition arrow_drop_up 1 more_vert I found directly using multiple neg ranking loss, the output of stage1 model rank well but it's score hard to use to choose thre for output. May be contrasive loss helps. Konni Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 5 more_vert Thank you @ohanegby and my congratulations to you for finishing on 10th place and also becoming a Kaggle Competitions Master 😊. I tried Multiple Negative Ranking Loss also right in the beginning of the competition, but nearly instantly switcht to InfoNCE , cause even on a single RTX 3090 my batch size was high enough to have enough negative pairs with a symmetric loss. But for sure the loss direction query->reference in our case topic->content is more important than the other direction. For translation I tested different approaches. Best offline translation with on huggingface available models are in my opinion the MarianMT models. Multilingual models were even worse . But in the end, I did a simple export in an .xlsx file where first column is a numerical ID and second column is what I want to translate and uploaded this to google translate -> document translation. The advantage is, this is much faster and generates more reliable results than MarianMT models, at least from my perspective. MAxx Yan Posted 2 years ago · 952nd in this Competition arrow_drop_up 0 more_vert @khabel Would you please explain the difference between Multiple Negative Ranking Loss and InfoNCE? Multiple Negative Ranking Loss includes the loss direction topic->content , and the infoCSE includes t opic->content and content->topic ? Thank you! Psi Posted 2 years ago · 4th in this Competition arrow_drop_up 3 more_vert Very impressive and awesome writeup. Congrats! Konni Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 2 more_vert Thank you. Your race on that challenge after finishing Player Contact Detection in the prize zone was incredible. And congratulations also on your 3rd place in the Efficiency Prize , that's awesome in such a short time. MALHOTRA Akul 56698828 Posted 2 years ago arrow_drop_up 1 more_vert Thanks so much for sharing this, so insightful!! Tariq Mahmood Posted 2 years ago arrow_drop_up 1 more_vert Dear @khabel , Thanks for sharing such a valuable post and an exemplary👍 work for the kagglers. Your compilations and elaboration have marvellous approach towards community. Adrian Dip Posted 2 years ago · 785th in this Competition arrow_drop_up 1 more_vert Thanks for the detailed writeup, Konni. Very educational. w2 Posted 2 years ago · 44th in this Competition arrow_drop_up 1 more_vert Great work and congratulations. Could you please tell your github? Can't wait to learn from your code. PilotLH Posted 2 years ago arrow_drop_up 1 more_vert Thanks for sharing, really learn a lot from these details. Also, amazing presentation! Ravi Shah Posted 2 years ago · 409th in this Competition arrow_drop_up 1 more_vert Congrats on 2nd place @khabel ! Thanks for sharing this very impressive solution! Trushant Kalyanpur Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Congratulations on the very strong solo finish @khabel and amazing work on a very elegant solution! Great work! Darek Kłeczek Posted 2 years ago · 9th in this Competition arrow_drop_up 2 more_vert This is an amazing level of insight and follow through with the implementation. I loved the optimal threshold idea. Congratulations!!! Konni Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 3 more_vert Thank you very much and congrats to your gold zone finishing and even more for becoming Kaggle Competition Grandmaster. A quantitative example when I switch from static -> dynamic thresholding: Model: 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2' Static Threshold: th=0.60 Public: 0.63509 Privat: 0.67885 Dyn. Threshold: margin=0.16 Public: 0.65236 Privat: 0.69374 So at least for my models that seems to work pretty well. chumajin Posted 2 years ago · 12th in this Competition arrow_drop_up 1 more_vert @khabel Congrats and thank you for sharing! I tried putting your dynamic threshold into my 1st stage and late submit it. cv : 0.653 → 0.670, dynamic threshold margin=0.17, public 0.695 → 0.715, private 0.732 →0.749 it's very strong! Konni Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Thank you for sharing your insights, glad to see that this works for others as well.",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules heng · 3rd in this Competition  · Posted 2 years ago arrow_drop_up 59 more_vert 3rd solution First of all, I would like to thank the organizers for hosting this high-quality competion, and my awesome teammates @xiamaozi11 @syzong @sayoulala @yzheng21 , we all worked hard for this competion. And I learned a lot from the great notebooks and discussions, basically all the methods we used are from the kaggle community. Thanks to these generous and smart kagglers! tips from hosts: https://www.kaggle.com/code/jamiealexandre/tips-and-recommendations-from-hosts text pre-processing: https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations/discussion/376873 stage1 and stage2 train and submit pipeline: https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations/discussion/373640 stage1 and stage2 modeling: https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations/discussion/381509 , https://www.kaggle.com/code/ragnar123/lecr-xlm-roberta-base-baseline Summary CV strategy Stage1: Retriever Stage2: Ranker Finding threshold Post-Processing Ensemble training pipeline CV strategy We only used 4,000 random topics which category != 'source' as hold out data. Those topics were as the validation data and never used in any training process. This simple CV strategy was unexpectedly stable. In the last month of the competition, we changed 4,000 topics to 1,000, which was still relatively consistent until we started to ensemble. Retriever We used unsupervised SIMCSE (Simple Contrastive Learning of Sentence Embeddings: https://github.com/princeton-nlp/SimCSE ) method for training retriever models. training retriever only used positive samples from correlations.csv for unsupervised simcse training random choice 100 negative samples per validation topic from same language, for validation set content text format: title [SEP] kind [SEP] description [SED] text , maxlen =  256 (string level) topic text format: title [SEP] channel [SEP] category [SEP] level [SEP] language [SEP] description [SEP] context [SEP] parent_description [SEP] children_description , maxlen = 256 (string level) simcse_unsup_loss def simcse_unsup_loss(feature_topic, feature_content) -> 'tensor': y_true = torch.arange( 0 , feature_topic.size( 0 ), device=device) sim = F.cosine_similarity(feature_topic.unsqueeze( 1 ), feature_content.unsqueeze( 0 ), dim= 2 ) sim = sim / 0 . 05 loss = F.cross_entropy(sim, y_true) loss = torch.mean(loss) return loss content_copy from: https://github.com/yangjianxin1/SimCSE/blob/master/model.py train code like: for step, (inputs_topic, inputs_content, labels) in enumerate (train_loader):\n        inputs_topic = collate (inputs_topic) for k, v in inputs_topic .items ():\n            inputs_topic [k] = v .to (device)\n        inputs_content = collate (inputs_content) for k, v in inputs_content .items ():\n            inputs_content [k] = v .to (device)\n        batch_size = labels .size ( 0 )\n        with torch .cuda .amp .autocast (enabled=CFG.apex):\n            feature_topic = model (inputs_topic)\n            feature_content = model (inputs_content)\n            loss = simcse_unsup_loss (feature_topic, feature_content) content_copy Performance on 1,000 topics validation data: model F2@5 max positive score top50 max positive score top100 paraphrase-multilingual-mpnet-base-v2 0.5250 0.9135 0.9443 all-MiniLM-L6-v2 0.4879 0.9045 0.9353 mdeberta-v3-base 0.4689 0.8938 0.9187 recall We didn't use KNN to clusting, simply calculate cosine similarity for each topic and all content samples of the topic's language and then choose topN samples. We also tested the retriever ensemble (weighted cosine similarity). Although the max positive score top50 score has been improved to 0.9235, but there is basically no change on the LB score. So we only used a single retriever model (paraphrase-multilingual-mpnet-base-v2) in the final submit. Ranker The ranker in stage2 basically is a binary classification model. We used our best simcse finetuned model (paraphrase-multilingual-mpnet-base-v2) to infer on train set topics, calculate cosine similarity for each topic and all content samples of the topic's language and then choose top100 samples. We also added all positive samples from correlations.csv. texts were prepared as same as stage1. Pair format: content [SEP] topic , maxlen = 256 (token level) Hard negative samples from retriever model can greatly improve the performance of ranker models. retrieve model (max positive score top100) ranker f2 score (LB) 0.80 0.585 0.94 0.688 We have used two model initialization methods. One is to directly load the model weight from huggingface, and the other is to load the model weight after simcse finetuning. The performance of the two methods is basically the same, while the latter is slightly higher and can converge faster. We also used FGM, EMA on training. FGM+EMA can impove score by 0.01. model validation (1,000 topics) LB score PB score mdeberta-v3-base (loading simcse weights) 0.7149 0.688 0.727 mdeberta-v3-base 0.6378 0.669 0.693 xlm-roberta-large (loading simcse weights) 0.6987 - - xlm-roberta-base (loading simcse weights) 0.6780 - - paraphrase-multilingual-mpnet-base-v2 (loading simcse weights) 0.6299 - - Finding threshold We set the threshold in loop to calculate the f2 metric on the 1,000 topics validation data, codes like: best_thres = 0 .\nbest_score = 0 .\nbest_n_rec = 10 for thres in tqdm (np .arange ( 0.01 , 0.2 , 0.005 )): for n_rec in range ( 30 , 50 ):\n        test_sub = test_data [test_data[ 'score' ] >= thres] .reset_index (drop=True)\n        sub_df = test_sub .groupby ( 'topic_id' ) .apply (lambda g : g .head (n_rec)) .reset_index (drop=True)\n        score = calc_f2 (sub_df, label_df) if score > best_score:\n            best_score = score\n            best_thres = thres\n            best_n_rec = n_rec content_copy When submitting a single model, this method basically CV-LB consistency (CV is about 0.02-0.03 higher than LB). But When it came to the last two weeks of the competition, when started to ensemble, we lost the CV-LB consistency, and I think the reason may be that 1,000 topics validation data is not big enough. Post-Processing When dividing the threshold, we will have a small number of topics that do not match any contents. We just simply using top4 contents ranked by the original scores. We tried recalling more contents for this part of topics, but LB score didn't improve. We also tried different languages using different threshold, both CV and LB score dropped a little. Ensemble We had trained 20+ ranker models, trained on different number of recall samples per topic, like 50, 70, 100. mdeberta (simcse weights, 4,000 validate topics) mdeberta (simcse weights, 4,000 validate topics, with FGM,EMA) mdeberta (simcse weights, 1,000 validate topics) mdeberta (simcse weights, 1,000 validate topics, with FGM,EMA) mdeberta (1,000 validate topics, with FGM,EMA) xlm-roberta-large (simcse weights, 1,000 validate topics, with FGM,EMA) xlm-roberta-base (simcse weights, 1,000 validate topics, with FGM,EMA) We used LinearRegression to fit on 1,000 topics validation model output score to get coef_ array, and then used as blending weights: pcols = [ c for c in valid_data.columns if c .startswith( 'score' )] for cols in tqdm([i for i in combinations(pcols, 10 )]):\n    cols = list (cols) X = valid_data[cols]. values y = valid_data[ 'label' ]. values lr = LinearRegression().fit( X , y )\n    coef = lr .coef_ print (get_score(valid_data, df_target_metric, cols, coef)) content_copy We started with 100 recall samples per topic, but due to time limits, we can only use up to 6 models. So we tried 70 and 50 recall samples in the later stage of the competition. number of recall samples per topic models validation (1,000 topics) LB score PB score 100 6 0.725 0.705 0.738 70 10 0.738 0.715 0.751 50 12 0.743 0.715 0.751 Train Code: https://github.com/syzong/2023-Kaggle-LECR-Top3-TrainCode Please sign in to reply to this topic. comment 32 Comments 1 appreciation  comment Hotness Felix M Neumann Posted 2 years ago · 994th in this Competition arrow_drop_up 4 more_vert Thanks so much for sharing the training code. This is what really adds value. Lots of people have similar ideas and just from the writeups of solutions it is often very difficult to gauge whether a method worked due to being better suited for the problem or just due to better execution. Big thumps up! chumajin Posted 2 years ago · 12th in this Competition arrow_drop_up 1 more_vert @hengzheng Congratulation and thank you for sharing. Reading this, I thought that I might have improved my score if I had increased the number of ensembles in the ranker models as well! Thank you. heng Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Congrats for your solo gold, I also learned a lot from your writeup! Masaki Gotou Posted 2 years ago · 439th in this Competition arrow_drop_up 1 more_vert Congratulation! I will study with your solution. chizhu Posted 2 years ago arrow_drop_up 1 more_vert Congrats ！喜提绿头牌 HZM Posted 2 years ago · 226th in this Competition arrow_drop_up 1 more_vert Congrats, Could you please share the train code? heng Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 0 more_vert Thanks. The stage1 train code we used are modified from https://github.com/yangjianxin1/SimCSE The stage2 train code basically from https://www.kaggle.com/code/ragnar123/lecr-xlm-roberta-base-baseline heng Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 0 more_vert update: added briefly training code of stage1 simcse model. HZM Posted 2 years ago · 226th in this Competition arrow_drop_up 1 more_vert thks so much for your effort, hope can work with you in the future Jaideep Posted 2 years ago · 18th in this Competition arrow_drop_up 0 more_vert @leehann Congrats & Thanks for your write up. Were you able to train Mdeberta on Fp16, as I read it dsnt supports same. What boosted the score considerably ? Ravi Shah Posted 2 years ago · 409th in this Competition arrow_drop_up 1 more_vert Congrats @hengzheng ! Thanks for the detailed solution write up! heng Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 0 more_vert Thank you! ADAM. Posted 2 years ago arrow_drop_up 1 more_vert Congrats! GM coming soon. sayoulala Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert nice job,mmd xia Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert nice job! my bro,dddd! Ori Hanegby Posted 2 years ago · 10th in this Competition arrow_drop_up 1 more_vert Congrats on the 3rd place and for thanks for sharing the solution! heng Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thanks! Congrats for your solo gold! xbustc Posted 2 years ago · 19th in this Competition arrow_drop_up 1 more_vert good solution, congrats! by the way, the simcse you used is supervised version, right? heng Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 0 more_vert unsupervised version, we only used positive samples, supervised version (add negative samples) we tried but not success. Jaideep Posted 2 years ago · 18th in this Competition arrow_drop_up 1 more_vert @hengzheng congrats Max pos score is computed for only holdnout set or whole train data because ours top50 was reaching 95+ will be interesting to find out the deltas 5 more replies arrow_drop_down Turbo Posted 2 years ago arrow_drop_up 1 more_vert Congratulations, Teacher Zheng! World Top 3. gezi Posted 2 years ago · 26th in this Competition arrow_drop_up 1 more_vert Cool！Congratulations! Simcse based recall model work so well! And I think your strong recall model help a lot for stage2 model. But I still wonder your strong stage1 mode might not perform worse then stage2 model with simple score transform. Siddharth Sah Posted 2 years ago arrow_drop_up 2 more_vert Wow, fantastic work on your 3rd place solution! 🎉👏 It's incredible to see the level of detail and experimentation that went into your approach, especially with the retriever and ranker models. Your collaboration with your teammates and learning from the Kaggle community really paid off. Thanks for sharing your insights and methods with us, as it'll definitely be helpful for others in the community as well. Congratulations once again on your achievement, and I'm excited to see what you'll accomplish in future competitions! 🚀😄 nayu555 Posted 2 years ago · 102nd in this Competition arrow_drop_up 0 more_vert Congratulations and thank you for sharing!! May I ask why you have set maxlen = 256 (token level) in Ranker ? According to your explanation, you have set maxlen = 256 for both topic and content in Retriever. So , I think maxlen = 512 is appropriate in order to create \"content [SEP] topic\" in Ranker. I'd really appreciate it if you could share your opinion. heng Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thanks. In Retriever part 256 is string length. Later in the ranker part I concatenate them together, then truncated with token length 256. nayu555 Posted 2 years ago · 102nd in this Competition arrow_drop_up 0 more_vert @hengzheng Thank you for your reply. May I ask why you use string length in Retriever part? Tariq Mahmood Posted 2 years ago arrow_drop_up 0 more_vert Thanks @chizhu2018 , for sharing your marvellous work and effort you have made to reach  that milestone. Your valuable efforts will prove a great source of inspiration and learning towards the community. Best of luck your future efforts. What a work!👍 Adrian Dip Posted 2 years ago · 785th in this Competition arrow_drop_up 0 more_vert Thanks for sharing your knowledge and the code. Very interesting and educational. ktr Posted 2 years ago · 27th in this Competition arrow_drop_up 0 more_vert Congrats!, @hengzheng I have a similar training strategy, but your significant score improvement in the 2nd stage is impressive. I would like to confirm something. Is there no hard negative added to retriever training? I had difficulty exceeding f2 0.5+ without hard negatives. How did you get the negative data to supply to Ranker, you got it from the entire topic not used for training? heng Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thanks! Yes, we only used positive samples, no any negative sample in the retriever training. We used the trained retriever to get 100 recalling contents per topic (hard negative from here), and all the postive, to supply to ranker training. You can check our code here: https://github.com/syzong/2023-Kaggle-LECR-Top3-TrainCode heng Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 0 more_vert update: sharing training codes: https://github.com/syzong/2023-Kaggle-LECR-Top3-TrainCode Frank Posted 2 years ago · 24th in this Competition arrow_drop_up 0 more_vert Thanks for sharing. How long does it take to train a reranker with top100 candidates per topic? heng Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 2 more_vert mdeberta about 3-4 days on A100 because fp16 not working on mdeberta, xlmr base about 1 day and large about 2-3 days. wagege Posted 2 years ago · 3rd in this Competition arrow_drop_up 0 more_vert old iron，six six six  。。 This comment has been deleted. Appreciation (1) YashNikel Posted 2 years ago arrow_drop_up 0 more_vert Thanks for the detailed solution!",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules Psi · 4th in this Competition  · Posted 2 years ago arrow_drop_up 46 more_vert 4th place solution Thanks a lot to Kaggle and the hosts, specifically @jamiealexandre , for hosting this competition. Unfortunately, we only could join after NFL competition finished, so we tried to do our best within an eleven days sprint, and we are very happy about the outcome. Improving every day can be quite thrilling - but at the same time we regret a bit not having more time for this competition as it was really interesting and fun participating here, and I believe our solution has not yet reached its ceiling. Due to time constraints, our solution is based only on an ensemble of embedding models with cosine similarity matching as well as additional post-processing. Data processing We only use the following features as text input for out models: language, category, title and description An \"issue\" with the data is, that some topics and content items do not have a proper description, or very inconclusive titles. So it is helpful to supplement them. For topics, we replace the title with an inverse track of the category tree titles, so basically adding the breadcrumbs, we do the same for description. For content, we concatenate the description and text fields as a single column. The benefit of this approach is, that it will add information to those records that have incomplete data. With tokenization and truncation, the models will prioritize the original information, if available, and otherwise use the supplemented information. So for instance, if a content item has description and text available, it will prioritize the description, but otherwise if description is missing, it will use the text. Validation setup and models Due to lack of time, we focused on a single holdout. For this, we split out non-source categories. Contents matching to these categories can then be either already be part of training, or completely unseen. We got strong correlation with this setup to public and private leaderboard in relative terms. For submissions, we always retrained models on full data. As we were running out of time, we could not always do both a validation and fullfit. So 3-4 days before end, we only did blind fullfits, and blended them on submission. And for local validation and testing post-processing we relied on some earlier models. While not ideal, it was a reasonable approach given the time constraints. Embedding models We only use ArcFace models. For input and training the models we use two different schemas: Topic-based labeling. Here, a single label is defined as a topic, and all content items that match to this topic. So for instance: Label 1: Topic A, Content A, Content C, … This means that content items occur as many times as they match topics, and each topic is only a single sample. This approach is strongest on its own with the F2-based metric. Content-based labeling. Here, a single label is defined as a content item, and a topic that matches to this content. So for instance: Label 1: Content A, Topic A This means that each topic-content pair is a single label. This method worked worse individually, but blended quite nicely with the topic-based approach. Our final blend contains 7 topic-based, and 2 content-based models. Backbones are mostly xlm-roberta-base, xlm-roberta-large, paraphrase-multilingual-mpnet-base-v2, or deberta-v3-large. Post-processing We played a lot with different post-processing techniques as this is always something that is useful in metric-learning matching. We optimize the treshold automatically in the kernel to a certain average number of matches per topic. Also, we found that penalizing the cosine similarities based on additional information helps. First, we slightly reduce similarity probabilities for content that only matches to a single topic in whole training. Second, we increase the probability of content items that are not available in training. We also always match new content to the top ranked topic, if above a certain threshold. Finally, we also additionally add new matches if we have less than five matches for a topic, but the additional probabilities are above a certain ratio to the higher ranked probabilities. What did not work (due to time) We spent 2-3 days trying to tune bi-encoder text models for second-stage, but could not get anything that improved our first-stage embedding models to be worth the additional runtime. So we decided to drop it and focus on first-stage only. Also, we spent some time on trying to tune LGB second-stage models. We were quite sure that they should be working well, and might replace also some manualy post-processing. But while CV looked reasonable, LB was dropping a bit, and we were not too confident in the validation setup for it, so we dropped it. Seeing other solutions, it definitely seems to be helpful, and I believe it could push our solution higher. Efficiency sub We also have an efficiency sub scoring 0.72 ensembling two smaller models on shorter token lengths running in 22 minutes. We use multiprocessing and ONNX. We probably lack a good 2nd stage LGB model to boost the score higher here. As always, cheers to my amazing team-mate @ilu000 . All training and inference code can be found online . Please sign in to reply to this topic. comment 12 Comments Hotness Siddharth Sah Posted 2 years ago arrow_drop_up 2 more_vert Congratulations on your 4th place finish, especially given the limited time you had to work on this competition! 🎉 Your approach to data processing by supplementing incomplete information and focusing on embedding models with ArcFace is quite interesting. It's great to see how you managed to blend the topic-based and content-based models to achieve better results. The post-processing techniques you used also seem to have played a crucial role in improving your solution. It's always fascinating to see the different approaches and what did or didn't work for each team. Your dedication and resourcefulness in such a short time frame is truly commendable. Best of luck in your future Kaggle competitions! 😊 Yuji.K Posted 2 years ago · 73rd in this Competition arrow_drop_up 1 more_vert Congratulations @philippsinger and @ilu000 and thank you for sharing! Do you plan to release the code? I would especially like to see the ArcFace part. This comment has been deleted. Darek Kłeczek Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Congratulations, it was impressive and inspiring seeing your progress in the last days of the competition! Interesting that you included deberta in your ensemble - have you done anything specific to make it work well? I tried it but it was much more difficult to train vs. e.g. xlm-roberta. Psi Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 1 more_vert No, it worked out of the box. Actually our efficiency solution is also using deberta-v3-xsmall. Darek Kłeczek Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Probably it's your ArcFace magic then :) Thanks! Ori Hanegby Posted 2 years ago · 10th in this Competition arrow_drop_up 1 more_vert Congrats on the great achievement in such short amount of time! Trushant Kalyanpur Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Congratulations @philippsinger and @ilu000 ! That is a very impressive high gold in just 11 days ! Chris Deotte Posted 2 years ago arrow_drop_up 2 more_vert Congratulations @philippsinger and @ilu000 on winning another top cash gold prize so soon after winning Kaggle's Player Contact Detection competition top cash gold prize! Very impressive! Ahmet Erdem Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert Congrats! Impressive performance as always. I would be happy to see how you improved speed with ONNX. I researched it but couldnt find a good tutorial. Please ping me if you open source your code. Also noticed torch was utilizing multi-core but multiprocessing by data splits can be more efficient. Did you benchmark it? Psi Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 1 more_vert We honestly never benchmarked it vs. pure Pytorch, but I did so a lot in past feedback competitions, and multiprocessed ONNX was always better for me. You are absolutely right that data-split multiprocessing is always better. So we also multiprocess by data, and not by model forward. Actually, the goal needs to be to see 400% CPU utilization in the kernel. Afaik, I can always only manage 200% with pure Pytorch multiprocessing, as it does not seem to properly use all virtual cores, only the two physical ones. With data-split MP, it is easier. Let's see where we end on the efficiency, first place will be interesting battle between @khabel and you :) Ahmet Erdem Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert I actually didnt have time to invest on multiprocessing and used torch as it is. Therefore I asked, thanks for sharing your observation. So I could probably get 2x faster then. I hope this mistake doesnt cost me much:) Psi Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 1 more_vert Probably not 2x faster, because the virtual cores will give you less boost. Somehow the behavior is sometimes also really weird in (Kaggle) kernels. And tokenization also needs CPU time, so everything is a bit tricky. For ONNX I shared some details here: https://www.kaggle.com/competitions/feedback-prize-english-language-learning/discussion/370020 This competition is in general quite useful to browse for different efficiency solutions. Konni Posted 2 years ago · 2nd in this Competition arrow_drop_up 3 more_vert Congratulations and thanks for pointing out the ONNX Runtime. I also had some weird behavior in the CPU kernel. If I use all available threads the runtime even gets worse. So I ended up with this setting for my Efficiency Submission: import os\nimport torch\nimport psutil\nos .environ [ \"OMP_NUM_THREADS\" ] = \"2\" os .environ [ \"OMP_SCHEDULE\" ] = \"STATIC\" os .environ [ \"OMP_PROC_BIND\" ] = \"CLOSE\" torch .set_num_threads (psutil .cpu_count (logical=False)) content_copy Setting everything to only the real core-count instead of utilizing all threads leads at least for my submission to better runtime. I don’t know what Pytorch sometimes does on CPU when using all threads, the utilization is 400% but without any gain in speed. I will have a look at ONNX next times. ToYou2U Posted 2 years ago · 17th in this Competition arrow_drop_up 0 more_vert Congratulations! I'm also looking foward to your code about how to make ArcFace work so well. I'd really appreciate it! Altair Farooque Posted 2 years ago arrow_drop_up 0 more_vert Congratulations @philippsinger and @ilu000 . Well, trying to get in top 4 in a span 12 days is not an easy task . I was too late to this competition , since I haven't been here for long time , had improve my knowledge. However , i try to implement this ,but due to lack of time and experience ,I couldn't do it . I want you to know me , is this reasonable approach . Thank You",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules Nikita Churkin · 5th in this Competition  · Posted 2 years ago arrow_drop_up 41 more_vert 5th place solution First of all, thanks to Kaggle, The Learning Agency Lab for interesting problem and kagglers for strong competiton! You can find inference code example notebook there: https://www.kaggle.com/code/churkinnikita/lecr-example-0-705-public It uses only 1 [SBERT + LightGBM] model and receives 0.705 public / 0.741 private and 11th place on the final LB. Validation setup From LB probing we know that there are approximately 9% of new channels (graphs) in the test set so I tried to mimic this logic. I created 7Fold validation scheme when for every training fold we have 9-10% of unseen channels in the corresponding validation fold. But instead of 7 folds I used 1 or 2 folds to check improvements almost all the time. Correlation to the public LB was perfect. After some time I switched to classic 5Fold scheme (based only on topics ids) because learning that way led to better results on public (train set and test set are mixed). For the Stage2 model I used exactly the same folds for validation. For computing F2 score I filtered only \"non source\" data. Stage 1 I basically used SBERT package for learning stage1 model. My solution includes usage of 2 models: paraphrase-multilingual-mpnet-base-v2 with long training (250 epochs) for every language and all-distilroberta-v1 for English language (learned only on English subset) with shorter training time. Full solution scheme is depicted below: MegaBatchMarginLoss was chosen as loss function, batch sizes in the range 270-310 provided the best performance. Text input for topics was computed according to this formula: topic_text_input = language + channel + category + level + topic_title + topic_description + context(aka breadcrumbs) + parent_description + cousines_titles + children_titles . Inputs for contents is much simplier: content_text_input = language + kind + content_title + content_description + content_text . I created an esquisse to illustrate how topic text input looks like: During training I used different weights for \"source\" and \"non source\" data (0.35 and 0.65 respectively) to sample \"non source\" instances more often: thery are much harder for the model to predict correctly. New Lion optimizer led to very good optimization results out-of-the box but carefully tuned good old AdamW won in terms of the final F2 score (but required much longer training time). Training for 250 epoch was extremely long: ~40 hours for 1 fold. I even bought RTX 4090 to be able to compute everything before the deadline but didn't manage to do that: for 250 epoch-model I computed only 3 folds (out of 5) and all-data model. Stage 2 Second stage involved looking up for top 100 nearest contents for every topic in the corresponding language (for example, for topic in French we search only in French contents). After retrieving the list of possible candidates I generated ~30 features based on distance, language, text similarity Jaro score between titles, number of shared words), \"geometry\" of distance space, etc. There is list of best features with explanation: rank – number of neighbor (or ranked distance for topic). distance. dist_std –  Variance of distances to contents (for particular  topic). dist_min – Minimum of distances to contents (for particular  topic). dist_range – dist_max – dist_min . dist_jump – argmax of distance differences for topic: where (on what neighbor number) the biggest “jump” in distances occurred. dist_apart – rank – dist_jump . dist_max_change – maximum change in distances to contents (for particular topic). margin_forward – distance(topic, nearest_content[i+1]) - distance(topic, nearest_content[i]) . margin_backward – distance(topic, nearest_content[i]) - distance(topic, nearest_content[i-1]) . dist_mm – MinMaxScaled distances for topic. margin_backward_mm – analogue of margin_backward but for dist_mm. topic_cumsum_dist  –  cumsum of distances for given topic: candidates.groupby('topic_id')['dist'].cumsum() . topic_language. topic_level. topic_len – length of topic’s title. jaro – Jaro similarity score between topic's title and current content's title. topic_max_jaro – maximum Jaro feature for topic. topic_median_jaro – median Jaro feature for topic. nshared_words – actually length of longest common substring between topic's title and content's title. nshared_words_lower – same as nshared_words but in lowercase scenario. is_max_nshared – does that particular pair (topic, content) have maximal nshared_words feature for that particular topic. jaro_forward  – jaro(nearest_content[i], nearest_content[i+1]) . content_desc_isnull – if content’s description is null. content_text_isnull – if content’s text is null. content_min_dist – minimum distance for content: candidates.groupby('content_id')['dist'].min() . content_max_dist – maximum distance for content. content_diff_dist – dist – content_min_dist . LightGBM in binary classification mode was used as the Stage2 model (we assign 0/1 labels for candidates based on the correlation file). I used only non-source data to train and evaluate GBM. Prediction For final prediction I normalized predicted probabilities using MinMaxScaling for every topic, something that looks like: prediction.groupby('topic_id')['proba'].apply(minmaxscale) This approach allowed to search for the optimal threshold that doesn't depend on particular topic id. Final solution is majority voting of models learned on full train set + fold models (5 models in total): content is considered relevant if it appears in recommendation of (at least) 3 out of 5 models. Please sign in to reply to this topic. comment 7 Comments Hotness chumajin Posted 2 years ago · 12th in this Competition arrow_drop_up 1 more_vert @churkinnikita Congratulations for your solo gold. I remember being surpassed towards the end of the contest. I also tried various features for LGBM, but couldn't get it to function sufficiently. Also, I would like to try minmaxscale too! Aman Kapoor Posted 2 years ago · 67th in this Competition arrow_drop_up 1 more_vert I am amazed to see how gbdt models are shinning in this competition… nevertheless good write up and Congrats on the great finish Ori Hanegby Posted 2 years ago · 10th in this Competition arrow_drop_up 1 more_vert Congrats on the great results and thanks for sharing your solution! heng Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Congrats for your solo gold! 👍 Siddharth Sah Posted 2 years ago arrow_drop_up 2 more_vert Congratulations on your 5th place finish! Your approach combining SBERT with LightGBM and the careful selection of features and validation setup is quite impressive. It's interesting to see how you managed to optimize the F2 score and the use of majority voting for the final solution. Looking forward to more details and code later on. Great work! Trushant Kalyanpur Posted 2 years ago · 9th in this Competition arrow_drop_up 2 more_vert Great work @churkinnikita ! Congrats on the solo gold! I tried to get GBDT to work as well and val score showed good improvement but it wasnt reflected on the LB. A couple of questions: How many negative samples did you take from stage1 to train the GBDT model? For me this was an imbalanced amount and I tried different approaches to tackle that. For features did you use embeddings from stage1 or TFIDF based features? I mainly had distance features like you did along with cos_sim scores from stage 1 . The scaling is a good idea and probably what helped with aligning the probabilities across different topics. Do you happen to have scores only with stage 1 and how much improvement GBDT models gave? Nikita Churkin Topic Author Posted 2 years ago · 5th in this Competition arrow_drop_up 2 more_vert Thank you! 1) Using stage1 model for every topic I pick 100 nearest neighbors (from contents). I label (topic, content) pairs according to correlations.csv file. I had ~3-4% of True Positives in my top-100 candidates. So I had 96-97 zeroes and 3-4 ones per topic on average. I haven't done anything to tackle imbalance, only tuned scale_pos_weight parameter a little bit. 2) Using TFiDF features is probably a good idea. I haven't used embeddings or TFiDF features. My main feature block was distance-based features and aggregations based on distance, topics and contents. For example, cumsum of distances for given topic: candidates.groupby('topic_id')['dist'].cumsum() . I will update my solution soon and add explicit list of features and explanations. 3) Fold #0: 0.67 F2 (without GBM) —> 0.687 F2 (with LGB). @trushk I hope I answered your questions, feel free to ask again, if I didn't. Trushant Kalyanpur Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Thanks @churkinnikita for the details! I might have to spend some time to try to get it to work just for my closure :)",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules s_shohei · 6th in this Competition  · Posted 2 years ago arrow_drop_up 46 more_vert 6th place solution Summary Retriever and reranker Transformer-based model for retriever GBDT-based model for reranker Inference code https://www.kaggle.com/code/iiyamaiiyama/llecr-ens8910-125128-149154-155157 CV strategy Simple 5fold random split with topic-id. For stage 1 I used all data, including \"source\". For stage 2, I used the same fold split as for stage 1, and created several models with and without \"source\" to ensure diversity. CV and LB were well correlated. I trained a model based on channel GroupKFold and select it for one of my final submissions, but a simple random kfold gave slightly better results for CV and LB. Retriever(stage1) Text The training text was created as follows: Topics Recursively traversed to the root node and added titles. Finally, the topic description was added. For example, (root title + parent1 title + … + topic title + topic description). Contents The title and description were concatenated. The \"text\" column was discarded. model    max positive score@50   CV F2@stage1    CV F2@stage2    public LB@stage2 sentence-transformers/LaBSE    0.8887  0.5462  0.6727  0.676 sentence-transformers/paraphrase-multilingual-mpnet-base-v2    0.8891  0.5429  0.6698  0.678 facebook/xlm-v-base    0.8869  0.532   0.669   0.671 xlm-roberta-base    0.8832  0.5388  0.6666  0.676 naive ensemble above four    0.9336  -   0.6916  (I didn't sub this) my published submission     -   -   0.7152  0.707 Model Each topic and its correlated contents were grouped together as one class. The model was trained with ArcFace. Each model produces 768-dimensional embeddings. Trained for 30 or 60 epochs, which took about 5 hours per fold. The margin was gradually increased from 0.2 to 0.6 during training. The following models were used for the final submission. sentence-transformers/LaBSE sentence-transformers/paraphrase-multilingual-mpnet-base-v2 facebook/xlm-v-base xlm-roberta-base KNN For each topic, find 50 nearest neighbor contents and these pairs are passed to the reranker(stage2). Reranker(stage2) When I submitted stage1 model only, I coudn't reach 0.6 on the public LB. I think this is because stage1 model doesn't have any topic-tree structure information. So I added many tree-based features for stage2 model. Model CatBoost and XGBoost were used for reranker. Features were created for each pair, then GBDT model predicts probability that the pairs were correlated or not. Example of features Embeddings distance TF-IDF distance Whether the topic's siblings correlated the target content How many times the content was correlated in the channel Ensemble stage1 Concatenate embeddings then find KNN stage2 Average GDBT predictions, then select pairs as positive predictions above the threshold. Post-processing After stage2, if there were no predictions for a topic, the content with the highest predicted score was added. If two contents that were always correrated together in \"correlations.csv\", and one of them appeared in the prediciton, add the other one. Search best threshold for each channel For channels not included in the training data, a fixed threshold was used. (based on CV across all channels) Not worked \"text\" information of contents I cannot find good way to use \"text\" column. Transformer-based reranker They were very prone to overfitting. Appendix: my models Model max positive score@50 CV F2@stage1 CV F2@stage2 public LB@stage2 sentence-transformers/LaBSE 0.8887 0.5462 0.6727 0.676 sentence-transformers/paraphrase-multilingual-mpnet-base-v2 0.8891 0.5429 0.6698 0.678 facebook/xlm-v-base 0.8869 0.5320 0.6690 0.671 xlm-roberta-base 0.8832 0.5388 0.6666 0.676 Naive ensemble above four 0.9336 - 0.6916 (I didn't sub this) My published submission - - 0.7152 0.707 Please sign in to reply to this topic. comment 13 Comments 1 appreciation  comment Hotness ToYou2U Posted 2 years ago · 17th in this Competition arrow_drop_up 1 more_vert Congratulations! Can you share your cv setting? How many data  are used for training trees? s_shohei Topic Author Posted 2 years ago · 6th in this Competition arrow_drop_up 0 more_vert Thank you very much. I added \"CV strategy\" section. ToYou2U Posted 2 years ago · 17th in this Competition arrow_drop_up 0 more_vert Congrats again for solo gold and Grandmaster! 3 more replies arrow_drop_down Ori Hanegby Posted 2 years ago · 10th in this Competition arrow_drop_up 1 more_vert Congrats on the great place and thanks for sharing the solution! chumajin Posted 2 years ago · 12th in this Competition arrow_drop_up 1 more_vert Congratulations @iiyamaiiyama I tried various feature selection techniques with GBDT, but I couldn't find anything that significantly increased the score. I will try what you suggested. Thank you very much. hhz Posted 2 years ago · 134th in this Competition arrow_drop_up 1 more_vert amazing solution!!! A brave way of using GBDT instead of transformer-based model qcqced Posted 2 years ago · 121st in this Competition arrow_drop_up 1 more_vert I really agree with you that transformer-based reranker were prone to overfitting. In my case, I used transformer-based reranker and experienced serious overfitting. Until my CV reach 0.51xx , CV & LB Scores are almost same. But After then CV reached 0.65xx, LB still stay in 0.51xx I never… didn't think about Several Distance Metrics with GDBT… What a nice solution. Congrats your Gold Medal. I really learned from your effort, Thanks! Siddharth Sah Posted 2 years ago arrow_drop_up 2 more_vert Wow, amazing work on your 6th place solution! 🤩🎉 Your approach to using a combination of transformer-based models for retrieval and GBDT-based models for reranking was quite innovative. It's also great to see that your CV and LB scores were well-correlated, ensuring a reliable evaluation process. Thank you for sharing the detailed explanation and your model's performance; it will definitely be helpful for others in the community. Congratulations on your achievement, and I'm looking forward to seeing what you'll come up with in future competitions! Keep up the great work! 🚀😊 AdilFaizanKhan Posted 2 years ago arrow_drop_up 0 more_vert Hi @iiyamaiiyama m trying to run your notebook but facing many error. Can you please help me with this one thanks in advance. Screenshot to the error is below. Tariq Mahmood Posted 2 years ago arrow_drop_up 0 more_vert @iiyamaiiyama , Thanks for sharing such an organized work with amaing elaboration. Congrats! upon your solution and dedicated work you have done. It is a motivation for us to follow. ktr Posted 2 years ago · 27th in this Competition arrow_drop_up 0 more_vert congrats solo gold and GM! @iiyamaiiyama I am not very familiar with Arcface training, could you tell me about the labels? With topic (or content) as the model input, and output topicID (or content_id) as the label? In other words, is there one sample for each label? s_shohei Topic Author Posted 2 years ago · 6th in this Competition arrow_drop_up 1 more_vert Thank you. In my pipeline, the label(class) in Arcface training is similar to the topic-id based, but not exactly the same. If a topic has four correlated contents, they all have the same label. It means the label has five(1+4) samples. However, if two topics share exactly same contents, they have same label. ktr Posted 2 years ago · 27th in this Competition arrow_drop_up 1 more_vert Thanks! It's a clear image. I hadn’t thought of correlations as a single class, I learned a lot from you. However, if two topics share exactly same contents, they have same label. so, If the topic does not share the content exactly , will it be in a different class? For example, if the lines T1 and C7 do not exist s_shohei Topic Author Posted 2 years ago · 6th in this Competition arrow_drop_up 1 more_vert Yes. In that case, these topics and contents are treated as two separate classes: {T1, C5, C6} {T2, C5, C6, C7} ktr Posted 2 years ago · 27th in this Competition arrow_drop_up 1 more_vert Thank you! I did not expect to be able to train a model in such a case. Everything is now clear. Yili Posted 2 years ago · 710th in this Competition arrow_drop_up 0 more_vert Congrats 🎉! Could u share your step2 code so that I can learn from it 🎉🎉. Appreciation (1) LK:)_kaggle123 Posted 2 years ago · 166th in this Competition arrow_drop_up 1 more_vert thanks for your sharing, I learn a lot.",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules Raja Biswas · 9th in this Competition  · Posted 2 years ago arrow_drop_up 21 more_vert 9th Place Solution Many thanks to Kaggle and the hosts for organizing such an amazing competition towards a novel cause! Special thanks to @jamiealexandre for creating and maintaining a welcoming and energetic atmosphere. Congratulations to all the winners and thank you for sharing your valuable insights. Kaggle is an awesome community! Congratulations to our teammate @thedrcat for becoming a Kaggle Competitions Grandmaster! Very well deserved 🎉🎉💯 Summary Our solution follows the classic stage 1 Retriever + stage 2 Re-Ranker pipeline, while using transformer based models for each stage. The final score of a (topic, content) pair is computed by blending retrievers similarity scores and re-ranker model probabilities. We then used a constant threshold to filter out candidates and performed minor post processing. Stage 1: Retrievers We used the standard dual-encoder setup for training of retrievers. We paid close attention to retrieval focused pre-training of backbones such at they are properly warmed up for retrieval task in the domain of interest. Specifically, we used 3 different pre-training approaches Inverse Cloze Task (ICT): basically a retrieval task where we defined the positive pairs to be: content title, content description content title, content text chunks (generate many examples by splitting content texts at fixed intervals) topic title, topic description Condenser pre-training of xlm-roberta-base A specific pre-training approach designed to modify LM’s internal attention structure such that they produce better dense vectors for retrieval Reference: Condenser: a Pre-training Architecture for Dense Retrieval ( https://arxiv.org/abs/2104.08253 ) RetroMAE pre-training of sentence-transformers/paraphrase-multilingual-mpnet-base-v2 Reference: RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder ( https://arxiv.org/pdf/2205.12035.pdf ) Our final solution included an ensemble of 8 retrievers, trained using Multiple Negatives Ranking Loss, using following backbones: sentence-transformers/paraphrase-multilingual-mpnet-base-v2 + ICT xlm-roberta-base + condenser sentence-transformers/paraphrase-multilingual-mpnet-base-v2 + RetroMAE intfloat/simlm-base-msmarco-finetuned xlm-roberta-large + ICT The models were trained using in-batch negatives. We implemented a custom sampler to construct batches in the following 3 ways - Language wise batching: all topics in a batch are from the same language Channel wise batching: all topics in a batch are from the same channel Random batch During training we used roughly 80% language wise batching + 15% channel wise batching + 5% random batching. We also implemented a two-pass approach for one of our retrievers Pass 1: Training with MNRL loss Pass 2: Select top 20 negative samples for each topic and continue training using Contrastive loss for additional epochs. This boosted recall while not affecting F2 too much. For topic / content representations we tried many different combinations, mainly similar to my post from the initial phase of the competition: https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations/discussion/376873 . We are not so sure if the topic encoder captured graph hierarchy properly. The information from ancestors were definitely helpful, but most likely it didn’t capture graph structural connotation of siblings, cousins and children. Stage 2: Re-rankers In order to prepare positive and negative examples for re-rankers we used Top 25 retrieved contents for each topic (out-of-sample) All positive contents from correlations.csv We adopted 2 different approached for re-ranker: Pointwise approach: only classify one (topic, content) pair at a time Listwise approach: classify all contents for a topic at once Pointwise Approach Even though it’s a standard binary classification task, the training of re-rankers with pointwise approach has been tricky with issues such as model de-generation / loss explosion etc. To overcome this, we enriched content representation by adding correlated topic breadcrumbs from source category enriched topic representation by adding breadcrumbs of very similar topics (> 0.95 cosine similarity / BM25 search) more dense supervision using auxiliary loss, as explained below The auxiliary labels are computed on ancestors up to a certain depth (hyper parameter set at 5). If a content is linked to any descendant topic of an ancestor then the corresponding auxiliary label is 1. One example: [CLS] [T_START] 9.4 Leaf electroscope ; Leaf electroscope: features, charging and discharging [T_END]  [T_START]  9.0 Electrostatics 1 (12 lessons); [T_END]  [T_START]  Physics; [T_END]  [T_START]  Form 1; [T_END]  [T_START]  Kolibri Library for Kenya; [T_END] [SEP] en; html5; Pinhole Magnifier; Who needs expensive optical equipment to see better?; Info = Pinhole Magnifier Who needs expensive optical equipment to see better? A pinhole in a card can act like a magnifying glass, helping your eye focus on an object that is very close to you. However, by limiting the amount of light that reaches your eye from the object, the pinhole also makes the object appear dimmer. Subjects: Keywords: Biology Anatomy &amp; Physiology Perception Light[SEP] content_copy Here the topic is 9.4 Leaf electroscope and content is en; html5; Pinhole Magnifier; Who needs expensive optical equipment to see better?; Info = Pinhole Magnifier Who needs expensive optical equipment to see better? ... . They are not directly correlated, hence main label is 0. Aux label for parent 9.0 Electrostatics 1 (12 lessons) is also 0 since none of the descendant topics of parent (siblings) is attached to the content. However, the grand parent of this topic physics has aux label of 1 since there exist at least one descendent topic of physics that is correlated with the content. This way training allowed dense supervision as compared to vanilla methods. Intuitively, this approach should help a model to better capture the domain of a topic, level of proficiency required and similar other dimensions. Listwise Approach In this approach, the candidate contents are classified together. The candidates are sorted as per their stage one cosine similarity scores. Hence this is some sort of meta model - using signals from stage 1 retrievers. We used the following backbones for re-ranker: microsoft/mdeberta-v3-base cross-encoder/mmarco-mMiniLMv2-L12-H384-v1 MoritzLaurer/multilingual-MiniLMv2-L12-mnli-xnli Additionally, we tried a QA based re-ranker approach as mentioned here: Reranking as QA ( https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations/discussion/395029 ) It gave good boost to our local validation score, but we couldn’t incorporate the model properly in our solution as it was trained on very last minutes… Blending + Post Processing We blended both stage 1 and stage 2 scores to compute the final score of a (topic, content) pair. The final list of contents for a topic was obtained by filtering the candidates using a constant threshold (irrespective of channel / language / new / old contents), which was tuned with a small subset of data (1/4 folds). This was definitely not optimal. After reading top place solutions, we realized, this step didn’t allow our pipeline to reach its full potential. In fact, our only gold place submission (completing 15 mins before deadline) happens to be the one where we increased the threshold slightly. We did the following post-processing: Handling null predictions: in case no candidate content was left after applying the threshold, we picked top 5 retriever candidates For each new content in the hidden set, Find the top matched unseen topic If the above topic has same language and has cosine similarity above 0.5, then add in the (topic, content) pair. What Didn't Work Second round training of retrievers with hard negatives / de-noised hard negatives We couldn’t make ArcFace loss work, we will revisit this, surely it’s a must try for metric learning tasks LGB / GNN based post-processing gave only minor boost, couldn’t focus on them more as we were pursuing many other ideas Query expansion Team Members Many thanks to my teammates (@trushk , @harshit92 , @syhens , @thedrcat ) for such an amazing collaboration and perfect teamwork! Thank you JarvisLabs ( https://jarvislabs.ai/ ) for empowering us with GPU hours! Trushant Kalyanpur @trushk Harshit Mehta @harshit92 Yao He @syhens Darek Kłeczek @thedrcat Raja Biswas @conjuring92 Please sign in to reply to this topic. comment 5 Comments Hotness chumajin Posted 2 years ago · 12th in this Competition arrow_drop_up 3 more_vert @conjuring92 congratulation and thank you for sharing! It was a strong finish. I also noticed that the optimal threshold value for validation and the optimal value for public LB were different, so I used that. Trushant Kalyanpur Posted 2 years ago · 9th in this Competition arrow_drop_up 3 more_vert It was great to take part in another NLP competition with Team Turing again! This was an excellent and well designed NLP competition and one which helped me learn a lot about this problem type. We need more competitons like these ! Anand Kumar Posted 2 years ago arrow_drop_up 3 more_vert I did not understand a thing here, seems like i have lot to catch up in this field. One thing I agree with you is \"kaggle is a awesome community\" Ravi Shah Posted 2 years ago · 409th in this Competition arrow_drop_up 1 more_vert Congrats @conjuring92 ! Thanks for sharing your fascinating approach! yao Posted 2 years ago · 9th in this Competition arrow_drop_up 2 more_vert These two collaborations were wonderful experiences and gave me great insight into completing NLP projects. Hopefully I will be able to join Team Turing again after I return to Kaggle. Great Team! (I will make sure I have enough time before returning to Kaggle) Hope you all can become Kaggle GM by then! (finger cross)",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules Ori Hanegby · 10th in this Competition  · Posted 2 years ago arrow_drop_up 22 more_vert 10th place Solution I want to thank the organizer for a fun competition for an important mission. I learned a lot through participating in it. Thanks also for everyone who is sharing the solutions, I've been learning a lot from the many approaches being presented. My solution had 3 stages which followed the most popular paradigm in this competition- retrieval, reranker (cross-encoder) and a stage to calibrate the results on the validation set for threshold selection. A high level diagram of the pipeline: My overall strategy was to keep stage1 inference simple and fast and do the heavy lifting in stage2 with the heavier transformer models. Stage 1: Bi-Encoder The baseline model was sentence-transformers/paraphrase-multilingual-mpnet-base-v2 and iteratively fine tune it with better training sets. K NN The output of stage1 is a set of content candidates assigned to each topic. Candidate selection was based on similarity score threshold (not top-K neighbors). This was helpful in several ways: It helped avoid unnecessary processing for many irrelevant candidates Initial experiments showed it gave better stage 1 CV The score from stage 1 helped define “sampling regions” that I used in stage2 and 3. More on this later. Stage 1 features: For each topic, I built a reverse topic tree from the training set. I’ll call it [topic path] in the rest of the note: (example) /12. 20: Bird Reproduction/12: Vertebrates/Book: Introductory Biology/Introductory and General Biology/Bookshelves/Libretext Open Educational Resource Library I used the following columns concatenated to strings as the input for the models: Content: [Kind]< K >[title]< D >[description]< T >[text] Topic: [topic path]< O >[description] Training The key for success in stage 1 was finding an optimal training set. The challenge has been how to select the negative samples for training as naive sampling leads to weak models. In order to create an optimal training set I created the following process: Predict topic-content assignments on the training set based on the latest trained model Use the topic-content assignments to generate a new training set. This would create a new training set with hard false positives on the top scores and easy false positives. The threshold in which I would cut off candidate would be when reaching an average of ~50 candidates per topic. Add all the missing topic-content assignments from the training set Train a new model with the new training set. Go to step 1 Multiple Negative Ranking Loss One of the key decision that helped my CV/LB early on was to use Multiple Negative Ranking Loss, which helped accelerate training significantly. From the training process described above, the false positive candidates work great with MNRL because in one batch we can have a good blend of hard to easy samples across multiple topics. There is one problem though- one of the requirements of MNRL is that for each sample pair (a_i, p_i): all p_j (j!=i) and all n_j are considered negative, which is likely to break especially in case of hard negative samples. In order to make it work I created a carefully crafted training set preparation process that “pre arranged” the batches before training started. For each batch: pick a positive topic-content assignment For each false positive content assignment for the above topic, find a correct topic to pair to that content and add it to the training set. Remember previously encountered topics and skip topic-content pairs where the topic has already been seen. Repeat the above process, in case there are no more valid samples to choose, pick another random topic-content pair. In case of a dead end- pick a sample from another language. And start over. Conveniently topic-content assignment are always valid within the same language so there are no collisions across languages. I passed the training data as-is to the data loader without shuffling where the batch size is the same one I used in the pre-processing step. Contrastive Loss After training enough iterations of the above process (eventually F2/Precision/Recall stops improving), I took the best model and further fine tuned it with contrastive loss on the latest training set using a standard training process with the training data as is. For stage1 the best F2 score in my CV is 0.65. I didn’t submit a solution based on stage1 when getting to these results so I don’t have the corresponding LB scores. The average number of candidates per topic vs. recall at different threshold was (threshold : avg. number of candidates : recall) 0.91 : 9.7 : 0.77 0.9 : 16.45 : 0.81 0.89 : 29.8 : 0.85 0.88 : 55.7 : 0.89 0.87 : 103.4 : 0.91 Stage2: Cross-Encoder I’ve been thinking about this step much like a “zero shot” learning approach because the topics in the test set were not seen in the training set. Furthermore, there is additional content that was not in the training set. Feature engineering From each topic-conent pair from the previous stage (both for positive and negative labels) I generated two samples: Training Sample 1: Same as the one for stage one but topic and content are concatenated and separated by the relevant [SEP] token Training Sample 2: An addition that I had in this stage is to add the correlated topic paths of the content to the content features: content: [Kind]< K >[title] < C >[correlated topic path 1]< C >[correlated topic path 2] .. < C >[correlate topic path n] < D >[description]< T >[text] example: The topic text is exactly the same as stage 1: Topic: [topic path]< O >[description] example: The final input text is concatenation of both the topic and content separated by the [SEP] token relevant for the model. The reason for the two variants is that I wanted to make sure that the model have two learned capabilities: Predict topic-content assignment based on text features alone - this would help with new unseen content in the test set. Predict topic-content assignments based on relationship of the content to other topics. This helped resolve assignment ambiguities where the attributes of the content alone were not informative enough to decide whether a certain content is relevant to the topic or not (e.g. similar math concept but for different grades) Sampling The cross-encoder was hard to train. The key for successful training was the sampling strategy. Similar to stage 1 it’s also important to find a good balance between hard and easy negatives so the model generalizes well with high predictive power. The best strategy that worked for me was - I took samples from stage one where the threshold was above 0.89. Between 0.88 and 0.89 I sampled 0.1 of the negative samples and below 0.88 I sampled 0.02 of the negative samples. I always added all the positive samples Models: I trained 4 models: XLM-Roberta-Base, seq max length 256 DeBERTa-v3-xsmall, seq max length 512 DeBERTa-v3-small, seq max length 512 DeBERTa-v3-base, seq max length 400 It’s interesting to point out that although the Deberta models are English only models their tokenizers had the non-english character sequences and they were able to train well on the competition dataset. I didn’t use mdeberta because I was unsuccessful in making it work with fp16 training and training was too slow. One interesting observation is that more epochs consistently resulted in higher LB scores both in public and private LB. I suspect that I had room to further improve scores by spending more hours in training more epochs, but eventually I ran out of time. And it was also becoming expensive given that I was using Colab Pro+ credit for training. Stage3: Logistic Regression The last model is taking the scores from stage1, the models from stage2 and trained on the validation set to produce the final score for topic-content assignments. Due to training on the validation set and using the results to find the threshold for maximizing F2 I decided to use a linear model in order to have low model complexity and less likelihood of overfitting. The approach I took here for creating separate features for each stage 1 threshold and further split the features based on whether the content was in the training set or not. It looks something like that. Here is a snapshot of the coefficients from the best model to illustrate the above Threshold above 0.95: Content In Training Set: stg_1_score, xlmr_score, deb_xs_score, deb_s_score, deb_base_score 0.16605078,  0.67237598,  3.0603439 ,  0.92530175,  2.55978453, Content Not In Training Set: stg_1_score, xlmr_score, deb_xs_score, deb_s_score, deb_base_score 3.01727545,  1.06343818,  0.9088478 ,  0.33693999,  2.01195161, Threshold between 0.94 and 0.95: Content In Training Set: stg_1_score, xlmr_score, deb_xs_score, deb_s_score, deb_base_score 0.25057614,  1.64266565,  1.98558391,  0.50577352,  1.99118334, Content Not In Training Set: stg_1_score, xlmr_score, deb_xs_score, deb_s_score, deb_base_score 2.60065444,  0.93282139,  0.68057603,  0.82555645,  1.23466772, For the submission I selected a different threshold that maximizes F2 on the validation set for content in the training set and out of the training set. My best submission had CV 0.747,  Private LB 0.741, Public LB: 0.708 Validation Set: For my evaluation set I sampled 10% of the topics in correlations.csv randomly, and removed half of their correlated content from the training set as well. In hindsight removing some channels in addition would have been better to mimic the distribution of the data in the competitions test set more closely. Because the validation set was used to calibrate the final predictions and the F2 maximizing thresholds I believe that a better validation set would directly translate to better LB scores. In this competition I used 1 fold CV shared across all stages. I was initially concerned about the high training cost for more than 1 fold, but over time I got more comfortable with the 1 fold CV due to a consistently good correlation between the CV and the LB results. I did have quite a bit of shake-up anxiety toward the end though so I’m happy it wasn’t a bad shakeup case. For training I used Sentence-Transformers for stage1, Hugging Face Transformers for stage2 and scikit-learn for stage3. Thanks and looking forward to more competitions! Please sign in to reply to this topic. comment 4 Comments Hotness Krisztián Boros Posted 2 years ago arrow_drop_up 1 more_vert Very detailed description, thank you for sharing it! Ravi Shah Posted 2 years ago · 409th in this Competition arrow_drop_up 1 more_vert Congrats on the solo gold @ohanegby ! Thanks for sharing your write up! TanjiroLL Posted 2 years ago · 705th in this Competition arrow_drop_up 1 more_vert Thanks for your description, and congrats for solo gold, well deserved) Could you please tell me what do you mean by correlated topic paths in training sample 2? Ori Hanegby Topic Author Posted 2 years ago · 10th in this Competition arrow_drop_up 1 more_vert Correlated topics paths are: Topic path: the reverse topic tree (like the example in the post) Correlated topics paths are the topics which the content was associated with in the correlations.csv files. I will add an example to the note to illustrate it TanjiroLL Posted 2 years ago · 705th in this Competition arrow_drop_up 0 more_vert Thank you for your reply and your updated description. For the new contents, did you use the predictions in stage 1 to calculate the related correlated topic paths? Ori Hanegby Topic Author Posted 2 years ago · 10th in this Competition arrow_drop_up 1 more_vert For new content I didn't add any correlated topic paths, I only added the correlated topics for the content that was already existing in the correlations.csv file that came with the training set. That is the reason that I created two training samples for each content when training the stage2 model: one with correlations and the other one without. That way the model learned to predict from both scenarios",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules chumajin · 12th in this Competition  · Posted 2 years ago arrow_drop_up 43 more_vert 12th place solution First of all, I would like to thank kaggle and the staff for hosting such an interesting competition. I'm really happy that I achieved my goal of reaching the solo gold tier. 1. Summary My solution consists of 3 stages. In the 1st stage, I created embeddings using Arcface and generated candidates. In the 2nd stage, I used a transformer-based rerank model with the distances from the first stage as features. In the 3rd stage, I used the 64-dimensional embeddings from the first stage obtained via SVD, distances, and predictions from the second stage to create rerank models using 1DCNN, LGBM, and MLP. I then performed rank ensemble, set a threshold, and selected final content ids. Finally, I added post-processing to fill in the gaps based on distances for cases where there were no content IDs. 2. 1st stage Arcface 2.0 Cross validation I used stratified group k-fold to group the data up to grandparents into one group. 2.1 Feature engineering My text creation was greatly boosted by @conjuring92 post REF topics: Level + [sep] + title : description + [sep] + context + [sep] + children + [sep] + parent_description + [sep] + brother title content : kind + [sep] + language + [sep] + title + [sep] + description + [sep] + text 2.2 MLM I conducted MLM using the features described in section 2.1, and referred to this code REF for the implementation. Thank you. MLM boosted my cv + 0.010 @ 1fold. But, it was necessary to verify that the model performed well on all folds (not execute). 2.3 Arcface architecture I believe that using Arcface was the most distinctive feature of my solution. The following is a dataset for training. The following is an overview of the 1 iteration process. First, I set the topic id and content id as outputs, and set the topic id as input. Then, I calculate the loss for each output (loss1, loss2). Next, I set the content id as input, and calculate the loss for each output (loss3, loss4). Finally, I took the average of the four losses, perform backpropagation, and train the model. The number of epochs was approximately 30 epochs, the margin was 0.0001, and the value of s was adjusted depending on the model (around 10-15). 2.4 Ensemble I created nine models including xlm-roberta-large, xlm-roberta-base, and mdeberta-v3-base, and then concatenated the outputs of these models to perform an ensemble. One of them, I used the pseudo labeling. I used the items with 'has content' equals False. While individually weak, they proved effective when combined in the ensemble. In order to avoid out of memory, I must devide the topics and content… (Adjusting the bugs was very difficult.) 2.5 Using fulltrain Using fulltrain was also one of my features. Initially, I used 4kfold, but I realized that fulltrain was extremely powerful. In the end, I did not use 4kfold to calculate distance and instead used the full train models trained with 3 different seeds (public LB + 0.007). 2.6 (just reference 1st stage + rule base submit result) cv : 0.65289, public lb : 0.695, private lb : 0.732 3. 2nd stage transformer base rerank I found comments about overfitting in the discussion, but I did not experience it. I used folds consistently with the 1st stage. Moreover, it was mentioned in some discussions that reranking using transformers works up to a certain point, but beyond that, it no longer works. I also experienced it. However, by adding the distance from the 1st stage as input, I was able to obtain more cv results. 3.1 Feature engineering I set the input as follows: Simirarity : str (int((1-distance)*1000)) + [sep] + topics title : topics description + [special original defined sep] + topics context + [sep] + content title : content description : content text 3.2 Ensemble model1 : xlm-roberta-large  cv : 0.66418 model2 : sentence-transformers/paraphrase-xlm-r-multilingual-v1  cv : 0.66039 model1 * 0.7 + model2 * 0.3 = cv 0.6668  ,public lb : 0.70118, private lb : 0.73884 4. 3rd stage LGBM,1dcnn, MLP In the 3rd stage, we used the SVD 64-dimensional embeddings generated in the 1st stage, as well as language, distance, and predictions generated in the 2nd stage, as features. The results for each are as follows. LGBM : cv 0.6644 1dcnn : cv 0.66450 mlp : cv 0.663156 5. Rank ensemble for final submission and postprocess I performed a mean ensemble of the results obtained in sections 3.2 and 4, ranked by their respective scores. In the post-processing, for the topics in which there were no results above the threshold, I established a specific number per language and used a filling technique based on the distance in the 1st stage. final cv : 0.668755, public lb : 0.7023, private lb : 0.74044 (12 th) 6. Not working for me Changes in margin and s for each epoch at Arcface AWP augmentation by mixup Catboost, XGboost, Tabnet Knowledge Distillation 7. Acknowledgments I couldn't get this score on our own. I am grateful to those who shared their knowledge in the past, those who teamed up with me, and everyone else! I respect to you. Special thanks to this competition (using the code, dataset, and strategy) @conjuring92 , @takamichitoda , @ragnar123 , @yasufuminakama (fb3 notebook) Please sign in to reply to this topic. comment 18 Comments Hotness Siddharth Sah Posted 2 years ago arrow_drop_up 3 more_vert Congratulations on your 12th place finish and achieving solo gold tier! 🎉 Your solution involving Arcface, transformer-based reranking, and using LGBM, 1DCNN, and MLP for the 3rd stage is quite impressive. It's great to see how you adapted and improved your model with various techniques and ensembles. Thank you for sharing your insights, approach, and the challenges you faced along the way. Your hard work and dedication have certainly paid off! Keep up the fantastic work, and best of luck in your future Kaggle competitions! 😊 chumajin Topic Author Posted 2 years ago · 12th in this Competition arrow_drop_up 1 more_vert @siddharthkumarsah Thank you very much! Let's meet again at some other competition. Raja Biswas Posted 2 years ago · 9th in this Competition arrow_drop_up 3 more_vert Many congratulations @chumajin on solo gold! Awesome work, specifically on Arcface 🔥! During the whole competition duration, we were trying to catch up with you! Thanks for sharing the great write-up 😊 chumajin Topic Author Posted 2 years ago · 12th in this Competition arrow_drop_up 1 more_vert @conjuring92 Thank you very much. Congrats too. Although I was surpassed in the end (maybe final day), I couldn't have achieved it without your input!  Let's meet again at a competition somewhere! Takamichi Toda Posted 2 years ago · 845th in this Competition arrow_drop_up 1 more_vert Great solution! I respect you for accomplishing this on solo team. Congratulations on your gold medal! chumajin Topic Author Posted 2 years ago · 12th in this Competition arrow_drop_up 0 more_vert @takamichitoda Thank you very much. I learned a lot from your blog and notebook. And it always helps. Thank you for the easy-to-understand explanation! yao Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Congratulations on your solo gold! 🎉🎉 Great work! chumajin Topic Author Posted 2 years ago · 12th in this Competition arrow_drop_up 1 more_vert @syhens Thank you. And congratulations to you too! Gaurav Rawat Posted 2 years ago · 69th in this Competition arrow_drop_up 1 more_vert First of all congrats 🎉 ,I had a small curious question why was language not included as part of the Topic context while it was part of the content context ? chumajin Topic Author Posted 2 years ago · 12th in this Competition arrow_drop_up 1 more_vert @gauravbrills Thank you very much! This is a good question. It's just experimental results. In my case, not including language in the topics slightly improved the score for fold 1. Trushant Kalyanpur Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Congratulations on the solo gold my friend! You are on your way to be competition GM soon! Thanks for the detailed write up. That is a pretty impressive pipeline and a lot of work for a one person team! This competition had so many avenues to explore! chumajin Topic Author Posted 2 years ago · 12th in this Competition arrow_drop_up 1 more_vert @trushk Thank you very much! And congratulations on winning the gold medal. Your team showed usual strength, especially final day! You're right, it was hard work! I'll take a break and then participate in another competition!! ToYou2U Posted 2 years ago · 17th in this Competition arrow_drop_up 1 more_vert Great job,Congrats!   What is the recall rate of the top 50 ? chumajin Topic Author Posted 2 years ago · 12th in this Competition arrow_drop_up 0 more_vert @toyou2u Thank you very much. And congratulations on your promotion to master. If my calculations are correct, it is 0.90792 in all validation data (fold0 : 0.922, fold1 : 0.903, fold2 : 0.916, fold3 : 0.889). My validation data does not include the source. xbustc Posted 2 years ago · 19th in this Competition arrow_drop_up 1 more_vert Congrats for the solo gold!   Great job! chumajin Topic Author Posted 2 years ago · 12th in this Competition arrow_drop_up 0 more_vert @xbustc Thank you, my friend!  I was watching you improve your score quite a bit towards the end! Ori Hanegby Posted 2 years ago · 10th in this Competition arrow_drop_up 2 more_vert Congrats for achieving a solo gold! Thank you for sharing the approach, great writeup. chumajin Topic Author Posted 2 years ago · 12th in this Competition arrow_drop_up 1 more_vert @ohanegby Thank you very much. And congratulations on your solo gold medal and promotion to master as well! I've been aiming for it since being surpassed halfway through, but couldn't catch up! You are strong. Let's meet at a competition somewhere.",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules Jaideep · 18th in this Competition  · Posted 2 years ago arrow_drop_up 14 more_vert 18th place Solution -Thank You ! Many thanks to organizers for setting up this competition which I believe not many competition we will find of this kind.  It was amazing to see 1st rank solution based on mere traditional machine learning quite a number of things to learn from  . I thank all my team members working hard on this competition @iafoss @rohitsingh9990 specially @evgeniimaslov2 whose last 2 weeks efforts put us in top 20. Lot more new things as take away from other Top solution Below is the outline of our solution We used 2 stage solution approach with channel level  5 fold StratifiedKfold (Alighned quite well with LB) Stage1 1) 64 seq length Paraphrase mpnet v2  Trained on Similarity loss (With different Seq l & Pos only +Pos/Neg combination) 2) 128 seq L Roberta L   Trained on Arcface Loss (Pos only samples) Max pos score of all our models on whole dataset was 0.95+ (Top 50) , while Validation fold was 0.85+ Stage2 Ensemble of 1) 64 seq Length 5 fold Paraphrase Mpnet v2  Pretrained on stage1 with Top 50 Neighbors  Trained on Contrastive loss  ( This gave significant boost compared to ReRanker based public approach) ( This boosted score from 0.57 to straight away 0.657 2) 64 Seq length  5 fold Roberta L as classifier trained on same contrastive loss   Top 64 Neighbors from corresponding Roberta model of stage1 3)  256-352 Seq length 5 fold ReRanker trained using public kernel approach Reducing  some of FPs count of Model 1  (This boosted the score by  0.01) from 0.657  to 0.66-0.67 series 4) Finally Light GBM  This took the score to 0.68 series ( a final boost) Micro level Approaches 1) Quite a number Top N selections  made for stage2 based on similarity ranking/ CV boost for Stage2 using these approaches @evgeniimaslov2 can throw some more light on this 2) Light GBM based on prob features (Ranking based)  and other Train features like Kind, category etc  after stage2 classification . This gave boost of around 0.02 to land us to our current score. using light gbm our CV reached to 0.74 to get private lb of 0.721 quite aligned. All in all it was progressive move using above approaches during entire period of competition. Regards Jaideep Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules Frank · 24th in this Competition  · Posted 2 years ago arrow_drop_up 8 more_vert 24th place solution No shakeup! Congrats to all the winners! I've learned a lot from this competition. Can't wait to see the winner solutions!!! For me, this is my first solo silver medal. I've published my inference code here . The image below shows a summary of my approach. TLDR : An ensemble of 3 retrievers using max_length of 64 + top10 neighbors + 6 rerankers using mixed max_length. The reranker ensemble strategy is just a simple average. Retriever candidates: Model Batch size Validation recall@10 sentence-transformers/paraphrase-multilingual-mpnet-base-v2 640 0.79869 sentence-transformers/all-distilroberta-v1 1280 0.78176 xlm-roberta-base 512 0.77429 Reranker candidates: Model max length validation F2 score MoritzLaurer/mDeBERTa-v3-base-mnli-xnli (with further pretraining) 128 0.6567 sentence-transformers/paraphrase-multilingual-mpnet-base-v2 (with further pretraining) 256 0.6448 sentence-transformers/paraphrase-multilingual-mpnet-base-v2 (with further pretraining) 128 0.6376 MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7 128 0.6361 xlm-roberta-large (with further pretraining) 128 0.6581 timpal0l/mdeberta-v3-base-squad2 256 0.6269 Ensemble / 0.6882 (public LB: 0.660; private LB: 0.693) CV setup Basically I followed what was proposed here . Split the topics which are not from \"source\" into 4 folds. The folds were stratified by \"channels\" because the hosts mentioned that the test topics could from seen channels. Put the topics from fold 0 to the validation set. Put all the topics from the \"source\" category, and the topics not from fold 0, to the training set. Use the correlation file to get all the matches of the training topics, and use them as positive pairs for sentence transformers. Retriever training (Stage #1) Use separate encoders for \"topic\" and \"content\" because intuitively their distributions are different. It turned out that it improved the validation recall by 2% or above compared with using one unified encoder. My best retriever is trained with \"paraphrase-multilingual-mpnet-base-v2\", it achieved a recall@10 of ~0.80 . One thing to note is that using longer sequence to train did not give me benefits, so I used 64 in all models. Meanwhile, a large batch size is very important in this stage. It is worth sacrificing the information in longer sequences. With contrastive loss, it was way much worse than MultipleNegativesRankingLoss . I assume I didn't use it in the right way. Retrieve (Stage #2 prep) This is the tricky part. It is difficult to retrieve more possible candidates and introduce less noise in the meantime. Naïvely retrieving more to increase recall will only lead to much longer training time and no better performer. I used 3 retrievers in my ensemble. I retrieved top10 candidates with each one and take the union set. Altogether I've got recall = 0.853 for the validation set. Reranker training (Stage #2) Here I used the same train/val splits for topics as the stage #1. I found that using a pretraining can boost the performance of reranker. Here the pretraining means the retriever training using a unified encoder. And before training the reranker, load the pretrained checkpoint. This way allows the model to gain some domain knowledge. But it could lead to data leakage to some extent. In the final ensemble, I mixed rerankers with and without the pretraining. In order to make the model converge faster and reduce training time, I use a customised balancing sampler, so that during each epoch the numbers of positive and negative pairs are the same. Final thoughts It looks like my pipeline is close to the 3rd solution except that they used top100 neighbours and I only used top10 instead. I would say this is a rather simple pipeline. Their success indicates that big models and rich resources are able to create a simple yet very performant model ! Please sign in to reply to this topic. comment 2 Comments Hotness Ravi Shah Posted 2 years ago · 409th in this Competition arrow_drop_up 1 more_vert Great job @xfffrank ! Thanks for sharing you write up! Frank Topic Author Posted 2 years ago · 24th in this Competition arrow_drop_up 0 more_vert Thank you.",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules Valerio Morelli · 28th in this Competition  · Posted 2 years ago arrow_drop_up 10 more_vert 28th place solution Thanks to the organizers for this interesting competition! I joined one month before the competition ended and am happy that a simple and direct approach got me to 28th place. Stage 1 retriever: Biencoder Teach model to embed related topics and contents closely together, using the following common setup: Embed topic and content using same transformer backbone and mean pooling Compute cosine similarity between all examples in a batch Multiple in-batch negative ranking loss with temperature 0.05 The following modifications gave a significant boost to the model: Modify sampler such that all examples in a batch share the same language. This makes the negative examples much more meaningful. For inference, I only consider candidate contents with matching language. Compute cross-entropy loss row-wise and column-wise (with respect to topics and contents) Penalize scores for the correct class with a margin The best retriever used XLM-RoBERTa (large) as backbone and was trained for 7 epochs. On my CV, it achieves a recall@50 of 91%. Stage 2: Crossencoder For every topic generate 50 candidates using the biencoder. The crossencoder feeds a joint representation of topic + content through a transformer and does binary prediction whether they match or not. The stage 2 model was trained using the out-of-fold predictions of the stage 1 model as input. The only things that worked for me in improving model performance were oversampling the positive class and using differential learning rates. My best stage 2 model was a multilingual BERT trained for 12 epochs achieving 63.6% on my CV (64.3% and 68.3% on public and private LB respectively). Looking at its performance and predictions, I realized that the crossencoder was having a hard time predicting a match from the text representations alone and experimented with GBDT as stage 2 model, but was not able to come up with a better-performing model. Input representation Could not find a input representation that worked better than simple concatenation. Topic representation: Title. Parent title. … Root title. Topic description. Content representation: Title. Description. Text. Sequence length was 128 tokens for both models. Cross validation I used 5 folds split on topics for stage 1, and 3 folds split on non-source topics for stage 2. I had good correlation between CV and LB using this setup. Splitting on channels instead led to unstable CV for me. Probing the public LB revealed that in fact there was a great amount (over 40%) of near-duplicate topics, so I settled in favor of a simple CV setup that gave me good correlation with LB. My evaluation metrics for stage 1 were average precision and recall@N, and directly the competition metric for stage 2. Post processing Topics with no predictions get assigned the nearest neighbor according to biencoder If a new topic shared the same title, parent title, and grandparent title as a seen topic, add the contents of the seen topic to the predictions (tiny boost). Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules Anil Ozturk · 31st in this Competition  · Posted 2 years ago arrow_drop_up 37 more_vert 31st Place Solution First of all, many thanks to the organizing team for the competition! I am just starting to focus on NLP domain and this competition was very educational for me, it also gave me my first solo silver 😀. It included a recommender-system problem and allowed me to practice text-embeddings. But I have reached my limits in creativity towards the end of the competition and tried unnecessary, overkill stuff. I will try to summarize my solution briefly. You can see the whole source-code from my GitHub here . You can also see the inference code of my submission from here . Solution Summary I used sentence-transformers library and the models from HuggingFace . I tried to implement the shared architecture here . The pipeline consists of: Splitting the Data as Train/Val Text Processing Training Sentence-Transformer (Stage 1) Retrieve with kNN using Stage 1 Embeddings Training Cross-Encoder (Stage 2) Inference Splitting the Data as Train/Val I've seen a lot of different approaches on the forum. I also wanted to use the imbalance in language distribution in my approach. I set all the data coming from source as train . For the remaining, I used: CV Scheme: Grouped Stratified K-Fold Folds: 5 (Used only the first) Group: Topic ID Stratifier Label: Language Text Processing Created topic tree Created special tokens for each value language and content kind can take. Created identifier separators for topic title , topic tree , topic description , content title , content description and content text . My final input for the model was like: Topic: [<[language_en]>] [<[topic_title]>] videos [<[topic_tree]>] maths g3 to g10 > maths > g6 > 17. geometrical constructions > perpendicular and perpendicular bisector > videos [<[topic_desc]>] nan Content: [<[language_en]>] [<[kind_exercise]>] [<[cntnt_title]>] level 3: identify elements of simple machine(axle,wheel,pulley and inclined plane etc [<[cntnt_desc]>] nan [<[cntnt_text]>] nan Training Sentence-Transformer (Stage 1) Base Model: AIDA-UPM/mstsb-paraphrase-multilingual-mpnet-base-v2 Sequence Length: 128 Epochs: 50 Batch Size: 128 Warm-Up Ratio: 0.03 Retrieve with kNN using Stage 1 Embeddings I used kNN from RAPIDS and get closest 100 content embedding for each topic embedding using cosine-similarity . Training Cross-Encoder (Stage 2) Base Model: Trained model from Stage 1 Output: Sigmoid Sequence Length: 128 Epochs: 15 Batch Size: 256 Warm-Up Ratio: 0.05 Inference Ran all the steps above sequentially in a single script. Tuned classification threshold on the hold-out validation set to maximize F2-Score. Imputed empty topic rows with the highest scoring content IDs. Didn't Work & Improve Language specific kNN Smaller models Lower sequence length Lower batch-size Union submission blending Please sign in to reply to this topic. comment 7 Comments Hotness Siddharth Sah Posted 2 years ago arrow_drop_up 3 more_vert Congratulations on your 31st place finish and your first solo silver! Your approach using sentence-transformers and HuggingFace models, as well as the thoughtful text processing and train/validation split, is impressive. It's great to see how you've been able to learn and experiment with various techniques in the NLP domain through this competition. Thanks for sharing the details of your solution, and I'm looking forward to seeing more of your work in future competitions. Keep up the great work! Ertuğrul Demir Posted 2 years ago · 129th in this Competition arrow_drop_up 1 more_vert Congrats Anıl, I was too obsessed with most compact solutions in this competition, it's nice to read some actual solutions :) Ravi Shah Posted 2 years ago · 409th in this Competition arrow_drop_up 1 more_vert Congrats and great solution @nlztrk ! Thanks for the explanation and write up! qcqced Posted 2 years ago · 121st in this Competition arrow_drop_up 1 more_vert Congratuations! Thanks for sharing you're amazing code. I learned a lot from your effort, Thanks! Tarik Karakas Posted 2 years ago · 105th in this Competition arrow_drop_up 1 more_vert Amazing explanation, congrats ! Tevfik Erkut Posted 2 years ago · 188th in this Competition arrow_drop_up 1 more_vert Fantastic write-up, thanks and congrats 🎉 @nlztrk Gaurav Rawat Posted 2 years ago · 69th in this Competition arrow_drop_up 2 more_vert Congrats , such a simple , classic and clear pipeline .. Nice topic representation , think we went this way but skipped at the end which didn't work so well for us . Thanks for sharing ur code so we can learn .",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules Moro · 32nd in this Competition  · Posted 2 years ago arrow_drop_up 21 more_vert 32th place solution Thank you to the organizers for the fun competition and everyone who participated. I share my solution. Summary 2-stage: Retrieval (Bi-Encoder) and Re-Ranker (Cross-Encoder) pipeline: below 1. Retrieval input data: topic: title + description + [SEP-Depth] + level + [SEP-context] + context + [SEP-children] + children context: title + description + text + [SEP-Kind] + kind split train/valid: StratifiedGroupKFold (y=channel, group=topic_id) only 1fold model: Bi-Encoder (Sentence-Transformers) loss: NT-Xent loss ( https://arxiv.org/pdf/2002.05709.pdf ) pretrained-model (1) xlm-roberta-base (2) sentence-transformers/paraphrase-multilingual-mpnet-base-v2 tokenizer: add special token ([SEP-Depth] etc.) batch_size: 256, max_len=128 pretrained-model training data Rec@10 Rec@50 f2@10 pub@10 pri@10 xlm-roberta-base train 76.8 91.1 50.3 46.9 46.9 paraphrase-multilingual-mpnet-base-v2 train 78.5 91.5 51.5 47.2 47.4 paraphrase-multilingual-mpnet-base-v2 train+valid 93.3 99.0 62.1 48.9 49.5 2. Select Candidate compute embedding vector by model, and calculate cosine-similarity between all topics and all contents select top50 by cosine-similarity per model-> select duplicate candidates top10 : public=53.4, private=55.4 3. Re-Ranker input data: title + description + [SEP-Depth] + level + [SEP-context] + context + \\\n    [SEP-children] + children + [SEP] + \\\n    title + description + text + [SEP-Kind] + kind content_copy split train/valid: same as stage 1 model: Cross-Encoder loss: BCE loss adversarial-learning: FGM batch_size: 128, max_len=256 thres: 0.1 # model training data local public private 1 xlm-roberta-base train 67.2 61.3 64.1 2 paraphrase-multilingual-mpnet-base-v2 train 67.6 61.8 64.7 3 paraphrase-multilingual-mpnet-base-v2 train+valid 68.0 63.4 66.3 final ensemble (weight=1:1:3) - 69.3 64.4 67.8 Did't work define graph data by topic's structure of curriculum, and train GNN (Link Prediction). But didn't work. use LightGBM in stage 2 (But higher team was using it, so my method was bad…) Thank you for reading. Please sign in to reply to this topic. comment 2 Comments 1 appreciation  comment Hotness Ravi Shah Posted 2 years ago · 409th in this Competition arrow_drop_up 1 more_vert Congrats @moromoromoro ! Interesting solution, thanks for sharing this write up! Appreciation (1) reboot Posted 2 years ago · 413th in this Competition arrow_drop_up 2 more_vert Thank you for sharing",
      "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules calpis10000 · 39th in this Competition  · Posted 2 years ago arrow_drop_up 13 more_vert 39th place solution Thanks to Kaggle and The Learning Agency Lab for this exciting competition, and thanks to kagglers and my great teammate @youjods . Summary Our model is a 2-stage configuration(Retriever and Reranker). We used models downloaded from huggingface and trained with the sentence-transformers library. The following points contributed significantly to the score: Using sentence-transformers/xlm-r-distilroberta-base-paraphrase-v1 for backbone model Using the Retriever model as a backbone of Reranker training Using OnlineContrastiveLoss for Reranker Codes Training Inference CV Strategy We used GroupKGold, which is handled differently depending on category. category=='source' topics were all used for training. The other categories are divided by GroupKFold keyed by channel, and the following data are used for validation. 1 fold topics (as unknown channel topics for train data) Other fold topics sampled same number of above fold (as known channel topics for train data) Valid-scores were calculated for known and unknown channels, respectively. preprocess We referred to @conjuring92 's discussion: Topic Context Matters in Supervised Pipeline topics: channel + language + level + title + description + context(title) + context(description) + children_title content: kind + language + title + description+ text We have cut the title and discussion to some length. The cut length differs between Retriever and Reranker. (Reranker is shorter) Stage1: Retriever We trained models from huggingface using sentence-transformer library, and we used MultipleNegativesRankingLoss. We have tried various backbone models and the following conditions produced the best recall score. backbone: sentence-transformers/xlm-r-distilroberta-base-paraphrase-v1 epoch: 20 batch_size: 128 lr: 2e-5 Recall@100 score resulted in: 0.8745 for whole valid-data 0.93929 for known channel 0.80972 for unknown channel Stage2: Reranker In stage2, top100 nearest contents were extracted for every topic using Reranker model. Then we finetuned Retriever model using sentence-transformer library with OnlineContrastiveLoss. We first trained with simple binary-classification, but OnlineContrastiveLoss boosted the f2-score as following: binary-classification: CV 0.4565, LB: 0.553 OnlineContrastiveLoss: CV 0.5414, LB: 0.619 Not worked Ensemble ensemble improved our validation-score, but worsed LB-score. Other pretrained model (e.g. sentence-transformers/all-MiniLM-L12-v2) LightGBM Reranker Please sign in to reply to this topic. comment 0 Comments Hotness"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Enhance learning by matching K-12 content to target topics The dataset presented here is drawn from the Kolibri Studio curricular alignment tool, in which users can create their own channel , then build out a topic tree that represents a curriculum taxonomy or other hierarchical structure, and finally organize content items into these topics, by uploading their own content and/or importing existing materials from the Kolibri Content Library of Open Educational Resources. An example of a branch of a topic tree is: Secondary Education >> Ordinary Level >> Mathematics >> Further Learning >> Activities >> Trigonometry . The leaf topic in this branch might then contain (be correlated with) a content item such as a video entitled Polar Coordinates . You are challenged to predict which content items are best aligned to a given topic in a topic tree, with the goal of matching the selections made by curricular experts and other users of the Kolibri Studio platform. In other words, your goal is to recommend content items to curators for potential inclusion in a topic, to reduce the time they spend searching for and discovering relevant content to consider including in each topic. Please note that this is a Code Competition , in which the actual test set is hidden. In this public version, we give some sample data drawn from the training set to help you author your solutions. When your submission is scored, this example test data will be replaced with the full test set. The full test set includes an additional 10,000 topics (none present in the training set) and a large number of additional content items. The additional content items are only correlated to test set topics. The training set consists of a corpus of topic trees from within the Kolibri Content Library, along with additional non-public aligned channels, and supplementary channels with less granular or lower-quality alignment. 4 files 891.27 MB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 891.27 MB content.csv correlations.csv sample_submission.csv topics.csv 4 files 21 columns ",
    "data_description": "Learning Equality - Curriculum Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The Learning Agency Lab · Featured Code Competition · 2 years ago Late Submission more_horiz Learning Equality - Curriculum Recommendations Enhance learning by matching K-12 content to target topics Learning Equality - Curriculum Recommendations Overview Data Code Models Discussion Leaderboard Rules Overview Start Dec 15, 2022 Close Mar 15, 2023 Entry Description link keyboard_arrow_up Goal of the Competition The goal of this competition is to streamline the process of matching educational content to specific topics in a curriculum. You will develop an accurate and efficient model trained on a library of K-12 educational materials that have been organized into a variety of topic taxonomies. These materials are in diverse languages, and cover a wide range of topics, particularly in STEM (Science, Technology, Engineering, and Mathematics). Your work will enable students and educators to more readily access relevant educational content to support and supplement learning. (opens in a new tab)\"> Context Every country in the world has its own educational structure and learning objectives. Most materials are categorized against a single national system or are not organized in a way that facilitates discovery. The process of curriculum alignment, the organization of educational resources to fit standards, is challenging as it varies between country contexts. Current efforts to align digital materials to national curricula are manual and require time, resources, and curricular expertise, and the process needs to be made more efficient in order to be scalable and sustainable. As new materials become available, they require additional efforts to be realigned, resulting in a never-ending process. There are no current algorithms or other AI interventions that address the resource constraints associated with improving the process of curriculum alignment. Competition host Learning Equality is committed to enabling every person in the world to realize their right to a quality education, by supporting the creation, adaptation, and distribution of open educational resources, and creating supportive tools for innovative pedagogy. Their core product is Kolibri, an adaptable set of open solutions and tools specially designed to support offline-first teaching and learning for the 37% of the world without Internet access. Their close partner UNHCR has consistently highlighted the strong need and innovation required to create automated alignment tools to ensure refugee learners and teachers are provided with relevant digital learning resources. They have been jointly exploring this challenge in depth for the past few years, engaging with curriculum designers, teachers, and machine learning experts. In addition, Learning Equality is partnering with The Learning Agency Lab, an​ independent nonprofit focused on developing science of learning-based tools and programs for social good, along with UNHCR to engage you in this important process. You have the opportunity to use your skills in machine learning to support educators and students around the world in accessing aligned learning materials that are relevant for their particular context. Better curriculum alignment processes are especially impactful during the onset of new emergencies or crises, where rapid support is needed, such as for refugee learners, and during school closures as took place during COVID-19. Acknowledgments Learning Equality and the Learning Agency Lab would like to thank Schmidt Futures and UNHCR for making this work possible. They also extend their gratitude to UNHCR for the ongoing collaboration in this effort to automate the process of curriculum alignment. This is a Code Competition. Refer to Code Requirements for details. Evaluation link keyboard_arrow_up Submissions will be evaluated based on their mean F2 score. The mean is calculated in a sample-wise fashion, meaning that an F2 score is calculated for every predicted row, then averaged. Submission File For each topic_id in the test set, you must predict a space-delimited list of recommended content_ids for that topic. The file should contain a header and have the following format: topic_id, content_ids t_00004da3a1b2, c_1108dd0c7a5d c_376c5a8eb028 c_5bc0e1e2cba0 c_76231f9d0b5e t_00068291e9a4, c_639ea2ef9c95 c_89ce9367be10 c_ac1672cdcd2c c_ebb7fdf10a7e t_00069b63a70a, c_11a1dc0bfb99\n... content_copy Timeline link keyboard_arrow_up December 15, 2022 - Start Date. March 7, 2023 - Entry Deadline. You must accept the competition rules before this date in order to compete. March 7, 2023 - Team Merger Deadline. This is the last day participants may join or merge teams. March 14, 2023 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up Leaderboard Prizes 1st Place - $ 12,000 2nd Place - $ 8,000 3rd Place - $ 5,000 4th Place - $ 5,000 Efficiency Prizes 1st Place - $ 12,000 2nd Place - $ 8,000 3rd Place - $ 5,000 Please see Efficiency Prize Evaluation for details on how the Efficiency Prize will be awarded. Winning a Leaderboard Prize does not preclude you from winning an Efficiency Prize. Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time Internet access disabled Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors. Efficiency Prize Evaluation link keyboard_arrow_up Efficiency Prize We are hosting a second track that focuses on model efficiency, because highly accurate models are often computationally heavy. Such models have a stronger carbon footprint and frequently prove difficult to utilize in real-world educational contexts. We hope to use these models to help educational organizations that have limited computational capabilities. For the Efficiency Prize, we will evaluate submissions on both runtime and predictive performance. To be eligible for an Efficiency Prize, a submission: Must be among the submissions selected by a team for the Leaderboard Prize, or else among those submissions automatically selected under the conditions described in the My Submissions tab. Must be ranked on the Private Leaderboard higher than the sample_submission.csv benchmark. Must not have a GPU enabled. The Efficiency Prize is CPU Only. All submissions meeting these conditions will be considered for the Efficiency Prize. A submission may be eligible for both the Leaderboard Prize and the Efficiency Prize. An Efficiency Prize will be awarded to eligible submissions according to how they are ranked by the following evaluation metric on the private test data. See the Prizes tab for the prize awarded to each rank. More details may be posted via discussion forum updates. Evaluation Metric We compute a submission's efficiency score by: Efficiency = 1 Benchmark − max F2 F2 + 1 32400 RuntimeSeconds where F2 is the submission's score on the main competition metric , Benchmark is the score of the benchmark sample_submission.csv , max F2 is the maximum F2 of all submissions on the Private Leaderboard, and RuntimeSeconds is the number of seconds it takes for the submission to be evaluated. The objective is to minimize the efficiency score. During the training period of the competition, you may see a leaderboard for the public test data in the following notebook, updated daily: Efficiency\nLeaderboard . After the competition ends, we will update this leaderboard with efficiency scores on the private data. During the training period, this leaderboard will show only the rank of each team, but not the complete score. Citation link keyboard_arrow_up Alex Franklin, Allan otodi , Carine Diaz, Jamie Alexandre, José L. Redrejo Rodriguez, Lauren Lichtman, Maggie, Natalie Rambis, Ryan Holbrook, Scott Crossley, and ulrichboser. Learning Equality - Curriculum Recommendations. https://kaggle.com/competitions/learning-equality-curriculum-recommendations, 2022. Kaggle. Cite Competition Host The Learning Agency Lab Prizes & Awards $55,000 Awards Points & Medals Participation 9,596 Entrants 1,308 Participants 1,057 Teams 25,877 Submissions Tags Education Primary and Secondary Schools Tabular F-Score Beta (Micro) Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Efficiency Prize Evaluation Citation"
  },
  {
    "competition_slug": "nfl-player-contact-detection",
    "discussion_links": [
      "/competitions/nfl-player-contact-detection/discussion/391635",
      "/competitions/nfl-player-contact-detection/discussion/391740",
      "/competitions/nfl-player-contact-detection/discussion/392182",
      "/competitions/nfl-player-contact-detection/discussion/391761",
      "/competitions/nfl-player-contact-detection/discussion/392290",
      "/competitions/nfl-player-contact-detection/discussion/392402",
      "/competitions/nfl-player-contact-detection/discussion/391609",
      "/competitions/nfl-player-contact-detection/discussion/391792",
      "/competitions/nfl-player-contact-detection/discussion/392162",
      "/competitions/nfl-player-contact-detection/discussion/394302",
      "/competitions/nfl-player-contact-detection/discussion/392226",
      "/competitions/nfl-player-contact-detection/discussion/391703"
    ],
    "discussion_texts": [
      "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago Late Submission more_horiz 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Overview Data Code Models Discussion Leaderboard Rules nvnn · 1st in this Competition  · Posted 2 years ago arrow_drop_up 136 more_vert 1st place solution Thanks to NFL and Kaggle for hosting this interesting competition. My approach comprises three main components A weak xgb model to remove easy negative samples A CNN to classify contact A xgb model to post-process the output. Since my xgb preprocessing was not really good compare to other teams (CV ~ 0.72),I will only elaborate on my CNN and post-processing method in this write-up. 1. 3D CNN for Video Classification 1.1 Input generator I separate the modeling and training of player-player (PP) and player-ground (PG) contacts. The PP model is trained using input from three sources, namely endzone video, sideline video, and tracking data. On the other hand, the PG model is trained using input from only two sources, namely endzone video and sideline video. Notably, including tracking data does not result in improved performance for the PG model. 1.1.1 Input generator for PP model The endzone and sideline videos are processed similarly. Firstly, I extract 18 images from neighboring frames, namely {frame[-44], -37, -30, -24, -18, -13, -8, -4, -2, 0, 2, 4, 8, 13, 18, 24, 30, frame[37]}. The frame[-44] represents 44 frames prior to the current sample's estimated frame. This sampling technique enables the model to observe more frames close to the estimated frame. Next, I mask the players' heads in contact with a black or white circle to guide the model's attention to the relevant players. Rather than using an additional channel, I mark the players' heads directly into the image. I made this decision to maintain the input's 3-channel format, which maximizes the utilization of the pretrained weight file. Finally, I crop each image around the players' contact area using a crop size of 10 times the mean helmet box size within the specified frame range. To enable the tracking data to be stacked with images from the endzone and sideline, I simulate the tracking data as images. To accomplish this, I use the OpenCV cv2.circle function to plot each player's position in a specific step on a black background. I assign two different colors to represent the two teams, and players in contact are depicted with bigger and brighter circles (radius is 5, and pixel value is 255), while background players are depicted with smaller and darker circles (radius is 3, and pixel value is 127). By integrating this information into the input, the model can learn the interaction of all players from a bird's eye view. The input to the PP model is displayed in the GIF below. 1.1.2 Input generator for PG model The endzone and sideline videos are processed similarly to the PP model, with the exception that the PG model uses a longer input sequence of 23 neighboring frames, ranging from [-54, -48, -42, -36, -30, -24, -18, -13, -8, -4, -2, 0, 2, 4, 8, 13, 18, 24, 30, 36, 42, 48, 54]. The PG model does not include simulated tracking images as they do not improve the PG CV score. Unlike the PP model, I can use a longer sequence of images in the PG model because the tracking images are not included. In the PP model, the maximum sequence length that can fit into my GPU is 18 images. 1.2 Model Given that the input appears to resemble an action classification task rather than a standard 3D classification, I opted to use an action recognition model to address this problem. After reviewing the mmaction2 repository , it became clear that the CSN series delivered the most impressive results in the Kinetics-400 dataset. As a result, I selected resnet50-irCSN and fine-tuned it for this particular task. 1.3 Training During training, I apply the following augmentations to the endzone and sideline images and randomly swap them. As for the tracking images, I only use horizontal and vertical flips as augmentations. base_aug = [\n        A.RandomResizedCrop( always_apply = False , p =1.0, height =cfg.img_size, width =cfg.img_size, scale=(0.7, 1.2), ratio=(0.75, 1.3), interpolation =1),\n        A.OneOf([\n            A.RandomGamma(gamma_limit=(30, 150), p =1),\n            A.RandomBrightnessContrast( brightness_limit =0.2, contrast_limit =0.3, p =1),\n            A.ColorJitter( brightness =0.2, contrast =0.2, saturation =0.2, hue =0.2, p =1),\n            A.HueSaturationValue( hue_shift_limit =20, sat_shift_limit =30, val_shift_limit =20, p =1),\n            A.CLAHE( clip_limit =5.0, tile_grid_size=(5, 5), p =1),\n        ], p =0.6),\n        A.HorizontalFlip( p =0.5), \n        A.ShiftScaleRotate( shift_limit =0.0, scale_limit =0.1, rotate_limit =15, interpolation =cv2.INTER_LINEAR, border_mode =cv2.BORDER_CONSTANT, p =0.8),\n        A.Cutout( max_h_size =int(50), max_w_size =int(50), num_holes =2, p =0.5),\n    ]\n\ncfg.train_transform = A.ReplayCompose(base_aug) content_copy I used a linear scheduler for the learning rate and trained the model for one epoch. In the final submission, I trained the model using all available data with 4 seeds. 2. XGB Postprocessing I employed a simple xgb model to combine the predictions of pre-xgb and cnn. Through experimentation, I discovered that the optimal feature for post-processing in PP and PG models slightly differs. 2.1 PP postprocessing First, I calculated an ensemble probability from the CNN and preprocessing xgb model as follows: prob = 0.2pre_xgb_prob + 0.8cnn_prob. Then, I used the probability from the 20 neighboring steps as features for the xgb model, i.e., {prob(-10), prob(-9), …, prob(0), prob(1), …, prob(9)}, where prob(-10) represents the probability of the same pair of players in the prior 10 steps. This postprocessing method improved my PP CV score by approximately 0.005. 2.2 PG postprocessing I calculated an ensemble probability from the CNN and preprocessing xgb model as follows: prob = 0.15pre_xgb_prob + 0.85cnn_prob. The feature to xgb model are The ensemble probability from the 30 neighboring steps {prob(-15), prob(-14), …, prob(0), prob(1), …, prob(14)}, The pre_xgb_prob and cnn_prob from the 20 neighboring steps. This postprocessing method improved my PG CV score by approximately 0.04. P/S. Thanks chatGPT for making my explanation better!! Please sign in to reply to this topic. comment 43 Comments 4 appreciation  comments Hotness Camaro Posted 2 years ago · 4th in this Competition arrow_drop_up 3 more_vert Congratulations on a strong finish and solo winning this hard competition! Different interval frame sampling and concatenating rgb image and tracking image are beyond my imagination, but seems surprisingly works well. Awesome!! nvnn Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thank you. Congratulations on your strong finish and becoming GM. well done Camaro Posted 2 years ago · 4th in this Competition arrow_drop_up 0 more_vert @nvnnghia Thanks for sharing code too, that's definitely a masterpiece!! Quick question, when I look through your code, I found that the dropout rate is set to 0.5, which seems too large for me. Is it natural for you? Have you experienced it working for other tasks too or did it specifically work for this task? Thanks, nvnn Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 4 more_vert dropout 0.5 is natural for me. At times, I utilize a larger dropout rate, such as 0.6 or 0.7. If my model appears to overfit easily, one of the initial experiments I aim to conduct is to increase the dropout rate and adjust the intensity of data augmentation. David Del Río Posted 2 years ago arrow_drop_up 0 more_vert What do you mean by adjusting the intensity of data augmentation? Thanks in advance!! DanielPuente Posted 2 years ago · 810th in this Competition arrow_drop_up 1 more_vert Congratulations. Very interesting way of solving the tournament, congratulations. I have a question about how to train the net. · Have you used a custom loss function to train the resnet? Thank you very much Majid Ahmad Khan Posted 2 years ago arrow_drop_up 1 more_vert Congratulations on your achievement on Kaggle! I'm really impressed with your work and wanted to let you know. By the way, if you have a moment, I would really appreciate it if you could take a look at my profile and give it an upvote if you find my work interesting as well. Sitraka_Forler Posted 2 years ago arrow_drop_up 1 more_vert Congrats on winning @nvnnghia ! And thanks for sharing your method wouldn't have though of it… ayushsingh05 Posted 2 years ago arrow_drop_up 1 more_vert Congratulations on winning and also thanks for sharing the approach Ravi Shah Posted 2 years ago arrow_drop_up 1 more_vert Congrats on winning @nvnnghia ! Thanks for sharing this write up! Chris Deotte Posted 2 years ago arrow_drop_up 1 more_vert Awesome work Nvnn. Congratulations on first place solo cash gold finish! Bilzard Posted 2 years ago · 19th in this Competition arrow_drop_up 1 more_vert @nvnnghia Congratulation. About 1st stage, what is the performance of negative sampling? Can I ask the recall and sample reduction ratio? e.g) For my case, player-player contact: keeping recall 0.992, reduced ~50% of samples player-ground contact: keeping recall 0.992, reduced ~75% of samples nvnn Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert thanks. After filtering, around 1.4 million samples remained for the PP model, with a recall rate of 99.3%. For the PG model, around 130k samples remained with a recall rate of over 97%. MS-05 ざこ Posted 2 years ago · 66th in this Competition arrow_drop_up 1 more_vert Congratulation! emoji_people Ahmed Samir Posted 2 years ago · 35th in this Competition arrow_drop_up 1 more_vert Really brilliant! Congrats on getting the 1st position 🙌 Ethan Posted 2 years ago · 10th in this Competition arrow_drop_up 1 more_vert Congrats and thanks for sharing your greate solution! How much does post-processing improve your cv score? nvnn Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert thanks. pp boost my cv around +0.015. I didn't check LB nadhir hasan Posted 2 years ago · 863rd in this Competition arrow_drop_up 1 more_vert congratulations!, you idea is amazing. may i know which algorithm did you used to find the bbox to crop the area. are you used YOLO or something else? nvnn Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 0 more_vert thanks. I used provided baseline helmet boxes emoji_people Dewei Chen Posted 2 years ago · 35th in this Competition arrow_drop_up 1 more_vert Congrats! Could you please tell us your Hardware device for training? and how much GPU RAM size for your model? nvnn Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 3 more_vert 1x RTX 3090 24Gb VRAM ( ͡° ͜ʖ ͡°) Posted 2 years ago arrow_drop_up 1 more_vert wow congrats KhanhVD Posted 2 years ago · 64th in this Competition arrow_drop_up 1 more_vert Congrats bro !!! Waiting for more detail from your solution. arutema47 Posted 2 years ago · 7th in this Competition arrow_drop_up 1 more_vert Awesome pipeline, thanks for sharing! Do you have any takes on the shake up? I think you were the only gold medal player who got PB>LB nvnn Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert To be honest, I'm unsure. From what I've observed, several teams have achieved a high public score within a short inference time, so I guess that that they might be unintentionally filtering out too many true positives from their private test data based on their CV score and public leaderboard feedback. In contrast, I used a filtering threshold that was five times lower than in my CV score to ensure that I didn't miss out on too many true positives in my submission. Psi Posted 2 years ago · 2nd in this Competition arrow_drop_up 2 more_vert I am quite confident that threshold filtering is not the reason. We tried a few different thresholds and both public and private behaves as expected for them for us. HongCheng Posted 2 years ago · 59th in this Competition arrow_drop_up 1 more_vert Congratulation! waiting for more sharing Dmytro Poplavskiy Posted 2 years ago · 3rd in this Competition arrow_drop_up 2 more_vert Congrats on winning the competition! Rob Mulla Competition Host Posted 2 years ago arrow_drop_up 2 more_vert Congrats @nvnnghia on your winning solution! I look forward to learning more about it. The idea of including the NGS data as part of the input image to your CNN is really clever. I have a few initial questions: How did you determine the frames you used {frame[-44], -37, -30, -24, -18, -13, -8, -4, -2, 0, 2, 4, 8, 13, 18, 24, 30, frame[37]} - was this decided through experimentation or intution? What made you select the resnet50-irCSN as your backbone? Did you have any succsess with other architectures? How did you handle cases where helmet boxes are not be seen for both players in sideline/endzone views? Did you only predict if both players were seen in both views? In your postprocessing step, you say you combined the 1st stage XGB and CNN outputs like this: prob = 0.2pre_xgb_prob + 0.8cnn_prob . Is there any reason you did not use pre_xgb_prob and cnn_prob directly as features to the postprocessing XGB model? Thanks again for your solution write up! nvnn Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thanks Rob. I have just added more detail to my writeup to make it more clear based on your questions. How did you determine the frames you used {frame[-44], -37, -30, -24, -18, -13, -8, -4, -2, 0, 2, 4, 8, 13, 18, 24, 30, frame[37]} - was this decided through experimentation or intution? The decision on the frame sampling was based on both intuition and experiments. Initially, I used an equal gap between frames such as […, 8, 4, 0, 4, 8, …]. However, I realized that the model should see more images near the estimated frame to improve performance, so I changed the sampling frames accordingly. It may seem strange that there is no frame[44], but this is because I pre-generated all inputs and saved them to disk for faster data loading (frame[44] is in my pre-generated data). However, a sequence of 19 images caused my GPU to run OOM, so I simply removed the last image (frame[44]) to avoid this issue. What made you select the resnet50-irCSN as your backbone? Did you have any succsess with other architectures? Given that the input appears to resemble an action classification task rather than a standard 3D classification, I opted to use an action recognition model to address this problem. After reviewing the mmaction2 repository, it became clear that the CSN series delivered the most impressive results in the Kinetics-400 dataset. As a result, I selected resnet50-irCSN and fine-tuned it for this particular task. I did tried 2.5D model, 3D model and other action recognition model such as slowfast, but CSN give me best CV score. How did you handle cases where helmet boxes are not be seen for both players in sideline/endzone views? Did you only predict if both players were seen in both views? I use a black image for those frames. In your postprocessing step, you say you combined the 1st stage XGB and CNN outputs like this: prob = 0.2pre_xgb_prob + 0.8cnn_prob. Is there any reason you did not use pre_xgb_prob and cnn_prob directly as features to the postprocessing XGB model? Thank you for the question. I have updated my post-processing part in the write up to explain this. Thank you once again for organizing this fascinating NFL competition series. I had the opportunity to participate in all three challenges, and I thoroughly enjoyed the experience. Rob Mulla Competition Host Posted 2 years ago arrow_drop_up 0 more_vert Thanks for the answers. That really helps. Congrats again. Heroseo Posted 2 years ago · 183rd in this Competition arrow_drop_up 2 more_vert Wow, Congrats on solo 1st place! It's a really cool, great job. Leon Posted 2 years ago · 137th in this Competition arrow_drop_up 2 more_vert Congrats! Great idea to put tracking data into CNNs. How much will performance drop if you don't add it? nvnn Topic Author Posted 2 years ago · 1st in this Competition arrow_drop_up 2 more_vert I added it when my cv were still low (~0.73) and it boosted my cv to 0.76. I don't know how much it contributed to my final model (cv 0.79+). Psi Posted 2 years ago · 2nd in this Competition arrow_drop_up 0 more_vert Is your 0.79+ CV on a single fold or full oof? 4 more replies arrow_drop_down SFR.LUX Posted 2 years ago arrow_drop_up 0 more_vert Bro CV hain't izy … mLiammm Posted 2 years ago arrow_drop_up 0 more_vert Appreciate all the effort you took here! Thanks for sharing Krishna Posted 2 years ago arrow_drop_up 0 more_vert Congratulations on your 1st place solution in the NFL and Kaggle competition! Your approach is well-structured and appears to be very effective. Here are some potential improvements you could consider just for fun and experimentation: Provide more detail on your xgb preprocessing method. While you mention that it was not as effective as the other teams' approaches, it could still be helpful to understand your thought process and what you tried. Consider testing other CNN architectures to compare their performance against the resnet50-irCSN model you used. This could help to identify which models are more effective for this type of task. Experiment with different augmentation techniques during training. Although you included several augmentations, there may be others that could further improve your model's performance. Consider using an ensemble of models. Ensembling multiple models can help to reduce the risk of overfitting and improve the overall accuracy of the predictions. Overall, great job on your winning solution!🎉 Tony Posted 2 years ago arrow_drop_up 0 more_vert Congratulations. Do you run inference for endzone and sideline respectively? If so ,how do you combine they.",
      "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago Late Submission more_horiz 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Overview Data Code Models Discussion Leaderboard Rules Pascal Pfeiffer · 2nd in this Competition  · Posted 2 years ago arrow_drop_up 78 more_vert 2nd place solution - Team Hydrogen Thank you for another great NFL challenge! As the previous NFL competitions it was well prepared and had quick feedback cycles anytime that the community had questions. We would like to highlight @robikscube , one of the hosts, who even supplied a strong tabular baseline to get started. Validation The test data is rather small compared to the large training set and only consists of 61 plays. Thus, local validation becomes even more important than usual. To evaluate our models, we used Stratified Group KFold cross validation on the game_key and public LB usually followed any local CV improvements with only a small random range of a few points and with blends being a bit more stable than single models (5 folds or a handful of fullfits). Our best local CV was 0.807 for the blend including 2nd stage and about 0.802 for a single model including 2nd stage. Models and architecture The core ideas and central building blocks of our models are based on our concepts of the previous DFL competition ( https://www.kaggle.com/competitions/dfl-bundesliga-data-shootout/discussion/359932 ) utilizing 2D/3D CNNs capturing temporal aspects of videos. This architecture has already served us well in multiple video sports projects and competitions and also turned out to be highly competitive here. In this competition we found longer time steps to work better and we got our best single model results using a time step of 24 frames, two times in both directions. We crop the region of interest for each potential contact based on helmet box information. In most models, we resize the crop, so that all boxes have about the same size. We concatenate endzone and sideline views horizontally to enable early fusion. Additionally, we encode tracking data directly into the CNN models. This has the main advantage that we can mostly rely on a single stage solution, and are less prone to overfitting on a 2-stage approach with out-of-fold CNN predictions. We step-wise encoded tracking features based on their importance in tabular models. The main architecture of our approach looks like the following: We will now explain in detail each of the channels. We have slight variations of these channels across models in our ensemble, but the core concept is the same. Please note that the order of channels is always the same, and the order is only changed for visual clarity in above architecture visualization, also only showing three channels, while our models use mostly five. Frames 552, 600 and 648 show the first channel in the foreground, while frame 576 shows the second frame, and 624 the fifth frame. First channel The first channel depicts the region of interest of the potential contact only using the grayscale image. For each view, we take the center of the two (or one) boxes and then crop a total rectangle of width 128 and height 256. We then put both views next to each other resulting in a 256x256 input size. For most of our models we try to keep the aspect ratio based on box information and crop more information downwards than upwards to better capture the full body of players. Second channel Here we put a mask of the boxes to allow the model to clearly learn which players it should try to predict the contact for. We mask the two boxes with a value of 255. If there is only one box, or if there is a ground contact, we only mask this one box. We additionally mask all other boxes in this crop with 128. Third channel The most important feature is the distance between two players. The CNN model itself can only learn the distance between players to some degree. So in this channel we directly decode the distance as derived from tracking information. Conveniently, there is a nice cutoff at around 2 yards where basically no contacts are present any longer. So we just multiply the distance by 128, giving us values between 0 and 255 that we encode in this channel. Fourth channel A very important feature was whether both players are from the same team. So here we just encode 255 if both are from the same team, and 128 otherwise. Fifth channel Finally, we saw that distance traveled of players from the last time point is helpful in tabular models. So similar to distance between players, we encode this feature separately for both players, or one in case of ground attack. For all tracking feature channels, we stick to uint8 encoding which means we lose some precision for the features, but it helps with overfitting to it and can be seen as a binning between 256 bins similar to what GBM models do. The great benefit of encoding these features is that the CNNs can learn all the spatial and temporal information of such tracking features directly. As the 2D backbone, we used tf_efficientnetv2_s.in21k_ft_in1k and tf_efficientnetv2_b3 architecture and pre-trained weights from the timm library. We train all our models for 4 epochs and cosine schedule decay and AdamW optimizer. Checkpoints are always on last epoch. Augmentations Specifically mixup proved to be very useful in preventing quick overfitting. While it may appear counterintuitive to work well with the encoded feature channels, it likely acted as a good regularization. During training, we randomly shifted the image frame within a range of +-3 frames to the closest matching frame calculated from the current step. Furthermore, we used a small shift of +-1 for a subset of the model as test time augmentation in the ensemble. Tracking and helmet interpolation For the random frame shift augmentation, it was helpful to interpolate the tracking information from 10 Hz to 60 Hz. We tried a few different methods, but simple linear interpolation proved to be sufficient and is robust. We also added missing helmet box information using linear interpolation. While this definitely added some noise and false positives, overall it seemed to have helped catching a few more contacts in very crowded situations. We also use this interpolation for inference in our submissions. Ensemble & Inference Our final ensemble consists of 6 models, and 3 seeds for each of them. All final models were retrained on the full data. We tried to add some diversity by different crop strategies and step sizes. Backbone Description Step size CV tf_efficientnetv2_s.in21k_ft_in1k No scaling of the crops 24 0.7899 tf_efficientnetv2_b3 Slightly zoomed-in crops 24 0.7953 tf_efficientnetv2_b3 Inverted feature channel encoding 24 0.7987 tf_efficientnetv2_b3 No interpolation for boxes of other players 24 0.7989 tf_efficientnetv2_b3 Inverted feature channel encoding 12 (4 times) 0.7988 tf_efficientnetv2_s.in21k_ft_in1k Smaller step size 6 0.7890 We made full use of the recently added kernel with 2 T4 GPUs by parallelizing the pipeline and spawning two threads (1 CPU core for each to preprocess) each covering one half of the plays. All model predictions were averaged and subsequently fed to a stage 2 LGBM model. The final blend has a CV score of around 0.805 before the second stage. Stage 2 We use a LGBM model with only a few carefully selected features including stage 1 ensemble probabilities, nfl_player_id_1 to nfl_player_id_2 distance and their lags. Other notable features are \"step_pct\", encoding the current step based on the play length and normalized X and Y positions on the field. Basically, using the average position of the two players and normalizing to one quarter of the field to prevent overfitting to single plays. In the early stages of the competition, our 2nd stage model gave a great boost in score, specifically after adding the extra tracking features, while in the end the stage 1 predictions were almost on-par, showcasing how the stage 1 CNNs already efficiently learn from the encoded tracking feature channels. Finally, we blend the LGB predictions with the smoothed raw predictions (window of 3) from the ensemble in a 50:50 ratio. Our final solution has a CV score of 0.807, a public LB of 0.796, and a private LB of 0.796, exhibiting strong consistency and generalizability. Huge shoutout to my teammates @philippsinger and @ybabakhin ! Please sign in to reply to this topic. comment 9 Comments Hotness pcullen11 Posted 2 years ago · 82nd in this Competition arrow_drop_up 3 more_vert Congrats.  I appreciate the clear write-up. Rob Mulla Competition Host Posted 2 years ago arrow_drop_up 4 more_vert Great writeup @ilu000 and team (@philippsinger, @ybabakhin ). This is a really elegant solution, congrats on the strong finish. Just a few quick questions that come to mind after reading: I'm trying to understand how the encoding of NGS features worked when you added them to channels 3-5. For these channels all values in the 256x256 image were the same - or did you only apply them to the helmet box areas? Did you experiment at all with modeling player-to-ground separately than player-to-player? I'm surprised a single model was best because of how different these types of contact look visually. How did you handle cases where players were not in view in both or either cameras? Did you only predict when both players could be seen and identified in the video? Did you account for those pairs in the stage 2 model? Did you filter out player pairs with high separation distance before training/evaluation? Am I reading it correctly that your stage 2 model improved the predictions from 0.805 -> 0.807 in your final CV? It's interesting that interpolating the NGS data to 60Hz was beneficial, and was a smart idea to interpolate the helmet boxes. Also interesting that mixup augmentation helped. Thanks again for sharing! Looking forward to discussing more. Psi Posted 2 years ago · 2nd in this Competition arrow_drop_up 2 more_vert Thanks @robikscube ! I'm trying to understand how the encoding of NGS features worked when you added them to channels 3-5. For these channels all values in the 256x256 image were the same - or did you only apply them to the helmet box areas? We only applied them to the helmet box areas. But we also tried adding them to all pixels, and it did not really make a difference, as the model has the helmet channel anyways. It only makes a difference if both boxes exhibit different features, such as for distance travelled. Did you experiment at all with modeling player-to-ground separately than player-to-player? I'm surprised a single model was best because of how different these types of contact look visually. We tried combined training and then finetuning separately, and it was not helpful. In general, such models can differentiate between these two types of contacts quite well, they can learn whether there is one box or two boxes marked, as well as how feature channels look like. We also checked the scores separately from time to time, and whenever we had a better model, both scores improved. Also blends and thresholds were super consistent across ground and player contacts. Maybe it is also one reason why our results are so stable, I would be curious if there are different number of ground and player contacts in public and private. How did you handle cases where players were not in view in both or either cameras? Did you only predict when both players could be seen and identified in the video? Did you account for those pairs in the stage 2 model? As we concatenate both views together, just having them in one view is not ideal, but not an issue, as the model can then just look at the other view. That's an advantage of the early fusion. Otherwise we did not handle it separately, if it cannot be seen at all we are feeding in an empty image. We tried a few things with the third view, but without success. Did you filter out player pairs with high separation distance before training/evaluation? Yes, we filter out distance > 2, except for ground attacks, and set those predictions to zero. Am I reading it correctly that your stage 2 model improved the predictions from 0.805 -> 0.807 in your final CV? Yes, it is only a tiny, but useful, boost in the end. It was very helpful early on, but the better we incorporated features into first stage, the more redundant it became. But for production one could absolutely consider ignoring it. Chris Deotte Posted 2 years ago arrow_drop_up 2 more_vert Congratulations! Great job guys! 정유석 Posted 2 years ago · 61st in this Competition arrow_drop_up 2 more_vert congratulations and thanks for sharing the solution! Tariq Mahmood Posted 2 years ago arrow_drop_up 0 more_vert Excellent work and execution. Congrats! HongCheng Posted 2 years ago · 59th in this Competition arrow_drop_up 0 more_vert thankyou for your great sharing and well wirteup! I have a question. what is \"shift the image frame\" in the augmentation part? Pascal Pfeiffer Topic Author Posted 2 years ago · 2nd in this Competition arrow_drop_up 1 more_vert With \"shift the image frame\", we refer to an augmentation where we shift all input frames by X frames from the closest match that was calculated based on labels that were given in 10 Hz. We have video data in 60 Hz, so about 6 frames (±3 frames) can get the same label (nearest match from the given labels in 10 Hz). Marc Burn Posted 2 years ago arrow_drop_up 0 more_vert V interesting",
      "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago Late Submission more_horiz 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Overview Data Code Models Discussion Leaderboard Rules Dmytro Poplavskiy · 3rd in this Competition  · Posted 2 years ago arrow_drop_up 37 more_vert 3rd place solution, single stage approach I'd like to thank the I'd like to thank organisers for a very interesting challenge (especially @robikscube for providing very useful answers and helping teams). It was interesting to participate. Overview The approach is single-stage, trained end-to-end with a single model executed per player and step interval (instead of per pairs or players) and predicting for all input steps range the ground contact for the current player and contact with 7 nearest players. The model has a video encoder part to process input video frames and a transformer decoder to combine tracking features and video activations. Video encoders The video encoders used a number of input video frames around requested steps and produced activations at corresponding steps at downsampled resolution, usually for 16 steps with corresponding 96 frames using every second frame for input. I used a few different models for video encoders: 2d imagenet pretrained models + 3d Conv layer (credits to the Team Hydrogen solution of one of previous competitions). 3 input frames around the current step are converted to grayscale and used as an input to 2d model, with the results combined using 3d conv. Usually larger models performed better for me, with the best performing model based on the convnext large backbone. Other Convnext based models or DPN92 also worked ok. 2d imagenet pretrained models + TSM, with the color inputs for every 2nd or 3rd frame and TSM like activation exchange between frames before every convolution. Worked better with smaller models like convnext pico or resnet 34 (would probably work better with larger models if the TSM converted model were pretrained on video tasks). 3D/Video models like CLIP-X (X-CLIP-B/16 was the second best performing model) or the Video Swin Transformer (performed okeish but not included in the final submission). Video frames were cropped to 224x224 resolution with the current player's helmet placed at the center/top part of the frame and scaled so the average size of helmets in surrounding frames would be scaled to 34 pixels. I applied augmentations to randomly shift, scale, rotate images, shift HUV, added blur and noise. For video model activations (at the 32x downsampled 7x7 resolution) I added the positional encoding and learnable separate sideline / endzone markers. Optionally the video activations may be encoded using transformers per frame in a similar way as done in DETR but I found it has little to no impact on the result. Transformer player features / video activations decoder The idea is to use attention mechanisms to combine the players features with other surrounding players information and to query the relevant parts of the images. For particular player and step, I selected the current player features for surrounding -7..+8 steps and for every step I selected up to 7 nearest players within 2.4 yards, so in total 16 steps * (7+1) players inputs. For every player/step input I used the following features, added together using per feature linear transformation to match the transformer features dim: position encoding for the helmet pos on the sideline and endzone video, if within 128 pixels from the crop. is it visible on sideline and endzone frames pos encoding for the step number is player the current selected player is player from the same team as the current player or not player position (not xy but the role from the tracking metadata) speed over +- 2 frames signed acceleration over +- 2 frames distance to the current player, both values and one hot encoding over +- 2 frames relative orientation, of the player relative to player-player0 and of player0 relative to player, encoded as sin and cos over +- 2 frames for visible helmets, I also added the activations from the video at the helmet position directly to player features. The idea was - it's most likely relevant and may help to avoid using the attention heads for the same task, but I found no difference in the final result. Player/step features are used as inputs/targets for a few iterations of transformer layers: For all step/player input, I applied the transformer decoder layer with the query over video activations from the same step. For all step/player inputs I applied the transformer encoder with the self attention over all  players/steps: # video shape is HW* 2 x T*B x C\n        # player_features shape is P, T, B, C\n        # where P - players, T - time_steps, B - batch, C - features, HW - video activations dims\n        x = player_features for step in range(self.num_decoder_layers):\n            x = x.reshape(P, T*B, C)  # reshape to move time steps to batch to use attention only over the current step\n            x = self.video_decoders[step](x, video)\n\n            x = x.reshape(P*T, B, C) # attention over all players/steps\n            x = self.player_decoders[step](x) content_copy I tested with the number of iterations between 2 and 8 and the results were comparable, so I used 2 iterations for most of models. Data pre-processing Mostly to smooth the predicted helmets trajectory, smoothed the prediction to find and remove outliers and interpolated/extrapolated. During the early test the impact on the performance was not very large, so not conclusive. Training For training I selected all players and steps with helmet detected on at least one video (so model would have the tracking features for a few steps before or after the player was visible for the first/last time). I have not excluded any samples using other rules. I used the AdamW optimiser with quite a small batch size of 1 to 4 and CosineAnnealingWarmRestarts scheduler with the epoch size of 1024-2048 samples, trained for 68 epochs. It takes about 6-10 hours to train a single model on 3090 GPU. I evaluated model every time the scheduler reaches the min rate at epochs 14, 36 and 68. I used the BCE loss with slight label smoothing of 0.001..0.999 (it was a guess, I have not tuned hyperparameters much). I added aux outputs to the video models to predict if the current player has contact with other players or ground and heatmap of other player helmets with contacts, but the impact on the score was not very large. Prediction The prediction is very straightforward, for model with the input interval of 11 or 16 steps I run it with the smaller offset of 5 steps to predict over the overlapped intervals for every player. predictions = defaultdict(list)  # key is (game, step, player1, player2) Every prediction between the current and another player, it's added to the list at the dictionary key (gameplay, step, min(player0, player), max(player0, player)) and all predictions are averages. Usually predictions for the pair of players at a certain step would include predictions with each player as the current one and a few step intervals when the current step is closer to the beginning, middle and end of the intervals. When ensembles multiple models, their predictions are added to the same predictions dictionary, with better models added 2-3 times to increase their weight. In total, I used 7 models for the best submission. Individual models performance Video model type, backbone Notes Private LB score Convnext large, 2D + 3D conv 16 steps/96 frames, skip 1 frame. 0.7915 Convnext base, 2D + 3D conv 16 steps/96 frames, skip 1 frame. 0.786 DPN92, 2D + 3D conv 16 steps/96 frames, skip 1 frame. 0.784 X-CLIP-B/16 11 steps/64 frames, skip 1 frame. 0.791 X-CLIP-B/32 11 steps/64 frames, skip 1 frame. 0.784 Convnext pico, TSM 63 steps/384 frames, skip 2 frames. 0.788 Convnext pico, 2D + 3D conv 64 steps/384 frames, skip 2 frames. Local CV slightly worse than TSM 2 best models ensemble Convnext large and X-CLIP-B/16, 0.7925 6 models ensemble Without DPN92, re-trained on full data with original helmets 0.7932 6 models ensemble Without DPN92, re-trained on full data with fixed helmets 0.7934 7 models ensemble Convnext large  added with weight 3 and X-CLIP-B/16 with weight 2. Models trained on different folds. 0.7956 What did not work Training Video Encoder model using aux losses before training transformer decoders. Video Encoder overfits. Adding much more tracking features to player transformer inputs. When added the history over larger number of steps for each player input, the transformer encoder overfits. Larger models with TSM Fix players/helmets assignment in the provided baseline helmets prediction. On some folds the impact was negligible, on some the score has improved by ~ 0.005 even without re-training models. On the private LB the score was similar with and without helmets fixed. One submitted model was using the original data pre-processing, another using more complex pipeline with helmets re-assigned. Local CV challenges To check for possible issues with models generalisation, I decided to split to folds using the sorted by game play list of games, with the first 25% of games assigned to fold 0 validation fold and so on. I found to have not only the difference between folds in score, but models/ideas performing well on one fold may work much worse on another. For example, I found on the fold 2, the models with the very large receptive field over time/steps (384 steps, over 6 seconds,  convnext pico based models in the submission) performed by about 0.008 better than the best larger models, while the score fo such models was by the similar 0.007 worse on the fold 3. All this made the local validation much more challenging and harder to trust. Taking into account the private dataset is even smaller than every fold, I expected to see a significant shakeup. Player helmets re-assignment Since it was not part of the best submission, added as a separate post: https://www.kaggle.com/competitions/nfl-player-contact-detection/discussion/392392 Instead of the data pre-processing described above, I used the estimated tracking -> video transformation to interpolate/extrapolate missing helmets information. The best result was when I discarded the first or the last predicted helmet position and extrapolated by 8 steps maintaining the difference with the position predicted from tracking and tracking->view transformation. The submission source is available at https://www.kaggle.com/dmytropoplavskiy/nfl-sub-place3 Please sign in to reply to this topic. comment 4 Comments Hotness Rob Mulla Competition Host Posted 2 years ago arrow_drop_up 1 more_vert Really great solution @dmytropoplavskiy - and congrats on the result! I'll probably need to read this again a few times before I can understand exactly how the archetecture works 😄. It's really interesting how your model predicted per player instead of per pair. Did you decide that using up to the 7th closest player was sufficient to capture any contact? Thats honestly slightly more than I'd expect. I'm not clear on how the model was able to identify which of surrounding players in the video were associated with the player tracking (NGS) features that you provided the decoder. Did you add any additional masking to the images or did the model learn these relationships on it's own? The helmet imputation is a clever idea. Did you use any of the helmet bounding box data in the model itself other than identifying the player's helmet to predict for. Also, how did you handle when helmets were not visible in either camera? Thanks for sharing your writeup. Dmytro Poplavskiy Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Hi, thank you again for organizing a very interesting competition, it was a pleasure to participate. It's really interesting how your model predicted per player instead of per pair. Did you decide that using up to the 7th closest player was sufficient to capture any contact? Thats honestly slightly more than I'd expect. I checked the distribution of Nth nearest player with contact (calculated for both players in the contact pair): Nearest player num number of contacts 1 69207 2 17584 3 5244 4 2000 5 785 6 271 7 135 8 59 9 22 10 23 11 15 12 14 13 8 14 12 15 37 When I checked contacts only within the distance of 2.4 (edited/fixed): Nearest player num number of contacts 1 69177 2 17569 3 5222 4 1992 5 768 6 263 7 119 8 48 9 15 10 12 11 10 12 4 13 1 Since the contact prediction is averaged when evaluated from both players of view, some (likely most or even all) contacts would still be checked. For example if player2 is 8th nearest player for player1 in contact, player1 may be the 5th nearest player for player 2, so the contact would still be evaluated from player2 point of view. I have not tested the model score with the different number of nearest players, but since the model can accept the variable size input, I tried one of the models on one of folds: Number of nearest players threshold for the best score score 15 0.5800 0.7626 13 0.5400 0.7708 11 0.5200 0.7784 9 0.4400 0.7881 8 0.4000 0.7910 7 0.3400 0.7926 6 0.3000 0.7938 5 0.2200 0.7921 4 0.1800 0.7900 3 0.1200 0.7873 2 0.1000 0.7824 1 0.0600 0.7499 So looks like the selected 7 players choice was reasonable, 6 players worked slightly better with the score of 0.7938. Maybe when trained on the 15 players input the model would learn better how such messy cases are annotated. I'm not clear on how the model was able to identify which of surrounding players in the video were associated with the player tracking (NGS) features that you provided the decoder. Did you add any additional masking to the images or did the model learn these relationships on it's own? I added the position encoding (grid of sin/cos values at different frequencies, like used with NLP) to 7x7 grid of video encoders activations (starting from -128, -128 pix to encode positions around the visible area) and I also added similar position encoding for the helmet position on the sideline and endzone views (with different linear projections to allow models to query both views). This way the similar position encoding is used for both key and query parts of the transformer decoder attention and allows to associate and query parts of images relevant to the player visible position. I allowed to encode positions within 128pix of the visible area to be able to query players with contact but the helmet not visible in the current step. I accidentally introduced a bug in the dataset class and provided the main player video position to all nearest players and this caused the significant degradation of model performance. I also tried to supply one of the activations from 7x7 grid with the player helmet directly to players features, but I have not noticed the significant difference, looks like the model is able to use the supplied position encodings. Did you use any of the helmet bounding box data in the model itself other than identifying the player's helmet to predict for. Also, how did you handle when helmets were not visible in either camera? I only used the position of the helmet on views (if visible). If the helmet is outside of [-128pix..crop+128pix] box, the pos encoding for corresponding view values are set to zero. I run prediction for the current player only for steps when the player is visible on at least one view, but since the prediction is done for a number of steps (for example 16 steps, or +-0.8s from the current timestamp, with the current timestamp sampled at 0.5s steps), it's possible the player will be not visible on the previous or next timestamp. But the model would still predict contacts for steps around the visible interval, using the previously visible frames and tracking information (the self attention part of the encoder which uses attention over all players and all time steps). If the nearest player is not visible on either view, I think it's still included but model would have access to only tracking information or images of this player from surrounding steps if he was visible (it may be hard to associate players only using the tracking info). Camaro Posted 2 years ago · 4th in this Competition arrow_drop_up 1 more_vert Congrats, what a cool and beautiful architecture!! if I understand correctly, you stack 96 frames of 224x224 image and feed them to Convnext large. How much GPU memory is required?? Dmytro Poplavskiy Topic Author Posted 2 years ago · 3rd in this Competition arrow_drop_up 0 more_vert Thanks! I should probably clarify, 96 frames is the slice length/duration, I used only every second frame (or even 3rd frame for the last 2 models). With 2D+3D approach in addition I converted 3 frames to monochrome and used it as an input to 2d CNN, so it was actually 96/(3*2) = 16 combined frames/runs of 224x224 convnext large. With the batch size of 2, it used 19GB of VRAM for ConvNext Large and ~13GB for ConvNext Base during training. Camaro Posted 2 years ago · 4th in this Competition arrow_drop_up 2 more_vert @dmytropoplavskiy Thanks for the clarification, now I understand it's actually a reasonable number! It seems stacking gray 3 neighbor images on channel dim is very useful technique to reduce memory usage in video deep learning.",
      "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago Late Submission more_horiz 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Overview Data Code Models Discussion Leaderboard Rules Leo0523 · 4th in this Competition  · Posted 2 years ago arrow_drop_up 37 more_vert 4th place solution Overall pipeline & tabular part - Osaka Tigers We really appreciated the hosts and the kaggle team for organizing the competition. Moreover, we would also like to thank all the participants who joined. We could enjoy this competition and write up our solutions. I would like to thank team members, @bamps53 , @nyanpn and @kmat2019 , who have the top talent to analyze the task. I could discuss and enjoy the competition. Overview Simple solution outline is attached pic. (opens in a new tab)\"> In the 1st stage we predict the contact by multiple CNN. In the 2nd stage CNN prediction(s), tracking and helmet data are aggregated and created features to input GDBT.  Lastly we compute 5 models averaged value and optimize threshold for both player-player and player-ground contact. 1st stage CNN k mat model Details are written in https://www.kaggle.com/competitions/nfl-player-contact-detection/discussion/391719 . We can obtain both Endzone and Sideline prediction values. camaro model will come up soon 2nd stage aggregation & binary classification models We excluded player-player pairs with distance > 3, and the remaining ~880K rows were used to train 2nd stage models. During inference time, we assigned 0 to pair with distance > 3 and predicted only the remaining data. Created features Because our CNN predictions are so strong, more than 90% of the top 30 important features were CNN-related features. Below are part of the features we have created. Tracking distance between two players distance/x_position/y_position from step0 distance from around player (full/same team/different team ) distance between team center distance to second nearest player current step / max step lag / lead of acc, speed, sa etc max/min/mean of x, y, speed, acc, sa, distance group by (play, step), (play, step, team) and (play, player1, player2) x/y positon diff from step=0 ”interceptor” features find playerC who meet the following conditions and add distance(A, C) and ∠BAC to the features of playerA-playerB (to detect that C intercepts between A-B) ∠BAC < 30deg distance(A, C) < distance(A, B) and distance(B, C) < distance(A, B) Helmet bbox aspect ratio bbox overlap lag / lead of bbox coordinates bbox center x,y std/shift/diff distance of bbox centers CNN prediction and  meta-features oof predictions of 1st stage CNNs max/min/std of predictions group by (play, step) and (play, player1, player2) 5/11/21 rolling features to complement CNN predictions on frames without helmets lag / diff around players’ player-ground prediction value Combinations registration errors from helmet-tracking coordinate transform (similar to 6th place solution, and previous NFL’s 1st place solution by K_mat) Models We trained four GBDT models with different combinations of 1st stage CNNs. We also added one NN model (\"camaro2\" in the figure above) and calculated the simple average of these 5 models. Predictions were binarized with separate thresholds optimized for player-player and player-ground respectively. LightGBM K_mat A + Camaro1 Public 0.795/Private 0.792 K_mat B + Camaro 1 K_mat B xgboost K_mat B + Camaro 1 Camaro 2 tips rolling features for CNN prediction values are most important in our models. judging from permutation feature importance, ‘minimum distance between players in the game_play’,  ‘distance between away team mean and home team mean’ and ‘player-player distance’ are important tracking features to increase score. We did not use early-stopping to train the GBDTs because the optimal number of rounds for MCC is always longer than AUC. not wroked for models Catboost Residual fit Meta Features by non CNN (e.g. logistic regression prediction values/ k-means clustering feature) Separate player-player and player-ground model 1DCNN External NFL data Focal loss not worked for overall Adding previous competition pseudo labeling data Removing noisy label all29 assignment and its prediction 2.5D or 3D CNN, but should have dug more.. Aggregate near frame information Please sign in to reply to this topic. comment 6 Comments Hotness Camaro Posted 2 years ago · 4th in this Competition arrow_drop_up 9 more_vert Camaro part 1st stage Kind of Object Detection like archiarchitecture. Predict all combination pairs' contact and ground contact at once.  (Should I name as YOLO?) Details Extract feature map by Yolox FPN. RoIAlign around the player by using helmet coordinates. a. Region size is adjusted by average helmet size in the frame b. Helmet is located in the same position in the crop area Concatenate tracking features after linear transformation For inter contact a. Creare pairwise player matrix and concat p1 and p2 features b. Multiply distance features c. Linear For ground contact, simply linear Additionally predict the player is in contact or not with any player Others As you can see there is no temporal information aggregation here. I tried 2.5d or 3d modeling like other teams, but somehow failed. One of the reasons is that our team table feature engineering includes many roll, shift or diff features, so the benefit of temporal modeling was may be less than other teams. And my model architecture is a way different from my teammates’ @kmat2019 models. I guess this is one of the many reasons why we finished in the prize zone, 2 diverse cnn engines:) https://www.kaggle.com/competitions/nfl-player-contact-detection/discussion/391719 2nd stage Other than the GBDT models, I built a simple 1d UNet as a 2nd stage model. The main motivation of this model is to predict for no helmet player’s contact. It’s not so good model compared to LightGBM or XGBoost.(CV0.780/LB0.764) But its prediction was very unique to other models, so it shined when ensembling. Where I failed? When we analyze failure cases, there are a lot of label noises. So we wasted a lot of time cleaning up labels, by pseudo labeling, manual error removal and so on… But all failed in overfitting to the validation set. No successful result in LB. I should have much more focus on pure CNN modeling, like other top teams! Rob Mulla Competition Host Posted 2 years ago arrow_drop_up 2 more_vert Really interesting writeup on your stage 1 approach @bamps53 - thanks for sharing. I'm not as familiar with Yolox FPN - was there a specific implementation you used? Was the feature map you extracted just for players? Great work. Camaro Posted 2 years ago · 4th in this Competition arrow_drop_up 1 more_vert Thanks! It's just a FPN which extracts 1/8 scale feature map. I used the one used in Yolox object detection model. Specifically this implementation. https://github.com/Megvii-BaseDetection/YOLOX/blob/main/yolox/models/yolo_fpn.py Was the feature map you extracted just for players? No, it's extracted for entire image. HongCheng Posted 2 years ago · 59th in this Competition arrow_drop_up 1 more_vert many thanks! waiting for your camaro model~ Leo0523 Topic Author Posted 2 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thank you for your comment. Camaro model detail is written in comment. Rob Mulla Competition Host Posted 2 years ago arrow_drop_up 1 more_vert Really great solution. Thanks for the writeup. It's interesting to see how you were able to ensemble individual team member's solutions so effectively. Do you know the CV score of the individual stage 1 models prior to stage 2? What type of improvement did you see from adding the 2nd stage with the tracking data? Did you have the same pipeline for P-P and P-G with the only difference being the difference in threshold? Appreciate you sharing this solution. Camaro Posted 2 years ago · 4th in this Competition arrow_drop_up 1 more_vert Since we started to team up at a very early stage of the competition, our solution is designed to be ensembled later. (ex. 2nd stage LGBM takes cnn prediction outputs and it can be null, cnn doesn’t care about missing helmet case.) But about my 1st stage CNN models typically score only around cv0.750~0.760. And if I feed it to LGBM with tabular data, it is boosted around cv0.780~0.790 area. As for my NN 2nd stage model, it is actually separately trained for P-P and P-G. Other GBDT model doesn’t have any difference other than threshold, it’s better than building separate models somehow.",
      "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago Late Submission more_horiz 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Overview Data Code Models Discussion Leaderboard Rules Tamo · 5th in this Competition  · Posted 2 years ago arrow_drop_up 18 more_vert 5th place solution Thanks to the host and kaggle for hosting such an interesting competition. I would also like to thank all of the participants and teammates( @takashisomeya @nomorevotch @fuumin621 ) for a great time. Our solution consists of two stages: NN and GBDT. We will show you how in detail. ■stage1 NN part overview tracking data and images as input(player-player distance < 2 and player-ground) inference of sequential frames at once CNN + LSTM Input to NN [1]tracking data Use the following tracking data. distance distance_1(player1) distance_2(player2) speed_1 speed_2 acceleration_1 acceleration_2 same_team(bool) different_team(bool) G_flag(bool) If player is G, fill distance and XXXX_2 values with -1. same_team and different_team are flags for whether the players are belong to the same/different team. G_flag means the player-ground pair flag. [2]Images + Bbox Concat the following three in the channel direction video frames of +-1 frame cropped around the helmet. helmet bbox mask Image size player-player pair   ：crop size = max(average bbox width, average bbox height) * 3 player-ground pair ：crop size = max(bbox width, bbox height) * 3 Resize the cropped image to 128x128. We used sequential frames containing at least one frame with a distance < 2.\n(At this time the data may contain frames of distance > 2.) [1]：B x N x 10 [2]：B x N x 3 x 128 x 128 (B:batch_size, N:Sequential frames (e,g. 16,32,48,64)) Sequential frames (N) are cut out with different strides during training and inference. training: No duplicate frames  (stride == N) inference: Duplicate frames(stride < N, Duplicate frame results are averaged.) Augmentations during training Use the following augmentations. HorizontalFlip RandomBrightnessContrast OneOf MotionBlur Blur GaussianBlur Ramdom frame dropout (40-60% for images and 20-60% for tracking data) NN Model The overall NN model architecture is as follows Endzone/sideline images go through a shared CNN backbone. The CNN backbone uses the TSM module. https://www.kaggle.com/competitions/nfl-impact-detection/discussion/209403 Concatenate features extracted by CNN with tracking features BiLSTM layers + FC layer infer sequential frames at once ■stage2 GBDT part overview The key feature in this model is the logit from stage1. The goal is to further improve the score by combining logit with tracking data and other data to create a binary classification model. Data distance <= 2 swap player1 and player2 features then concatenate them vertically to the original data. average swap and original  features for final prediction Features Raw value x_position, y_position, speed, distance, orientation, acceleration, direction, sa, jersey_number of each player distance between players frame number nn_pred Helmet https://www.kaggle.com/code/ahmedelfazouan/nfl-player-contact-detection-helmet-track-ftrs Simple computational features The following are calculated for x_position, y_position, speed, distance, orientation, acceleration, direction, sa Absolute difference between players, multiplied by Difference from the average of all players in each frame Aggregate features For distance, nn_pred, sa, distance, speed Aggregate features for (game_play, position), (game_play, player), (game_play, team), (game_play, step) Aggregate features for each (game_play, player_1, player_2) shift, diff(-3~3) for each (game_play, player_1, player_2). model lgbm xgboost ■Ensemble stage1 (NN part) Created models on different backbones and different sequence lengths as follows backbone resnet18,34,50 resnext50 efficientnet b0,b1 sequence length 16,32,48,64 stage2 (GBDT part) Two models were created with the same features LightGBM XGBoost Forward Selection Created models for (almost) all combinations of the above, and use Forward Selection Forward Selection was based on the excellent kernel by chris here. https://www.kaggle.com/code/cdeotte/forward-selection-oof-ensemble-0-942-private/notebook It is a simple method. so we expected to avoid overfit. The following models were finally selected by Forward Selection sequence length backbone gbdt cv 64 resnext50 xgb 0.7918 64 resnext50 lgb 0.7906 64 effib0 lgb 0.79 32 resnext50 lgb 0.7935 32 effib0 lgb 0.7881 16 resnext50 xgb 0.7906 Final submit is CV:0.8016 ,LB : 0.7902, PB : 0.7913 Threshold We simply blend predictions of selected models (x5fold), and determined by a single threshold. We used two threshold. predictions themselves percentile of the predictions We also tried voting ensemble , but decided not to use it because the LB score was better with a single threshold. Other tips In the inference notebook, the following were introduced to avoid OOM and timeout. using lru_cache for read image at high speed PyTurboJPEG loads images faster than OpenCV Polars helps reducing submission time. Acknowledgments zzy's excellent kernel is very helpful in our pipeline. https://www.kaggle.com/code/zzy990106/nfl-2-5d-cnn-baseline-inference Please sign in to reply to this topic. comment 5 Comments Hotness yuki Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Thanks for sharing great solution. I have a question about the input to the CNN. Does the Sequential frames(N) stack up on the channel axis? Or do you input each frame to the CNN and concatenate the output of the CNN? The input to the CNN is usually B*C*H*W, but I would like to know how you have handled the \"N\" in your solutions that deal with B*N*C*H*W. Tamo Topic Author Posted 2 years ago · 5th in this Competition arrow_drop_up 1 more_vert Thanks, @yururoi The input to the CNN is the latter, that is (BxN)xCxHxW. yuki Posted 2 years ago · 9th in this Competition arrow_drop_up 0 more_vert Understood! Thank you! Rob Mulla Competition Host Posted 2 years ago arrow_drop_up 2 more_vert Great work and thanks for sharing the solution writeup. It seems many teams had good results with a 2-stage approach, but it's fun to see so much diversity in the architectures used. Do you happen to know the CV gain from stage 1 vs stage 2? Looking forward to learning more about your solution! Tamo Topic Author Posted 2 years ago · 5th in this Competition arrow_drop_up 1 more_vert Thank you, @robikscube The stage 2 gain depends mainly on the sequential length. If the sequential length is small, the gain is large, but if the sequential length is large, the gain is almost none. sequence length backbone stage1 cv stage2 cv 16 resnext50 0.7868 0.7906(xgb) 32 resnext50 0.7929 0.7935(lgb) 32 effib0 0.785 0.7881(lgb) 64 resnext50 0.7923 0.7918(xgb) 64 resnext50 0.7923 0.7906(lgb) 64 effib0 0.7893 0.79(lgb) HongCheng Posted 2 years ago · 59th in this Competition arrow_drop_up 1 more_vert Thanks for sharing great solution and explaination. your Forward Selection and ensemble solutions are nice ~ Cliche Posted 2 years ago · 632nd in this Competition arrow_drop_up 0 more_vert Hi Tamo, thank you for sharing this great solution! Your explanation is very straight forward, but I still got one question about the sequential frames N. Are they extracted by 6 frames interval? When you say N=16, so there are 16 sample steps with contact label? I would very appreciate if you can explain my question above!",
      "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago Late Submission more_horiz 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Overview Data Code Models Discussion Leaderboard Rules tomo20180402 · 9th in this Competition  · Posted 2 years ago arrow_drop_up 31 more_vert 9th place solution - Team JK I would like to thank the organizers for such an interesting competition! We share the Team JK's solution. Team Member: @vostankovich , @tereka , @anonamename , @yururoi , @tomo20180402 Overview 1st stage yuki part (1) of fig. See yuki's post . Vladislav part (2) of fig. Features are mainly created from sensor data, but helmets bboxes information is also used. Trained XGB and LGBM models for P2P and P2G individually. P2P and P2G have different features. There are 133 features for pair contact and 119 features for ground contact. Here is explanation of some features: Excluded speed, since it correlates to distance. Step (or frame_number), it boosts score a lot. Player position on field (defense, offense or special) Twist feature (direction-orientation) Same team feature Is home team feature Number of players/opponents in (1,3,5 meters) is quite good feature Number of players in opposite orientation Acceleration of player ratio to mean acceleration of all players per step Diff of features of same player (in time domain) Time features (just copy of previous and future steps features) Difference of features between two players Euclidean distance is the main feature and other features based on it as well Features from helmets dataframe (bboxes coordinates, bboxes height & width for each view and perimeter) IoU helmets features XGB/LGBM models were trained with common hyperperameters that can be seen on public notebooks. Only added reg_alpha = 0.1 for both models. anonamename part - combined knowledge of team members (3) of fig. 2-stage model of 2.5D/3D CNN and GBDT (5fold CV:0.778/Public:0.775/Private:0.773) 2.5D/3D CNN based public notebook . input image 15frames (±7frame, skip_frame=1) use both view (Endzone and Sideline) tracking data 64 features (created by @vostankovich ) model based DFL competition 1st solution . pipeline : 15frames 2.5D -> Residual3DBlock -> GeM (created by @tereka ) 2.5D backborn : tf_mobilenetv3_small_minimal_100.in1k multi-label classification (created by @anonamename ) num_classes=2(Player-Player contact(P2P) and Player-Ground contact(P2G)) + nn.BCEWithLogitsLoss fold : StratifiedGroupKFold(n_splits=5).split(y=\"contact_org\", groups=\"game_id\") (created by @tomo20180402 ) Set different labels for contacts between same team, different teams and ground. train data under sampling : positive:negative = 1:5 (change under sampling data for each epoch) optimaizer : AdamW(lr=1e-3->1e-5 CosineAnnealingLR, weight_decay=1e-5) epoch : 15 augmentation HorizontalFlip, ShiftScaleRotate, MotionBlur, OpticalDistortion, CoarseDropout Mixup at the last layer (like a Manifold mixup . created by @tereka ) TTA : HorizontalFlip GBDT Create xgboost and lightgbm for P2P and P2G individually. tracking feature (created by @vostankovich ) 2.5D/3D CNN prob feature groupby([\"game_play\", \"nfl_player_id_1\", \"nfl_player_id_2\"]) : shift(), diff(), mean(), max(), min(), std() tomo part (4) of fig. single-stage NN model (3fold CV:0.771/Public:0.759/Private:0.760) multi-class classification : P2P (same team), P2P (different team), P2G output is 6 labels which are used as features of team's 2nd stage execution time : 2h validation : StratifiedGroupKFold(n_splits=3).split(y=\"contact_org\", groups=\"game_id\") same as anonamename part dataset train data under sampling : Reduce negative sample of P2P contact (same team) by one-third. feature table feature : 54 3 types distance : euclidean, chebyshev, cityblock 3 types distance rank : among all, same team, different team median of helmet width and height normalized distance by mean of helmet width and height The mean of helmet width and height are calculated from all players. total rank from the center coordinates of 2player's helmets ratio of helmet detection exist : both players, each player cosine similarity : direction, orientation predicted euclidean distance other simple features : step, is_same_team, ground_flag, etc. image feature 10 images in 2.5D CNN 5frames each for Sideline and Endline (n-4, n-2, n, n+2, n+4) image_size = (256, 256) cropping method Change the cropping method depending on whether both players’ helmets exist. both players exist : Make sure both players are visible. one player exist : Make sure the player is in the center. Crop the image with the mean of helmet width and height as a variable. Give priority to the downward direction. mean of image exist : 4 each for Sideline and Endline TTA flip sensor and image in one of three models inferences sensor : exchange player1,2 image : HorizontalFlip 2nd stage model : lgbm × 4 feature : shift features of each models’ predictions and sensor data (-13~+13) postprocessing : 4 predictions by lgbm -> simple average -> moving average -> final prediction Please sign in to reply to this topic. comment 4 Comments Hotness tereka Posted 2 years ago · 9th in this Competition arrow_drop_up 12 more_vert I explain about Residual3DBlock. In this competition, 3d(t, h, w) is very important, so I decided to use any time analysis method. 15 frames reshape this ({-28, -24, -20}{-16, -12, -8}{-4, 0, 4}{8, 12, 16}{20, 24, 28}) and extract feature using backbone, then hidden output is applied it. here is a sample for using residual3d block(final architecture is very similar) class Residual3DBlock (nn.Module):\n    def __init__ (self): super (Residual3DBlock, self). __init__ ()\n\n        self.block = nn. Sequential (\n            nn. Conv3d ( 512 , 512 , 3 , stride= 1 , padding= 1 ),\n            nn. BatchNorm3d ( 512 ),\n            nn. ReLU ( 512 )\n        )\n\n        self.block2 = nn. Sequential (\n            nn. Conv3d ( 512 , 512 , 3 , stride= 1 , padding= 1 ),\n            nn. BatchNorm3d ( 512 ),\n        )\n\n    def forward (self, images):\n        short_cut = images\n        h = self. block (images)\n        h = self. block2 (h)\n\n        return F. relu (h + short_cut)\n\nclass Model (nn.Module):\n    def __init__ (self): super (Model, self). __init__ ()\n        self.backbone = timm. create_model ( \"tf_efficientnet_b0_ns\" , pretrained=True, num_classes= 1 , in_chans= 3 )\n        self.mlp = nn. Sequential (\n            nn. Linear ( 68 , 256 ),\n            nn. BatchNorm1d ( 256 ),\n            nn. LeakyReLU (),\n            nn. Dropout ( 0.2 ),\n        )\n        n_hidden = 1024 self.conv_proj = nn. Sequential (\n            nn. Conv2d ( 1280 , 512 , 1 , stride= 1 ),\n            nn. BatchNorm2d ( 512 ),\n            nn. ReLU (),\n        )\n\n        self.neck = nn. Sequential (\n            nn. Linear ( 1024 , 1024 ),\n            nn. BatchNorm1d ( 1024 ),\n            nn. LeakyReLU (),\n            nn. Dropout ( 0.2 ),\n        )\n\n        self.triple_layer = nn. Sequential ( Residual3DBlock (),\n        )\n\n        self.pool = GeM () \n\n        self.fc = nn. Linear ( 256 + 1024 , 1 )\n\n    def forward (self, images,feature,  target=None, mixup_hidden = False,  mixup_alpha = 0.1 , layer_mix=None):\n        b, t, h, w = images.shape\n        images = images. view (b * t // 3 , 3 , h, w)\n        feature_maps = self. conv_proj (self.backbone. forward_features (images))\n        _, c, h, w = feature_maps. size ()\n        feature_maps = feature_maps. contiguous (). view (b * 2 ,c,t // 2 // 3 , h, w)\n        feature_maps = self. triple_layer (feature_maps)\n        middle_maps = feature_maps[:, :, 2 , :, :]\n        #b, h, w= middle_maps. size ()\n        #middle_maps = middle_maps. view (b, 1 , h, w)\n        nn_feature = self. neck (self. pool (middle_maps). reshape (b, - 1 ))\n        feature = self. mlp (feature)\n        cat_features = torch. cat ([nn_feature, feature], dim= 1 )\n        if target is not None:\n            cat_features, y_a, y_b, lam = mixup_data (cat_features, target, mixup_alpha)\n            y = self. fc (cat_features)\n            return y, y_a, y_b, lam\n        else:\n            y = self. fc (cat_features)\n            return y content_copy YujiAriyasu Posted 2 years ago arrow_drop_up 0 more_vert I tried to run the code, but it seems to give me an error. What am I doing wrong? model = Model() im = torch.randn(( 8 , 15 , 512 , 512 )) feature = torch.randn(( 8 , 68 )) model (im, feature) content_copy YujiAriyasu Posted 2 years ago arrow_drop_up 1 more_vert Oh, do you use the following because of the end and side? im = torch.randn(( 8 , 30 , 512 , 512 )) content_copy tereka Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert yes, it's correct. b, t(side + endzone), height, width. im = torch.randn((8, 30, 512, 512)) HongCheng Posted 2 years ago · 59th in this Competition arrow_drop_up 0 more_vert May I ask a question? what is moving average? I have seen that it is mentioned several times anonamename Posted 2 years ago · 9th in this Competition arrow_drop_up 1 more_vert Moving average is the average of previous and next data. In short, df['pred'].rolling().mean() . Below is the pseudo code. test_df['pred_ma'] = test_df.groupby(['game_play', 'nfl_player_id_1', 'nfl_player_id_2'])['pred'].rolling(3, center=True, min_periods=1).mean().to_frame('pred_ma').reset_index()['pred_ma']",
      "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules kurupical · 14th in this Competition  · Posted 2 years ago arrow_drop_up 38 more_vert 14th Place Solution First of all, I would like to thank our hosts for organizing the competition. It was a task I've never solved before, and it was both educational and a lot of fun trying different approaches! Summary Model Detail 3D-CNN (cv: 0.770) backbone: r3d_18 (from torchvision: https://pytorch.org/vision/stable/models/generated/torchvision.models.video.r3d_18.html#torchvision.models.video.R3D_18_Weights ) use 63 frames(20fps) predict 19 steps train every 9 steps StepLR Scheduler(~2epochs: lr=1e-3/1e-4) 2.5D3D-CNN (cv: 0.768) Almost same as DFL's 1st solution by Team Hydrogen ( https://www.kaggle.com/competitions/dfl-bundesliga-data-shootout/discussion/359932 ) backbone: legacy_seresnet34 use 123 frames(20fps) predict 3 frames down sampling (g: 10%, contact: 30%) label smoothing (0.1-0.9) Both 3D, 2.5D3D Linear layers for g and contact # x: (bs, cnn_features) x_contact = model_contact(x) # (bs, n_predict_frames) x_g = model_g(x) # (bs, n_predict_frames) not_is_g = (is_g == 0 )\nx = x_contact * not_is_g + x_g * is_g # (bs, n_predict_frames) content_copy output 3 prediction and calculate loss: only sideline, only endzone, concat sideline-endzone feature. # pseudo code def forward ( self, x_sideline_image, x_endzone_image ):\n  x_sideline = cnn(x_sideline_image) \n  x_endzone = cnn(x_endzone_image) return fc(torch.cat([x_sideline, x_endzone])), fc_sideline(x_sideline), fc_endzone(x_endzone) content_copy LGBM (cv: 0.740) about 1100 features feautres player's distance (tracking, helmet) lag, diff top_n nearest player's distance (n: parameters) number of people within distance n (n: parameters) groupby game_play is_g is_same_team number of people within distance n ensemble Weighted ensemble, G and contact respectively. What worked for me image preprocessing draw bbox -> draw bbox and paint out use 2 colors(g, contact) -> use 3 colors(g, same team contact, different team contact) crop the image with keeping the aspect ratio bbox_left_ratio = 4.5 bbox_right_ratio = 4.5 bbox_top_ratio = 4.5 bbox_down_ratio = 2.25 for col in [ \"x\" , \"y\" , \"width\" , \"height\" ]:\n      df[col] = df[[ f\" {col} _1\" , f\" {col} _2\" ]].mean(axis= 1 )\n  df[ \"bbox_size\" ] = df[[ \"width\" , \"height\" ]].mean(axis= 1 )\n  df[ \"bbox_size\" ] = df.groupby([ \"view\" , \"step\" , \"game_play\" ])[ \"bbox_size\" ].transform( \"mean\" )\n\n  series = df.iloc[ 0 ] # sample left = int (series[ \"x\" ] - series[ \"bbox_size\" ] * bbox_left_ratio)\n  right = int (series[ \"x\" ] + series[ \"bbox_size\" ] * bbox_right_ratio)\n  top = int (series[ \"y\" ] + series[ \"bbox_size\" ] * bbox_top_ratio)\n  down = int (series[ \"y\" ] - series[ \"bbox_size\" ] * bbox_down_ratio)\n  img = img[down:top, left:right]\n  img = cv2.resize(img, ( 128 , 96 )) content_copy StepLR with warmup scheduler label smoothing (worked for 2.5D3D, but not worked for 3D) What not worked for me Transformers use top 100~400 features of lgbm feature importances tuned hard but got cv 0.02 lower than lgbm. 2D->1D CNN contact score is same as 2.5D3D, 3D but very poor G score in my work. interpolate bbox Other tools: I make tools to investigate wrong inference and make a hypothesize to improve score. https://github.com/kurupical/nfl_contact_detection/blob/master/58218_003210_contact_0.506591796875_score0.0_H23_V10.gif Please sign in to reply to this topic. comment 2 Comments Hotness Rob Mulla Competition Host Posted 2 years ago arrow_drop_up 0 more_vert Great work @kurupical and thanks for sharing this solution write up. nhatntt Posted 2 years ago · 63rd in this Competition arrow_drop_up 0 more_vert First of all congratulations, thanks for sharing this great solution. I have some questions that in 3D CNN you predict 19 steps, that is, given that input sequence you will predict which step it will belong to? (classification)",
      "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules anyai · 16th in this Competition  · Posted 2 years ago arrow_drop_up 18 more_vert 16th place solution - Team : Deimon Devil Bats Thanks to the organizers and the kaggle team for organizing the contest. EDA(match watching) was a lot of fun. Thanks to all participants for their hard work. I'll be reading and learning from your solutions! Also, thanks to the team, I could do best until the finish. Thanks @yokuyama @shimishige ! During the first half of the competition, each team member tried to create models in their own way (3D segmentaion, CenterNet , etc.), but unfortunately, the scores did not increase at all (LB score < 0.7). With 3 weeks remaining, the policy was changed to proceed on the basis of public notebooks. Summary This is a 2-stage model of Deep Learning (2.5D CNN, Transformer) and GBDT. Each is based on two public notebooks. Thanks @zzy990106 ( 2.5DCNN ) , @columbia2131 ( GBDT ). Deep is poor at the level of worrying about the correctness of the CV calculation, but it seems to have been sufficient as a feature to GBDT. deep has CV calculation with dist<2 only. 1st stage 2.5D CNN We created a 1-class output model that predicts player contact and G in the same class, and a 2-class output model that predicts them separately. Two models were created for Endzone and Sideline, respectively, for a total of 4 models. Common settings input : Image (±4frame), Tracking data backbone : tf_efficientnet_b0_ns Image cropping based on predicted player helmet size (max(width, height)*5) Prediction only for distance<2 mixup 1class Train data downsmpling (negative sample to 40,000sample) 2class Helmet position heatmap for player 1 and 2 ( reference ) Temporal Shift Module ( reference code Thanks @bamps53 ) Transformer + LSTM 30% skip connection Transformer ( reference code Thanks @cdeotte ) LSTM in last layer 25 features based on tracking data Scaling with RobustScaler 2nd stage catboost was a little better than XGB Features (public notebook +) Tracking data : diff, shift, product Deep model prob : shift, cummax, cumsum helmet size, etc. ↓Adding Deep model predictions (especially CNN) improves the score CV Table only 0.7030 + 2.5D CNN 1class 0.7540 + 2.5D CNN 2class 0.7681 + Transformer+LSTM 0.7683 Not work Interpolation of box undetected helmets by homography transformation. Probably there was a lot of noise, and the score worsened when the box interpolated by the transformation was used honestly. Using embedding of CNN Batch prediction of players in images (3D segmentaion, CenterNet, etc.) Please sign in to reply to this topic. comment 2 Comments Hotness HongCheng Posted 2 years ago · 59th in this Competition arrow_drop_up 0 more_vert thanks for your sharing， may I ask you how you do the shift in the second stage? o    Tracking data : diff, shift, product\no    Deep model prob : shift, cummax, cumsum content_copy anyai Topic Author Posted 2 years ago · 16th in this Competition arrow_drop_up 0 more_vert Thank you for your comment. The implementation is as follows gb = test.groupby([ \"game_play\" , \"nfl_player_id_1\" , \"nfl_player_id_2\" ])\n    test[ f\" {col_name} _next\" ] = gb[col_name].shift(- 2 )\n    test[ f\" {col_name} _prev\" ] = gb[col_name].shift( 2 ) content_copy",
      "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules emoji_people Thomas · 18th in this Competition  · Posted 2 years ago arrow_drop_up 10 more_vert 18th place solution : 2d-cnn / 1d-cnn / XGB / 1d-cnn First of all, I want to thank the hosts of this competition and my team: @chenlin1999 and @hanzhou0315 Summary Our solution is made of 4 stages : 2d-cnn : The model predicts for each player if the player is in contact as well as if the player is on the ground 1d-cnn : This stage is intended to smooth the prediction of 1. using the temporality XGB : It is at this moment that we associate the contacts between players 1d-cnn : This stage is intended to smooth the prediction of 3. using the temporality Validation methodology We opt for a Stratified group 5-Fold cross-validation by game_play : this strategy seemed to be the most correlated with LB and the most obvious. We have a final solution that reaches a CV score : 0.77174 for a LB : 0.77004 and a public LB : 0.76219 Stage1 : 2d-cnn At this stage it is very easy to overfit on the data so we only trained for 2 epochs. We used the timm models: efficientnetv2_rw_s and convnext_base_in22k for the final submission. The input is composed of 2 RGB images for the Endzone and the Sideline, then we concatenate the features to make a prediction. To add supervision to this model we used features created from the tabular data. Tt's a bit similar to this Stage2 : 1d-cnn It is a simple CNN with 5 layers and kernels of 3. To ensure temporal consistency we sorted by [\"game_play\",\"nfl_player_id\",\"step\"] Stage3 : XGB It's XGB like this by @columbia2131 and we added the features from the previous stage Stage4 : 1d-cnn It is a simple CNN with 5 layers and kernels of 3. To ensure temporal consistency we sorted by [\"game_play\",\"nfl_player_id_1\",\"nfl_player_id_2\",\"step\"] Final results Stage1 Stage2 Stage3 Stage4 CV public LB private LB ✓ 0.65 0.645 0.645 ✓ ✓ 0.718 0.715 0.716 ✓ ✓ ✓ 0.731 0.729 0.725 ✓ ✓ ✓ ✓ 0.771 0.762 0.770 Computer Vision Image Classification Tabular Image Beginner Please sign in to reply to this topic. comment 3 Comments Hotness emoji_people Chen Lin Posted 2 years ago · 18th in this Competition arrow_drop_up 4 more_vert Many thanks to @robikscube for organising this interesting competition and to my two teammates @thomasdubail and @hanzhou0315 This is my first time participating in a competition using computer vision and tabular data together, and I have seen a very wide variety of completely different solutions in the top solution, each using images and tabular data in a different order and in different ways, as well as being able to achieve very close results in the end. I noticed that the first half of our solution was very similar to the stage1 of the solution shared by @haqishen here , but in the second stage we only used the tracking data and some manual features created to train XGB, and did not use the CNN again to classify the images of the player pairs, perhaps this was the last step we needed to take to get to the gold zone. Besides, we saw from the top solution in the previous NFL competition that the 2.5d window size was typically 9 frames, but this window size did not lead to a boost in our stage1. Due to time constraints in the end, we decided to abandon the 2.5d which consumes a lot of computing power and did not explore more window size, and now it seems that others got a boost on the larger windows. Bilzard Posted 2 years ago · 19th in this Competition arrow_drop_up 1 more_vert Thank you for sharing solution. It's interesting that the result shows 1D-CNN is more effective in stage2 than stage4. How many window size do you use for 1D-CNN in stage 2 and 4? emoji_people Thomas Topic Author Posted 2 years ago · 18th in this Competition arrow_drop_up 1 more_vert Thank you for the feedback: indeed for me stage1 was already very effective in highlighting the people in contact, stage2 just had the effect of smoothing its values. However in stage3 the association of the players was much more noizy so the gain after this stage was much greater using time. For training I used 64 steps windows (for batch optimization) and then in inference mode I used the whole game as input Bilzard Posted 2 years ago · 19th in this Competition arrow_drop_up 1 more_vert Thanks. One more question: Why stage 2 only focus on single player? Do you train the model to the contact event for a single player to any other players? (like Qishen's solution ?) [\"game_play\",\"nfl_player_id\",\"step\"] Do you think modeling with contact event for single player is better than that of two players? 3 more replies arrow_drop_down",
      "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago Late Submission more_horiz 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Overview Data Code Models Discussion Leaderboard Rules Bilzard · 19th in this Competition  · Posted 2 years ago arrow_drop_up 15 more_vert Result of Late Submission: 2D-CNN + XGB + 1D-CNN (Private LB: 0.78703) I will share experiment result after competition (LB: 0.78703) for future reference. My original solution is available here (Private LB: 0.76741). First of all, I appreciate all the solution write-ups by others. I get much insight from their solutions. For example, giving numeric features on CNN with isolated channels is from Team Hidrogen's solution, and using player-anyone & player-ground contact information is adopted from Qishen and Bo's solution (and from 18th place team). What I Did on the Late Submissions use less-player-duplicated fold split 5-channel 2D-CNN (channel design is the same as Team Hydrogen's solution except for not using 2.5D) add group feature & lag feature of 1st/2nd stage prediction score (+group +lag) add player-anyone contact feature (+group +lag) add player-ground contact feature on player-player model (+group +lag) apply player-player sequence-level pruning add 4th-stage 1D-CNN Discussion 5-channel 2D-CNN gave me notable boost from my original architecture: 3-channel 2.5D-CNN (+0.54%) it also much reduces train/scoring time. One of the largest gains are from strictly split CV. As I already wrote on the post , the fold split with less player duplication drastically improves CV/LB correlation. Thanks to this fold split, I can boost LB further with additional features on the 3rd stage (+0.87%). Sequence level pruning (+0.16%) and 1D-CNN (+0.26%) boosted score further although there are no gain on CV. One of the possible reason is the CV I used is too strict and test data may contain few players that also appeared on train data. Tricks to speed up scoring time I also reduced scoring time by the following tricks. use 2D CNN instead of 2.5D (3-4h -> 2h) use @lru_cache when loading image (2h -> 1h) use numpy array instead of JPEG (1h -> 45 min) What didn't worked using prediction score of additional CNN which trains labels with player-anyone contact (it only scores tie comparing to the group features extracted from player-player contact CNN) 2.5D-CNN (it only tie scores to 2D-CNN) Score Results Submissions CV Public LB Private LB architecture description #features(p2g) #features(p2g) 1 0.7950 0.7701 0.7738 XGB + 2D-CNN(5-channel) + XGB stage 1 feats. + stage-2 pred score 1032 1032 2 0.8038 0.7806 0.7773 XGB + 2D-CNN(5-channel) + XGB #1 + lag & group feats. of stage-2 pred score 1057 1057 3 0.8035 0.7788 0.7799 XGB + 2D-CNN(5-channel) + XGB #2 + lag & group feats. of stage-1 pred score 1083 1083 4 0.8039 0.7799 0.7782 XGB + 2D-CNN(5-channel) + XGB #3 + stage 2 p2anyone feats (+group) 1087 1087 5 0.8040 0.7800 0.7808 XGB + 2D-CNN(5-channel) + XGB #3 + stage 2 p2anyone feats (+group +lag) 1104 1104 6 0.8055 0.7815 0.7820 XGB + 2D-CNN(5-channel) + XGB #5 + stage 1 p2anyone feats (+group) 1108 1108 7 0.8053 0.7808 0.7819 XGB + 2D-CNN(5-channel) + XGB #5 + stage 1 p2anyone feats (+group +lag) 1125 1125 8 0.8064 0.7827 0.7825 XGB + 2D-CNN(5-channel) + XGB #6 + p2g feat on p2p model (+group +lag) 1108 1130 9 0.8051 0.78368 0.78412 XGB + 2D-CNN(5-channel) + XGB #8 + sequence level pruning 1108 1130 10 0.8053 0.78687 0.78672 XGB + 2D-CNN(5-channel) + XGB + 1D-CNN #9 + 4th stage (1D-CNN; input stages 1-3 output) 1108 1130 11 0.80577 0.78713 0.78703 XGB + 2D-CNN(5-channel) + XGB + 1D-CNN #9 + 4th stage (1D-CNN; input only stage 3 output) 1108 1130 Please sign in to reply to this topic. comment 2 Comments Hotness HongCheng Posted 2 years ago · 59th in this Competition arrow_drop_up 0 more_vert It is very nice that you could share more experiment after the competition finished! This comment has been deleted.",
      "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules Taro_pan · 41st in this Competition  · Posted 2 years ago arrow_drop_up 13 more_vert 41st solution(2D-CNN, 1D-CNN, Stacking) Thank you to all the organizers and participants for this amazing competition! I really enjoyed this competion! Overview 2D-CNN prediction [zoom out, zoom in, masked] input single frame, predict Endzone & Sideline by same model 1D-CNN prediction [tracking, helmet] input tracking and helmet position data shift(-6~6) (pos, speed, acc,distance, orientation, direction, sa, helmet position) Stacking predcitions and 5 features by 2D-CNN (zoom out, zoom in, masked = 3 models) 3 tracking features by 1D-CNN tabel features Moving average post processing after concat distance > 2 data Score CV(Group K fold by game_play) : 0.740 Publie LB : 0.73699 Private LB : 0.7302 Please sign in to reply to this topic. comment 2 Comments Hotness HongCheng Posted 2 years ago · 59th in this Competition arrow_drop_up 1 more_vert very clear model! Ravi Shah Posted 2 years ago arrow_drop_up 2 more_vert Congrats @stgkrtua ! Nice solution and write up!",
      "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago Late Submission more_horiz 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Overview Data Code Models Discussion Leaderboard Rules yoyobar · 45th in this Competition  · Posted 2 years ago arrow_drop_up 9 more_vert 45th place solution (the most simple method to get seliver madel) First of all thanks to our teammates. Our method is makeup by a tabular model and cnn model. In the tabular model, we are based on this model second, we add (mean, std, max, min) from helmet trajectory data(train_player_tracking.csv) to increase LB from 0.650 to 0.684. Third, we add step_rate (0.684->0.693) from the video. And then We apply TTA to cnn model to increase LB from 0.667 to 0.671. Finally, we add cnn model's prediction to the tabular model. After doing this, we get 0.724 (public score) and 0.728 (private score). Our code . Cheers! Please sign in to reply to this topic. comment 2 Comments Hotness JEANMPIA Posted 2 years ago arrow_drop_up 1 more_vert wow, impressive emsembling of both tabuar and deeplearning, I have never seen that before, is that better than using a model that predicts on both the data and the transformed one ? yoyobar Topic Author Posted 2 years ago · 45th in this Competition arrow_drop_up 0 more_vert We have thought this way too, I think it will be better. we did not have enough time to debug our cnn model(a lot of bugs), so we gave up this way."
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Detect Player Contacts from Sensor and Video Data In this competition, you are tasked with predicting moments of contact between player pairs, as well as when players make non-foot contact with the ground using game footage and tracking data. Each play has four associated videos. Two videos, showing a sideline and endzone view, are time synced and aligned with each other. Additionally, an All29 view is provided but not guaranteed to be time synced. The training set videos are in train/ with corresponding labels in train_labels.csv , while the videos for which you must predict are in the test/ folder. This year we are also providing baseline helmet detection and assignment boxes for the training and test set. train_baseline_helmets.csv is the output from last year's winning player assignment model. train_player_tracking.csv provides 10 Hz tracking data for each player on the field during the provided plays. train_video_metadata.csv contains timestamps associated with each Sideline and Endzone view for syncing with the player tracking data. This is a code competition. When you submit, your model will be rerun on a set of 61 unseen plays located in a holdout test set. The publicly provided test videos are simply a set of mock plays (copied from the training set) which are not used in scoring. The associated test_baseline_helmets.csv , test_player_tracking.csv , and test_video_metadata.csv are available to your model when submitting. A sample_submission.csv will be available when submitting and will contain all rows required for a valid submission. [train/test] mp4 videos of each play. Each play has three videos. The two main view are shot from the endzone and sideline. Sideline and Endzone video pairs are matched frame for frame in time, but different players may be visible in each view. This year, an additional view is provided, All29 which should include view of every player involved in the play. All29 video is not guaranteed to be time synced with the sideline and endzone. These videos all contain a frame rate of 59.94 HZ. The moment of snap occurs 5 seconds into the video. train_labels.csv Contains a row for every combination of players, and players with the ground for each 0.1 second timestamp in the play. Note: Labels may not be exact but are expected to be within +/-10Hz from the actual moment of contact. Labels were created in a multistep process including quality checks, however there still may be mislabels. You should expect the test labels to be of similar quality as the training set labels. sample_submission.csv A valid sample submission file. [train/test]_baseline_helmets.csv contains imperfect baseline predictions for helmet boxes and player assignments for the Sideline and Endzone video view. The model used to create these predictions are from the winning solution from last year's competition and can be used to leverage your predictions. [train/test]_player_tracking.csv Each player wears a sensor that allows us to locate them on the field; that information is reported in these two files.  [train/test]_video_metadata.csv Metadata for each sideline and endzone video file including the timestamp information to be used to sync with player tracking data. 734 files 5.01 GB mp4, csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 5.01 GB test train sample_submission.csv test_baseline_helmets.csv test_player_tracking.csv test_video_metadata.csv train_baseline_helmets.csv train_labels.csv train_player_tracking.csv train_video_metadata.csv 734 files 81 columns ",
    "data_description": "1st and Future - Player Contact Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. The National Football League · Featured Code Competition · 2 years ago Late Submission more_horiz 1st and Future - Player Contact Detection Detect Player Contacts from Sensor and Video Data 1st and Future - Player Contact Detection Overview Data Code Models Discussion Leaderboard Rules Overview Start Dec 5, 2022 Close Mar 2, 2023 Merger & Entry Description link keyboard_arrow_up Goal of the Competition The goal of this competition is to detect external contact experienced by players during an NFL football game. You will use video and player tracking data to identify moments with contact to help improve player safety. Context The National Football League (NFL) has teamed up with Amazon Web Services (AWS) to strengthen its commitment to predict player injuries. The NFL aspires to have the best injury surveillance and mitigation program in any sport. With your machine learning and computer vision skills, you can help the NFL accurately identify when players experience contact throughout a football play. In prior years, the NFL challenged the Kaggle community to create helmet impact detection and identification algorithms. This year the NFL looks to automatically identify all moments when players experience contact. This competition will be successful if we can reliably detect moments when players are in contact with one another and when a player’s body is in contact with the ground. Currently, the NFL uses its tracking system to monitor a large number of statistics about players’ load during the season. The league has a solution that predicts contact between players, but it only leverages the player tracking data. This competition hopes to improve the predictive power by including video in addition to tracking data. Categorizing ground contact will also provide a more comprehensive view of impacts, improving analysis for player health and safety. More accurate data is an important step toward the NFL’s injury surveillance and mitigation goals. With complete contact detection, the league can identify correlations between certain types of contact and injury, a contributor to future prevention. Your efforts could help mitigate unsafe situations to reduce injury to all players. The National Football League is America's most popular sports league. Founded in 1920, the NFL developed the model for the successful modern sports league and is committed to advancing progress in the diagnosis, prevention, and treatment of sports-related injuries. This competition is part of the Digital Athlete, a joint effort between the NFL and AWS to build a virtual, 360-degree representation of an NFL player’s experience. The Digital Athlete hopes to generate a precise picture of what they need when it comes to preventing and recovering from injuries while performing at their best. Health and safety efforts include support for independent medical research and engineering advancements as well as a commitment to work to better protect players and make the game safer, including enhancements to medical protocols and improvements to how our game is taught and played. For more information about the NFL's health and safety efforts, please visit the NFL Player Health and Safety website . This is a Code Competition. Refer to Code Requirements for details. Evaluation link keyboard_arrow_up Submissions are evaluated on Matthews Correlation Coefficient between the predicted and actual contact events. Submission File For every allowable contact_id (formed by concatenating the game_play_step_player1_player2 ), you must predict whether the involved players are in contact at that moment in time. sample_submission.csv provides the exhaustive list of contact_id s. Note that the ground, denoted as player G , is included as a possible contact in place of player2. The player with the lower id is always listed first in the contact_id . The file should contain a header and have the following format: contact_id ,contact 58168_003392_0_38590_43854 , 0 58168_003392_0_38590_41257 , 1 58168_003392_0_38590_41944 , 0 etc . content_copy Timeline link keyboard_arrow_up December 5, 2022 - Start Date. February 22, 2023 - Entry Deadline. You must accept the competition rules before this date in order to compete. February 22, 2023 - Team Merger Deadline. This is the last day participants may join or merge teams. March 1, 2023 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - $50,000 2nd Place - $25,000 3rd Place - $13,000 4th Place - $7,000 5th Place - $5,000 Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time Internet access disabled Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors. Citation link keyboard_arrow_up Addison Howard, Britt Evans, Rob Mulla, Ryan Holbrook, Sam Huddleston, and Will Cukierski. 1st and Future - Player Contact Detection. https://kaggle.com/competitions/nfl-player-contact-detection, 2022. Kaggle. Cite Competition Host The National Football League Prizes & Awards $100,000 Awards Points & Medals Participation 6,945 Entrants 1,334 Participants 939 Teams 12,559 Submissions Tags Health Football Video Tabular Matthews correlation coefficient Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Citation"
  },
  {
    "competition_slug": "open-problems-multimodal",
    "discussion_links": [
      "/competitions/open-problems-multimodal/discussion/366961",
      "/competitions/open-problems-multimodal/discussion/366453",
      "/competitions/open-problems-multimodal/discussion/366428",
      "/competitions/open-problems-multimodal/discussion/366460",
      "/competitions/open-problems-multimodal/discussion/366409",
      "/competitions/open-problems-multimodal/discussion/366417",
      "/competitions/open-problems-multimodal/discussion/366471",
      "/competitions/open-problems-multimodal/discussion/366667",
      "/competitions/open-problems-multimodal/discussion/366455",
      "/competitions/open-problems-multimodal/discussion/366504",
      "/competitions/open-problems-multimodal/discussion/366392",
      "/competitions/open-problems-multimodal/discussion/368052",
      "/competitions/open-problems-multimodal/discussion/366578",
      "/competitions/open-problems-multimodal/discussion/367195",
      "/competitions/open-problems-multimodal/discussion/366372",
      "/competitions/open-problems-multimodal/discussion/368421"
    ],
    "discussion_texts": [
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules Shuji Suzuki · 1st in this Competition  · Posted 3 years ago arrow_drop_up 103 more_vert 1st Place Solution Summary First of all, thank you to the organizers and kaggle management and to everyone who participated with me. Since I needed to gain experience in analyzing single cell data, this competition was an excellent experience for me. I would like to introduce the overview of my solution. Multiome Model Overview Input Preprocessing Target Preprocessing tSVD-based imputation method: Perform dimensionality reduction on the data with tSVD And then, Transform the data back to the original space Copy the value of the 0 part of the original data from the transformed values. Model Output Postprocessing and Loss In the inference phase, the model outputs the average of the five predicted target data. CITEseq Model Overview Input Preprocessing In selecting important genes in CITEseq, the correlation coefficient is calculated for each batch and select only genes with high correlation in many batches. Genes were selected from those related to the target proteins and pathway. I use Reactome as pathway database. Target Preprocessing Model Output Postprocessing and Loss In the inference phase, the model outputs the average of the five predicted target data. Local evaluation I used two evaluation schemes. Evaluation with cross validation: 5-fold cross validation grouped by donor and day Evaluation for hyperparameter optimization with Optuna: Training data set is divided into training and validation data sets. ( Training data set: 80%, validation data set: 20%. ) Ensemble I used the weighted average of predictions of the following models. Models trained with changing the seed Models fine-tuned on only some batches Batch combination pattern examples: males only, female only, Day 4, 7 only, etc. Use a model trained on the full training data set as a pre-training model Code https://github.com/shu65/open-problems-multimodal Update 2022/11/20 add the repository url of my solution 2022/11/26 fix some figures Please sign in to reply to this topic. comment 21 Comments Hotness Barry Posted 3 years ago · 29th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for posting your great solution Shuji Suzuki! Liilili Posted 3 years ago · 57th in this Competition arrow_drop_up 2 more_vert Big old 🙏🙏🙏 May i ask some questions emmm The part of multi's model is fantastic, why  not  apply to Cite problems,  by residual target this way, it is not work in the cite problems? andreylalaley Posted 3 years ago · 127th in this Competition arrow_drop_up 2 more_vert Congratulations for thq 1st place and thank you for sharing your solution! As I see, you ve added metadata to both models. What increase (cv/ lb) did you get from adding metadata to models? vialactea Posted 3 years ago · 8th in this Competition arrow_drop_up 2 more_vert Congratulations for the 1st position and great solution!! I tried combining multiple forms of inputs/targets (initial inputs, SVD and binarized) and weight averaging their losses, but dropped it because the added complexity wasn't helping. I missed the ingenuity of your preprocessing chain and the averaging of the outputs for prediction (I took only the prediction of the original targets, and used the other outputs solely to regularize training). I have seen big PB jumps to gold medal positions - @cpmpml comes to mind, but I don't recall ever seeing such an impressive finish in a winner. What a great job! I'm curious about how surprised you where with taking the top position after ending the competition in a position far from the gold medals in the LB. CPMP Posted 3 years ago arrow_drop_up 3 more_vert You jumped quite a bit too. Shuji Suzuki Topic Author Posted 3 years ago · 1st in this Competition arrow_drop_up 3 more_vert Thank you! I couldn't believe it at first either and showed the screenshot of LB to my colleagues at work to make sure I was in first place and not wrong. Syed Siddique Mridul Posted 3 years ago arrow_drop_up 0 more_vert One of the best posts I have seen Artem Fedorov Posted 3 years ago · 88th in this Competition arrow_drop_up 1 more_vert Thank you for this detailed post. I see you made a lot of work preprocessing the data. But how did you come to this sort of preprocessing? Where does the idea to divide all the inputs by non-zero medians come from? Shuji Suzuki Topic Author Posted 3 years ago · 1st in this Competition arrow_drop_up 7 more_vert I noticed that the library-size normalization + log1p data is not being returned well after dimensionality compression with tSVD. The correlation coefficient between the original data and the converted and reverted data is only about 0.70. For this reason, I tried to find a better preprocessing method and found it. Mikhael Manurung Posted 3 years ago arrow_drop_up 1 more_vert Dividing by non-zero medians is something I've never seen in single-cell literature, but hey, it WORKS! How do you get to this approach? Could you elaborate on your thought process? Thanks and congratulations! Daniel Burkhardt Competition Host Posted 3 years ago · 1145th in this Competition arrow_drop_up 1 more_vert @shujisuzuki65 am I right in understanding you looked at many different pre-processing approaches and optimized for one that had high correlation after low-rank-approximation with tSVD? This led to selecting normalizing by median non-zero values? Matt Yang Posted 2 years ago arrow_drop_up 0 more_vert Very cool stuff Alex Posted 2 years ago · 492nd in this Competition arrow_drop_up 0 more_vert Hello Shuji. I tried to run your code with the minor modification that I only use 996 data points for both the train and test set to speed up the calculations. I get this output/error when training the cite model and don't know what to do. The error persists even when changing batch-size to 1000, for some reason it always looks for 8 batches and can't find 6 of them: git_hexsha Failed to get git load input values completed loading input values. elapsed time: 4.2 load targets values completed loading targets values. elapsed time: 0.2 load input values completed loading input values. elapsed time: 1.2 use test_inputs_values. total size: 1992 train sample size: 996 dump params kfold type  group None skip pre_post_process fit model input shape X:(664, 217) Y:(664, 128) dataset size 664 KeyError                                  Traceback (most recent call last) /content/drive/MyDrive/bio/suzuki/open-problems-multimodal-main/script/train_model.py in 570 571 if name == \" main \": --> 572     main() 6 frames /content/drive/MyDrive/bio/suzuki/open-problems-multimodal-main/script/train_model.py in main() 475 476     cv = CrossVaridation() --> 477     result_df, k_fold_models, k_fold_pre_post_processes = cv.compute_score( 478         x=train_inputs, 479         y=train_target, /content/drive/MyDrive/bio/suzuki/open-problems-multimodal-main/script/train_model.py in compute_score(self, x, y, metadata, x_test, metadata_test, params, build_model, build_pre_post_process, dump, dump_dir, n_splits, n_bagging, bagging_ratio, use_batch_group) 145                 model = build_model(params=params[\"model\"]) 146                 print(f\"model input shape X:{preprocessed_x_train.shape} Y:{preprocessed_y_train.shape}\") --> 147                 model.fit( 148                     x=x_train_bagging, 149                     y=y_train_bagging, /content/drive/MyDrive/bio/suzuki/open-problems-multimodal-main/ss_opm/model/encoder_decoder/encoder_decoder.py in fit(self, x, preprocessed_x, y, preprocessed_y, metadata, pre_post_process) 221         self.model.to(device=self.params[\"device\"]) 222 --> 223         dummy_batch = next(iter(data_loader)) 224         dummy_batch = self._batch_to_device(dummy_batch) 225         self._train_step_forward(dummy_batch, 1.0) /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py in next (self) 626                 # TODO( https://github.com/pytorch/pytorch/issues/76750 ) 627                 self._reset()  # type: ignore[call-arg] --> 628             data = self._next_data() 629             self._num_yielded += 1 630             if self._dataset_kind == _DatasetKind.Iterable and \\ /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py in _next_data(self) 1331             else: 1332                 del self._task_info[idx] -> 1333                 return self._process_data(data) 1334 1335     def _try_put_index(self): /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py in _process_data(self, data) 1357         self._try_put_index() 1358         if isinstance(data, ExceptionWrapper): -> 1359             data.reraise() 1360         return data 1361 /usr/local/lib/python3.8/dist-packages/torch/_utils.py in reraise(self) 541             # instantiate since we don't know how to 542             raise RuntimeError(msg) from None --> 543         raise exception 544 545 KeyError: Caught KeyError in DataLoader worker process 0. Original Traceback (most recent call last): File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop data = fetcher.fetch(index) File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch data = [self.dataset[idx] for idx in possibly_batched_index] File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in data = [self.dataset[idx] for idx in possibly_batched_index] File \"/content/drive/MyDrive/bio/suzuki/open-problems-multimodal-main/ss_opm/model/torch_dataset/citeseq_dataset.py\", line 79, in getitem info = torch.as_tensor(self.metadata.iloc[index, :][self.metadata_keys].values.astype(float), dtype=torch.float32) File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 1007, in getitem return self._get_with(key) File \"/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\", line 1047, in _get_with return self.loc[key] File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\", line 1073, in getitem return self._getitem_axis(maybe_callable, axis=axis) File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\", line 1301, in _getitem_axis return self._getitem_iterable(key, axis=axis) File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\", line 1239, in _getitem_iterable keyarr, indexer = self._get_listlike_indexer(key, axis) File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexing.py\", line 1432, in _get_listlike_indexer keyarr, indexer = ax._get_indexer_strict(key, axis_name) File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 6070, in _get_indexer_strict self._raise_if_missing(keyarr, indexer, axis_name) File \"/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\", line 6133, in _raise_if_missing raise KeyError(f\"{not_found} not in index\") KeyError: \"['batch_sv2', 'batch_sv3', 'batch_sv4', 'batch_sv5', 'batch_sv6', 'batch_sv7'] not in index\" Oscar Aguilar Posted 3 years ago arrow_drop_up 0 more_vert Congrats @shujisuzuki65 . Thanks for sharing all your approach details. Well done👍 zhez017 Posted 3 years ago arrow_drop_up 0 more_vert Congratulations to get the 1st palce, and thanks for sharing. Kexin Wang Posted 3 years ago arrow_drop_up 0 more_vert Big Congrat! And thanks a lot for sharing your idear Chris Deotte Posted 3 years ago arrow_drop_up 0 more_vert Congratulations @shujisuzuki65 ! Amazing work, great job! Ravi Ramakrishnan Posted 3 years ago arrow_drop_up 0 more_vert Thanks a lot for the detailed approach note @shujisuzuki65 ! Hearty congratulations and best regards! I wish to know of the approaches that did not work for you too. I eagerly await the adjutant code too. Arthur Mulikhov Posted 3 years ago · 85th in this Competition arrow_drop_up 0 more_vert Congratulations! I see the combination of MSE/MAE in \"compressed\" space and correlation loss in original space as a approach to model regularization Did you select weights of these two losses ? If so, how ? Why you used MAE in citeseq part, but not MSE as in multiome part ? Shuji Suzuki Topic Author Posted 3 years ago · 1st in this Competition arrow_drop_up 0 more_vert The MAE/MSE weights were set at 1.0 at the beginning of training and the weights were gradually reduced as training progressed. The detail of weight schedule is here: https://github.com/shu65/open-problems-multimodal/blob/3d57dd3837b17079fed5678043e681749ba32324/ss_opm/model/encoder_decoder/cite_encoder_decoder_module.py#L81 I tried MAE as in multiome part. But, the score of the model with MAE is lower than that with MSE. This comment has been deleted.",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules senkin13 · 2nd in this Competition  · Posted 3 years ago arrow_drop_up 95 more_vert 2nd place solution(senkin part with code) Thanks to all the organizers and kaggle team hosting such a challengeable competition.Thanks my team mate @baosenguo , I have no knowledge about  bioinformatics ,learned a lot from him.I thought my team could win this competition as we were at the 1st place of LB from start to end,but the time domain shift is unpredictable,we accept this result and congratulas to @shujisuzuki65 ,great shakeup! Overview Cite ](https://postimg.cc/s1XNhL6m) (opens in a new tab)\">[ ](https://postimg.cc/s1XNhL6m) Multi ](https://postimg.cc/sMwW7T0P) (opens in a new tab)\">[ ](https://postimg.cc/sMwW7T0P) preprocessing 1) centered log ratio transformation (CLR) is the best normalization method for both of cite and multi, I found the method from nature articles. https://www.nature.com/articles/s41467-022-29356-8 2) high correlation raw features with target 3) @baosenguo designed fine tuned process using raw count: normalization:sample normalization by mean values over features transformation:sqrt transformation standardization:feature z-score batch-effect correction:take \"day\" as batch, for each batch, we calculate the column-wise median to get a \"median-sample\" representing the batch, and then subtract this sample from each sample in this batch. This method may not bring much improvement, but it is simple enough to avoid risks. 4) row-wise zscore transformation before input to neural network validation The biggest challenge in this competition is how to build a robust model for unseen donor in public test and unseen day&donor in private test. At the early stage I used random kfold, cross validation and LB score matched very well, so we don't need to worry about donor domain shift.But time domain shift is unpredictable, after team merge, we check our features one by one with out-of-day validation(groupkfold by day) to make sure all the features can improve every day. model Lightgbm train 4 lightgbm models with different input features,then transform oof predictions to tsvd as nn model's meta features -- library-size normalized and log1p transformed counts -> tsvd -- raw counts -> clr -> tsvd -- raw counts -- raw counts with raw target one trick is input sparse matrix of raw count to lightgbm directly with small \"feature_fraction\": 0.1,it brings nn model much improvment. NN Basiclly 3layers MLP works well,one trick is to use GRU to replace first dense layer or add GRU after final dense layer. Cite target is transformed to dsb having negative values, compared to ReLU, ELU is much better to deal with negative target values,Swish is also work well for both of cite and multi. At the early stage I found cosine similarity is best as loss funtion for my model, after team merge, I learned from teammate to use MSE and Huber to build more different models. notebook [simple cite version] https://www.kaggle.com/code/senkin13/2nd-place-gru-cite github https://github.com/senkin13/kaggle/tree/master/Open-Problems-Multimodal-Single-Cell-Integration-2nd-Place-Solution Please sign in to reply to this topic. comment 29 Comments Hotness Chris Deotte Posted 3 years ago arrow_drop_up 3 more_vert Congratulations @senkin13 and @baosenguo Great job leading the public LB during the competition and great job building a generalizing model to stay top on private LB! senkin13 Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert source code uploaded Barry Posted 3 years ago · 29th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for posting your great solution senkin13! NicoF Posted 3 years ago arrow_drop_up 1 more_vert Congratulation! and thanks for sharing the notebook, its super useful! Alexander Chervov Posted 3 years ago · 234th in this Competition arrow_drop_up 1 more_vert Congratulations again  with the great solution! May I ask you - you write: \"we check our features one by one with out-of-day validation(groupkfold by day) to make sure all the features can improve every day\" How exactly that check can be done ? And how much resources it takes ? I mean you have something like 1000 features - so train new models dropping features one by one - 1000 trains - probably not the way you used ? senkin13 Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert sorry, I mean one group by one group, for example clr of raw count is one group, lightgbm oof predictions is one group. JulyHappy Posted 3 years ago · 111th in this Competition arrow_drop_up 1 more_vert Thank you for sharing! I still don't understand the data processing. Where can I see more detailed information about（train_cite_X.shape）→(70988, 1009)，Can you explain why it is 1009 columns？ senkin13 Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert it includes clr transformation(200), lgb oof(400), fine-tuned transformation(164), high correlation and important features(245) Jiwei Liu Posted 3 years ago · 73rd in this Competition arrow_drop_up 1 more_vert Do you use the raw data for cite exclusively? and not use the kaggle preprocessed version at all? Thank you. senkin13 Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert yes, I only use the raw data for cite exclusively Dmitriy Ershov Posted 3 years ago · 92nd in this Competition arrow_drop_up 1 more_vert Hi! Tried to upgrade your notebook, changed your GRU net on my best CNN with 1D and 2D convs. Got a little improvements senkin13 Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 0 more_vert good job, it seems lower cv,higher plb, our random kfold didn't fit private very well time's up Posted 3 years ago · 124th in this Competition arrow_drop_up 1 more_vert @senkin13 Big congratulation and thanks a lot for sharing your team solutions. May I ask in the early stage why you typically choose the cosine similarity as your loss function? Did you try a batch of the different loss function and find cosine-similarity was the best score? senkin13 Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 0 more_vert I tried mse,mae,pearson correlation,cosine similarity, cosine similarity is best for my model no-magic Posted 3 years ago · 32nd in this Competition arrow_drop_up 1 more_vert Thank you for sharing! Would you like to share the consideration why gru works in this dataset, does it mean that through forget and add new information through features, gru layer extract more information than noise compared to mlp? senkin13 Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 2 more_vert I don't have precise theory supported,but I assume you are right,gru layer extract some more and different information than mlp. there were two successful experience in the past。 https://www.kaggle.com/competitions/favorita-grocery-sales-forecasting/discussion/47582 https://www.kaggle.com/competitions/talkingdata-adtracking-fraud-detection/discussion/56262 no-magic Posted 3 years ago · 32nd in this Competition arrow_drop_up 1 more_vert awesome solution, thanks! yjyang Posted 3 years ago · 413th in this Competition arrow_drop_up 1 more_vert May I know how much the row-wise z-score step help you score up? senkin13 Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert I remember it boosted cv 0.0003 Jiwei Liu Posted 3 years ago · 73rd in this Competition arrow_drop_up 1 more_vert Super strong solution! Thank you for sharing! Alexander Chervov Posted 3 years ago · 234th in this Competition arrow_drop_up 1 more_vert @senkin13 Thank you very much for your efforts on the competition ! You were the one who broke all the limits which seemed to be unbrokable, thus giving the others the example to follow  ! So mainly due to your efforts we kind of can estimate how much we can extract from that data - that it is important for the research community. PS If you would have time to share your experience in a zoom webinar - it would be very great ! PSPS and welcome to join our telegram chat: https://t.me/sberlogacompete senkin13 Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Thank you for invitation, I am going to share more details at NeurIPS workshop at 7/Dec, you can follow that. TESUZI Posted 3 years ago · 21st in this Competition arrow_drop_up 1 more_vert Interesting.  I did not have ideas about TF-IDF, it seems that this approach is always used in NLP. Glad to see it works. senkin13 Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert yes, it also can be used for GBDT or ridge model.By the way,TF-IDF is the old version data provided,you can check Dataset Description page. Dmitriy Ershov Posted 3 years ago · 92nd in this Competition arrow_drop_up 1 more_vert I also used cosine similarity on some models, but at the end I also used weighted combination of mse, cs, and corr loss, and it works well senkin13 Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert yes, we should ensemble models as many as possible, I regret I didn't ensemble lots of models. Trí Phạm Posted 2 years ago arrow_drop_up 0 more_vert I am interested in your solutions. So, I tried to run your code, but I running LightLBM on the CPU very slowly. Thus, I wonder whether your team uses GPU when training the LightLBM model. I try to config the LightLBM model to access GPU, but I meet the bug environment. Noé Tran Posted 2 years ago arrow_drop_up 0 more_vert I'm really new to those topic. I'm just confused about the purpose of LightBGM model, could you please briefly explain it to me? Thank in advance!!! Trí Phạm Posted 2 years ago arrow_drop_up 0 more_vert I think, LightBGM use extracting features",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules Makotu · 3rd in this Competition  · Posted 3 years ago arrow_drop_up 70 more_vert 3rd place solution First of all, to the organizers and kaggle management, and to everyone who participated with me, thank you for organizing a great competition! Before starting this competition, I had no knowledge of the biological field.To be honest, I still don't know much about it. (Since this was a completely unprofessional field, I may have been able to try various things without bias…？) I would like to share my solution below. Please forgive me if it is difficult to read in many respects, as my English skills are not very good. Summary: multiome preprocess use okapi bm25 instead of tfidf dimensionality reduction：use lsi(implemented in muon) instead of svd https://muon.readthedocs.io/en/latest/api/generated/muon.atac.tl.lsi.html muon.atac.tl.lsi(rawdata(with okapi preprocessing), n_comps=64) feature Basically, pre-processing contributed greatly to the accuracy, but the following features also contributed somewhat to the accuracy. binary feature transformed 0/1 binary and reduced 16 dimensions(svd) as features w2v vector feature For each cell, the top100 with the highest expression levels were lined up and vectorized by gensim to get feature vector(16dims) for each gene. https://radimrehurek.com/gensim/models/word2vec.html Ex. CellA: geneB → geneE → geneF → … CellB: geneA → geneC → geneM → … top100 genes vector average in each cell used as features. leiden cluster mean feature I made Clusters using muon's leiden clustering(23 cluster). https://muon.readthedocs.io/en/latest/api/generated/muon.tl.leiden.html#muon.tl.leiden After taking the average of the features for each cluster(23 cluster × 228942 feat), they were reduced to 16 dimensions by svd and used as features(23 cluster × 16feat). After that, join on clusters. connectivy matrix feature Since muon's leiden clustering generates an adjacency matrix between cells as a byproduct, I also use it 16-dimensional with svd as a feature. model mlp Simple 4-layer mlp; no major differences from mlp in public notebooks target has been reduced to 128 dimensions with svd. use rmse loss catboost target has been reduced to 128 dimensions with svd. ensemble I made a model of nearly 20 mlp and 3 catboosts with various feature combinations. and cv-based weighted averaging. cite preprocess The same process as in the organizer was applied. use sc.pp.normalize_per_cell and sc.pp.log1p (excluding the gene that are significantly related to the target protein) feature I've made a lot of features, and here are some of them that have worked to some degree. leiden cluster feature I made Clusters using muon's leiden clustering. Average of features per cluster and reduce dimensions with svd. (excluding Important genes. It were not used svd, and use raw count's average for each cluster was used as features as is.) w2v vector feature Same as multiome. model mlp Simple 4-layer mlp; no major differences from mlp in public notebooks Using correlation_loss. No different from public notebook. catboost ensemble I made a model of nearly 20 mlp and 2 catboosts with various feature combinations. and cv-based weighted averaging. Validation It goes without saying that one of the key elements of this competition is validation. I tried binary classification to classify test data used in PB and others. (At this point, the classification accuracy was so high. So I think it is dangerous to trust LB.) The 10% of the training data that is close to the PB, is used as validation data. This method seemed to work well, the submit with my highest cv was highest pb score. update: Code & Model & Data share code and models in github and kaggle datasets https://github.com/makotu1208/open-problems-multimodal-3rd-solution Please sign in to reply to this topic. comment 18 Comments 1 appreciation  comment Hotness luddite^ Posted 3 years ago · 22nd in this Competition arrow_drop_up 1 more_vert Congratulations for your solo gold, and thank you for your informative solutions. I want to ask a question about w2v vector features. Did you input numerical data into w2v and converted them to 16 dimensional vector? And I couldn't surely understand the part \"top100 genes vector average in each cell used as features”. Could you explain more details? Thank you in advance! Makotu Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thank you for comment! I think it would be faster to have you look at code, so I created a notebook. https://www.kaggle.com/code/mhyodo/w2v-feature-sample luddite^ Posted 3 years ago · 22nd in this Competition arrow_drop_up 1 more_vert I understood! Thank you, and again congratulations! Artem Fedorov Posted 3 years ago · 88th in this Competition arrow_drop_up 1 more_vert cite model catboost target has been reduced to 128 dimensions with svd. Did you reduce target for CITE to 128 dimentions also? Makotu Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 0 more_vert Oops, sorry. cite used the 140 dimensional target as is without dimensional reduction. I will correct this. Thanks for pointing that out! sing4it Luo Posted 3 years ago · 121st in this Competition arrow_drop_up 1 more_vert a little aside, your avatar is so charming on LB😆 Anthony Chiu Posted 3 years ago · 13th in this Competition arrow_drop_up 1 more_vert Interestingly you used word2vec instead of autoencoder, have you compared them? Makotu Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thanks for your comment!  I also tried making feature using autoencoder, but it did not work that well within my experiments. Anthony Chiu Posted 3 years ago · 13th in this Competition arrow_drop_up 1 more_vert I think it is also interesting to fit the vectors for each position directly to an RNN layer rather than taking an average. Barry Posted 3 years ago · 29th in this Competition arrow_drop_up 2 more_vert Congratulations and thanks for posting your great solution Makotu! Chris Deotte Posted 3 years ago arrow_drop_up 2 more_vert Congratulations @mhyodo Fantastic job winning 3rd place Cash Gold Solo! pineapple Posted 3 years ago arrow_drop_up 2 more_vert Thanks for sharing! As a beginner, this is really helpful for illustrating the importance of validation sets. time's up Posted 3 years ago · 124th in this Competition arrow_drop_up 2 more_vert @mhyodo Big Congrat on your solo gold. And thanks a lot for sharing your solution. Could you please help me to understand the validation part? Why did you start to doubt the LB with a high binary classification result? Does it mean that with a high score, the test data for PB and others have a huge variance, and this variance will lead to the final score shift? Makotu Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 2 more_vert Thanks for your question! Also, sorry for the delay in responding. Sorry, I need to add something. Precisely, I clearly felt that it might be dangerous to trust LB because when I performed the classification with the LB test data as 0 and the PB test data as 1 I felt this was dangerous because of the high accuracy of the classification. If the test data used in LB and the test data used in PB are similar I would believe LB, but since this was not the case, I thought it was dangerous to believe LB. time's up Posted 3 years ago · 124th in this Competition arrow_drop_up 1 more_vert Ah， got it! thanks again for your explanation Riccardo Posted 3 years ago · 19th in this Competition arrow_drop_up 2 more_vert @mhyodo adversarial validation was surely a nice way to go! Congrats! Moses Aborisade Posted 3 years ago arrow_drop_up 0 more_vert This is really a nice idea Appreciation (1) Pavithra Devi M Posted 3 years ago arrow_drop_up 0 more_vert Thanks for sharing :)",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules Oliver Wang · 4th in this Competition  · Posted 3 years ago arrow_drop_up 38 more_vert 4th place solution (with code) Intro To begin with, thanks to the Kaggle team and Open Problems team for hosting such a wonderful contest. I would also like to share my gratitude to all the competitors, especially those generous competitors who are willing to share their codes and thoughts like @ambrosm @alexandervc @baosenguo @pourchot and so on. I couldn't have gone so far without their help. You can find full code on Github here Cite Data preprocessing At first, all of my feature engineering methods are based on the original data, but my public score raised from 0.812 to 0.813 after I merely change the data source so all the feature engineering processes and based on the raw data. My preprocessing method is using np.log1p to change the raw data. I have also tried other preprocessing methods like MAGIC and TF-IDF but they can't improve my CV score. Feature engineering The final inputs of the models consist of mainly six parts. Three of them are dimension reduction parts including Tsvd , UMAP , and Novel’s method . The rest are feature selection parts including name importance , corr importance, and rf importance . Tsvd : TruncatedSVD(n_components=128, random_state=42) UMAP : UMAP(n_neighbors = 16,n_components=128, random_state=42,verbose = True) Novel’s method : The original method can be found here . At first, I wanted to implement the preprocessing method to replace simple log1p but after I replaced the Tsvd results of log1p by  the Tsvd results of Novel’s method I found that my CV went down. But if I kept both of them, the CV score would increase a little bit. So I kept the Tsvd results of Novel’s method . name importance : It 's mainly based on AmbrosM's notebook . But I added additional information from mygene while matching. I will release my complete preprocessing code later and specific results can be found there. corr importance : As the name suggested, I chose the top 3 features that correlated with the targets. There was overlap and the number of selected features was about 104 rf importance : Since the feature importances of random forest may apply to NN and other models as well. So I selected 128 top feature importances of the random forest model. I have also tried other mothed including PCA , KernelPCA , LocallyLinearEmbedding , and SpectralEmbedding . PCA gives little help and it will cause severe overfitting when used with Tsvd . I could' t finish the manifold methods in 24 hours so I gave them up. Models I have implemented the CV strategy like the private test, but it turns out that the strategy like the public test is better. So all of the results are based on GroupKFold on donors . I have done there-layers stacking in the competition. and I have also done the ensemble on the stacking results and the results of independent models. Here are the models I used and I will also release the code later. Method Stacking NN NN_online CNN kernel_rigde LGBM Catboost CV 0.89677 0.89596 0.89580 0.89530 0.89326 0.89270 0.89100 NN : A personal-designed NN network, trying to do something like the transformers. I used MLP to replace the dot product in the mechanism of attention. This may not be so reasonable and I am also aware of the importance of feature vectors and dot products. But I was so fascinated by attention and I also tried tabnet and rtdl but they didn't work very well. But my method seemed to work even better than simple MLP. Demo notebook CNN : Inspired by the tmp method here and also added multidimensional convolution kernel like the Resnet. NN(Online) : This model is mainly based on pourchot's method here and only some tiny change was made. Demo notebook Kernel Rigde : This model is inspired by the best solution of last year's competition. I used Ray Tune to optimize the hypermeters Demo notebook with ray tune Catboost : There are many options for catboost here. Using MultiOutputRegressor or MultiRMSE as objective .But we can't do earlystopping to prevent overfitting in the first method and the result of the second method is not good enough so I made a class MultiOutputCatboostRegressor personally, using MSE to fit the normalized targets. LGBM : I also wrote MultiOutputLGBMRegressor and the results seem to be better and the training process was so slow that I had to give it up in the stacking. However, I still trained a independent LGBM model and used it in the final training. Demo notebook stacking : I used KNN , CNN , ridge , rf , catboost , NN in the first layer and only CNN , catboost , NN in the second and just a simple MLP in the last layer. To avoid overfitting, I used KFold and oof predictions between layers, and every stacking model are using GroupKFold (so there are 3 stacking models here). It seems to be a little bit to understand so you may refer to the picture. If you still have confusion please feel free to ask me. Demo notebook train Demo notebook predict CV Results Model Ⅰ (vaild 32606) Model Ⅱ (vaild 13176) Model Ⅲ (vaild 31800) Fold 1 0.8989 0.8967 0.8947 Fold 2 0.8995 0.8967 0.8951 Fold 3 0.8985 0.8959 0.8949 Fold Mean 0.89897 0.89643 0.89490 Model Mean 0.89677 - - Ensemble notebook Multi To be honest, I put most of my efforts on cite part so there is nothing very special here and I will make a brief introduction. Data preprocessing & Feature engineering inputs: TF-IDF normalization np.log1p(data * 1e4) Tsvd -> 512 targets: Normalization -> mean = 0, std = 1 Tsvd -> 1024 Models NN : A personal-designed NN network as mentioned above. The output of the model is 1024 dim and make dot product with tsvd.components_ (constant) to get the final prediction than use correl_loss to calculate the loss then back propagate the grads. Catboost : The results from online notebook LGBM : The same as the MultiOutputLGBMRegressor mentioned above. Using MSE to fit the tsvd results of normalized targets. Ensemble The same notebook as mentioned above. notebook Please sign in to reply to this topic. comment 17 Comments Hotness jcerpent Posted 3 years ago · 5th in this Competition arrow_drop_up 1 more_vert Very clear visualization, and congrats on the solo gold! Oliver Wang Topic Author Posted 3 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thanks! Hope they can help you! luddite^ Posted 3 years ago · 22nd in this Competition arrow_drop_up 1 more_vert Congratulations for your solo gold, and thank you for sharing your solution. I have a question. I also did stacking, but its score is lower than simple average or even single model's score. Did you do something special? Oliver Wang Topic Author Posted 3 years ago · 4th in this Competition arrow_drop_up 2 more_vert Thanks. From my perspective, the reason why stacking is lower is often related to overfitting. The CV score is very high but the LB score is relatively low. So in order to avoid or alleviate overfitting, I used the special KFold strategy and simple MLP as the last layer, which is illustrated in the picture. 6 more replies arrow_drop_down Laurent Pourchot Posted 3 years ago · 117th in this Competition arrow_drop_up 1 more_vert @oliverwang15 , congrats for your work ! 👍 Oliver Wang Topic Author Posted 3 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thanks and your work is very profound and helpful ! 👍 time's up Posted 3 years ago · 124th in this Competition arrow_drop_up 2 more_vert Warm Congratulations! and thanks for sharing your solution. May I ask about the stacking parts, What do you mean about the first layer and second layer? Does it mean that you first select all features to train several models (e.g KNN,CNN,ridge,rf,catboost,NN) and get the prediction results for each of them, after that connate all \"first layer model\" predicted results as input for the \"second layer\" input? Oliver Wang Topic Author Posted 3 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thanks. Yes, you are right. But in the second layer input there are not only the output of those models in the first layer but the original features. Actually I was inspired my Mu Li‘s idea here . If you are interested you may have a look Akki Posted 3 years ago · 32nd in this Competition arrow_drop_up 0 more_vert Thank you for your sharing. I have a question. You used some dimension reduction method for cite part. How do you use them differently? Did you simply want diversity for the sake of the ensemble? Oliver Wang Topic Author Posted 3 years ago · 4th in this Competition arrow_drop_up 1 more_vert Hi, Aesop. Thanks for your question. Indeed, for the sake of diversity, we should use different data to train different models and ensemble them to get the best results. But actually, when I was doing this part since the training part was time-consuming, and trying the best coefficient or doing stacking would also be time-consuming. So I reduced the diversity in data and trained models with all the data I mentioned above, even in the stacking part. So maybe we could try to use different data to train the models and then ensemble them to see whether the results would be better. Akki Posted 3 years ago · 32nd in this Competition arrow_drop_up 0 more_vert I understood. Thanks!!",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules A Beginner · 5th in this Competition  · Posted 3 years ago arrow_drop_up 36 more_vert Private 5th Solution (A Beginner part) Intro This is a scheme from a beginner. Public notebooks, the top scheme of last year, and ensemble have helped me a lot. Citeseq Data preprocessing and feature engineering I used two different methods ①Preprocessing method of public notebook from @pourchot . ②Using PCA to reduce dimensions to 128 + Direct features based on absolute correlation to targets. I have normalized the row after both methods. The first method is more effective, and the second method is only used for ensemble. Model ①MLP without BN and drop (Adam as the optimizer). I tried different activation functions and ensemble them, and this greatly improved CV. ②LGBM. I trained two LGBM models (different data preprocessing). CV ①groupkfold on donor ②groupkfold on donor and day The first one scored higher on public LB, but the second one performed slightly better on private LB. ensemble Through oof prediction, I selected 4 NN models to mix with 2 LGBM models, and determined the weight. The best CV score was 0.896(donor). Multiome Data preprocessing and feature engineering I used the top method of last year and made some adjustments. This method has the following steps: ①tf-idf ②log1p ③sklearn.preprocessing.Normalizer(norm=\"l2\") or sklearn.preprocessing.Normalizer(norm=\"max\") ④PCA(512) ⑤row normalization ⑥Select the first 64 items in 512 and the first 100 items in 512 generated by direct dimension reduction as all features. Model MLP with drop(AdamW as the optimizer). I used different activation functions and ensemble them. CV ①groupkfold on donor ②groupkfold on donor and day ensemble I used 4 NN and ensembled them. The best CV score was 0.670(donor). In this way, I got the submission of 0.814 public LB and 0.772 private LB. I think this is probably the easiest way to win the gold medal. Finally, I would like to thank my two teammates @jcerpentier @ahmedelfazouan . I have learned a lot from them. Looking forward to the next progress! Please sign in to reply to this topic. comment 4 Comments Hotness jcerpent Posted 3 years ago · 5th in this Competition arrow_drop_up 4 more_vert Thanks to the organizers and my teammates for this well-organized and interesting competition! Aside from @qqzzxxdd 's great writeup, I'd like to add a brief summary of things we did that did not end up working. Initially, we were working on CV splits by donor. This resulted in our highest public solution, and was 1 of the 2 submissions that we picked as the final (1 CV, 1 LB). As some may have noticed, it is extremely challenging to push the CV score above 0.9 with single model in this context. We tried using model soups ( https://arxiv.org/abs/2203.05482 ), which did not help much. Alternatively, I manually implemented a similar method that reached 0.8997 CV score. But we noticed that CV and LB did not align as well as expected for donor. A lot of keras tuner/manual experiments for the best MLP. Turned out the best model was just always the same 4 layer MLP. Ridge ensemble (different alphas) also reached good CV results in donor context, but did not perform well for us on LB. Lastly, I think it is mandatory to thank @pourchot , @jsmithperera and @vslaykovsky (sorry if I forgot any) for their great notebooks! They were very helpful to us, and probably many others, throughout this competition. Laurent Pourchot Posted 3 years ago · 117th in this Competition arrow_drop_up 1 more_vert Hi @jcerpentier , Congrats 🎉🍾 ! Laurent Pourchot Posted 3 years ago · 117th in this Competition arrow_drop_up 3 more_vert Congrats the team ! You were better than some grand masters and it is a big challenge 😉 Shreya Mishra 0307 Posted 3 years ago arrow_drop_up 2 more_vert Very useful for  beginners like me thanks @qqzzxxdd for sharing",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules minhtu.mt.mt · 6th in this Competition  · Posted 3 years ago arrow_drop_up 40 more_vert [6th private - 3rd public] Summary of our solution First of all, thanks to the Kaggle team and the host for providing this cutting-edge technology dataset and hosting this great challenge. I learned a lot from the competition. Many thanks to my teammates @mathormad @nguyenvlm for hard-working days. This is a summary of what we did to get 3rd place in LB and 6th place in private Local CV (for both cite and multiome task) We use stratified-5-Fold (stratify by day/donor/celltype) CV strategy. We do not use GroupKFold to avoid overfitting in public, and the private dataset day is also quite \"far\" from the training dataset, so it's a bit risky to use a CV split by day (we've tried it, but both CV and LB drop). In our opinion, this is the key to our stable placement in both public and private leaderboards. Our best submission in CV are also the best on Public and Private leaderboard. Feature engineering (cite) Feature selection: remove all 0 cols group by metadata (day/donor/cell_type/train/test) --> remove ~4,500 features Dimension reduction: we use different methods to increase the diversity for the final ensemble: 240 n_components SVD/quantiledPCA and denoise Autoencoder (256 latent dims). For SVD, increase n_iter params slightly improve CV by 2e-4 (but run longer) (ours is 50 iter, the default of sklearn library is only 5). The denoise autoencoder helps us improve CV by 1e-3 (comparing to SVD/PCA) Feature importance Use name matching from this discussion https://www.kaggle.com/competitions/open-problems-multimodal/discussion/349242 Search using xgb feature importance: for cite, we fit multiple xgb for each target (full 22k feature) then choose the top 5  important features of each target. In total, we get around 500 important features for our model. TargetEncoder for xgb: we apply target encode the cell_type for each target (each cell_type will be represented by a 140-dims vector) Feature engineering (multiome) Feature selection: remove chY features, which is not correlated to our target. We also remove all 0 cols group by metadata (day/donor/cell_type/train/test) --> remove ~ 500 features Dimension reduction: we use 256 n_components SVD fitted with 200iter Training process Use custom loss (weighted correlation and MSE loss) Use c-mixup to increase the diversity for tabnet SWA when training MLP, Denoise Autocoder, Tabnet and 1D-CNN Adam optimizer with high learning rate (1e-2) (Multiome) XGB is trained with the PCA of target as label External data For cite task, we apply the whole training process above for the raw count dataset , then ensemble with the original one. This significantly boosts the performance by 1e-3 For multiome task, we do not use the raw count dataset because it's lacking some rows, we cannot match the OOF between the original and raw count so we do not have CV score to validate. I think it should work but we do not have enough time to rematch the OOF Stacking feature & Pseudo-labeling We concatenate the prediction of 1 model with current features as input to another model (e.g. use output of xgb as input of MLP, and vice versa), which improves the performance of a single model around 5e-4. We also use pseudo labeling (not much improvement, about 2-3e-4) Post process (Cite) Apply standard scaler (axis 1) for the output of each fold before taking the average (Multiome) Some of the target is all 0, so we remove them from the training process and replace 0 later. This helps us increase our CV by 1e-3 Ensemble (Cite) Our final submission is a blending of the following models: MLP XGB Tabnet 1D-CNN Each model is trained on the original and raw count dataset -> 8 models in total (Multiome) Blending of: MLP XGB Tabnet Tryhard We worked 6 hours/day, from the very beginning of the competition to the last hour. Especially for this competition when we have 2 tasks with 2 datasets, a lot of work and experiments to do. The most important thing I learned during this competition is that the more time you spend, the higher place you will be. What does not work Encoder for multiome task Use important features in multiome task Some bio techniques such as Ivis (dimension reduction), Magic (denoising) Use raw label for training … Please sign in to reply to this topic. comment 9 Comments Hotness Ravi Ramakrishnan Posted 3 years ago arrow_drop_up 1 more_vert Hearty congratulations for the result! Hats off to all of you for the dedication and perseverance! 6 hours per day for the length of the competition is a commendable perseverance!! All the best @minhtu123 and team!! Dmitriy Ershov Posted 3 years ago · 92nd in this Competition arrow_drop_up 2 more_vert Important features were working for me in both multiome and siteseq. But I used random forest and another method for selection (thresholding). Also I use correlation features for siteseq (feature-target correlation for whole pairs of it) minhtu.mt.mt Topic Author Posted 3 years ago · 6th in this Competition arrow_drop_up 0 more_vert To my mind, any important features searching strategy works as well. I tried some of them (correlation features, shap, random selection, etc) and they all lead to a common subset of features (let's say all strategies result overlap more than 80%) Alexander Chervov Posted 3 years ago · 234th in this Competition arrow_drop_up 0 more_vert @minhtu123 Thanks for sharing and congratulations with the gold medal ! May I ask you about the following, you write: \"Search using xgb feature importance: for cite, we fit multiple xgb for each target (full 22k feature) then choose the top 5 important features of each target. In total, we get around 500 important features for our model.\" Is is possible to share these results ? Thanks in advance ! LearningAndChanging Posted 3 years ago · 91st in this Competition arrow_drop_up 0 more_vert Thank you for sharing, may I ask what is the difference between raw count dataset and original dataset?  Does raw count dataset means dataset composed by 500 important features? minhtu.mt.mt Topic Author Posted 3 years ago · 6th in this Competition arrow_drop_up 0 more_vert The raw count dataset is this one: https://www.kaggle.com/competitions/open-problems-multimodal/discussion/359355 LearningAndChanging Posted 3 years ago · 91st in this Competition arrow_drop_up 0 more_vert Thank you for your reply, I understand now, thank  you very much! Akmal Azzam o'g'li Posted 3 years ago · 178th in this Competition arrow_drop_up 0 more_vert @minhtu123 Thank you for sharing, one starter question, which is first, feature selection and then dimension reduction or vice versa? minhtu.mt.mt Topic Author Posted 3 years ago · 6th in this Competition arrow_drop_up 1 more_vert Feature selection first, then the reduction. We want the embedding feature learned from clean data",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules kss · 7th in this Competition  · Posted 3 years ago arrow_drop_up 55 more_vert 7th Place Solution Summary First of all, we would like to thank the organizers for an interesting challenge, as well as for the opportunity to use Saturn cloud. CV Scheme Since the organizer initially provided the information how public and private parts are splitted, we decided to utilize this info and design our local validation as similar as possible to the private part. So, we can say that the public part of the leaderboard was of little interest to us. The validation scheme is shown in the picture below (for citeseq part). Since we have three days in train dataset (citeseq task), the number of splits equals 6, for the multiome part the scheme was the same. However, there were 9 splits due to the larger number of days (at the very end we slightly modified this scheme so that validation fold always contained only one (nearest) day). How to submit Since the last available day (in train dataset) was always in validation folds, we could not use this validation scheme for the traditional folds blending. So, when we wanted to make submit, we used the usual KFold validation with shuffle. The approach was the following: all hypotheses and experiments, including hyperparameters tuning, were tested on true cv and afterwards these models were retrained on KFold. Features and dimensionality reduction We used PCA for dimensionality reduction for both tasks. Autoencoder was tested on citeseq dataset too, but performed slightly worse than PCA. We also used \"important\" features in their raw form for citeseq task. Moreover we created some additional features based on aggregations of \"important\" columns over metadata (for example, \"mean_feature_1_by_donor\"). Such features gave us + 0.0003 for GBDT model on local CV. Models (ensemble) Citeseq task : 3x multilayer perceptrons, 1x Conv1D, 1x pyBoost (best single model). Multiome task : 1x multilayer perceptron, 1x TabNet, 1x pyBoost (best single model). pyBoost seems to be the new SOTA on multioutput tasks (at least among GBDT models). It's extremely fast to train as it uses GPU only and super easy to customize. Paper Code Some remarks about models Multiome task : all neural nets had 23418 output neurons. For pyBoost we reduced targets' dimension to 64 components using PCA. pyBoost was the best single model on True CV and KFold validation on citeseq data. For multiome task, it was the best model according to True CV and the worst by KFold. We noticed that splitting targets  into groups and building a separate pyBoost model for each group improved our local CV a lot. By default, pyBoost can split targets into groups randomly, so we decided to try to improve it by splitting targets into groups based on their clusters, however, in the end it worked nearly the same as random splitting. Data used to train Citeseq task : all available data Multiome task : day 7 data only. Solving multiome task, we noticed that there is a significant performance drop on day 7 (on True CV). There was an idea that the reason is data drift in time, so we tried to train the model not on all available days, but only on the last available one. Locally, this improved our score by + 0.02. However, the problem was that there were no unseen days on the public leaderboard, so training the model only on the last available day seriously dropped our public score (from 0.814 to 0.808). Nevertheless, we decided to follow the mantra \"trust your cv\" and as a result, this particular submission became our best on private. We also conducted a study on the similarity of days and found out that among available training days, day 3 is the most similar to private day 10 (day 7 is the second most similar). Nevertheless, since for other days the most similar day was always the closest in time, we decided to train our models on day 7. Please sign in to reply to this topic. comment 12 Comments Hotness Leandro Destefani Posted 3 years ago · 860th in this Competition arrow_drop_up 1 more_vert nice catch at Multiome CV. congratulations for your results Alexander Chervov Posted 3 years ago · 234th in this Competition arrow_drop_up 1 more_vert Great ! Thanks for the summary and congratulations with the Gold ! You fairly deserved it! Question 1: how did you measure the quality of the model - AVERAGE scores over folds, is it correct ? Or some weights for folds - since sizes are different ? In classical cv we have two options - average over the folds VS create Out-Of-Fold prediction and score(Y_true, Y_oof) (without fold averaging). In that scheme you do not have OOF, is it correct ? Question 2: Have you considered blend and tuning/optimizing coefficients for blend ? If yes - how do approach it ? Is it - blending over each validation fold and again taking the average ? Question 3: Do you think about having something like a additional  \"holdout\"  - because if you do something like extensive hyperparameter search or feature selection over CV you might get better results due to randomness ( similar  problem in statistics is resolved by Bonferroni correction -  which should depend on  the number of parameter trials https://en.wikipedia.org/wiki/Bonferroni_correction ). So to control it - one may introduce \"holdout\" - which should NOT be seen during hyperparam optiomization/feature selection. =============== Remarks: the proposed CV scheme https://www.kaggle.com/competitions/open-problems-multimodal/discussion/358860 is quite similar to yours. In particular you folds 3,4 are exactly the same as in our scheme you excluded from the train day 4 and donors 31800 for one fold and 32606 for other. So our idea was just to be symmetric - exclude day 2, exclude 3 - and do the same as you for excluded day 4  - and thus we  get 6 folds. With our CV we can get two interesting things: 1) We can create \"public like score\" and \"private like score\" - one corresponds to predict other  donor , another to predict the other day 2) We can make  Y_{oof} of the FULL SIZE - so we can score , not by averaging over folds -  but like in classical scheme, to do that - we need a trick: we average predictions from two subfolds, e.g. OOF_{day4} =  1/2 ( Y_{pred from days 2,3 exclude 31800} +   Y_{pred from days 2,3 exclude 32606} )  - so difference with the classical scheme that we need to average predictions to form the Y_{oof} ! Not sure I am very clear, may be the code is more clear: Create folds: https://www.kaggle.com/code/alexandervc/mmscel-cv-modeling-advanced?scriptVersionId=111042574&cellId=21 Create predictions: https://www.kaggle.com/code/alexandervc/mmscel-cv-modeling-advanced?scriptVersionId=111042574&cellId=26 ==== To conclude in some sense your CV scheme is more \"stronger\" - you train one just single day and required the model to perform good on TWO days (your folds 1-3),   but our CV in some sense is more general - you can use it for any similar task when  you have two groups and \"two leaderbords\" - public and private - we have scores for the both of them. kss Topic Author Posted 3 years ago · 7th in this Competition arrow_drop_up 1 more_vert Answer_1: yep, you're right, we measured model performance by taking simple average. Answer_2: blend weights were tuned with optuna (we've tried out stacking with pyBoost but in the end simple linear combination of predictions worked better for us). Since the dataset with predictions was quite large, the weights were selected not for all folds, but only for a part of them. Alexander Chervov Posted 3 years ago · 234th in this Competition arrow_drop_up 0 more_vert Thanks for answers ! For the answer 2 - so you blend on each fold, and average the scores from each fold - that what you need to optimize with Optuna - is it correct ? PS Also added the question 3 - how to control possible overfit to validation scheme - if extensive hyperparam search / feature selection was done - please take a look when you have free time. something4kag Posted 3 years ago · 74th in this Competition arrow_drop_up 0 more_vert Congratulations on your finish.  Thanks for sharing the links on PyBoost. Wondered about the days and differences or similarities.  Day 7 consistently had the lowest number of cell ids for all Donors, 6960-7466 and seemed to score poorly for all, best for the donor with the highest number. Whereas Day 4 which had more cell ids for the Train Donors (no day 4 in Public though), 9407-10933 always seemed to score the best in folds.  If trying to balance the number of cell ids or generate some artificial data or from other sources might have helped models. Did you find any donor for day 10 that was more similar to the Test donor? kss Topic Author Posted 3 years ago · 7th in this Competition arrow_drop_up 0 more_vert Good question! Frankly speaking, no. As we observed model performance across all folds, it seemed that new day was always much more problematic than new donor. So we concentrated on time component of this problem only. Akki Posted 3 years ago · 32nd in this Competition arrow_drop_up 0 more_vert Thank you for publishing this. I have one question. How did you decide to use Kfold for the final submission? I mean, how were you convinced that using the features and parameters which scored highest in the CV mimicking Private would also score well in Kfold? kss Topic Author Posted 3 years ago · 7th in this Competition arrow_drop_up 1 more_vert Hello! We were not interested in how our model performs on KFold cv. The only purpose of its usage was the possibility to train multiple models and blend them (instead of training one model on the whole train dataset). As an alternative for Kfold validation, a common bootstrap could also be used here. Akki Posted 3 years ago · 32nd in this Competition arrow_drop_up 1 more_vert Thanks a lot! Alvin.ai Posted 3 years ago · 691st in this Competition arrow_drop_up 0 more_vert Congrats! I got a question that how do u compute the similarity of days? Would u introduce more details. Thanks for sharing, u did a great job. kss Topic Author Posted 3 years ago · 7th in this Competition arrow_drop_up 1 more_vert Thank you very much for the kind words! In fact, the most similar day to private day 10 was determined in two ways: Visual analysis of the distribution of individual features + comparing the number of non-zero elements for different days We trained CatBoost classifier that tried to distinguish the day from the training dataset from day 10. The day with lowest ROC AUC we considered as the most similar to day 10. According to both of these approaches, day 3 looked as the most similar day (the next one was day 7) Alvin.ai Posted 3 years ago · 691st in this Competition arrow_drop_up 0 more_vert Great! thanks for sharing.",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules vialactea · 8th in this Competition  · Posted 3 years ago arrow_drop_up 27 more_vert 9th place solution – strong fundamentals Thanks to Open Problems in Single-Cell Analysis and Kaggle for organizing this exciting competition. Ever since I was a little kid, I have had a passion for understanding how things worked. Three topics fascinate me the most: how the universe works (astrophysics and quantum mechanics), how the brain works (AI), and how life works (genetics and molecular biology). This competition was a wonderful opportunity to combine two of these topics. Unfortunately, I failed to live up to the expectations I had coming in. I was unable to come up with any good ideas with the potential to advance the field. Many years ago, I read a few books on genetics, starting with “Shadows of forgotten ancestors” (big Carl Sagan fan), but unfortunately, they are all very superficial on the science. I believe my illiteracy in the area was too big a handicap to overcome. I’d like to congratulate the winners and thank everyone who shared their knowledge and ideas, particularly @ambrosm , @psilogram and @pourchot . I’d also like to thank and commend @alexandervc for his contributions and tireless effort to motivate others and advance the science around the competition. My journey Upon reading the competition description, looking at the data, reading some posts and browsing a couple of notebooks, I listed a set of ideas to try. At that stage, my hopes rested mainly on two of them: pseudo labeling unused test data and NLP transformer models. My plan was to build a strong model, generate pseudo labels, improve the model to get better labels, incorporate transformer models, gain a better understanding of the problem, come up with some breakthrough idea and have plenty of time to explore it. I knew that cite weighted more on the PB score, but I thought that multi’s low scores presented a better opportunity to differentiate. Despite having my sights on multi, I typically would build new code (create inputs, train, analyze results, predict, ensemble) first for cite and then adjust for multi. The thinking was that cite’s smaller data would make be easier to get the code right. The need for a strong model to produce pseudo labels led to a big focus on ensembling, which I believe was detrimental to my performance. Also, the size of the data proved to be a much bigger challenge than I anticipated. Easily more than 90% of the time I dedicated to this competition was spent on trying to make things work within the memory available in Kaggle. It took me a while to build my first ensemble and conduct training with pseudo labels. The results were very disappointing. I wondered if it was because the model was generalizing better to unseen data, or if pseudo labeling just didn’t work. I spent some more time improving the base models and gave it another try, but with the same poor results. I then focused on a few other small improvements to my models, while building a more manageable overall pipeline (I broke processing in several pieces to deal with the memory and disk constraints). Then family demands and the need to revisit my analysis of several prospect investments during the recent stock market rout, ate away at my personal time. Before I knew it, the competition was over, and I hadn’t moved past the more fundamental stuff. If I were to do it again, I’d focus more on better understanding the science, better feature engineering and better individual models. CV strategy The CV strategy was the first thing I defined. The way I understood the problem, we had to make two types of predictions for which I devised two different CV approaches: Prediction CV method Future day for 3 known donors day (d): 3 folds based on day (4 for multi) Future day for an unknown donor day/donor (dd): 9 folds based on day/donor (12 for multi). Training data excludes any records for the donor or day in the validation fold. Although this strategy gave me confidence on the results, it had the downside of leaving a lot of data on the plate during training. I decided to edge this approach by using a variation of day/donor in which I would extend the training sample to any data that wasn’t for the same donor/day as the validation (dde). The downside of this approach is that it will overfit, and its results cannot be fully trusted. The upside is that it can lead to a better model as it uses more data. The following diagram illustrates the 3 methods, and how I used them for my two selected submissions. As you may notice, I didn’t bother about the LB range. I didn’t expect the absolute LB score to be of significance, so I only used it to check if CV improvements would translate to the LB. All methods provided good CV/LB correlation with one notable exception, which I suspected was caused by a bug. Solution Feature engineering I used combinations of the following input features in models optimized by Optuna: Raw: This refers to the original input features. I noticed that several people asked for the actual raw data, which was made available at some point. I decided to forego using it, because my intuition was that it wouldn’t make a difference and it would save me precious time. Based on other writeups, my intuition might have been wrong. PCA: input features reduced through PCA to up to 2048 principal components. Going beyond 2048 components didn’t seem to help. The PCA input was complemented by important features (borrowed from @ambrosm ). I generated a version of PCA using all columns and another excluding important columns. I trained models using both the regular version of the PCA output and normalized versions. These latter versions didn’t seem to help, so I discarded them. Type: cell type used only for cite. I theorized that I could build a good classification model to predict it for multi, but using the training data available didn’t seem to help CV. Gender: donor gender Day KNN mean: mean value of input features for N nearest neighbors, reduced through PCA. Mean:  mean value of input features for donor/day, reduced through PCA. I used both the raw PCA and a normalized version. Non-zero: mean number of non-zero features per cell for donor/day, reduced through PCA. Again, both the raw PCA and a normalized version. Binary: input features converted to binary. My intuition was that the value matter less than whether the feature was present or not. Contrary to my expectation, PCA input data performed better than raw data. It seems that it worked better to denoise data than other approaches I tried. After verifying that the public versions of important cite features helped training, I started using them, with the intention of conducting my own selection later. I never got to do it, though. Towards the end, I concluded I should have prioritized it. I used the following targets: Raw: original targets Binary: original targets converted to binary. I thought this would help denoise the target data without degrading performance (same intuition I applied to the input data). Although the correlation between the actual targets and the binary form was very high, this didn’t seem to help CV, so I discarded the approach. PCA: original targets reduced through PCA to up to 2048 principal components. Training with a large number of components and using a smaller number for the prediction boosted the CV of individual models (up to 0.002, with the high end of the range seen in weaker models), but hindered ensembling. I tried these approaches independently and in combination, using multiple heads and loss functions. I discarded the combination models because the added complexity didn’t seem to help CV. model architecture I used the following types of models: NNs, mostly MLPs, residual networks, and 1dCNNs. XGBD ElasticNet NNs covered a range of options: dropout, normalization, gaussian noise, activation and others. For loss I used correlation, mse and binary cross entropy (for binary targets). I tried incorporating autoencoders into the models to denoise data, but it didn’t help CV. Non NN models performed clearly worse and didn’t even help with ensembling. With better tunning they might have helped with ensembling, but I never made that a priority. Conceptually, this felt like a time series problem. However, as I understand it, the data collection process is intrusive and prevents us from having historical data for the same cell. I discarded using timeseries approaches given the small number of days we had and the risk of overfitting. In the last weekend, I tried enriching cell data with data from other cells for the same day/donor. This data was added as additional channels in a 3D input stream (batch, channel, features). I fed that to both LSTM and CNN models for a couple of dd folds with about the same results as my best existing models. With better tunning the results might have been different. Also, due to lack of time I only used the prediction of the first channel. I initially intended to ensemble the predictions from all channels as I theorized that the diversity would help. However, I wondered if it would offset the improvements of ensembling with other models, so I decided not to spend the time writing the corresponding code. NLP Transformers In my mind this problem is a classic case of language translation: from DNA to RNA, and from RNA to proteins. Transcription errors occur all the time and the measurement also introduces errors, hence the data ends up being noisy. I envisioned the following approach when I started the competition: Assign a token to each input feature, to be defined as the column_number + a constant; build the input streams using the tokens of non zero features. For multi, assign a token to each target column, to be defined as the column_number + a constant; build the target streams using the tokens of non zero columns. Split the data in some way that makes sense to accommodate the maximum length limitations of the transformer. For example, for multi I intended to break data per chromosome and then merge the outputs. I had a few ideas for the merge, but wanted to see the output data before making a decision. My expectation was that each protein would be produced only by one of the chromosomes and that could facilitate the merging. In the cases in which the stream length for a chromosome was still too large, I'd further split it in partially overlapping segments, which the model would run through the transformer, to subsequently concatenate the outputs and run through the rest of the model; Use any specific transformers for genetic data or regular ones for text (deberta, for example) and pretrain them with the competition data (e.g. MLM) or any other data publicly available (not needing tokens might facilitate that). For multi, build both sequence-to-sequence models that take the binary input features and produce binary targets, and models that directly predict the value of each target column. For cite, use only the latter approach. Depending on the performance of the transformer models, use their predictions directly in ensembling or to adjust the predictions of other models, e.g., merge both models and use the output of the transformer as a multiplier for the output of the other model, while also merging the losses of both models. Last Monday I finally gave it a go for multi. Given the limited time I had left, I went with a direct prediction of the target values using deberta. It didn’t take me long to write a draft version of the code, but I kept running into memory issues. I tried to work through them for a while, but it was late and after a week of not much sleep I sadly concluded that I had ran the clock. From the moment I started I was conscious that there wasn’t enough time left on that day to still use this approach in the competition, but I wanted to know if it would work. ensembling I used linear regression on oof data for ensembling. I intended to use only the cells that better matched the PB sample but ended up discarding that idea, because the adversarial approach I used suggested that the vast majority of the training data was easily distinguishable from the PB data. To address the fact that the split of cells per day/donor was significantly different between training and LB, I balanced the oof data to have an equal number of cells per day/donor. As mentioned earlier, I produced three sets of ensembles based respectively on d, dd and dde models. In the latter case, I excluded any models that used feature combinations that performed poorly with d and dd models, especially in the last day of the training data. A notable example were models using only PCA and day, which were the best performers for dde, but did poorly on d or dd. That suggested overfitting that would not generalize well to unseen days. The following diagram summarizes the main characteristics of the models that compose my best solution (dde). Final thoughts My two final selections were an ensemble of dde models and an ensemble of merged d/dd models. The former performed better, but I suspect there is a problem with the merging of the d and dd ensembles. Out of curiosity, while writing this I submitted my best cite and multi models and got a 0.770982 PB score, which would rank 37th in the PB. Ensembling boosted it by +0.0012, which is consistent with what I measured with the CV. I should note though that the best cite model was not part of my selected ensemble. I’m pleased that I selected the ensemble with the highest PB score. I suspected there was going to be a big shakeup in the LB, except perhaps for the top positions, and for the most part that ended up happening. In the absence of any novel ideas, I credit my final position to strong fundamentals, particularly a solid CV strategy. That’s not what I expected to accomplish when I started, but I had a lot of fun participating in this competition and look forward to doing things differently, and hopefully better, next time. Please sign in to reply to this topic. comment 0 Comments Hotness",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules Silogram · 11th in this Competition  · Posted 3 years ago arrow_drop_up 53 more_vert 12th-Place Solution First, congratulations to the winners, especially @senkin and @tmp for leading throughout and @shujisuzuki65 for his big jump from public to private LB. I'm eager to hear from both teams about their techniques. Overall, it was an interesting competition that gave me a greater appreciation for the challenges that bioinformaticians face. I don't know if others feel the same, but it seemed like a very long competition to me. I ran out of gas about midway and didn't really work on it much the last 4 weeks or so, which means I didn't use the raw counts at all. My solution, therefore, is fairly simple. CV setup: I assigned each batch (unique user/day) to a separate fold, so I had 9 folds for citeseq and 12 for multiome. This was expensive, but had the virtue that it wasn't optimized towards either new donors (public LB) or new days (private LB). LB scores tracked local CV scores very closely. Even small gains in local CV almost always led to similar gains on the LB. This CV scheme was probably the reason that I fared well on the private LB (46->12). Data transformations: I tried a lot of ways to denoise and transform the data, but most of them failed. In the end, I just used PCA and tSVD of the original data. Feature engineering: None for Multiome. For Citeseq, I trained 140 shallow LGB models (one for each target) using the full set of data. The goal here was not to use the models themselves, but to see which features were important for each target. I used the top 100-200 features per target in the later, deeper modeling. Modeling: For Citeseq, I trained both single-target (140 separate models) and multi-target NN models (using Fastai). I also trained LGB and CatBoost models for each target. Altogether, I trained over 20 sets of models using different variations of the PCA data combined with selected features from the feature engineering. Individually, the models had local CV scores in the range 0.8995 - 0.9017. For multiome, I trained 3 multi-target NNs on the tSVD-reduced targets, and one CatBoost model. Ensembling: I blended the models together using a very simple optimized weighting scheme where the only possible weights were 0,1,2, or 3. I tried other ensembling techniques that had higher CV scores, but they performed worse on the LB. I was afraid this might be due to some hidden leakage between folds, so I stuck to the simpler weighting scheme. This led to local CV scores of 0.9039 for Citseq and 0.669 for Multiome. Thoughts about trends in data: I was intrigued by @AmbroseM 's posts arguing that the data is a time series. There are undoubtedly trends over the 7 days of training data, but I was concerned about whether these trends would continue to day 10. I don't know enough about the biology, but it seems likely that cell behavior has both long-term trends (aging) and short-term trends based on things like diet, exercise, illness, etc. I decided that the trends visible in the 7 days of training data could very easily be short-term trends that would reverse themselves after 7 days, or could be just coincidental due to technical aspects of the data collection. Based on @AmbroseM 's post here ( https://www.kaggle.com/competitions/open-problems-multimodal/discussion/366395) , it seems like the trends did, in fact, continue to the test set. I would be very interested to hear from some cell scientists about what these trends might signify, whether they're cyclical in nature, and if yes, how long each cycle is typically. Please sign in to reply to this topic. comment 10 Comments 1 appreciation  comment Hotness Anthony Chiu Posted 3 years ago · 13th in this Competition arrow_drop_up 3 more_vert The goal here was not to use the models themselves, but to see which features were important for each target. I used the top 100-200 features per target in the later, deeper modeling. I think this is really cool! Alexander Chervov Posted 3 years ago · 234th in this Competition arrow_drop_up 1 more_vert Thanks again for sharing and comments ! May I ask you about the following, would you be so kind to share the results you mention: \"I trained 140 shallow LGB models (one for each target) using the full set of data. The goal here was not to use the models themselves, but to see which features were important for each target. I used the top 100-200 features per target in the later, deeper modeling.\" It would be great - if you can share these importances (and/or those 100-200 selected features) ! We plan to make further analysis of the data from the biological perspective - if you would have time to take part - that would be honor for us. PS Technically:  may be create some like \"Kaggle dataset\" and save csv files with importances  (and/or list of those 100-200 selected features) Something like that: https://www.kaggle.com/datasets/kaggledummie007/msci-cite-importances Silogram Topic Author Posted 3 years ago · 11th in this Competition arrow_drop_up 2 more_vert @alexandervc Sorry for the delay. I've uploaded the feature importance tables here: https://www.kaggle.com/datasets/psilogram/citeq-feature-importance there are a couple different versions from different runs. Let me know if you have any questions. Alexander Chervov Posted 3 years ago · 234th in this Competition arrow_drop_up 1 more_vert Thank you very much for sharing ! https://www.kaggle.com/code/alexandervc/silogram-cd36-feature-importances Here is some brief look on importances for CD36. We can see quite some biology from them - e.g. - CD36 protein is higly activated in erythroid like cells - similar to red blood cells - so you can see genes like HBD, HBB, HBA1 as important features - that various forms of the hemoglobin - so quite as expected from biology. Also look on the so-called genes enrichment analysis with KEGG pathways - again biologically reasonable results, and compared with importances by other methods - they are quite consistent. That is a first look - we need some time to get more insights from the data. Barry Posted 3 years ago · 29th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for posting your great solution Silogram! Alexander Chervov Posted 3 years ago · 234th in this Competition arrow_drop_up 1 more_vert Thanks for sharing ! And congratulations with the gold ! May I ask a question about the validation scheme: from the naive point of view it is not that much strong -  validation fold and train folds have the same day and the donor - so from the naive point of view it is not easy to expect that it would be similar to LB which contains completely new day and donor. What do you think ? Senkin13 writes that even random validation worked well for him, but they carefully checked the features. Validation scheme by KSS (and similar our scheme) https://www.kaggle.com/competitions/open-problems-multimodal/discussion/366471 might be stronger, but for after private LB was opened - I do not see full correspondence between private LB and our  scheme. So for you, Senkin some more simple validation scheme works, but for me even more sophisticated does not fully work, so it seems the validation scheme is not the guarantee for the full correspondence between CV and LB. May be there are some additional trick to control features to include ? What are your opinion about that ? Silogram Topic Author Posted 3 years ago · 11th in this Competition arrow_drop_up 3 more_vert I looked at it from a slightly different perspective. The public LB data has the same days but a different donor, so to optimize CV for the public score, we would want 3 folds, one for each donor in the training set. But this set-up could potentially penalize the private LB since it would not optimize for unseen days. Conversely, to optimize for the private LB, we would want separate folds by day (or even possibly past/future days if we believe it is a time series problem), but such a set-up might give us misleading feedback on the public LB if it turns out that the data is highly specific to donors. My system is a compromise between the donor/day approaches. The fact that it aligned so closely with the public LB gave me confidence that it was generalizing well for new donors, and the fact that each training fold contained data from all days meant that it wasn't biased too much by any particular day. The one thing my CV set-up doesn't do is optimize for future days, and this turned out to be its biggest failing.  Despite some clear trends in the data, I didn't know enough about the underlying biology to be persuaded that the trends would continue into the private test days. In retrospect, this is my biggest regret. Also, that I didn't bite the bullet and re-run my entire pipeline with the new raw data when it was released. SergioMiguelM Posted 3 years ago · 36th in this Competition arrow_drop_up 0 more_vert I agree with this!!! I thought about the same thing!! I guess that I was a bit skeptical that in fact the trend was going to continue, so I decided to make a general solution considering both public and private LB. Chris Deotte Posted 3 years ago arrow_drop_up 2 more_vert Congratulations Silogram. Great CV scheme and jump upward from public to private LB! Ravi Ramakrishnan Posted 3 years ago arrow_drop_up 2 more_vert Hearty congratulations @psilogram , thanks for the approach too. All the best!! Appreciation (1) Olqa_842 Posted 3 years ago arrow_drop_up 0 more_vert Thanks for sharing @psilogram",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules Little Boat · 12th in this Competition  · Posted 3 years ago arrow_drop_up 46 more_vert 13th place and how to Hey guys, it's been a while since my last actively participated competition and this is a good experience! Though I am not active any more, I frequently come back to kaggle to browse new ideas. One thing I notice is that it is not really straightforward to learn new things by just reading top solutions if you were not active in that competition… And one factor could be that we mostly share what finally worked (and/or what didn't work), but not the thought process of getting there. For someone like me, either a bit too lazy or a bit too busy to actively participate, or someone who might be a bit inexperienced, I think it would be more beneficial to share \"how I got here\" more than \"here I am\". So in that spirit, I would like to start sharing solutions this way… Some background: My job can be demanding so whenever I can delegate the work to computers, I do, whenever I cannot, I minimize the time needed for coding such that I can leverage fragamented time as much as possible. And I mainly participate in this competition to learn how feature extraction could work with high dimension inputs and outputs. And they shape what next experiments I decided to try, and mistakes I made along the way so I thought it is important to share. My journey: started by reading the Discussion to understand what the data is about, and walk through popular public kernels to understand what can be used as baselines. I noticed that mostly TruncatedSVD was used to reduce dimensions. I figured it might be a good idea to just train a mlp model with all features included. And I was too lazy to build a local validation pipeline so I just randomly sampled 10% as validation set. And the result wasn't so good on public LB compared to public kernels. Then I was thinking, ok, maybe it was because mlp was bad. But xgboost/lightgbm on cpu would take forever to train, and would run out of memory on gpu. So need a better neural network model. What about Tabnet (terrible). Ok. There was one paper I remember claiming similar performance to xgboost. OK. found it. https://arxiv.org/pdf/2112.02962.pdf (It is called DANets). Better but still not as good… Now back to public kernel as the baseline. Maybe instead of SVD, we can use autoencoder? OK, only linear autoencoder  performed ok-ish, any nonlinearity didn't work… no matter what tricks (e.g. swap noise augmentation) used. Hmmm… Back to TSVD + MLP as baseline again… Let's make this baseline better first. First swap MLP with DANets. And let's just focus on cite since it only have high dim inputs. Whatever works for cite should work for multi right? (TimeMachine: Nope!) Let's standardize the data since PCA likes it. OK. Slightly better. The explained variance seems quite low, but adding more components as features doesn't seem too helpful. So likely the inputs are quite noisy. Don't really know what to do. Well, we can always add different decomposition methods if no better ideas. Added NMF, FactorAnalysis, FastICA. Ok. All of them worked. Now let's also train some xgboost model since now we can train on gpu. Ok cool. Averaging xgboost with DANets improves results significantly. Maybe should try autoencoder again…. Read some papers. Ok. Still didn't work. Let's try some popular nonlinear dimension reduction techniques. UMAP, TriMAP, PaCMAP. Doesn't seem working. XGBoost or LightGBM train one model per target which seems wasteful and may not consider the correlation between outputs. Let's see if there is a better way out there. https://arxiv.org/abs/1909.04373 found this GBDT-MO. and it has code. Tried. Didn't work so well. Read in the Discussion that we can select features by matching input/output names for Cite. Tried and it worked. Neat. Also read in the Discussion that the 0s in inputs may not be actual 0s could also be missing. OK. Tried to calculate the mean/std by considering all 0s as missing and then calculate PCA. Adding to the features improved the model a bit. Realized that if 0s can be treated as missing then we can calculate PCA differently too according to this old paper ( https://www.sciencedirect.com/science/article/abs/pii/S016974399600007X) . Helped a bit. OK… stop the laziness and spend the weekend on building proper cross validation pipeline and retrain the models for cite. Nice. a huge jump from 0.812 to 0.814 on leaderboard. Saw some discussion about the supplymentary raw data. Well, I believe the host has done the best preprocessing and probably not helpful, also it is a pain to handle two datasets, so ignored. (TimeMachine: Big Mistake!) Approaching the last week of competition so rewire the cite pipeline for multi. Hoping to see a huge jump on score for multi as seen on cite. But that didn't happen. So… They are actually very different… Maybe I should have accepted some team merging invite earlier…. stack a few DANets and GBDT models. picked the wrong submissions for final evaluation. But what can you do. Model Summary Cite: (PCA + NMF + FA + ICA + NanPCA + Missing Value PCA (i.e. NIPALS) ) + (DANets + XGBoost) Multi: (PCA + NMF) + (DANets + XGBoost with SVDed Output) Looking Back, to improve the score further Should have spent more time on data preprocessing. Should anticipate and prepare for a complex ensemble pipeline so save all the models and predictions properly along the way for multi layer stacking. Shouldn't have assumed Cite and Multi being similar and go for team up. Hopefully it is helpful (also to those who didn't participate!) Please sign in to reply to this topic. comment 17 Comments Hotness Vladimir Posted 3 years ago arrow_drop_up 1 more_vert @xiaozhouwang , thank you very much for sharing! Can you clarify which cross-validation pipeline did you use in the end? Alexander Chervov Posted 3 years ago · 234th in this Competition arrow_drop_up 1 more_vert Thank you very much for sharing your wonderful ideas ! May I kindly ask you: what Python packages you used for 1) NanPCA 2) Missing Value PCA (i.e. NIPALS) ? I cannot google anything like \"NanPCA\". For NIPALS there is package https://python-nipals.readthedocs.io/en/latest/usage.html there is not any single line of example how to use it. Or you are using \"R\" ? ======= PS For Python I found: https://www.statsmodels.org/dev/generated/statsmodels.multivariate.pca.PCA.html missing = ‘fill-em’ - use EM algorithm to fill missing value. ncomp should be set to the number of factors required. Barry Posted 3 years ago · 29th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for posting your great solution Little Boat! Anthony Chiu Posted 3 years ago · 13th in this Competition arrow_drop_up 1 more_vert haha yeah… I wished you accepted our merge request. I am impressed that you can do that many experiments within such a short time and with your startup work. I am also a startup guy, I am interested in how did you manage/track your experiment? I tried using excel or W&B, but non of them truly sped up or organized my workflow.  Lately, I am considering setting on MLOps framework like kubeflow in my personal computer to see if it is helpful, what do you think? Little Boat Topic Author Posted 3 years ago · 12th in this Competition arrow_drop_up 2 more_vert just a lot of messy code + W&B runs… I don't know the best way to trade off code quality with the speed of experiments actually. The good thing is that you have to do things with quality at work. So I guess that balances things 😄 luddite^ Posted 3 years ago · 22nd in this Competition arrow_drop_up 1 more_vert Thank you for sharing. This is very helpful for a newbie like me. About DANets, I couldn't find its implementation. Could you share your code? Little Boat Topic Author Posted 3 years ago · 12th in this Competition arrow_drop_up 1 more_vert if I got time to clean up the code will post here Chris Miles Posted 3 years ago · 360th in this Competition arrow_drop_up 1 more_vert Thank you very much. This full-story format is really awesome. even for those who did participate in the competition, it is most helpful for learning. Jiwei Liu Posted 3 years ago · 73rd in this Competition arrow_drop_up 1 more_vert did you use the xgb multioutput mode? https://xgboost.readthedocs.io/en/stable/tutorials/multioutput.html ? Little Boat Topic Author Posted 3 years ago · 12th in this Competition arrow_drop_up 1 more_vert Yes. And you can run it on GPU too! pineapple Posted 3 years ago arrow_drop_up 2 more_vert Thank you so much! This is extremely helpful, especially for illustrating the importance of cleaning data and creating a good validation set. Congrats on an amazing result! Mahyar Arani Posted 3 years ago arrow_drop_up 2 more_vert Tons of learning here. I usually has  the same issue with finding dimensionality reduction technique that works and get better results too. (like you mentioned, UMAP, TriMAP, PaCMAP. Doesn't seem working) rump roast Posted 3 years ago · 803rd in this Competition arrow_drop_up 2 more_vert Appreciate the \"thought process\" details here. Thank you. JohnM Posted 3 years ago arrow_drop_up 0 more_vert Always great to hear your thoughts, LB. I learned a lot from you when first starting out on Kaggle. Akmal Azzam o'g'li Posted 3 years ago · 178th in this Competition arrow_drop_up 0 more_vert @xiaozhouwang Thank you for sharing, one starter question, what is MLP? Little Boat Topic Author Posted 3 years ago · 12th in this Competition arrow_drop_up 1 more_vert it is multi layer perceptron, a.k.a fully connected neural network https://en.wikipedia.org/wiki/Multilayer_perceptron Jiwei Liu Posted 3 years ago · 73rd in this Competition arrow_drop_up 0 more_vert wow, amazing. Great to have you back and share your journey!",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules Anthony Chiu · 13th in this Competition  · Posted 3 years ago arrow_drop_up 23 more_vert Public 6th Private 14 Solution Intro I have been mainly working on the Cite part. I tried many things, multi part by @paragkale https://www.kaggle.com/paragkale/private-14th-public-6th-multiome-portion The following tricks gave me the most gain. I will update in this post about the code and what didn't work. Extra Data The raw count data released by the host. Dimensionality Reduction I think the most helpful one are: sklearn.decomposition.TruncatedSVD (128 comps) Self-made denosing auto encoder (128 hidden nodes) Direct Features Direct features based on matching the names Direct features based on absolute correlation to targets Direct features based on the list shared by the hosts in this thread https://www.kaggle.com/competitions/open-problems-multimodal/discussion/366392 Use base models outcomes as NN inputs feature for ensembling It is known that MSE is not a really good loss function for the competition metric. Therefore within each fold, we trained 4 base models and used their features for the NN input. sklearn.linear_model.Ridge sklearn.linear_model.MultiTaskElasticNet sklearn.kernel_ridge.KernelRidge sklearn.ensemble.HistGradientBoostingRegressor We added heavy noise to their predictions to make sure the NN can learn from other features as well self.blender = torch. nn .Sequential(\n            GaussianNoise(self.blend_noise),\n            torch. nn .Linear(out_dim * 4 , 128 ),\n            torch. nn .LayerNorm( 128 ),\n            activation(),\n            torch. nn .Dropout(self.blend_dropout),\n        ) content_copy GroupK Cross Validation on Target Clusters The tricks in this section increased both public and private LB, but we cannot compare the CV because it is a CV scheme change. Luckily it is (relatively, I guess?) performing well on both public and private. It is known that there are some subtle domain shifts between train, private and public test sets. However, the difficulty is that the shift is happening in at least 3 directions (donor, day, cell types). To create a hard but not too hard CV scheme, we find that clustering the target values performed very well on both of the public and private leaderboard. Let's consider the CV scheme selection as a spectrum: The easiest CV scheme: Random K fold (Downside: not representative of the test set) The hard CV scheme: GroupKfold by day/donor (Downside: too few fold to train) The hardest CV scheme 1: Time series split (Downside: wasting the last day data) The hardest CV scheme 2: Excluding the 1 day or 1 donor completely from the training set (Downside: too hard/defensive) Another reason of doing the clustering is that the day here is categorical, however in real life, time is continuous. GroupK CV by day is not that satisfying. The first image shows the target kmeans result (colors)  visualized with the tsvd targets (points): Next, you can see the target clusters capture the cell type differences: And the shifts of day and donor are not that significant compared to the cell types in the context of target clustering: Regularization/Augmentation The tricks in this section increased both CV and LB. Seed-bagging I think most people have done this, we trained the same model a few more times with the different seeds for blending. Mixup Augmentation and Stochastic Weight Averaging The training is done on roughly 3 stages 1. Mix up augmentation stage Since all features are numerical values, mixup worked well. def mixup_augmentation( x : torch.Tensor, y : torch.Tensor, alpha: float = 5 ):\n    lam = np. random .beta(alpha, alpha)\n    rand_idx = torch.randperm( x .shape[ 0 ])\n    mixed_x = lam * x + ( 1 - lam) * x [rand_idx, :]\n    target_a, target_b = y , y [rand_idx] return mixed_x, target_a, target_b, lam content_copy 2. Normal training stage 3. SWA stage https://pytorch.org/docs/stable/optim.html#putting-it-all-together This is similar to seed-bagging, I am not sure if they are overlapping or if they have their benefits here. Please sign in to reply to this topic. comment 12 Comments Hotness kmkm1234 Posted 3 years ago · 49th in this Competition arrow_drop_up 1 more_vert Congrats! The idea of GroupK Cross Validation on the Target Clusters is so novel and interesting for me! I learned a lot from it. Anthony Chiu Topic Author Posted 3 years ago · 13th in this Competition arrow_drop_up 2 more_vert Thanks for you kind words. As always, I can't conclude it is really helpful based on only the final result; I think we need more experiments for most tricks reported in this comp to \"conclude\" what tricks are actually helpful to both unseen donors and unseen days. If you look at our sub sorted by private score in the below image, groupk by donor got the gold range private scores, but the public score is very low… target clustering seems to be ok-ish… If I could redo the entire competition again, I think I would do a simple K-fold, but validate/early-stop on the last day of the validation fold data. This trick has been used on kaggle many times to allow us to train on full data but still fit towards the latest data in time. You can see @AmbrosM mentioned here as well https://www.kaggle.com/competitions/open-problems-multimodal/discussion/366395#2031471 My old team has used this trick before as well (3 years ago): https://www.kaggle.com/competitions/nfl-big-data-bowl-2020/discussion/119395 I think I was overthinking about the \"domain shift\" here. There are always some domain shifts in kaggle data, if you compare the shakeup this time to other historical kaggle competitions, this time is not huge… And looking at the gold solutions, nothing crazy/fancy domain adaptation skills have been performed. Mostly is about careful feature generation/selection if I hasn't missed anything. Anthony Chiu Topic Author Posted 3 years ago · 13th in this Competition arrow_drop_up 1 more_vert To be clear, I am not saying we should ignore shifts in data. In the context of competition, the highest chance is that all participants cannot make a significantly closer public-private leaderboard score gap, which means the gap is more like a hidden difference between datasets, which is really hard to solve in a 3-month competition (or even years of research). Btw, I learned the concept and the difficulties of dataset shift in this paper: https://arxiv.org/abs/2007.00644 Most research on robustness focuses on synthetic image perturbations (noise, simulated weather artifacts, adversarial examples, etc.), which leaves open how robustness on synthetic distribution shift relates to distribution shift arising in real data. …. most current techniques provide no robustness to the natural distribution shifts in our testbed. The main exception is training on larger and more diverse datasets For research, of course, we want to make the public-private leaderboard gap as close as possible.  So, my reflection on this is we should have the leaderboard ranking based on the public-private leaderboard score gap in this type of competition. Then the competition ranking aligns with the host's objective of studying domain shift and domain adaptation. I think this is a feature request to @ryanholbrook , because it seems like people now care more about robustness then performance. Lastly, out of curiosity, I want to ask for the host's comment @danielburkhardt on the current finalized public-private leaderboard gap (0.81x ~ 0.77x). Is this gap  \"good enough\", \"can be improved\" or \"totally unacceptable\"? Without domain knowledge, I think the gap is not crazily huge, is it expected? Anthony Chiu Topic Author Posted 3 years ago · 13th in this Competition arrow_drop_up 1 more_vert I tried to rank the teams by 2 different \"robustness measures\" in this notebook https://www.kaggle.com/code/kingychiu/robustness-on-open-problems-multimodal?scriptVersionId=112466254 haha, I realized it is tricky to use robustness for a leaderboard because, generally, underfit models / poorly performed teams got quite robust scores… Parag Posted 3 years ago · 13th in this Competition arrow_drop_up 1 more_vert Here is the link to the multiome portion of our solution https://www.kaggle.com/paragkale/private-14th-public-6th-multiome-portion Very simple script, uses 12 folds for donor x day. emoji_people Patrick Chan Posted 3 years ago · 115th in this Competition arrow_drop_up 1 more_vert Congrats! I would like to clarify the sentence \"We added heavy noise to their predictions to make sure the NN can learn from other features as well\".. Do you mean adding a big dropout value to the base model predictions? Anthony Chiu Topic Author Posted 3 years ago · 13th in this Competition arrow_drop_up 1 more_vert We have both noise and dropout. We have added this layer copied from the intenet  with stddev ~ 0.8 class GaussianNoise (torch.nn. Module ): def __init__ ( self , stddev ): super ().__init__() self .stddev = stddev def forward ( self , din ): if self . training: return din + torch.autograd. Variable (\n                torch.randn(din.size(), device=din.device) * self .stddev\n            ) return din content_copy also 0.8 dropout as well emoji_people Patrick Chan Posted 3 years ago · 115th in this Competition arrow_drop_up 0 more_vert Thanks for the clarification nlgn… Good work. Jiwei Liu Posted 3 years ago · 73rd in this Competition arrow_drop_up 1 more_vert great solution! congrats! Seed-bagging Can I ask how much improvement you got with seed-bagging? Anthony Chiu Topic Author Posted 3 years ago · 13th in this Competition arrow_drop_up 0 more_vert For example, this is 4-seed groupk cv by donor result cv cite_tsvd_50_torch_nn_oof_0 . 894400 .npz 0 . 8943999611714102 cite_tsvd_50_torch_nn_oof_0 . 894471 .npz 0 . 8944711303264004 cite_tsvd_50_torch_nn_oof_0 . 894500 .npz 0 . 8945004071945007 cite_tsvd_50_torch_nn_oof_0 . 894614 .npz 0 . 8946143443923297 0 . 8956681046650627 content_copy I haven't compare the lb of seed bagging for so long time, so cannot give you a number now,. Jiwei Liu Posted 3 years ago · 73rd in this Competition arrow_drop_up 1 more_vert so for the cv score, 4 seed bagging got 0.001 improvement over 1 seed? Impressive! KhanhVD Posted 3 years ago arrow_drop_up 2 more_vert Congrats on results and condolences to gold @kingychiu and @paragkale",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules AyPy · 16th in this Competition  · Posted 3 years ago arrow_drop_up 19 more_vert 16th Place Solution Summary Thanks to the competition host, Kaggle team, Saturn cloud team and congrats to all the winners! Although many great solutions have already been posted and my solution may not contain new approaches, I would like to leave my efforts over the past two months. In this term I learned a lot. Thanks to the all competitors for a great game. Please forgive me if it is difficult to read this post or see the schematic diagram below, as my command of English is not very good and my educational background was different from computer science or machine learning area. Overview: The machine learning algorithms used were as follows: Two MLPs (4 and 9 hidden layers, the variants of Laurent Pourchot's model ) Conv1d (almost all the same as the tmp’s 1D-CNN model for tabular data on MoA competition ) LGBM In my case, stacking scheme boosted the score. The outputs of level 1 models were concatenated and then used as input for level 2. It was effective to apply dimensionality reduction to concatenated level 1 outputs after standardization. When the outputs were just concatenated without dimensionality reduction, the score of level 2 was rather lower than that of level 1. Also, ensemble worked well. I created several models with slight difference (different dimensionality reduction algorithms, feature extraction methods and loss functions) and blended them. In addition, each learning process was performed on 15 random-seeds and results were averaged. CV scheme: I used simple KFold (k = 5). Fortunately, I resulted in shakeup in private LB. Citeseq: The diagram of my Citeseq stacking scheme is as follow. Preprocess Before dimensionality reduction or feature extraction in level 1, the set of all features which are constant in the train or test were eliminated according to the AmbrosM’s Code . Dimensionality reduction: tSVD and PCA (n_components = 64) were used separately and inference results were finally blended. Feature extraction: According to the Fabien Crom's Code , I picked features with high Pearson correlation coefficients for the targets. In order to gain diversity as much as possible, I changed the picking query for each models. For example: ・Extract the RNAs in order of highest average of correlation to 140 proteins ・Extract the RNAs that have a high correlation value for a single protein, not the average ・Change how many of the top RNAs are extracted ・With or without dimensionality reduction after extracted Multiome: The scheme is almost the same as that of Citeseq. The differences are as follows: Extracted features were not used. In the Multiome case, the score was deteriorated when they were concatenated with 64 dims compressed features. For NN algorisms, only MSE was used as loss function. The target vectors were compressed to 512 (for NN) or 128 (for LGBM) dims by tSVD and used in training. The inferred vectors were decompressed to 23418 dims by inverse SVD. Please sign in to reply to this topic. comment 1 Comment Hotness Barry Posted 3 years ago · 29th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for posting your solution AyPy!",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules",
      "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Late Submission more_horiz Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Overview Data Code Models Discussion Leaderboard Rules"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict how DNA, RNA & protein measurements co-vary in single cells The dataset for this competition comprises single-cell multiomics data collected from mobilized peripheral CD34+ hematopoietic stem and progenitor cells (HSPCs) isolated from four healthy human donors. More information about the cells can be found on the vendor website . Measurements were taken at five time points over a ten-day period. During this time, cells were cultured with StemSpan SFEM media supplemented with CC100 and thrombopoietin (TPO) and incubated at 37ºC. Media was changed every 2-3 days. No additional media supplements were added to the cell culture conditions. From each culture plate at each sampling time point, cells were collected for measurement with two single-cell assays. The first is the 10x Chromium Single Cell Multiome ATAC + Gene Expression technology (Multiome) and the second is the 10x Genomics Single Cell Gene Expression with Feature Barcoding technology technology using the TotalSeq™-B Human Universal Cocktail, V1.0 (CITEseq). If you've never worked with this data type before, we've included some links at the bottom of this description. Each assay technology measures two modalities. The Multiome kit measures chromatin accessibility (DNA) and gene expression (RNA), while the CITEseq kit measures gene expression (RNA) and surface protein levels . Following the central dogma of molecular biology: DNA --> RNA-->Protein , your task is as follows: To help guide your analysis, we performed a preliminary cell type annotation based on the RNA gene expression using information from the following paper: https://www.nature.com/articles/ncb3493 . Note, cell type annotation is an imprecise art, and the concept of assigning discrete labels to continuous data has inherent limitations. You do not need to use these labels in your predictions; they are primarily provided to guide exploratory analysis. In the data, there are the following cell types: The experimental observations are contained in several large arrays. We provide these arrays in HDF5 format. The data splits are arranged as follows:  Your task is to predict the labels corresponding to the inputs in the test set. To facilitate submission scoring, we only require predictions on a subset of the Multiome data. This subset was created by sampling 30% of the Multiome rows, and for each row, 15% of the columns. The sample of columns varies from row-to-row. All of the CITEseq labels are scored. 11 files 28.84 GB h5, csv Attribution 4.0 International (CC BY 4.0) Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 28.84 GB evaluation_ids.csv metadata.csv metadata_cite_day_2_donor_27678.csv sample_submission.csv test_cite_inputs.h5 test_cite_inputs_day_2_donor_27678.h5 test_multi_inputs.h5 train_cite_inputs.h5 train_cite_targets.h5 train_multi_inputs.h5 train_multi_targets.h5 11 files 15 columns ",
    "data_description": "Open Problems - Multimodal Single-Cell Integration | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Open Problems in Single-Cell Analysis · Featured Prediction Competition · 3 years ago Open Problems - Multimodal Single-Cell Integration Predict how DNA, RNA & protein measurements co-vary in single cells Open Problems - Multimodal Single-Cell Integration Late Submission more_horiz Overview Data Code Models Discussion Leaderboard Rules Overview Start Aug 15, 2022 Close Nov 16, 2022 Merger & Entry Description link keyboard_arrow_up Goal of the Competition The goal of this competition is to predict how DNA, RNA, and protein measurements co-vary in single cells as bone marrow stem cells develop into more mature blood cells. You will develop a  model trained on a subset of 300,000-cell time course dataset of CD34+ hematopoietic stem and progenitor cells (HSPC) from four human donors at five time points generated for this competition by Cellarity, a cell-centric drug creation company. In the test set, taken from an unseen later time point in the dataset, competitors will be provided with one modality and be tasked with predicting a paired modality measured in the same cell. The added challenge of this competition is that the test data will be from a later time point than any time point in the training data. Your work will help accelerate innovation in methods of mapping genetic information across layers of cellular state. If we can predict one modality from another, we may expand our understanding of the rules governing these complex regulatory processes. Context In the past decade, the advent of single-cell genomics has enabled the measurement of DNA, RNA, and proteins in single cells. These technologies allow the study of biology at an unprecedented scale and resolution. Among the outcomes have been detailed maps of early human embryonic development, the discovery of new disease-associated cell types, and cell-targeted therapeutic interventions. Moreover, with recent advances in experimental techniques it is now possible to measure multiple genomic modalities in the same cell. While multimodal single-cell data is increasingly available, data analysis methods are still scarce. Due to the small volume of a single cell, measurements are sparse and noisy. Differences in molecular sampling depths between cells (sequencing depth) and technical effects from handling cells in batches (batch effects) can often overwhelm biological differences. When analyzing multimodal data, one must account for different feature spaces, as well as shared and unique variation between modalities and between batches. Furthermore, current pipelines for single-cell data analysis treat cells as static snapshots, even when there is an underlying dynamical biological process. Accounting for temporal dynamics alongside state changes over time is an open challenge in single-cell data science. Generally, genetic information flows from DNA to RNA to proteins. DNA must be accessible (ATAC data) to produce RNA (GEX data), and RNA in turn is used as a template to produce protein (ADT data). These processes are regulated by feedback: for example, a protein may bind DNA to prevent the production of more RNA. This genetic regulation is the foundation for dynamic cellular processes that allow organisms to develop and adapt to changing environments. In single-cell data science, dynamic processes have been modeled by so-called pseudotime algorithms that capture the progression of the biological process. Yet, generalizing these algorithms to account for both pseudotime and real time is still an open problem. Competition host Open Problems in Single-Cell Analysis is an open-source, community-driven effort to standardize benchmarking of single-cell methods. The core efforts of Open Problems include the formalization of existing challenges into measurable tasks, a collection of high-quality datasets, centralized benchmarking of community-contributed methods, and community-focused events that bring together diverse method developers to improve single-cell algorithms. They're excited to be partnering with Cellarity, Chan Zuckerbeg Biohub, the Chan Zuckerberg Initiative, Helmholtz Munich, and Yale to see what progress can be made in predicting changes in genetic dynamics over time through interdisciplinary collaboration. There are approximately 37 trillion cells in the human body, all with different behaviors and functions. Understanding how a single genome gives rise to a diversity of cellular states is the key to gaining mechanistic insight into how tissues function or malfunction in health and disease. You can help solve this fundamental challenge for single-cell biology. Being able to solve the prediction problems over time may yield new insights into how gene regulation influences differentiation as blood and immune cells mature. Competition header image by Pawel Czerwinski on Unsplash Evaluation link keyboard_arrow_up We use the Pearson correlation coefficient to rank submissions. For each observation in the Multiome data set, we compute the correlation between the ground-truth gene expressions and the predicted gene expressions. For each observation in the CITEseq data set, we compute the correlation between ground-truth surface protein levels and predicted surface protein levels. The overall score is the average of each sample's correlation score. If a sample's predictions are all the same, the correlation for that sample is scored as -1.0 . Submission File For each id in the evaluation set, you should predict a target value for that row_id . Your submission should contain a header and have the following format: row_id,target\n0,0.0\n1,0.0\n2,0.0\n3,0.0 ... content_copy Your submission file should contain only a subset of the test set observations. See the Data Description for the specific ids that should be included. Timeline link keyboard_arrow_up August 15, 2022 - Start Date. November 8, 2022 - Entry Deadline. You must accept the competition rules before this date in order to compete. November 8, 2022 - Team Merger Deadline. This is the last day participants may join or merge teams. November 15, 2022 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up The competition is part of the NeurIPS 2022 Competition Track. The prizes for top performing methods is as follows: 1st Place - $15,000 2nd Place - $7,000 3rd Place - $3,000 In addition to cash prizes, winners will be invited to present at the NeurIPS 2022 Competition Workshop, held virtually in November. Citation link keyboard_arrow_up Daniel Burkhardt, Malte Luecken, Andrew Benz, Peter Holderrieth, Jonathan Bloom, Christopher Lance, Ashley Chow, and Ryan Holbrook. Open Problems - Multimodal Single-Cell Integration. https://kaggle.com/competitions/open-problems-multimodal, 2022. Kaggle. Cite Competition Host Open Problems in Single-Cell Analysis Prizes & Awards $25,000 Awards Points & Medals Participation 7,770 Entrants 1,602 Participants 1,220 Teams 27,149 Submissions Tags Tabular Genetics Biotechnology Custom Metric Table of Contents collapse_all Description Evaluation Timeline Prizes Citation"
  },
  {
    "competition_slug": "amex-default-prediction",
    "discussion_links": [
      "/competitions/amex-default-prediction/discussion/348111",
      "/competitions/amex-default-prediction/discussion/347637",
      "/competitions/amex-default-prediction/discussion/349741",
      "/competitions/amex-default-prediction/discussion/348097",
      "/competitions/amex-default-prediction/discussion/350538",
      "/competitions/amex-default-prediction/discussion/347668",
      "/competitions/amex-default-prediction/discussion/347786",
      "/competitions/amex-default-prediction/discussion/347740",
      "/competitions/amex-default-prediction/discussion/348014",
      "/competitions/amex-default-prediction/discussion/347641",
      "/competitions/amex-default-prediction/discussion/347908",
      "/competitions/amex-default-prediction/discussion/347858",
      "/competitions/amex-default-prediction/discussion/347850",
      "/competitions/amex-default-prediction/discussion/349250",
      "/competitions/amex-default-prediction/discussion/347651",
      "/competitions/amex-default-prediction/discussion/348530",
      "/competitions/amex-default-prediction/discussion/347880",
      "/competitions/amex-default-prediction/discussion/347722",
      "/competitions/amex-default-prediction/discussion/347966"
    ],
    "discussion_texts": [
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules Correlation · 1st in this Competition  · Posted 3 years ago arrow_drop_up 294 more_vert 1st solution(update github code) First time be a solo winner, I must say there is luck in winning the competition. My best result is a heavy ensemble with LGB and NN. Data Model for NN model, all data fillna(0) and using nn.utils.rnn.pack_padded_sequence to pad. I have to look for my early stage model, sorry for small font size in figure. update: I release a clean code at https://github.com/jxzly/Kaggle-American-Express-Default-Prediction-1st-solution . note: You may not be able to reproduce the best result due to random fluctuations. 8 Please sign in to reply to this topic. comment 112 Comments 1 appreciation  comment Hotness Marília Prata Posted 3 years ago · 4725th in this Competition arrow_drop_up 5 more_vert A huge congratulation Daishu! 1st time solo winner is an achievement to be remembered. Mostly in this AMEX competition. Though I can't even understand any solutions since I'm a beginner. Thank you for sharing yours. delai50 Posted 3 years ago · 228th in this Competition arrow_drop_up 4 more_vert Thanks for sharing and congratulations for your first solo winner! Some questions from my side: How user-based and month-based rank features are calculated? Simply applying scipy.stats.rankdata to raw data per user and per month before making the aggregations? diff : Is it last value - nth value or nth value - nth-1 value ? LGB series oof : Let's call the number of customers in train set N_1 and the number of features N_2 . The feature matrix you used to calculate LGB series oof has size (N_1, N_2*13) , correct? What are GreedyBins ? Could you point some resource to take a look at it? How was your iteration process for trying new features? I mean, how did you decided which features to try/add to your already selected features each time? Answers from anybody are welcome :) Correlation Topic Author Posted 3 years ago · 1st in this Competition arrow_drop_up 12 more_vert df.groupby('cid')[num_features].rank(pct=True), df.groupby('year-month')[num_features].rank(pct=True) nth value - nth-1 value the training set is train_data.merge(train_y,how='left',on='cid') GreedyBins is a operation in LGB. https://blog.katastros.com/a?ID=01800-4e3a4f7c-6981-40af-b4dd-3224074d705a when cv and lb boosting, the feature were selected. This comment has been deleted. Muhammad Anas Mahmood Posted 3 years ago arrow_drop_up 1 more_vert Thanks for sharing this amazing way to summarize the solution guansuo Posted 3 years ago · 459th in this Competition arrow_drop_up 1 more_vert Congrats on the win! Thank you for sharing！ peterclerk Posted 3 years ago arrow_drop_up 1 more_vert Well done!! Mezue Posted 3 years ago arrow_drop_up 1 more_vert Well done!! Stefan Burger Posted 3 years ago arrow_drop_up 1 more_vert Congrats! Ed Kent-Hazledine Posted 3 years ago arrow_drop_up 1 more_vert Congrats, well done! 💪 Gizachew Alemu Posted 3 years ago arrow_drop_up 1 more_vert Congratulations on the win! Ranjeet shrivastav Posted 3 years ago arrow_drop_up 1 more_vert Congrats on the solo win🎉 SgangX Posted 3 years ago · 717th in this Competition arrow_drop_up 1 more_vert Thanks for sharing amazing way and l look forward to your github code!👍 Lamsaoub Posted 3 years ago arrow_drop_up 1 more_vert Thanks for sharing amazing way to summarize solution ChenSN Posted 3 years ago · 688th in this Competition arrow_drop_up 1 more_vert Congrats on the win! 🎉Thank U for sharing！ Athar Sayed Posted 3 years ago arrow_drop_up 1 more_vert Thanks for sharing amazing way to summarize solution Ivan Kireev Posted 3 years ago · 3185th in this Competition arrow_drop_up 1 more_vert Really lucky shake ) My congratulation and thanks for sharing! Nuriel Reuven Posted 3 years ago arrow_drop_up 1 more_vert Thank you for sharing! And congratulations for the win Matthew Setiadi Posted 3 years ago arrow_drop_up 1 more_vert congratulation @daishu and Thank you for sharing your interesting solution. Ahmed Gomaa Posted 3 years ago arrow_drop_up 1 more_vert Congrats on the win! 🎉 Felipe Posted 3 years ago · 848th in this Competition arrow_drop_up 1 more_vert Very interesting @daishu . Enjoy your first solo. Im also beginning in competitions and is very instructive to see your workflow. By the way, feature selection didnt improve my public also. Anyway… a tough competition in my opinion. Congrats!!! shahilpravind Posted 3 years ago · 3390th in this Competition arrow_drop_up 1 more_vert Congrats on the win! 🎉 Robert Hatch Posted 3 years ago · 45th in this Competition arrow_drop_up 1 more_vert Thanks for sharing! My first competition, I'm not sure what's normal: are you planning to share any Notebook(s) of the above work? Correlation Topic Author Posted 3 years ago · 1st in this Competition arrow_drop_up 6 more_vert I run all on my local machine. Maybe I will share code at github. Santiago Mota Posted 3 years ago · 92nd in this Competition arrow_drop_up 0 more_vert Please, do it This comment has been deleted. Mike Mazurov Posted 3 years ago arrow_drop_up 1 more_vert Congratulations! Interesting solution👍 Oscar Aguilar Posted 3 years ago · 4383rd in this Competition arrow_drop_up 1 more_vert Thanks for sharing your solution @daishu ! Very insightful. Ravi Ramakrishnan Posted 3 years ago · 1277th in this Competition arrow_drop_up 1 more_vert Hearty congratulations for the result!! The achievent is fantastic!! Great approach though!! Jiwei Liu Posted 3 years ago · 10th in this Competition arrow_drop_up 1 more_vert Congrats! Thank you for sharing. I'm interested in the \"global rank\" feature. How is it different from the raw feature? I thought they are the same for xgb/lgb. Thanks. Correlation Topic Author Posted 3 years ago · 1st in this Competition arrow_drop_up 4 more_vert Sorry for my mistake, it's user-based rank.",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules Konstantin Yakovlev · 2nd in this Competition  · Posted 3 years ago arrow_drop_up 181 more_vert 2nd place solution - team JuneHomes (writeup) Note: Full code to retrain single model will be shared here in a 2 weeks. I would like to say thank you to competition hosts and Kaggle - it was a great pleasure to participate in tabular data competition after many months and years without one. Thank you to all participants (and of course winners - @daishu huge jump during last 3 days and fantastic solo result) - our success is your success - you forced us to try harder - without all of you It would be impossible to learn so many new things and achieve such result. And special \"thank you\" goes to my fantastic teammates: Danila Alexey Igor No words would be enough to say how much each of you contributed to the end result. \"Do Data Scientists have hobby? Yes -> DS competition\". Competition announcement We were very hyped by the new tabular data competition release (sorry for the external link: link ) and immediately decided to participate. Slack notification -> rules and perspective advertisement (100% chance too lose summer holidays and all free time) -> and here we are - four members. Only one member of the team had previous experience in DS competitions participation. Few rules were established from the beginning: Only free time for competition No \"second\" accounts on Kaggle (even wife/friends to exclude any cheating suspicion) No competition discussion outside of the team We are here to learn and try our best Infrastructure and pipelines: Each of us had own machines / resources (GCP/AWS/local). We used Kaggle platform just for a few times. So the first thing we wanted to solve - unified machine to save all artefacts / experiments. We decided to go with AWS. I would say that it is possible to achieve the same result that we have just with Kaggle resources, but it would be bit more stressful for team management. We didn't want to spend a lot of money on AWS but sometimes (during very hot hours) RAM spikes were 500GB+ to permit simultaneous work. We tried to use neptune.ai for ML tracking but from July it was not very effective as we entered in brute force zone. Advise: Resources management is very critical - find bottleneck and remove it to make your team most effective. At the same time don't burn money recklessly - limit your budged. If any optimization possible - do it as soon as possible to save time and resources. Project Structure Each run was internally versioned (ex. v1.1.1 - (major version).(fe version).(model version)) Overall project structure: Initial preprocess -> artifact cleaned and joined df FE -> Many aligned (by uid) dfs with separated features Features selection -> dictionary we selection metadata Holdout Model (fe check and tuning) -> Local validation oof preds / holdout preds/ model / model metadata Full model run -> Model / Model metadata Prediction -> each fold oof predictions / cv split metadata / test predictions All these permitted us to go back and forward and check what worked well and what did not and restore experiments in each particular step. Initial preprocess We wanted to achieve several things with this step: Join Train and Test -> due to many people involved I was afraid that some missed transformation on private test part will be unnoticed. So we sacrifice memory and speed optimization for overall stability and security. Remove detected noise -> (we had options here but ended with unified single one) Transform Customer ID to unified uid Create internal subset feature -> Train / Public / Private Create unified kfold and holdout split -> To align all experiments Separate columns by type and store them separately to minify memory use and load time Remove detected noise We didn't use public notebooks for cleaning. Radar's Dataset is fantastic and it is 99% similar to our own transformations. We used \"isle\" identification without any pre-build coefficients. dummy code is something like this: for col in process_columns: \n\n        df = temp_df[[col]].sort_values(by=[col]) \n        df = df[df[col].notna()].drop_duplicates(subset=[col]).reset_index(drop= True )\n\n        df[ 'temp' ] = np.floor(df[col] * 100000 )\n        df[ 'group' ] = ((df[ 'temp' ] - df[ 'temp' ].shift()).abs() >= 100 ).cumsum()\n\n        i = 0 while True :\n            min_val = df[df[ 'group' ]==i][ 'temp' ].min()\n            if min_val> 0 :\n                break\n            i += 1 df[ 'temp2' ] = np.where(df[ 'temp' ]>= 0 , \n                                np.floor(df[ 'temp' ]/min_val).astype(np.int32),\n                                np.round(df[ 'temp' ]/min_val).astype(np.int32))\n\n        mapping = dict(zip(df[col],df[ 'temp2' ]))\n        temp_df[col] = temp_df[col].map(mapping)\n\n        print(col, df[ 'group' ].nunique(), df[col].nunique())\n        print(df.groupby([ 'group' ])[ 'temp' , 'temp2' ].agg([ 'min' , 'max' , 'count' , 'nunique' ]).head( 40 )) content_copy Create internal subset feature We used last statement month to create 0/1/2 feature and store in in \"index\" df Create unified kfold and holdout split Fixed random seed (of course 42) to make spits and then took 20% of customers to holdout group (to test stacking / blending / etc) Separate columns by type After cleaning we had several columns \"groups\". all_files = [ 'p_columns' , -> just p columns as we thought that they are very different ( and P_2 is internal amex \"scoring\" model) 'objects_radar_columns' , -> order encoding (we were checking where out cleaning differs from public approaches and here was the unique place) 'objects_columns' , -> onehot encoding 'categorical_cleaned__D__columns' , -> no noise categoricals 'categorical_binary__S__columns' , -> cleaned binary 'categorical_binary__R__columns' , -> cleaned binary 'categorical_binary__D__columns' , -> cleaned binary 'categorical_binary__B__columns' , -> cleaned binary 'categorical__D__columns' , -> removed noise categoricals 'categorical__B__columns' , -> removed noise categoricals 'cleaned__B__columns' , -> removed noise continuous 'cleaned__D__columns' , -> removed noise continuous 'cleaned__R__columns' , -> removed noise continuous 'cleaned__S__columns' , -> removed noise continuous 'rest__B__columns' , -> have no idea what to do with it -> floor 'rest__D__columns' , -> have no idea what to do with it -> floor 'rest__R__columns' , -> have no idea what to do with it -> floor 'rest__S__columns' , -> have no idea what to do with it -> floor \n] content_copy Thanks again to @raddar we always used your preprocess as a baseline. We were able to load just portion of data -> do fe -> concat to \"index\" as all dfs were aligned by index. Also such split permitted us to do fe by feature type to accelerate process and see more statistically valuable metric change. FE We started with careful fe column by column or small subset and it worked well until 1xx features and then any metric improvement or degradation was not statistically significant and many features \"overlapped\" on importance and significance. Note: I believe that it is possible to build silver zone robust model with only 3xx features So we started from scratch with brute force))) Of course there was no need to apply \"nunique\" (for example to binary features) and our previous step helped us to limit fe. all_aggregations = { 'agg_func' : [ 'last' , 'mean' , 'std' , 'median' , 'min' , 'max' , 'nunique' ], 'diff_func' : [ 'first' , 'mean' , 'std' , 'median' , 'min' , 'max' ], 'ratio_func' : [ 'first' , 'mean' , 'std' , 'median' , 'min' , 'max' ], 'lags' : [ 1 , 2 , 3 , 6 , 11 ], 'special' : [ 'ewm' , 'count_month_enc' , 'monotonic_increase' , 'diff_mean' , 'major_class' , 'normalization' , 'top_outlier' , 'bottom_outlier' , 'normalization_mean' , 'top_outlier_mean' , 'top_outlier_mean' ]\n} content_copy pca (horizonal and vertical) + horizontal combinations + horizontal aggregations. agg_func -> normal aggregations by uid diff_func -> diff last - xxx -> std diff worked better than any other ratio_func -> ratio_func last/xxx lags -> diff last - Nx special -> some special transformations -> count_month_enc worked well for categorical / emw for continous We ended up with about 7k features (stored file by group and by agg type for faster loading). Next thing was to figure out what works and what not -> this topic was the most challenging for us. Normalizations It's better to call it Standardization (x - m) / s -> as we had also normalization test the name became constant \"normalization\"))) df .groupby ( [ 'dt_month' , 'subset' ] ) [col] .agg ( [ 'mean' , 'std' ] ) content_copy dt_month -> month of the statement subset -> train / public / private and mean and std from clients that had full statement history. We have to have temporal shift to make it work. So we did a \"trick\" removed last statement for each client and applied exactly same transformation  for each client and merged appropriate labels. So we had 2 lines in training set for almost each client BUT validated results only on last statement during CV runs and Holdout checks. It more or less same as adding noised data but we had temporal drift and model was able to work better on unknown future data with  \"possible\" data drift. Features selection Ooohh that was really fun. We used gbdt boosting type during experiments as it was very aligned with dart mode but was significantly faster. Also, we used ROC AUC score during our experiments as we believed that due to amex instability we can't use it for decision making  (of course we tracked log loss and amex). In previous step we brute forced many features and now is time to clean them out. All feature selection was done with 5 CV folds training + independent check on 20% holdout data. Zero importance -> Right from the start we were able to through away 1.5k features that had exactly 0 importance (lgbm importance). That means that with 250 bins and 2**10 data in leaf those features are not participating in any split. Stepped hierarchical permutation importance -> we defined 300 initial features and looped over all other features subsets (600+) - was very time consuming but very stable. Note: we shuffled order of the subset to force model try different combinations. Add features subset -> train model -> permutate -> drop negative features (negative mean over 5 seeds) -> add new subset -> … During this part that took almost 3 days we limited features to 3k -> 0.800 lb Stepped permutation importance. Take all features -> train model -> permutate -> drop 20% of worst performed features (only negative) -> repeat. Final subset was 25xx features (and different from previous step) -> 0.800 lb Forward feature selection. We defined 300 initial features and simply added subset by subset and compared ROC AUC if metric change was > 0.0003 we kept the subset. -> 0.800 lb Time series CV. For very doubtful features as PCA and Normilized values we used to different validation stratagies: Train on first 6 month values (last statement of the first 6 months went to train set) and validate on last 6 (also just last statement of the last 6 months). We trained model without temporal feature and then with if result was better on CV and on holdout we added to final features subset. We used P_2, B_1, B_2 as a proxy target and MSE loss with combined Train and Test to see if we did right transformation and result did not degrade. Many other options we tried but result was not stable. Final subset came from \"Forward feature selection\" plus overlapped features from other technics minus overlapped negative combination.  -> lb 0.801 single model. We tried to blend many models with different subset as we believed that it should give huge LB boost (based on holdout blending tests) but it didn't work well for lb. Model In my own experience, DART never worked better and here we have proof that in DS \"all depends.\" We did experiments with DART in the beginning and it did not show any metric improvement with our params and baseline model features subset. Later we found @ragnar123 notebook and gave it one more try and it worked marvelously. From the beginning, we tried to build a more complex model with 2**7+ leaves and 0.7+ features but failed. It still puzzles me why a simple model with a very low number of features works here. I saw such behaviour mostly on synthetic data and stacking - so we tried to find out if data is syntetic (at least partly) and deanonimize internal scoring values - but didn't make it. Our best single lgbm model was trained on 29xx features. 5 folds CV - no stratification by any option. Training data - 2 last staements for each client (transformed independently). Params: lgb_params = { 'boosting_type' : 'dart' , 'objective' : 'cross_entropy' , 'metric' : [ 'AUC' ], 'subsample' : 0.8 , 'subsample_freq' : 1 , 'learning_rate' : 0.01 , 'num_leaves' : 2 ** 6 , 'min_data_in_leaf' : 2 ** 11 , 'feature_fraction' : 0.2 , 'feature_fraction_bynode' : 0.3 , 'first_metric_only' : True , 'n_estimators' : 17001 ,  # -> 5000 for gbdt 'boost_from_average' : False , 'early_stopping_rounds' : 300 , 'verbose' : -1 , 'num_threads' : -1 , 'seed' : SEED,\n} content_copy Blend -> Power (2) rank blend of Dart lgbm (0.801 public) / GBDT lgbm  (0.799 public) / Catboost models (0.799 public) Single lgbm with 3 last statements showed even better CV by we didn't have enough time to retrain it (full DART run for 5 folds took 12+ hours there). It was obvious that clients with a little number of statements will not get benefit from all 2k features. So we created a special model that was trained only on 300 features with custom params (also dart). Predictions for clients with <=2 statements came exclusively from such model and were not blended with other models. How did we combine the result from 2 independent models to not destroy the final ranking? Client id Number of statements Basic ranking <=2 prediction Final ranking 1 13 5 … 5 1 2 4 0.1 3 1 13 8 … 8 1 1 3 0.5 4 1 13 7 … 7 1 13 2 … 2 we kept ranking for >2 statements and for rest resorted within initial ranking group (hope it's clear enough))). Was is perfect - no, but it was very stable with really tiny improvement (because of number of such clients in public and private test parts). What also worked well: Train on all data without folds splitting and stop just 1000 rounds further then CV showed. What didn't work: Stacking by any mean Many models with different seed and fe order Massive blend of different models with different types and features (blend worked well till 0.799 and then any low performed model 0.795- made public score worse - we use public CV as additional holdout set and used approaches that worked well on local CV and LB - anything that worked partially was not used in the end). Again, nothing really fancy here. The main thing that helped us align Train / CV with LB was very high  'min_data_in_leaf'. No optuna used -> just manual old school tuning based on data feeling. We did many experiments with weights and loss functions but none of them worked. Due to AMEX metric specification it was obvious that focal loss should work but it didn't. We tried several times to switch loss function during the competition period and the result was the same. Error analysis showed that model makes errors without any \"pattern\" -> stacking didn't work for holdout set (25% of data)  and we had doubts that it will work on private/public test parts. We kept only LR/Lasso(0.02) for blending options to choose submissions. Cross validation -> standard 5folds CV split by client ID. The unique thing that we did here is \"prespliting\" to align all CV between team members to be able to compare results directly. What left without mentions: EDA on data Denoising experiments Data deanonymization -> didn't manage to make it Features pairs and triples combinations -> that didn't work well NaN filling -> didn't work Clusterization -> didn't work Hundreds of experiments with features selection process and internal discussions about it. Adding noised data (noise / swap noise) that leaded to interesting but doubtful results Model tuning Removing absolute values and keep only diff or ratios -> should be more stable for future data but we saw some lb degradation and didn't proceed pseudo labeling What we always wanted but didn't found time to do: NN - we have no NN in our final blend P_2 or any other column prediction (1/2/3/4 months ahead) with combined data and use it as meta information for lgbm main model 11 / 12 / 13 statements joined training on different subsets (df was too large and training was slow) Internal initial plan ###### ###### ###### ###### ### Data preprocessing and Data evaluation\n### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ## Added noise removal -> GOOD2DO # There is no doubt that some Noise was injected in data # https://www.kaggle.com/competitions/amex-default-prediction/discussion/328514 # https://www.kaggle.com/competitions/amex-default-prediction/discussion/327649 # https://www.kaggle.com/competitions/amex-default-prediction/discussion/327651 # We need to find a way to remove it # the best option to not follow public approach # At least with columns where columns have overlaped population ## Data minification for FE -> GOOD2DO # Datatype downcasting # Pickle/Parquet/Feather # Be careful with floats16 as it may lead to bad agg results # Also float16 may lead to some signal degradation due to precision and values changes ## Evaluate values distributions and NaNs -> GOOD2DO # Full 13 months history # Train against Test Public and Test Private # https://www.kaggle.com/competitions/amex-default-prediction/discussion/327926 # # Need to try: # Kolmogorov–Smirnov test -> GOOD2DO # # Adversarial validation -> GOOD2DO # https://www.kaggle.com/code/carlmcbrideellis/what-is-adversarial-validation/notebook (just as simple example) # # Entropy / Distances / etc.... # # Visual checks))) # # We need to find if ANY feature has very different distribution in PRIVATE test set # If that feature works for Public part it doesn't mean that it will work for Private ###### ###### ###### ###### ### Targets\n### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### # We need to find a way to get more targets -> GOOD2DO # as we currently training on a single point by client we could greatly improve results # by extending our training set with new targets # # Find default periods in current client history and make appropriate labeling -> GOOD2DO # Make 2 level model -> predict p_2 values as normal time-series model and feed it to 2nd level GBT ###### ###### ###### ###### ### Separate Models\n### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### # Probably it's a good idea to make separate models for each subdatasets # Full history (13 months) # Less than 13 months ###### ###### ###### ###### ### External Data\n### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### # We can try to add \"Consumer index\" or any other independent temporal feature # Will not work if we will not be able to expand targets and add temporal feature ###### ###### ###### ###### ### FE\n### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### # We didn't make anything special here -> July # AGGS (Stats by client) # Rollings # History length feature (not sure if it will help with Private Test) # Should we correct statements dates and add NaNs? # ReRanking categorical features by P_2 or Target # Clusterization (4+ groups feature by feature) # Count and Mean encodings for categorical features # Features combinations (sum/prod/power) -> bruteforce # PCA or any other dimension reduction by features groups # We need to find if there is \"connection\" between clients in Train -> Public Test -> Private Test # we have 458913 + 924621 -> 1383534 If I were AMEX I would export 1M clients (or other round number) # so may be 384 534 Clients are overlaps # Clip by 5 - 95 percentile ###### ###### ###### ###### ### Features Selection\n### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### # Permutation importance (use all fold only!!!) -> recursive elimination (because of quantity of features -> 3-4 rounds with 0 and 50% negative drop) # SHAP # Highly correlated features (.98+?) # Forward selection (may take ages and due aggs may be not effective - probably by feature block) # Backward elimination (may take ages and due aggs may be not effective - probably by feature block) ###### ###### ###### ###### ### CV\n### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### # Mean Target differs my \"history length\" -> could be wise to do GroupedStratifeidFolds by history length # For sure Splits should be done by client # Target stratification to balance folds ###### ###### ###### ###### ### Loss function / Metric\n### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### # Clean and fast np/torch metric # Now it's in helper (need to cleanup that) # https://www.kaggle.com/code/rohanrao/amex-competition-metric-implementations # # I don't believe that we will have better results with different loss function # But it worth to try at least focal loss # https://maxhalford.github.io/blog/lightgbm-focal-loss/ # # Weights -> we should try change weights there # weights by class # weights by some history length # weights by internal fe group # # We need custom metric for catboost # example https://catboost.ai/en/docs/concepts/python-usages-examples#logloss1 ###### ###### ###### ###### ### Models\n### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ## First level choice # LGB/XGB/CTB -> our main models here for sure # After stabilizing the baseline model and base feature we need to make 1st round tuning ## Catboos specials # Categorical features # Embeding features ## NN (GPU/TPU) -> RNN / LSTM / Transformer # TPU -> tensorflow (as it works better there) ## NN -> AE / VAE / DAE -> as a denoising model hidden layer as input for GBT models # No need complex approach - just fast check the idea and in case of success move to big model ###### ###### ###### ###### ### Blending\n### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### # Weighted Average # Power Average # Weighted Rank Average # Linear/SVM # Postprocessing? content_copy Please sign in to reply to this topic. comment 57 Comments 3 appreciation  comments Hotness Chris Deotte Posted 3 years ago · 14th in this Competition arrow_drop_up 13 more_vert @kyakovlev Congratulations Konstantin and team. I am so happy for you. You are the best tabular data feature engineer that I know. I'm glad to see your skills achieved great success! I'm looking forward to learning about your solution! yukiya Posted 3 years ago · 170th in this Competition arrow_drop_up 1 more_vert I think you groom him well from IEEE competition 3 years back :)  Congratulations to you both ! 7 more replies arrow_drop_down delai50 Posted 3 years ago · 228th in this Competition arrow_drop_up 5 more_vert A huge congratulations and thanks for such a detail explanation. As Chris said, the best tabular feature engineer! Some questions from my side: 1. Based on your experience, doing brute force feature engineering (i.e. create a ton of potential new features and then perform feature selection) gives better results than going column by column? 2. How did you choose the initial LGBM hyperparameters before starting the feature selection phase? 3. Did you retune the hyperparameters at some point of the feature selection phase (e.g. when you added/removed a significant number of features) or kept them fixed all the time? 4. Could you give some examples of what type of feature subsets you include at each step of the feature selection phase (e.g. one subset could be the mean of cleaned__B__columns )? 5. Your final selected features came from \"Forward feature selection\". I tried it a couple of times at work and saw that tends to overfit, do you have the same experience? 6. How did you realize that increasing the value of min_data_in_leaf favour CV/LB alignment? Also waiting for the code 👀 Konstantin Yakovlev Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 8 more_vert Thank you for your kind words and very nice questions. 1 | Brute force is a dead end by all meanings - it will not work on production, it will not provide any interpretability or explainability, it is not reproducible result. Column by column and distilled carefully created features are always better and preferable. Even in this competition, manual feature engineering would work better if we could have access to raw data and columns description (meaning). 2 | Initial params always depend on data, but I start most of times with 0.7/0.7 sampling/features sampling, 2^7 leaves and 2^8 min data in leaf (all other params as default). First baseline run normally gives you a lot of information how to tune it further: if train score is much higher than CV and holdout -> try regularizations / lower subsampling / less complex model if naive score is better than lgbm or difference is minimal -> bad features or/and need more complex model (2**7 is complex enough for majority of tasks) if train score is low but very aligned with CV/holdout -> underfitting -> more complex model with less data in leaf Then try to \"tweak\" loss function that is more appropriate for your task. Then try to fit goss/dart/extra_trees -> extra_trees works very nice sometimes -> DART never worked better before))) 3 | Params tuning normally takes several iterations during model creation - for competitions it means a slight tuning every 2 weeks (after significant feature sets changes or data structure changes) and final deep tuning round 2 weeks before competition ends. 4 | Not much to add here - you are right with your assumption: mean + cleaned__B__columns produces: agg_func__mean__cleaned__B__columns|__B_4__mean agg_func__mean__cleaned__B__columns|__B_16__mean agg_func__mean__cleaned__B__columns|__B_20__mean agg_func__mean__cleaned__B__columns|__B_22__mean agg_func__mean__cleaned__B__columns|__B_41__mean … 5 | It works really nice for the start but with more features it becomes less informative and slower. It didn't show any overfitting but we didn't trust it fully as we were training and validating on the same time period (and didn't have any aligned validation set in the “future”). 6 | We saw that ROC AUC score on train data went up to 0.9999 and CV/Holdout had cap of 0.96xxx -> many teams decided to use regularization (l1 - punish features with low importance but we would like to keep even small signals, l2 - adds regularization to features with high importance -> that is good move here but we didn't see any statistically significant CV score boost). We decided to make less complex model with 2^6 leaves and increase min data in leaf to force our model generalize better and found that increasing this param to 2^9 - 2^11 works very well on CV/holdout and also worked well on LB. delai50 Posted 3 years ago · 228th in this Competition arrow_drop_up 1 more_vert Thanks for such a complete answer! Konstantin Yakovlev Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 5 more_vert It's becoming a long writeup))) will need bit more time Robert Hatch Posted 3 years ago · 45th in this Competition arrow_drop_up 1 more_vert Take your time! Great details already Tonghui Li Posted 3 years ago · 66th in this Competition arrow_drop_up 3 more_vert Huge congrats Konstantin @kyakovlev , this is incredible commitment your team had! I subscribed 2 months of Colab Pro+ for this comp and that was it… Curious if you know besides diff_mean which of your 'special' agg ideas worked best for you? Konstantin Yakovlev Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 2 more_vert btw diff_mean is: df[f'{ col }__diff_mean] = df[ col ] - df.groupby(['uid'])[ col ].shift()\ndf[f'{ col }__diff_mean] = df.groupby(['uid'])[f'{ col }__diff_mean]]. transform (' mean ')\n\ndf = df.drop_duplicates( subset =['uid'], keep=' last ').reset_index(drop=True) content_copy std features worked better than others - diff with std for example normal diff with client mean was in 'diff_func': ['first','mean','std','median','min','max'] and my bad - which of specials. monotonic functions worked really well Tonghui Li Posted 3 years ago · 66th in this Competition arrow_drop_up 0 more_vert Thanks for the reply! monotonic functions - Did you create a T/F binary feature or did you calculate for example spearman correlation or similar to quantify monotonicity? Konstantin Yakovlev Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Binary if current more or equal to previous and then took mean. We wanted to pass to the model information if last statements increase or decrease is normal for the client or not. If due payments are constantly increasing for us it was a sign that risk is growing. And such information is not based on absolute values - we constantly wanted to avoid absolute values to be able to generalize well on future data and data with drifts. Gaurav Rawat Posted 3 years ago · 15th in this Competition arrow_drop_up 3 more_vert Congrats guys .. cant wait for your solution to reach 0.809 .. Ahmed Taha Posted 3 years ago arrow_drop_up 1 more_vert very insightful tank youu Runzhong Xu Posted 3 years ago · 299th in this Competition arrow_drop_up 1 more_vert Congratulations! Konstantin Yakovlev Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Here is example how we augmented training Data: We were removing last statement per client as it would never exists and then applied exactly the same transformations as we did for full history. Last values are different but within distributions Agg values are different but within distributions Model has temporal component We wanted to completely avoid absolute values and with data augmentation the CV and LB were almost on the same level as with absolute values - of course it would not work with clients with 1-2 statements but we had a separate model for them. I believe that that would give much more robust results but on kaggle even slight metric increase is important and we kept \"lasts\". Also, we were planning to go in -2 max depth and used only 1,2,3,6,11 (most common time-series lags) diff lags to have fe aligned. Combining original data with -1 statement or with -2 statement gave significant CV boost on ROC AUC score (our main metric) and tiny boost on AMEX metric. Combination of 3 sets gave just a tiny boost on ROC AUC but was significantly more memory consuming and slower to train model. Also, CV splits per original client uid to exclude leaks and validation was performed only on original data to exclude metric degradation tracking for shorten history predictions. Konstantin Yakovlev Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Few examples how noise cleaning works: For the most features nunique groups within 998 -> 1001 is a sign of noised cleaning possibility. Simple feature cleaning - in this case group is our new value: More complex feature with negative \"categories\" we keep negative values (temp2 is a new value): Very \"noised\" feature (we find initial coefficient and then apply it to groups with low population) - temp2 is a new value: Oscar Aguilar Posted 3 years ago · 4383rd in this Competition arrow_drop_up 1 more_vert Thanks for sharing your solution! Very insightful. Robert Hatch Posted 3 years ago · 45th in this Competition arrow_drop_up 1 more_vert Normalizations. Sorry, I'm having trouble understanding. For all EXCEPT last month you took mean and std for all customers fort that exact col, month and subset (train/pub/pri)? What about last? I'm actually super confused what the trick was, and why? Maybe try just restating/rephrasing it and maybe I'll figure it or at least have more precise questions… This comment has been deleted. Alexander Guldbrand Posted 3 years ago · 2550th in this Competition arrow_drop_up 1 more_vert Congratulation! Looking forward to dive into your solution! Mert Burabak Posted 3 years ago · 1758th in this Competition arrow_drop_up 1 more_vert Good Job! well deserved Santiago Mota Posted 3 years ago · 92nd in this Competition arrow_drop_up 1 more_vert Congrats to the team. One more waiting for the extended explanation Vadim Imaev Posted 3 years ago · 3602nd in this Competition arrow_drop_up 1 more_vert Congratulations ! :) iliiiiiili Posted 3 years ago · 322nd in this Competition arrow_drop_up 1 more_vert Congratuation! Nischay Dhankhar Posted 3 years ago · 22nd in this Competition arrow_drop_up 1 more_vert Glad to see you back on Kaggle competitions with a bang. Congratulations, amazing finish. Daisy Posted 3 years ago · 1888th in this Competition arrow_drop_up 1 more_vert Congratulations! Thank you for sharing model params!! Very useful for next competitions! shahilpravind Posted 3 years ago · 3390th in this Competition arrow_drop_up 1 more_vert Congratulations! 🎉  Not a lot of experience on Kaggle so not sure if this is normal but was surprised to see that 3/4 of your team is novices and contributors. Gives me hope 😅 Konstantin Yakovlev Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 4 more_vert @shahilap96 our team was formed at the very early stage of the competition and exclusively from my company DS department - many other DS colleagues wanted to participate but were not able to made such 3month commitment and decided not to join. Kaggle experience is nice to have but it’s not an obligatory condition - the most important is a “fire in the eyes” and passion to learn and try something new. Yes, my teammates are novice on kaggle but not novice in analytics and DS. One thing that I can say - all my teammates work with the same passion with regular work tasks and trying really hard to achieve goals. I feel very proud of them. shahilpravind Posted 3 years ago · 3390th in this Competition arrow_drop_up 0 more_vert Aha, that explains the novice part 😅 People with passion and fire in their eyes are always great to work with. The energy and drive always pushes me to do better. Congrats once again. Kim Keonho Posted 3 years ago · 2034th in this Competition arrow_drop_up 1 more_vert Congrats! I'm waiting for your notebook to knwo what had been updated from IEEE competition :) Sixian Chen Posted 3 years ago · 2029th in this Competition arrow_drop_up 1 more_vert Congratulations！ Leandro Destefani Posted 3 years ago · 525th in this Competition arrow_drop_up 1 more_vert Congratulations, guys, you deserved it! Really well done reaching this score Neha Posted 3 years ago · 3985th in this Competition arrow_drop_up 1 more_vert Congratulations 🎉🎉💥 cong wang Posted 3 years ago · 311th in this Competition arrow_drop_up 1 more_vert Congratulations！ Gaju Ahmed Posted 3 years ago · 1577th in this Competition arrow_drop_up 2 more_vert Congratulations 🎉🎉 and Thanks for the solution Konstantin Yakovlev Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 3 more_vert Solution is still in process. Was posted just initial plan and some information about model. Sorry for the delay, but will need 3-4h more to finish it. Hope you'll find it interesting.",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules Alex · 3rd in this Competition  · Posted 3 years ago arrow_drop_up 79 more_vert 3rd solution--simple is the best feature engineering is all you need First of all,  Big thanks  to Amex and Kaggle for organizing a very interesting competition.I am too busy my work so that I haven't done this competition for about a month and a half. I came back a few days before the competition ended. I am very happy to rank 3rd in this competition. Thanks to @raddar denoise idea and @ragnar123 public notebook. my solution I will share my code after winning call.Thanks for you reading! Please sign in to reply to this topic. comment 23 Comments Hotness Fagbemi Tunji Posted a year ago arrow_drop_up 1 more_vert Hello Alex, Trust you are doing great Did you later drop the code? Chris Deotte Posted 3 years ago · 14th in this Competition arrow_drop_up 6 more_vert Congratulations Alex. Amazing solo gold 3rd place Gold. You will be Kaggle competition Grandmaster soon! I like your simple solution. Looks like you made some strong features! Data student Posted 3 years ago arrow_drop_up 1 more_vert Hi Alex ( @lihuajing ), congratulations on the solo 3rd place gold. Looking forward to learn from your code. Quân Trần Đình Đại Posted 3 years ago arrow_drop_up 1 more_vert looking forward the code! Muhammad Anas Mahmood Posted 3 years ago arrow_drop_up 1 more_vert Congratulations @lihuajing The Devastator Posted 3 years ago · 540th in this Competition arrow_drop_up 2 more_vert Wow nice! Congrats Can't wait for the code, this is insane! Anthony Bernardi Posted 7 months ago arrow_drop_up 0 more_vert Thank you for sharing the diagram and detailing the simple solution! Just a Duck Posted 2 years ago arrow_drop_up 0 more_vert Hello Alex, where can I find your code? I love to learn it! Johnny Torres Posted 3 years ago · 4443rd in this Competition arrow_drop_up 0 more_vert Amazing ! Thanks! Eagerly waiting for your code… nianliu107 Posted 3 years ago arrow_drop_up 0 more_vert Wow! Congratulations Alex. The feature engineering part is the killer. Looking to hear more of your solution. Thanks. nianliu107 Posted 3 years ago arrow_drop_up 0 more_vert congratulations on the solo 3rd place gold. Looking forward to learn from your code. Liming Posted 3 years ago arrow_drop_up 0 more_vert Congratulations！ Looking forward to learning from your code. skw1990 Posted 3 years ago · 2436th in this Competition arrow_drop_up 0 more_vert How do you select the initial num_features and cat_features to use? Also, love to know the magic parts =) Ezzaldin Mamdouh Posted 3 years ago arrow_drop_up 0 more_vert Amazing Alex, Congratulations raccoon Posted 3 years ago arrow_drop_up 0 more_vert Wow! Congratulations Alex. The feature engineering part is the killer. Looking to hear more of your solution. Thanks. Sandy Posted 3 years ago arrow_drop_up 0 more_vert can you share code, for  for 1st place already code shared ? Anubhav Kundu Posted 3 years ago arrow_drop_up 0 more_vert awesome! Awaiting your code to be trained. backpack Posted 3 years ago arrow_drop_up 0 more_vert can't wait for your code to learn! Ravi Ramakrishnan Posted 3 years ago · 1277th in this Competition arrow_drop_up 0 more_vert Great result emanating from a baroque approach! Keep up the great work, wishing you the best and hearty congratulations @lihuajing !! C4rl05/V Posted 3 years ago · 611th in this Competition arrow_drop_up 0 more_vert Hello @lihuajing thanks for sharing your solution Jiwei Liu Posted 3 years ago · 10th in this Competition arrow_drop_up 0 more_vert Congratulations! Very impressive approach! I'm very interested in your LGB model and I would like to accelerate the inference part using NVIDIA Triton. Please keep me posted when the code is available. Thank you! Jackson You Posted 3 years ago · 148th in this Competition arrow_drop_up 0 more_vert Keep me too! This comment has been deleted.",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules zakopuro · 5th in this Competition  · Posted 3 years ago arrow_drop_up 61 more_vert 5th Place Solution - Team 💳VISA💳(Summary&zakopuro's part) I would like to thank the organizers for organizing this competition, the participants for sharing their many insights, and my teammates( @baosenguo @wimwim @scumufeng ). Now @scumufeng and I are promoted to kaggle master and @wimwim is reaching for GM. This competition will be unforgettable for me:) Summary Our solution is the result of ensembling several GBDT models , Transfomr, 2d-CNN, and GRU. We noticed that ensemble weights are determined based on Public LB and overfit if based on CV. Features We are using the dataset shared with us by raddar. The features are based on those shared by ragnar.(Thanks to both of you.) meta feature I did not know this is called a meta feature. This feature was useful not only in GBDT, but also in Transformer. If added to chris's Transformer , the LB will increase from 0.790 to 793. Pivot Combine all features horizontally. P_2_0 , P_2_1 , P_2_3 , P_2_4 , ... , P_2_12 , B_30_0 , ... XXX  ,  XXX  ,  XXX  ,  XXX  , ... ,   XXX  ,   YYY  , ... content_copy Model GBDT LightGBM Almost no change from this Notebook stratfiedKfold: 10 Catboost Use GPU(I was surprised at how fast it was.) parameter : default Transformer zakopuro Based on chris's Notebook Some additional features.(Mainly meta features) Patrick Yam This is his solution. https://www.kaggle.com/competitions/amex-default-prediction/discussion/348118 mufeng Add meta feature to Patrick's transformer. LB increased from 0.795 to 0.798. Ensemble Use 21 models. Ensemble weight Determined based on Public LB In our case, the LB score will be lower if based on CV.(CV is 0.8016 or higher.) We trusted Public LB more than CV because it is close to Private and has a sufficient amount of data. Weights are not complicated. (For example, 0.1,0.2,… etc.) Select Submit Best LB Public : 0.80199(2nd) Priavte : 0.80881(6th) Best LB*0.5 + Best CV *0.5 Public : 0.80154 Private : 0.80862(Gold zone) Correlation check Check the Public and Private correlation values for all predictions used in the ensemble to see that there are no significant differences. All posts above 0.801 in Public LB were in the Gold zone in Private LB.(I prayed on the last day not to Shake down😣) Let's enjoy kaggle! Thank you!!!! Please sign in to reply to this topic. comment 11 Comments Hotness Chris Deotte Posted 3 years ago · 14th in this Competition arrow_drop_up 6 more_vert Congratulations team. Great solution. I didn't know about these \"meta features\" but i see that many top teams used them. I will use them in my future competitions. Thanks for sharing your solution. zakopuro Topic Author Posted 3 years ago · 5th in this Competition arrow_drop_up 4 more_vert Thank you very much. I have learned a lot from you in this competition as well! 3 more replies arrow_drop_down delai50 Posted 3 years ago · 228th in this Competition arrow_drop_up 1 more_vert Congrats for all of you that were promoted and thanks for sharing! I would like to ask a couple of questions: 1. The \"pivot\" part is only for the Transformers, correct? 2. From picture I interpret that each team member developed his own features, how different are from Martin's features? 3. I'm curious about the 2d-CNN model, do you have more info? zakopuro Topic Author Posted 3 years ago · 5th in this Competition arrow_drop_up 1 more_vert Thank you! The \"pivot\" part is only for the Transformers, correct? -> This is a feature for the GBDT, not for the Transformer. From picture I interpret that each team member developed his own features, how different are from Martin's features? -> There are no major differences; diff features are added, rounding with float, etc. I'm curious about the 2d-CNN model, do you have more info? -> It is similar to this content.( https://www.kaggle.com/competitions/lish-moa/discussion/202256#1106810 ) delai50 Posted 3 years ago · 228th in this Competition arrow_drop_up 1 more_vert Thanks for the answers! Then the GBDT used pivoted features (i.e. the 13 pivoted statements) instead of aggregations of them? Or maybe It used both? zakopuro Topic Author Posted 3 years ago · 5th in this Competition arrow_drop_up 1 more_vert It used both. nymfree Posted 3 years ago · 1683rd in this Competition arrow_drop_up 1 more_vert Congrats on the gold medal. Nice solution and nice write-up. The \"meta-features\" approach is very interesting. If I understand correctly you did a number of rounds of adding these meta-features -- taking the average of the previous ones and keeping the last one intact. After how many rounds did CV/LB improvements stagnate? zakopuro Topic Author Posted 3 years ago · 5th in this Competition arrow_drop_up 1 more_vert Thank you. Four \"meta-features\" were used. However, in the case of GBDT, the use of \"meta-features\" does not improve the score of the single model much. The model using this feature is more effective for ensembles. In Transformer's case, increasing the number by more than four had no effect. Kevin Morgado Posted 3 years ago · 4345th in this Competition arrow_drop_up 0 more_vert Congratulations. I found very interesting the process of ensembling that wide variety of models and it is a good reference for future competitions. Thank you so much for this answer.",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules George Reus · 9th in this Competition  · Posted 3 years ago arrow_drop_up 32 more_vert 9th Place Solution ( XGBoost+LGBM+NN ) Many thanks to AMEX,Kaggle and all contributors of discussion during the entire competition ( @raddar , @cdeotte , @ragnar123 ,…). Congratulations to all winners and new Experts, Masters and Grandmaster! Score & Result My best submission CV: 0.799106        Public: 0.80062        Private:0.80875 My result CV: 0.799194        Public: 0.80057        Private:0.80868 Feature Engineering I only used the integer dataset provided by @raddar . Base features aggregated features like mean,max,min,std,sum,medium,last,first other rate and diff features with datediff last-first,last1-last2,last1-last3,last-mean,max/last,sum/last  and so on Date features is it a holiday Model LightGBM 3 models of LGBM with different data representation & parameters give CV in the range [0.796-0.799] and LB in [0.797-0.799]  (2 model with dart-LGBM , 1 model with goss-LGBM) XGBoost 6 models of XGB, with different data representation & parameters give CV in the range [0.794-0.796] and LB in [0.795-0.796] NeuralNet 4 models  of NeuralNet  with different parameters give CV in the range [0.788-0.790] and LB in [0.790-0.792] (I am not so proud of NNs  Thank again @cdeotte for sharing his great public kernel NN.) Ensemble(stacking) Using 13 models to stack with 10-fold cross-validation , Hyperparameter-tuning and appropriate early stopping can give   Private in the range [0.80853-0.80875] some ideas Predict if a customer will default  when customers have already used credit to consume ,trending of the features changed over time(like consumption frequency ,Change in consumption amount) is very important ,especially in the last few months. I am very grateful to this competition. I learned a lot in this competition for a newbie in kaggle .Thanks you all😎 Please sign in to reply to this topic. comment 8 Comments Hotness delai50 Posted 3 years ago · 228th in this Competition arrow_drop_up 1 more_vert Congratulations for the gold in your first Kaggle comp! How did you find useful features besides the base features? I started with almost the same base features as you (Martin's notebook features) but was sooo hard to find new features that improved the existing ones. On the other hand, what are the differences between different data representations? Thanks! George Reus Topic Author Posted 3 years ago · 9th in this Competition arrow_drop_up 3 more_vert Thanks delai! I didn't use all features in one big model, because it's difficult to find useful new features from feature importance and mabye some new features is useful but the result is not significantly improved. Actually, every single-model is created by different base features and other new features. For example, XGBoost-model-1 is created by base features like 'mean', 'max', 'sum' and diff/rate/date features between first month and last month, XGBoost-model-2 is created by base features like 'medium','max,'min' and diff/rate/date features between the last two months.So every model has different data representations and some new useful features can give a relatively obvious improvemen. Amirdha Varshini S Posted 3 years ago arrow_drop_up 2 more_vert congrats…great job Ruslan Sadykov Posted 3 years ago · 4534th in this Competition arrow_drop_up 0 more_vert Great job! Shibata Posted 3 years ago · 377th in this Competition arrow_drop_up 0 more_vert Congrats 🎉 I learned how important it is to create various models with different features from your solution. Actually, I could not make the most of stacking successfully in this competition. I am very curious about What was the CV gain when you added \"is it holiday feature\"? What was the model for 2nd stage stacking? Was it Ridge or Lasso? Thanks in advance! George Reus Topic Author Posted 3 years ago · 9th in this Competition arrow_drop_up 0 more_vert Thanks Shibata ! Using Holiday features can  give about +0.0002 boost in CV. The model is LinearRegression for stacking with rounded oof score ( oof=oof.round(5) ) Santiago Mota Posted 3 years ago · 92nd in this Competition arrow_drop_up 0 more_vert Well done. Congrats George Reus Topic Author Posted 3 years ago · 9th in this Competition arrow_drop_up 0 more_vert Thanks Santiago !",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules Jiwei Liu · 10th in this Competition  · Posted 3 years ago arrow_drop_up 93 more_vert 10th Place Solution: XGB with Autoregressive RNN features What a competition! I really enjoyed it and only hope I could have found more time. First of all, I would like to thank Raddar @raddar , Martin @ragnar123 , and many others who generously shared codes and datasets! The public solutions are of amazing quality. And it also determines my game plan: create something original and blend it with the best public solution. My solution is based on RAPIDS cudf for dataframe processing, XGB for training, and pytorch lightning for feature extraction. There are two vital observations of this dataset: the big test data which is from the future is available. short sequences (<13) are culprits of bad performance. Let's start with the latter observation: the sequence length of each customer profiles plays a critical role in the model performance: import cudf path = '/raid/amex' train = cudf .read_parquet (f '{path}/train.parquet' , columns =[ 'customer_ID' ])\ntrainl = cudf .read_csv (f '{path}/train_labels.csv' )\ntrain = train .merge (trainl,on= 'customer_ID' ,how= 'left' )\ntrain [ 'seq_len' ] = train .groupby ( 'customer_ID' ) [ 'target' ] .transform ( 'count' )\ntrain = train .drop_duplicates ( 'customer_ID' ,keep= 'last' )\ntrain .groupby ( 'seq_len' ) .agg ({ 'target' : [ 'mean' , 'count' ] }) .sort_index (ascending=False) content_copy output: target mean count seq_len 13 0 . 231788 386034 12 0 . 389344 10623 11 0 . 446737 5961 10 0 . 462282 6721 9 0 . 450164 6411 8 0 . 447300 6110 7 0 . 418430 5198 6 0 . 387670 5515 5 0 . 392635 4671 4 0 . 416221 4673 3 0 . 358602 5778 2 0 . 318465 6098 1 0 . 335742 5120 content_copy It is obvious that sequence length 13 is the most common but also with a significantly lower mean default rate. At first glance, I thought it meant shorter sequences are easier to predict since they have more positive samples. But I'm quickly proven wrong when checking my cross-validation results: Fold 0 amex 0 . 7990 logloss 0 . 2144 Fold 0 L13 amex 0 . 8214 logloss 0 . 1928 Fold 0 Other amex 0 . 6724 logloss 0 . 3289 content_copy The 1st line is the overall score. The 2nd line is the score of sequences of length 13 and the 3rd line is the score of all the rest sequences. Apparently, shorter sequences have a much worse score than the full sequences of length 13. This is also an implication of how the short sequences are truncated: the more recent profiles are deleted, which could explain the big degradation of the score because more recent profiles have more predicting power in general.  For example, let's say for 13 consecutive months (M1~M13) and sequence A is of length 13 and sequence B is of length 8: M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 A 1 1 1 1 1 1 1 1 1 1 1 1 1 1 B 1 1 1 1 1 1 1 1 1 0 0 0 0 0 content_copy where 1 means features exist and 0 means features missing. Of course, there is another possibility: M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 A 1 1 1 1 1 1 1 1 1 1 1 1 1 1 B 0 0 0 0 0 1 1 1 1 1 1 1 1 1 content_copy We can actually find out which one is more plausible by unstacking dataframes in the above two ways and run xgboost with them, respectively. As expected, the former has a better CV score which indicates it is likely how the truncation of short sequences is done. If we could somehow predict the missing profiles of sequence B , the life of the downstream XGB models would be made much easier. An intuitive choice is to generate the missing profiles using a one-dimension auto-regressive RNN. Bascially we want to predict the features of the next month based on the feature values of the current month and all previous months. And when we have the prediction for the next month, we can use it as part of the input and predict again and so on so forth. This is also where the availability of the big test data really shines. Since we are predicting features, not target , we can train our models using both train and test data. The RNN structure is very simple:  just one GRU layer and some FC layers. The RNN performance is pretty decent. In terms of RMSE of all 178 numerical features, the GRU achieves validation RMSE 0.019 . For simplicity, all features are log-transformed and fillna(0) . You might wonder how good is RMSE 0.019 . We can simply compare it with the naive baseline: just repeat the last available month. For example, if I'm asked to predict features of M2, the naive baseline is just output features of M1. The RMSE of this naive baseline is 0.03 so our RNN actually learns something and could be useful. The rest would be straightforward, after predicting missing months, now every sequence is of length 13 so I just unstack the dataframe to increase the number of features 13x. For example, instead of having one feature P_2 of the last month, now we have 13 features P_2_M_1 to P_2_M_13 . These are the most useful features I created. For downstream classifiers I only use XGB so that it is not similar to the great LGB DART notebook . By varying RNN hyperparameters, XGB hyperparameters and different combination of features, I end up with 7 XGB models, whose ensemble is  0.7993 CV and 0.799 public LB. Averaging it with the best public solution and with extraordinary luck, my final submission ended up in the gold zone. The final thought is my best auto-regressive features are generated 1 hour before the deadline. I'm very happy it worked! Please sign in to reply to this topic. comment 29 Comments 3 appreciation  comments Hotness Jiwei Liu Topic Author Posted 3 years ago · 10th in this Competition arrow_drop_up 3 more_vert I forgot to thank @pyagoubi Art Vandelay for his great kernel: https://www.kaggle.com/code/pyagoubi/amex-eda-evolvement-of-numeric-features-over-time here is a screenshot of his drawing: The pattern shows a clear and predictable trend, which motivates me to go down this path. Art Vandelay Posted 3 years ago · 729th in this Competition arrow_drop_up 0 more_vert wow thanks! Jonathan Mallia Posted 3 years ago arrow_drop_up 1 more_vert Really great idea to generate features for the missing periods! Therefore just to understand, every cid has 13 records in the end, right? After generating the features, did you also predict the target for these new records, or what value for target did you use? Can you also give more info on the one-dimension auto-regressive RNN? Does the RNN predicts all the features at once or loop / predict for every feature? Thanks Jiwei Liu Topic Author Posted 3 years ago · 10th in this Competition arrow_drop_up 1 more_vert Thank you for the question! After generating the features, did you also predict the target for these new records, or what value for target did you use? I just used their original targets. The purpose is to generate more recent features with respect to the original targets. Does the RNN predicts all the features at once? Yes. please also refer to my answer here rajashekar vt Posted 3 years ago arrow_drop_up 1 more_vert that is nice keep it up! XiaFire Posted 3 years ago · 1379th in this Competition arrow_drop_up 1 more_vert Thanks for sharing, I've learned a lot! Congratulations! Priyanshu Chaudhary Posted 3 years ago · 16th in this Competition arrow_drop_up 1 more_vert Congratulations @jiweiliu ,  that's a great insight. tarick.morty Posted 3 years ago · 133rd in this Competition arrow_drop_up 1 more_vert Big Congratulations @jiweiliu . I was trying something similar with KNN regressors, couldn't get a good CV LB to make it work. Feels good that this approach worked like magic. Chris Deotte Posted 3 years ago · 14th in this Competition arrow_drop_up 2 more_vert Congratulations Jiwei! Great solution. I love your use of RNN and test data. Did you try giving the customers with 13 statements more statements? Like predict them a 14th, 15th, 16th etc etc statement? I wonder if that would boost CV LB Jiwei Liu Topic Author Posted 3 years ago · 10th in this Competition arrow_drop_up 4 more_vert Yes, I did. I only tried to predict the 14th statement of customers with 13 statements and it does help. Maybe predicting more is better. I stopped because the prediction of customers with 13 statements is already very accurate, above 0.82 AMEX metric. So I focused more on shorter sequences. 3 more replies arrow_drop_down Tonghui Li Posted 3 years ago · 66th in this Competition arrow_drop_up 2 more_vert @jiweiliu Thanks for sharing and congrats on the solo gold! One thing I wanted to learn more is your construction of short sequences. According to S_2 , the sequence alignment should be the second possibility. Am I thinking about something different here? Jiwei Liu Topic Author Posted 3 years ago · 10th in this Competition arrow_drop_up 4 more_vert Thank you for the comment! Yes, I think you are right and I wasn't clear in my post. If we align two sequences with the date S_2 like Feb/2018, it is the 2nd scenario as you suggested. But if we consider aligning two sequences relative to when the defaults actually happened, it might become the 1st scenario. For example, if M1 to M13 represent Feb/2018 to Mar/2019 , it is aligned as the 2nd case. M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 A 1 1 1 1 1 1 1 1 1 1 1 1 1 1 B 0 0 0 0 0 1 1 1 1 1 1 1 1 1 content_copy so when do the defaults happen? My assumption is default of B happened sometime after default of A , something like this: M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 M14 M15 M16 M17 M18 A 1 1 1 1 1 1 1 1 1 1 1 1 1 1 x B 0 0 0 0 0 1 1 1 1 1 1 1 1 1 x content_copy where x indicates default. The point is sequence B is harder to predict which indicates its more recent profiles are missing. So if we align the default dates x , it looks like scenario 1 in my original post: M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 M14 M15 M16 M17 M18 A 1 1 1 1 1 1 1 1 1 1 1 1 1 1 x B 1 1 1 1 1 1 1 1 1 0 0 0 0 0 x content_copy Of course, this is all my hypothesis and by no means a precise alignment since the true default dates are unknown. However, the idea might be roughly correct and useful. Eventually, it all comes down to what the new feature should mean. P_2_M_13 could mean the value of P_2 in Mar/2019 or it could mean the value of P_2 in the most recent month relative to when the default happens. I think the downstream XGB learns better with the latter. delai50 Posted 3 years ago · 228th in this Competition arrow_drop_up 0 more_vert Congrats and thanks for sharing such a great solution! I read that you trained only one RNN with length 8 and only predicted 1 step ahead each time, but I didn't get which statements covers that length 8 sequence (you mentioned truncating/padding and right alignment). Following your example: customer_ID M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 A 1 1 1 1 1 1 1 1 1 1 1 1 1 B 1 1 1 1 1 1 1 1 0 0 0 0 0 C 1 1 1 1 0 0 0 0 0 0 0 0 0 The data used for training would be the following? (X's are features, Y the target): customer_ID M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 M11 M12 M13 A - - - - X1 X2 X3 X4 X5 X6 X7 X8 Y B X2 X3 X4 X5 X6 X7 X8 Y - - - - - C X6 X7 X8 Y - - - - - - - - - And in the end: customer_ID S1 S2 S3 S4 S5 S6 S7 S8 Target A X1 X2 X3 X4 X5 X6 X7 X8 Y B Pad X2 X3 X4 X5 X6 X7 X8 Y C Pad Pad Pad Pad Pad X6 X7 X8 Y Is that correct? Chris Miles Posted 3 years ago · 2627th in this Competition arrow_drop_up 0 more_vert Thank you very much for the well written, detailed explanation. I have some questions, but if you don't have time or desire to answer, I will not take offense. This is already very enlightening so feel free to ignore. RNN: 1) Is the below impression nearly correct? My impression from we have the prediction for the next month, we can use it as part of the input and predict again and so on so forth is that the model predicts only 1 month ahead. Do you train different models for different number of previous months? If not, it would seem to me that the model would need features for up to 12 previous months, and instances with fewer than 12 would have NaN for those features. Also, you could have a large amount of training data, since an instance with no missing months provides 12 points of training data for this RNN model; the second month (with 1 previous month of know data), the third (with 2 previous months …) and so on. 2) Did you provide a flag to the XGB models that the feature sequence of a row had filled data? CV 3) Do you do a simple k-fold cv (early stop on the validation set and count the score on the same set for your CV score), or did you use nested cv, early stopping on the inner valid set. Or maybe not early stopping, but retraining with an average of best number of iterations. Is there ever a time in which you don't use any early stopping with XGBoost? Sorry for the detail seeking question about CV. I am just really curious as to best practices around cv are. I always figured nested cv was the only \"true\" CV, but I don't find people differentiating which scheme they are using for some reason. Jiwei Liu Topic Author Posted 3 years ago · 10th in this Competition arrow_drop_up 4 more_vert Hi Chris, these are great questions! I'll come back to this and give more details. The short answers are: I only trained one RNN model of length 8 to generate future profiles. The input data are truncated or padded to make length 8 and they are right aligned. No, I don't give a flag to xgb to indicate the features are generated or real. But this is an interesting idea. might work. Yes, I used simple k-fold. but I found very late in the competition that a stratified k-fold on sequence-length might be better. Yes, I used xgb early stopping but very large early_stopping_rounds . 3 more replies arrow_drop_down Elias Posted 3 years ago · 192nd in this Competition arrow_drop_up 0 more_vert Beatiful and original idea. Well done! iliiiiiili Posted 3 years ago · 322nd in this Competition arrow_drop_up 0 more_vert Nice Job! Congrats! Oleg Khudyakov Posted 3 years ago · 204th in this Competition arrow_drop_up 0 more_vert Amazing way to analyze data and generate features! Thanks a lot! This comment has been deleted. Jiwei Liu Topic Author Posted 3 years ago · 10th in this Competition arrow_drop_up 1 more_vert Thank you for the question. The short answer is I ignored the gaps in the sequence. I think what's important is if the more recent profiles of a customer are available, not what's missing in the past. Please also refer to my response to Tonghui Li here . Appreciation (3) Santiago Mota Posted 3 years ago · 92nd in this Competition arrow_drop_up 1 more_vert Thanks for the explanation and congrats SgangX Posted 3 years ago · 717th in this Competition arrow_drop_up 1 more_vert Thanks for sharing ~ Robert Hatch Posted 3 years ago · 45th in this Competition arrow_drop_up 1 more_vert Great job! Thanks for sharing",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict if a customer will default in the future The objective of this competition is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event. The dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories: with the following features being categorical: Your task is to predict, for each customer_ID , the probability of a future payment default ( target = 1 ). Note that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric. 4 files 50.31 GB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 50.31 GB sample_submission.csv test_data.csv train_data.csv train_labels.csv 4 files 384 columns ",
    "data_description": "American Express - Default Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. American Express · Featured Prediction Competition · 3 years ago Late Submission more_horiz American Express - Default Prediction Predict if a customer will default in the future American Express - Default Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start May 25, 2022 Close Aug 25, 2022 Merger & Entry Description link keyboard_arrow_up Whether out at a restaurant or buying tickets to a concert, modern life counts on the convenience of a credit card to make daily purchases. It saves us from carrying large amounts of cash and also can advance a full purchase that can be paid over time. How do card issuers know we’ll pay back what we charge? That’s a complex problem with many existing solutions—and even more potential improvements, to be explored in this competition. Credit default prediction is central to managing risk in a consumer lending business. Credit default prediction allows lenders to optimize lending decisions, which leads to a better customer experience and sound business economics. Current models exist to help manage risk. But it's possible to create better models that can outperform those currently in use. American Express is a globally integrated payments company. The largest payment card issuer in the world, they provide customers with access to products, insights, and experiences that enrich lives and build business success. In this competition, you’ll apply your machine learning skills to predict credit default. Specifically, you will leverage an industrial scale data set to build a machine learning model that challenges the current model in production. Training, validation, and testing datasets include time-series behavioral data and anonymized customer profile information. You're free to explore any technique to create the most powerful model, from creating features to using the data in a more organic way within a model. If successful, you'll help create a better customer experience for cardholders by making it easier to be approved for a credit card. Top solutions could challenge the credit default prediction model used by the world's largest payment card issuer—earning you cash prizes, the opportunity to interview with American Express, and potentially a rewarding new career. Evaluation link keyboard_arrow_up The evaluation metric, M , for this competition is the mean of two measures of rank ordering: Normalized Gini Coefficient, G , and default rate captured at 4%, D . M = 0.5 ⋅ ( G + D ) The default rate captured at 4% is the percentage of the positive labels (defaults) captured within the highest-ranked 4% of the predictions, and represents a Sensitivity/Recall statistic. For both of the sub-metrics G and D , the negative labels are given a weight of 20 to adjust for downsampling. This metric has a maximum value of 1.0. Python code for calculating this metric can be found in this Notebook . Submission File For each customer_ID in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format: customer_ID ,prediction 00000469ba ..., 0 . 01 00001bf2e7 ..., 0 . 22 0000210045 ..., 0 . 98 etc . content_copy Timeline link keyboard_arrow_up May 25, 2022 - Start Date. August 17, 2022 - Entry Deadline. You must accept the competition rules before this date in order to compete. August 17, 2022 - Team Merger Deadline. This is the last day participants may join or merge teams. August 24, 2022 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes And Hiring link keyboard_arrow_up 1st Place - $40,000 2nd Place - $30,000 3rd Place - $20,000 4th Place - $10,000 In addition to cash prizes to the top winners, American Express is hiring! Highly ranked contestants who indicate their interest will be considered by American Express for interviews, based on their work in the competition and additional background. JOB DESCRIPTION American Express is seeking experienced data scientists and machine learning researchers to join our Global Decision Science team. Members of Global Decision Science are responsible for managing enterprise risks throughout the customer lifecycle by developing industry-first data capabilities, building profitable decision-making frameworks and creating machine learning-powered predictive models. Our Global Decision Science team uses industry-leading modeling and AI practices to predict customer behavior. We develop, deploy and validate predictive models and support the use of models in economic logic to enable profitable decisions across credit, fraud, marketing and servicing optimization engines. Positions are available in the US, UK and India. If you'd like your work to be considered for review by the American Express team: Please upload your resume through the Team tab on the competition’s menu bar. Scroll down to “Your Model” and “Upload file” with your solution. You acknowledge that at the end of the competition, the American Express team may request to review your model for purposes of reviewing your capabilities for the job. This license is limited for recruiting and review purposes only. Note that applicants who are one member of a team may be requested to provide documentation of their specific contribution to a team model. Citation link keyboard_arrow_up Addison Howard, AritraAmex, Di Xu, Hossein Vashani, inversion, Negin, and Sohier Dane. American Express - Default Prediction. https://kaggle.com/competitions/amex-default-prediction, 2022. Kaggle. Cite Competition Host American Express Prizes & Awards $100,000 Awards Points & Medals Participation 29,924 Entrants 6,003 Participants 4,874 Teams 90,058 Submissions Tags Tabular Binary Classification Finance Custom Metric Table of Contents collapse_all Description Evaluation Timeline Prizes And Hiring Citation"
  },
  {
    "competition_slug": "foursquare-location-matching",
    "discussion_links": [
      "/competitions/foursquare-location-matching/discussion/336055",
      "/competitions/foursquare-location-matching/discussion/336090",
      "/competitions/foursquare-location-matching/discussion/338112",
      "/competitions/foursquare-location-matching/discussion/335810",
      "/competitions/foursquare-location-matching/discussion/348399",
      "/competitions/foursquare-location-matching/discussion/335800",
      "/competitions/foursquare-location-matching/discussion/335928",
      "/competitions/foursquare-location-matching/discussion/336415",
      "/competitions/foursquare-location-matching/discussion/335924",
      "/competitions/foursquare-location-matching/discussion/336051",
      "/competitions/foursquare-location-matching/discussion/336124",
      "/competitions/foursquare-location-matching/discussion/335818",
      "/competitions/foursquare-location-matching/discussion/335860",
      "/competitions/foursquare-location-matching/discussion/335857",
      "/competitions/foursquare-location-matching/discussion/335883"
    ],
    "discussion_texts": [
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules",
      "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Match point of interest data across datasets The data presented here comprises over one-and-a-half million place entries for hundreds of thousands of commercial Points-of-Interest (POIs) around the globe. Your task is to determine which place entries describe the same point-of-interest. Though the data entries may represent or resemble entries for real places, they may also contain artificial information or additional noise. To help you author submission code, we include a few example instances selected from the test set. When you submit your notebook for scoring, this example data will be replaced by the actual test data. The actual test set has approximately 600,000 place entries with POIs that are distinct from the POIs in the training set. 1 files 215 B csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 215 B sample_submission.csv 1 file 2 columns ",
    "data_description": "Foursquare - Location Matching | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Foursquare · Featured Code Competition · 3 years ago Late Submission more_horiz Foursquare - Location Matching Match point of interest data across datasets Foursquare - Location Matching Overview Data Code Models Discussion Leaderboard Rules Overview Start Apr 14, 2022 Close Jul 8, 2022 Merger & Entry Description link keyboard_arrow_up When you look for nearby restaurants or plan an errand in an unknown area, you expect relevant, accurate information. To maintain quality data worldwide is a challenge, and one with implications beyond navigation. Businesses make decisions on new sites for market expansion, analyze the competitive landscape, and show relevant ads informed by location data. For these, and many other uses, reliable data is critical. Large-scale datasets on commercial points-of-interest (POI) can be rich with real-world information. To maintain the highest level of accuracy, the data must be matched and de-duplicated with timely updates from multiple sources. De-duplication involves many challenges, as the raw data can contain noise, unstructured information, and incomplete or inaccurate attributes. A combination of machine-learning algorithms and rigorous human validation methods are optimal to de-dupe datasets. With 12+ years of experience perfecting such methods, Foursquare is the #1 independent provider of global POI data. The leading independent location technology and data cloud platform, Foursquare is dedicated to building meaningful bridges between digital spaces and physical places. Trusted by leading enterprises like Apple, Microsoft, Samsung, and Uber, Foursquare’s tech stack harnesses the power of places and movement to improve customer experiences and drive better business outcomes. In this competition, you’ll match POIs together. Using a dataset of over one-and-a-half million Places entries heavily altered to include noise, duplications, extraneous, or incorrect information, you'll produce an algorithm that predicts which Place entries represent the same point-of-interest. Each Place entry includes attributes like the name, street address, and coordinates. Successful submissions will identify matches with the greatest accuracy. By efficiently and successfully matching POIs, you'll make it easier to identify where new stores or businesses would benefit people the most. This is a Code Competition. Refer to Code Requirements for details. Evaluation link keyboard_arrow_up Submissions are evaluated by the mean Intersection over Union (IoU, aka the Jaccard index) of the ground-truth entry matches and the predicted entry matches. The mean is taken sample-wise, meaning that an IoU score is calculated for each row in the submission file, and the final score is their average. Submission File For each place entry id in the test set, you should submit a space-delimited list of matching place id s. Places always self-match , so the list of matches for an id should always contain that id . The file should contain a header, be named submission.csv , and have the following format: id, matches E_00001118ad0191, E_00001118ad0191 E_000020eb6fed40, E_000020eb6fed40 E_00002f98667edf, E_00002f98667edf E_001b6bad66eb98, E_001b6bad66eb98 E_0283d9f61e569d E_0283d9f61e569d, E_0283d9f61e569d E_001b6bad66eb98 content_copy You should predict matches for every id . For example, if you believe A matches B and C , your submission file should include rows A,A B C , but also B,B A C and C,C A B . Timeline link keyboard_arrow_up April 14, 2022 - Start Date. June 30, 2022 - Entry Deadline. You must accept the competition rules before this date in order to compete. June 30, 2022 - Team Merger Deadline. This is the last day participants may join or merge teams. July 7, 2022 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - $12,500 2nd Place - $7,500 3rd Place - $5,000 Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time Internet access disabled Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors. Citation link keyboard_arrow_up Addison Howard, Anastassia Etropolski, Ariel Segal Eck, cflaming, Gabriel Pelican, Kashuk, Ryan Holbrook, Saurav Bose, and Zisis Petrou. Foursquare - Location Matching. https://kaggle.com/competitions/foursquare-location-matching, 2022. Kaggle. Cite Competition Host Foursquare Prizes & Awards $25,000 Awards Points & Medals Participation 6,649 Entrants 1,290 Participants 1,079 Teams 22,050 Submissions Tags Tabular Geography Business Custom Metric Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Citation"
  },
  {
    "competition_slug": "jpx-tokyo-stock-exchange-prediction",
    "discussion_links": [
      "/competitions/jpx-tokyo-stock-exchange-prediction/discussion/359151",
      "/competitions/jpx-tokyo-stock-exchange-prediction/discussion/359227",
      "/competitions/jpx-tokyo-stock-exchange-prediction/discussion/361127"
    ],
    "discussion_texts": [
      "JPX Tokyo Stock Exchange Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Japan Exchange Group · Featured Code Competition · 3 years ago Late Submission more_horiz JPX Tokyo Stock Exchange Prediction Explore the Tokyo market with your data science skills JPX Tokyo Stock Exchange Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "JPX Tokyo Stock Exchange Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Japan Exchange Group · Featured Code Competition · 3 years ago Late Submission more_horiz JPX Tokyo Stock Exchange Prediction Explore the Tokyo market with your data science skills JPX Tokyo Stock Exchange Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "JPX Tokyo Stock Exchange Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Japan Exchange Group · Featured Code Competition · 3 years ago Late Submission more_horiz JPX Tokyo Stock Exchange Prediction Explore the Tokyo market with your data science skills JPX Tokyo Stock Exchange Prediction Overview Data Code Models Discussion Leaderboard Rules"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Explore the Tokyo market with your data science skills This dataset contains historic data for a variety of Japanese stocks and options. Your challenge is to predict the future returns of the stocks. As historic stock prices are not confidential this will be a forecasting competition using the time series API. The data for the public leaderboard period is included as part of the competition dataset. Expect to see many people submitting perfect submissions for fun. Accordingly, the active phase public leaderboard for this competition is intended as a convenience for anyone who wants to test their code. The forecasting phase leaderboard will be determined using real market data gathered after the submission period closes. stock_prices.csv The core file of interest. Includes the daily closing price for each stock and the target column. options.csv Data on the status of a variety of options based on the broader market. Many options include implicit predictions of the future price of the stock market and so may be of interest even though the options are not scored directly. secondary_stock_prices.csv The core dataset contains on the 2,000 most commonly traded equities but many less liquid securities are also traded on the Tokyo market. This file contains data for those securities, which aren't scored but may be of interest for assessing the market as a whole. trades.csv Aggregated summary of trading volumes from the previous business week. financials.csv Results from quarterly earnings reports. stock_list.csv - Mapping between the SecuritiesCode and company names, plus general information about which industry the company is in. data_specifications/ - Definitions for individual columns. jpx_tokyo_market_prediction/ Files that enable the API. Expect the API to deliver all rows in under five minutes and to reserve less than 0.5 GB of memory. Copies of data files exist in multiple folders that cover different time windows and serve different purposes. train_files/ Data folder covering the main training period. supplemental_files/ Data folder containing a dynamic window of supplemental training data. This will be updated with new data during the main phase of the competition in early May, early June, and roughly a week before the submissions are locked. The supplemental data will also be updated once at the very beginning of the forecasting phase so that the test set will start with the trading day after the last trading day in the supplemental data. example_test_files/ Data folder covering the public test period. Intended to facilitate offline testing. Includes the same columns delivered by the API (ie no Target column). You can calculate the Target column from the Close column; it's the return from buying a stock the next day and selling the day after that. This folder also includes an example of the sample submission file that will be delivered by the API. 24 files 1.33 GB csv, so, py Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 1.33 GB data_specifications example_test_files jpx_tokyo_market_prediction supplemental_files train_files stock_list.csv 24 files 511 columns ",
    "data_description": "JPX Tokyo Stock Exchange Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Japan Exchange Group · Featured Code Competition · 3 years ago Late Submission more_horiz JPX Tokyo Stock Exchange Prediction Explore the Tokyo market with your data science skills JPX Tokyo Stock Exchange Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Apr 5, 2022 Close Oct 8, 2022 Merger & Entry Description link keyboard_arrow_up Success in any financial market requires one to identify solid investments. When a stock or derivative is undervalued, it makes sense to buy. If it's overvalued, perhaps it's time to sell. While these finance decisions were historically made manually by professionals, technology has ushered in new opportunities for retail investors. Data scientists, specifically, may be interested to explore quantitative trading, where decisions are executed programmatically based on predictions from trained models. There are plenty of existing quantitative trading efforts used to analyze financial markets and formulate investment strategies. To create and execute such a strategy requires both historical and real-time data, which is difficult to obtain especially for retail investors. This competition will provide financial data for the Japanese market, allowing retail investors to analyze the market to the fullest extent. Japan Exchange Group, Inc. (JPX) is a holding company operating one of the largest stock exchanges in the world, Tokyo Stock Exchange (TSE), and derivatives exchanges Osaka Exchange (OSE) and Tokyo Commodity Exchange (TOCOM). JPX is hosting this competition and is supported by AI technology company AlpacaJapan Co.,Ltd. This competition will compare your models against real future returns after the training phase is complete. The competition will involve building portfolios from the stocks eligible for predictions (around 2,000 stocks). Specifically, each participant ranks the stocks from highest to lowest expected returns and is evaluated on the difference in returns between the top and bottom 200 stocks. You'll have access to financial data from the Japanese market, such as stock information and historical stock prices to train and test your model. All winning models will be made public so that other participants can learn from the outstanding models. Excellent models also may increase the interest in the market among retail investors, including those who want to practice quantitative trading. At the same time, you'll gain your own insights into programmatic investment methods and portfolio analysis―and you may even discover you have an affinity for the Japanese market. This is a Code Competition. Refer to Code Requirements for details. Evaluation link keyboard_arrow_up Submissions are evaluated on the Sharpe Ratio of the daily spread returns. You will need to rank each stock active on a given day. The returns for a single day treat the 200 highest (e.g. 0 to 199) ranked stocks as purchased and the lowest (e.g. 1999 to 1800) ranked 200 stocks as shorted. The stocks are then weighted based on their ranks and the total returns for the portfolio are calculated assuming the stocks were purchased the next day and sold the day after that. You can find a python implementation of the metric here . You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow this template in Kaggle Notebooks: import jpx_tokyo_market_prediction env = jpx_tokyo_market_prediction.make_env() # initialize the environment iter_test = env .iter_test() # an iterator which loops over the test files for (prices, options, financials, trades, secondary_prices, sample_prediction) in iter_test:\n    sample_prediction_df[ 'Rank' ] = np.arange(len(sample_prediction)) # make your predictions here env .predict(sample_prediction_df) # register your predictions content_copy You will get an error if you: Use ranks that are below zero or greater than or equal to the number of stocks for a given date. Submit any duplicated ranks. Change the order of the rows. Timeline link keyboard_arrow_up This is a forecasting competition with an active training phase and a second period where models will be run against real market data. Training Timeline April 4, 2022 - Start Date June 28, 2022 - Entry deadline. You must accept the competition rules before this date in order to compete. June 28, 2022 - Team Merger deadline. This is the last day participants may join or merge teams. July 5, 2022 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Forecasting Timeline: Starting after the final submission deadline there will be periodic updates to the leaderboard to reflect market data updates that will be run against selected notebooks. Updates will take place roughly every two weeks. October 7, 2022 - Competition End Date - Winner's announcement Prizes link keyboard_arrow_up 1st     $20,000 2nd     $10,000 3rd     $7,000 4th     $6,000 5th     $5,000 6-10th     $3,000 All winning models will be made public. Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time Internet access disabled Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv . The API will generate this submission file for you. Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors. Citation link keyboard_arrow_up AkihiroSugiyama, Chihiro Hio(Alpaca), Eiichiro Kaji, n-onishi, s-meitoma - JPX, Shun Takato, Shun Takato - JPX, Sohier Dane, and Tomoya Kitayama(Alpaca). JPX Tokyo Stock Exchange Prediction. https://kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction, 2022. Kaggle. Cite Competition Host Japan Exchange Group Prizes & Awards $63,000 Awards Points & Medals Participation 19,206 Entrants 1,324 Participants 2,033 Teams 1,573 Submissions Tags Tabular Finance Custom Metric Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Citation"
  },
  {
    "competition_slug": "ubiquant-market-prediction",
    "discussion_links": [
      "/competitions/ubiquant-market-prediction/discussion/338220",
      "/competitions/ubiquant-market-prediction/discussion/338615",
      "/competitions/ubiquant-market-prediction/discussion/338561",
      "/competitions/ubiquant-market-prediction/discussion/338400",
      "/competitions/ubiquant-market-prediction/discussion/338293",
      "/competitions/ubiquant-market-prediction/discussion/338236",
      "/competitions/ubiquant-market-prediction/discussion/338239"
    ],
    "discussion_texts": [
      "Ubiquant Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Ubiquant · Featured Code Competition · 3 years ago Late Submission more_horiz Ubiquant Market Prediction Make predictions against future market data Ubiquant Market Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "Ubiquant Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Ubiquant · Featured Code Competition · 3 years ago Late Submission more_horiz Ubiquant Market Prediction Make predictions against future market data Ubiquant Market Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "Ubiquant Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Ubiquant · Featured Code Competition · 3 years ago Late Submission more_horiz Ubiquant Market Prediction Make predictions against future market data Ubiquant Market Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "Ubiquant Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Ubiquant · Featured Code Competition · 3 years ago Late Submission more_horiz Ubiquant Market Prediction Make predictions against future market data Ubiquant Market Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "Ubiquant Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Ubiquant · Featured Code Competition · 3 years ago Late Submission more_horiz Ubiquant Market Prediction Make predictions against future market data Ubiquant Market Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "Ubiquant Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Ubiquant · Featured Code Competition · 3 years ago Late Submission more_horiz Ubiquant Market Prediction Make predictions against future market data Ubiquant Market Prediction Overview Data Code Models Discussion Leaderboard Rules",
      "Ubiquant Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Ubiquant · Featured Code Competition · 3 years ago Late Submission more_horiz Ubiquant Market Prediction Make predictions against future market data Ubiquant Market Prediction Overview Data Code Models Discussion Leaderboard Rules"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Make predictions against future market data This dataset is no longer available for download. This dataset contains features derived from real historic data from thousands of investments. Your challenge is to predict the value of an obfuscated metric relevant for making trading decisions. This is a code competition that relies on a time-series API to ensure models do not peek forward in time. To use the API, follow the instructions on the Evaluation page. When you submit your notebook, it will be rerun on an unseen test. This is also a forecasting competition, where the final private leaderboard will be determined using data gathered after the training period closes. train.csv example_test.csv - Random data provided to demonstrate what shape and format of data the API will deliver to your notebook when you submit. example_sample_submission.csv - An example submission file provided so the publicly accessible copy of the API provides the correct data shape and format. supplemental_train.csv - Once submissions are locked on April 18th, the hidden test set copy of this file will be replaced with the data currently used for the public leaderboard. Until then it will contain randomly generated noise of the correct shape to assist with debugging. ubiquant/ - The image delivery API that will serve the test set. You may need Python 3.7 and a Linux environment to run the example test set through the API offline without errors. Time-series API Details The API serves the data in batches, with all of rows for a single time time_id per batch. Expect to see roughly one million rows in the test set. The API will require roughly 0.25 GB of memory after initialization. The initialization step (env.iter_test()) will require meaningfully more memory than that; we recommend you do not load your model until after making that call. The API will also use less than 15 minutes of runtime for loading and serving the data. 1 files 173 B txt Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 173 B readme.txt 1 file ",
    "data_description": "Ubiquant Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Ubiquant · Featured Code Competition · 3 years ago Late Submission more_horiz Ubiquant Market Prediction Make predictions against future market data Ubiquant Market Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Jan 19, 2022 Close Jul 19, 2022 Merger & Entry Description link keyboard_arrow_up Regardless of your investment strategy, fluctuations are expected in the financial market. Despite this variance, professional investors try to estimate their overall returns. Risks and returns differ based on investment types and other factors, which impact stability and volatility. To attempt to predict returns, there are many computer-based algorithms and models for financial market trading. Yet, with new techniques and approaches, data science could improve quantitative researchers' ability to forecast an investment's return. Ubiquant Investment (Beijing) Co., Ltd is a leading domestic quantitative hedge fund based in China. Established in 2012, they rely on international talents in math and computer science along with cutting-edge technology to drive quantitative financial market investment. Overall, Ubiquant is committed to creating long-term stable returns for investors. In this competition, you’ll build a model that forecasts an investment's return rate. Train and test your algorithm on historical prices. Top entries will solve this real-world data science problem with as much accuracy as possible. If successful, you could improve the ability of quantitative researchers to forecast returns. This will enable investors at any scale to make better decisions. You may even discover you have a knack for financial datasets, opening up a world of new opportunities in many industries. See more information about Ubiquant below: (opens in a new tab)\"> This is a Code Competition. Refer to Code Requirements for details. Evaluation link keyboard_arrow_up Submissions are evaluated on the mean of the Pearson correlation coefficient for each time ID. You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow this template in Kaggle Notebooks: import ubiquant env = ubiquant.make_env() # initialize the environment iter_test = env .iter_test() # an iterator which loops over the test set and sample submission for (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df[ 'target' ] = 0 # make your predictions here env .predict(sample_prediction_df) # register your predictions content_copy You will get an error if you submission includes nulls or infinities and submissions that only include one prediction value will receive a score of -1. Timeline link keyboard_arrow_up EDIT: The final submission deadline has been extended 48 hours due to reports of an inability to submit This is a forecasting competition with an active training phase and a second period where models will be run against real market data. Training Timeline January 18, 2022 - Start Date. April 11, 2022 - Entry Deadline. You must accept the competition rules before this date in order to compete. April 11, 2022 - Team Merger Deadline. This is the last day participants may join or merge teams. April 20, 2022 - Final Submission Deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Forecasting Timeline: Starting after the final submission deadline there will be periodic updates to the leaderboard to reflect data updates that will be run against selected notebooks. Updates will take place roughly every two weeks. July 18th, 2022 - Competition End Date Prizes link keyboard_arrow_up 1st Place - $40,000 2nd Place - $20,000 3rd Place - $10,000 4th Place - $5,000 5th Place - $5,000 6th Place - $5,000 7th Place - $5,000 8th Place - $3,500 9th Place - $3,500 10th Place - $3,000 Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time Internet access disabled Freely & publicly available external data is allowed, including pre-trained models Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors. Ubiquant Contact Information link keyboard_arrow_up During the competition, participants are encouraged to use the Discussion Forums to discuss techniques and ask questions. Should you have other questions for the competition sponsor, or be interested in employment with Ubiquant, see the links below. For more information, check out the following: Official website of Ubiquant: https://www.ubiquant.com/website/home Official website for recruitment: https://app.mokahr.com/apply/ubiquantrecruit/37030#/ Competition email address: hackathon@ubiquant.com Recruitment email address: recruiter@ubiquant.com Citation link keyboard_arrow_up Addison Howard, Seth Syun, Sohier Dane, tangyuexia, and 韭足饭饱. Ubiquant Market Prediction. https://kaggle.com/competitions/ubiquant-market-prediction, 2022. Kaggle. Cite Competition Host Ubiquant Prizes & Awards $100,000 Awards Points & Medals Participation 21,049 Entrants 2,949 Participants 2,893 Teams 4,159 Submissions Tags Tabular Finance MeanPearson Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Ubiquant Contact Information Citation"
  },
  {
    "competition_slug": "g-research-crypto-forecasting",
    "discussion_links": [
      "/competitions/g-research-crypto-forecasting/discussion/323098",
      "/competitions/g-research-crypto-forecasting/discussion/323703",
      "/competitions/g-research-crypto-forecasting/discussion/323250",
      "/competitions/g-research-crypto-forecasting/discussion/324180",
      "/competitions/g-research-crypto-forecasting/discussion/313386",
      "/competitions/g-research-crypto-forecasting/discussion/323602",
      "/competitions/g-research-crypto-forecasting/discussion/308325",
      "/competitions/g-research-crypto-forecasting/discussion/322886"
    ],
    "discussion_texts": [
      "G-Research Crypto Forecasting  | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. G-Research · Featured Code Competition · 3 years ago Late Submission more_horiz G-Research Crypto Forecasting Use your ML expertise to predict real crypto market data G-Research Crypto Forecasting Overview Data Code Models Discussion Leaderboard Rules Nathaniel Maddux · 2nd in this Competition  · Posted 3 years ago arrow_drop_up 80 more_vert 2nd place solution Hi everyone, This is great. I'm relieved to have maintained second place in the final update, as I was nervous that anything could happen. By my estimate, there was a 60% chance I would drop below 2nd on the 6th update, due to the noisy nature of this kind of data. I also estimate that on a 7th update, Meme Lord Capital would have a 75% chance of maintaining first place. They finished with a big lead on everyone. Good job, Meme Lord Capital! I'd like to thank G-Research for sponsoring the competition, and everyone involved in organizing and hosting it. Introduction In this writeup I'll strive to provide insight into my methods, but without giving away model details that could be profitable for the host. When a good financial indicator becomes common knowledge, everyone uses it, therefore it loses profitability. As such, I won't be sharing my code or any specific insight on features. I decided to enter this competition because it concerns asset price prediction using actual market data (not pre-engineered features). That's the kind of data I like: having an explicit time dimension, many interacting entities, a lot of noise, and a need for creative method development. In 2014-2017, I researched stock price analysis and prediction in my own time. It didn't end up being profitable long-term as a single DIY investor, but I did learn a lot about predicting time series and asset prices. Toolkit My toolkit consisted of CPython, Numba, Jupyter, Pandas, LightGBM, Matplotlib, and scikit-learn. Using the Numba compiler instead of CPython for feature generation resulted in high performance. Doing so, I was able to experiment with a wide variety of features, utilizing the entire dataset and iterating quickly. Cross validation Setting up good N-fold cross validation (CV) is essential. We should try to see if our ideas fail in the most real environment we can make. Every time we have a research question, it should (eventually, after a period of creative exploration) be answered in that manner. If we don't have a good CV setup, our decisions will mostly be mere guesses. Good N-fold CV is easy to set up, taking less than 100 lines of code, without using scikit-learn or any timeseries framework. In this competition I used 6-fold, walk-forward, grouped cross validation. The group key was the timestamp. In a typical setup, train folds were 40 weeks long, test folds were 40 weeks long, there was a gap of 1 week between test and train folds, and the ends of training folds were incremented by 20 weeks each fold. I chose to overlap my folds so that they could be long, but I could still have 6 folds. With non-overlapping folds, I would have to decide between many short folds or a few long ones. The advantage of having many folds is that average CV scores will have lower variance. On the other hand, the advantage of having long folds is that the model sees more data, so you get a better picture of how it will perform with the full dataset. Instead of choosing between many short folds or a few long ones, I let them overlap so I could have many long folds. I could have used more than 6 folds, or run CV multiple times, changing seeds. I didn't do these things because CV score variance was decently low, and CV was already almost too slow. (If you don't pay attention to CV variance, you could waste days optimizing your model, only to find that your early decisions were based on noise and incorrect. An easy way to see if your CV scores have too much variance is to look at a plot of CV score vs. a parameter you're tuning. A good plot will usually be smooth, with a knee and a plateau, or maybe a peak or a valley. If the plot looks too noisy to clearly see those things, then try it again, with different seeds, and see if you get different results. If you do, then your CV results are too noisy. You can try using more folds, or running CV several times with different random seeds and averaging.) The one week gap between train and test data was to prevent CV results from being too rosy. With no gap, a model can cheat at the the beginning of the test period, because the end of the train period is very similar. There was no gap between train and test data in the final submission. Public leaderboard scores As I was developing my model, naturally I wanted to have an idea of how it would place against competitors. Because public leaderboard scores were unrealistically high due to probing and overfitting, I had to find a way to interpret them. So I looked at masters and grandmasters with \"low\" scores. I figured these competitors were good enough to submit good kernels, but had been careful to keep the public leaderboard period out of their training data. I did not expect true scores to go above 0.1, based on experience and discussions here. Plotting the sorted scores of the \"low\" scoring masters and grandmasters, there was a clear plateau around 0.08, as I recall. That was the score I aimed to beat in my local CV. Of course, I did not use the public leaderboard as a guide in any optimizations. That's always a bad idea (see the paragraph above about CV variance). I only made a total of two submissions in the competition. Feature engineering Because my labor and computational resources are finite, it’s important to know where to direct them. As I developed my model, feature engineering was guided by feature importance. As I made new features, I focused further effort on developing features sets that already performed well (had high importance), or for which transformations led to easy increases in importance. As I said in the introduction, I won't be discussing specific feature insight or give my code. I'll only be giving my two cents on methods. Learner The learner was trivial: a LightGBM GBDT regressor with squared loss. There was no ensembling other than the gradient boosting in GBDT. The only parameters I changed from defaults were the number of estimators, number of leaves, and the learning rate. There was no regularization, augmentation, or feature neutralization. I checked with CV whether these things would help, found that they wouldn't, and decided to go with a simpler submission. LightGBM parameters were not tuned exhaustively. Tuning was taking a long time, and CV results were indicating that regularization would have little effect on model performance. So I decided to not push on that wall and instead placed my focus back on feature engineering. CV is essential in making these resource allocation decisions. Submission kernel Because this was a code competition with a large dataset, performance was crucial. Our kernels were limited to running in 9 hours or less, using up to 16GB of RAM. As we all can attest, it's easy to bump into those limits. The hardest limitation I dealt with was the 16GB RAM limit. I wanted to train with the entire dataset, because CV had shown me that scores just kept improving with longer training data. To use the entire dataset, I had to find and fix code that resulted in unnecessary RAM copies of arrays. The Pandas operation df.update() was a frustrating offender. The solution was to select small subsets of data, use df.update() on those subsets, then append to a list of dataframes, and then at the end, concatenate the list into one dataframe. On the other hand, my kernel is really fast. It takes 25 minutes to import data, generate features, and train the model. When submitting, predictions are generated at a rate of about 50 timestamps per second, not including the overhead of the submission mechanism. Generating predictions for 2 weeks of data takes around 10 minutes, on top of the the 25 minutes to train the model. Speed wasn't a limiting factor in my submission because I used Numba to write feature generation code and my learner was simple. My main goal was to have a codebase that allowed me to test feature ideas quickly and easily. The fast submission kernel was a side effect. Please sign in to reply to this topic. comment 24 Comments Hotness Dehma Posted 2 years ago arrow_drop_up 12 more_vert Could you tell us, nearly 1 year after the competition, if your model was able to significantly outperform the market? Jenmar Posted 2 years ago arrow_drop_up 2 more_vert This is something I also want to know Doctor Posted 3 years ago arrow_drop_up 5 more_vert Very good job and congratz! Could you share any further insight into how you went on to select meaningful features? Was it just adding and subtracting features, looking at lgbm feature importance etc. Or something more sophisticated? Nathaniel Maddux Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert That's a good question. You guessed a topic that I decided to leave out. It was more sophisticated. Lucas Morin Posted 3 years ago · 797th in this Competition arrow_drop_up 5 more_vert Congratulation. Nice to see Feature Engineering and (mostly untuned) lgbm for the win. Of course I have some questions regarding modelling, but feel free to not answer; How do you handle coind id ? (one model for all coin ? with the coin as a feature ? or individual models ?). Regarding FE, what would be your opinion on what has been shared ? (are common factor a  good idea ? technical analysis ? or did you go into more complex stuff ?) Nathaniel Maddux Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Good questions. You too guessed a topic I decided to leave out. I tried all of it. CV and feature importance guided the way. valindor Posted 3 years ago arrow_drop_up 3 more_vert Anything you would have like to explored if you had more resources and time? Nathaniel Maddux Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 2 more_vert Absolutely, there are a lot of things. All the way up to the deadline, feature engineering was the best avenue for improvement. So I'd continue developing features. I'd want to improve my codebase a little first. I'd revisit some decisions I made based on CV. With more computational resources, CV scores would have less variance, and I might be able to see, for example, that regularization actually improved the model a little. I'd try more complex models, especially using autoencoders, convolutional NN's, recursive NN's, etc. There's a feature engineering research idea I'd like to try, but I probably shouldn't discuss it publicly. This competition got me interested again in something I was researching a while ago. Essentially, I\"m thinking of supervised learning problems where the data is nonlinear, high dimensional and has a lot of noise. There are toy datasets that seem like they should be easy to fit, but quickly become challenging as noisy dimensions are added. Neural nets do the best on these toy datasets, with decision forest methods coming in a distant second, and basically every other method fails with even a few dimensions. It's interesting to try to make new models that do better than neural nets in that playground. I particularly enjoy a certain area of research that has applications to asset price prediction. That's unsupervised data decompositions in general (PCA being an example), but especially if there's any connection to statistical mechanics, quantum mechancs, information theory, or emergence. Fritz Cremer Posted 3 years ago · 112th in this Competition arrow_drop_up 4 more_vert Congratulations! I know you said you don't want to go into your FE or other details, but I just wonder, how much do you think your success can be attributed to model choice/hyperparameter choice/cross validation setup, and how much is feature engineering? Do you think you that finding the right features was the key in this competition? As you said you did not any \"fancy\" models, I assume there must be something special in your feature engineering. Nathaniel Maddux Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 6 more_vert It was mostly feature engineering, and yes, finding the right features was key. Minkyu Kim Posted 3 years ago arrow_drop_up 1 more_vert Congraturation!! Arthur Hsu Posted 3 years ago arrow_drop_up 1 more_vert Simple but useful suggestion,thanks sharing! Maria Aboulaala Posted 3 years ago arrow_drop_up 1 more_vert congrats mate SRK Posted 3 years ago arrow_drop_up 1 more_vert Congratulations @nathanrm and thanks for sharing the solution summary. Satoshi Datamoto Posted 3 years ago arrow_drop_up 1 more_vert This is a great post! Thank you for sharing so much information. Your writeup has given me a lot to think about in terms of how I approach future data competitions. Lachlan Gillian Posted 3 years ago arrow_drop_up 1 more_vert @nathanrm congratulations! very impressive :) JM Posted 3 years ago arrow_drop_up 1 more_vert Congratulations and thanks for sharing! There was no ensembling Impressive! 💥Tesla, Inc.💥 Posted 3 years ago arrow_drop_up 1 more_vert @nathanrm Wow congratulations on getting 2nd place, amazing work PsiqueR Posted 3 years ago arrow_drop_up 2 more_vert Any recommendation/resources for learning about time series and asset prediction for a newbie? Nathaniel Maddux Topic Author Posted 3 years ago · 2nd in this Competition arrow_drop_up 2 more_vert Giving a recommendation without knowing your background would be difficult. You could search \"best timeseries book\" and find one that's highly recommended, specific to your programming language and application domain. I come from a physics background, and timeseries are a natural extension of that, so I learned by experimenting and reading here and there. Timeseries learning is essentially supervised learning that respects causality. You avoid doing anything in your algorithm that would depend on data from the future. That's both harder and easier than it sounds. Technically, it's easy because you're avoiding doing something. In practice, you can sometimes take shortcuts and make mistakes that end up causing your algorithm to need future data. Learning how to avoid these pitfalls is one of the main things to learning timeseries forecasting. You want to be as creative as possible, clustering assets by market, etc., without making these mistakes and breaking causality. You have to develop a way of thinking about it. The only way I know to do that is with experience, but there might be a resource that teaches that way of thinking. There are some things to learn that are very useful for timeseries, like de-trending data by differencing, lagged features, etc. A good timeseries book should show you these things, but a lot of it is almost self-suggesting, based on understanding the data you're modelling. For example, you could learn on your own about de-trending data by diagnosing a timeseries algorithm that didn't work (and then you'd also learn about diagnosing timeseries models). Because understanding the data is important, domain knowledge is important. The rest of it (actually a huge portion of it) is just machine learning. Every technique, method, or algorithm that can be applied to a supervised or unsupervised learning problem can be used for timeseries prediction. LG Posted 3 years ago · 1557th in this Competition arrow_drop_up 2 more_vert Congrats! I would have loved to hear a bit more about the feature engineering part but also understand why you are keeping that secret. Well done! Hinz Posted 3 years ago · 8th in this Competition arrow_drop_up 2 more_vert Congratulations! I believe feature engineering is the key. :) ABHAY SINGH Posted a month ago arrow_drop_up 0 more_vert Will your model is able to predict short term crypto price movement ? Sahand Nazarzadeh Posted 2 years ago arrow_drop_up 0 more_vert Congratulations! Too many requests error Too many requests",
      "G-Research Crypto Forecasting  | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. G-Research · Featured Code Competition · 3 years ago Late Submission more_horiz G-Research Crypto Forecasting Use your ML expertise to predict real crypto market data G-Research Crypto Forecasting Overview Data Code Models Discussion Leaderboard Rules sugghi · 3rd in this Competition  · Posted 3 years ago arrow_drop_up 66 more_vert 3rd place solution First of all, I would like to thank G-Research, kaggle and all competitors. I have learned a lot through this interesting competition. It was very difficult for me (actually, it was the third machine learning challenge in my life) and without the knowledge shared by all the kagglers, I would not have been able to handle it. Also, since this is a competition with a strong element of luck, it was only by sheer luck that I was able to finish in third place, but I am glad that my rank was somewhat stable in the last few updates. In coding, I referred to the knowledge of kagglers. I am especially grateful to @jagofc for providing local api . Without @jagofc 's local api, I would not have been able to successfully submit even once. Here is my solution. If you have any suggestions for improvement or anything else you think I should know, I'd be glad to hear from you! The link to the notebook is as follows; training: https://www.kaggle.com/code/sugghi/training-3rd-place-solution inference: https://www.kaggle.com/code/sugghi/inference-3rd-place-solution Overview of my model The characteristics of my model are as follows Only 'Close' is used. The model is trained for each coin using a common set of features for all the coins. The difference between the change of each currency and the change of all currencies is provided as features. Single model of LightGBM (7-fold CV) Considering the definition of the forecasting target in this competition, I felt it was necessary to prepare features with information about the entire market. I also thought that some currencies might be affected by the movements of other currencies, so I made it possible to refer to information about other currencies as well. Since I thought that memory and inference time would become more demanding with this kind of processing, I reduced the amount of data to be used. Specifically, I considered 'Close', which is used to calculate the target, to be the most important, so I decided to use only it. Even so, the ensemble could not be performed because of the limited inference time (and lack of coding skill). For CV, I used EmbargoCV by @nrcjea001 . About the data used for training The start date of train data differed greatly among currencies. Since my model deals with the average of the changes of each currency, I considered it undesirable for the existing currencies to differ significantly between the training period and the evaluation period. As I expected that all currencies would have few missing values during the evaluation period of the competition, I decided not to use all of the train data, but to use the data from the period when there were enough currencies present. The selection of the starting date was done by looking at the CV scores. However, this was a mistake in hindsight, since it meant that I was comparing CV scores across different data. Also, each currency had several long and short blank periods. I attempted forward fill to prevent missing data as a result of rolling. On the other hand, I thought that forward fill for the entire period might cause a decline in data quality when there is a long blank period, so I set a limit on forward fill. In the evaluation phase, the code was designed to have forward fill without a limit, but I thought this would not be a problem since there are no long blank periods in the evaluation phase. Feature engineering Since the value of cryptocurrencies is increasing, I tried to make sure to pick up the magnitude of the change independent of the evaluation period. For 'Close', I prepared two features for multiple lag periods: the log of the ratio of the current value to the average during the period, and the log of the ratio of the current value to the value a certain period ago. For these, I took the average for all currencies (Due to missing data, no weighted averaging was performed). In addition, the difference between each currency and the average of all currencies was also prepared as a feature. As a result, this feature seems to have worked well. Dealing with time limit The most difficult part for me was the time limit. When inference is performed, it should be sufficient to generate only one row of features, but my programming skills did not allow me to do this well, so I gave up on this and generated features for all data. So, I tried to avoid using pandas as much as possible to speed up the process, and managed to finish the inference within 9 hours. To be honest, I was quite worried about the timeout at the final update. What I would have worked on if I had more time Speed up feature generation to save time and perform ensemble. Learning with less missing data (external data?) Parameter optimization The first successful submission was a week before the end of the competition, so I have not been able to optimize much. I would like to continue to improve so that I can achieve results in competitions where the element of luck is small. See you at the next competition! Please sign in to reply to this topic. comment 27 Comments 2 appreciation  comments Hotness theDoors Posted 3 years ago arrow_drop_up 2 more_vert In your feature engineering code in training, can you explain the following line of code - df[f'log_close/mean_{lag} id{id}'] = np.log( np.array(df[f'Close {id}']) /  np.roll(np.append(np.convolve( np.array(df[f'Close_{id}']), np.ones(lag)/lag, mode=\"valid\"), np.ones(lag-1)), lag-1)  ) Thanks for your wonderful solution! sugghi Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 4 more_vert Thanks for your question. I used such confusing code to avoid using pandas to accelerate inference. The original and simpler code was as follows; df[f'log_close/mean_{lag}_id{id}'] = np.log( df[f'Close_{id}'] / df[f'Close_{id}'].rolling(lag).mean() ) Volgoesupndown Posted a year ago arrow_drop_up 1 more_vert Stumbled upon this, doesn't your numpy version introduce a bug? roll != shift, as roll will not introduce NaNs but instead roll values from the end of the array to the front. This comment has been deleted. Thomas Tellier Posted 6 months ago arrow_drop_up 0 more_vert Yes, the first |lag| values will use the last |lag| values 🫤 James Cann Posted 3 years ago · 1809th in this Competition arrow_drop_up 4 more_vert @sugghi Many congratulations! And thank you for mentioning the local_api - I'm very glad it was useful :) sugghi Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 1 more_vert I greatly appreciate your local_api! Thank you once again! Nathaniel Maddux Posted 3 years ago · 2nd in this Competition arrow_drop_up 2 more_vert I used your API as well, @jagofc , and forgot to mention it. Thank you. James Cann Posted 3 years ago · 1809th in this Competition arrow_drop_up 1 more_vert Haha, you're welcome @nathanrm ! I wonder if Meme Lord Capital did too… :D Carlos Oblitas Villegas Posted 3 years ago arrow_drop_up 1 more_vert Great solution and thanks for sharing the code…!! 🙌🙌 I downloaded, implemented and run it, but I still dont understand it… :( Could someone helpme to study this code or tellme where to begin?? (my email is: oblitasss@hotmail.com) I really read the code and the info of this contest… PosixHouse Posted 3 years ago arrow_drop_up 1 more_vert Congratulations on your achievement, and many thanks for sharing your work! Arthur Hsu Posted 3 years ago arrow_drop_up 1 more_vert Nice share! Niek van der Zwaag Posted 3 years ago arrow_drop_up 1 more_vert @sugghi Congratulations on 3rd! 🎉 Thanks for sharing your work and approach; very insightful. sugghi Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thank you for celebrating! Aditya sinha Posted 3 years ago arrow_drop_up 1 more_vert very good one sugghi Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 0 more_vert Thanks for your comment! Jean-Michel Nairac Posted 3 years ago · 1538th in this Competition arrow_drop_up 1 more_vert Thank you for the mention :) sugghi Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 2 more_vert I have greatly benefited from your code. Thank you! Never$ Posted 3 years ago · 17th in this Competition arrow_drop_up 1 more_vert Congratulations on winning the gold medal 🎉💯 Also, thanks for sharing the training code with us. I' ll learn about your model. I look forward to seeing you again at the finance competition 😃 sugghi Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thank you for celebrating with me! I look forward to competing in another competition! Lachlan Gillian Posted 3 years ago arrow_drop_up 1 more_vert @sugghi thanks for this post! I learned some valuable things reading through the feature engineering section :) sugghi Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 0 more_vert Thanks for your comment! I am glad you found it useful :) Viji Posted 3 years ago · 1627th in this Competition arrow_drop_up 2 more_vert Congratulations @sugghi…very well done! Thanks for sharing the notebooks. Hope to learn some feature engineering aspects. Wish you the best for other competitions. sugghi Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 0 more_vert Thanks for your message! I am glad you found it useful! Lucas Morin Posted 3 years ago · 797th in this Competition arrow_drop_up 2 more_vert Yay for Feature Engineering and lgbm ! Also many thanks for sharing the whole code :) sugghi Topic Author Posted 3 years ago · 3rd in this Competition arrow_drop_up 0 more_vert Thanks for your comment! I hope my code is helpful :) This comment has been deleted. This comment has been deleted. Appreciation (2) yuanzhe zhou Posted 3 years ago · 204th in this Competition arrow_drop_up 1 more_vert thanks for sharing! julinChou Posted 2 years ago arrow_drop_up 0 more_vert thanks a lot for sharing your code, Too many requests error Too many requests",
      "G-Research Crypto Forecasting  | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. G-Research · Featured Code Competition · 3 years ago Late Submission more_horiz G-Research Crypto Forecasting Use your ML expertise to predict real crypto market data G-Research Crypto Forecasting Overview Data Code Models Discussion Leaderboard Rules Patrick Yam · 7th in this Competition  · Posted 3 years ago arrow_drop_up 73 more_vert 7th place solution First of all, thank you G-Research and Kaggle for hosting the competition. I really like the data provided in this competition, actual market data give us more room for creative method development and is much more interesting compared with anonymized tabular data. My approach is 99% on modeling and the only feature I added is the time of day. Here is a very brief description of my model, the final submission is an ensemble of 4 models trained with different sequence lengths. 3 Please sign in to reply to this topic. comment 29 Comments 2 appreciation  comments Hotness Vitor Posted 3 months ago arrow_drop_up 1 more_vert Thanks for sharing this great solution @wimwim . I was wondering how many training samples in total did you have for the (90, 14, 9) tensors. Did you consider the whole span of years (2018-2021) or only the most recent period? MarkFromItaly Posted 3 years ago · 245th in this Competition arrow_drop_up 3 more_vert Congratulations! I am really amazed when I see solutions without heavy feature engineering that use well designed deep learning models. Thank you for sharing your solution. Nathaniel Maddux Posted 3 years ago · 2nd in this Competition arrow_drop_up 4 more_vert Wow, that's so different from what I did, really cool. Patrick Yam Topic Author Posted 3 years ago · 7th in this Competition arrow_drop_up 2 more_vert Congrats on 2nd place! katsu1110 Posted 3 years ago · 484th in this Competition arrow_drop_up 1 more_vert I came across your solution and found it wonderful. Congrats to you! I'd like to ask, how did your model normalize input? Patrick Yam Topic Author Posted 3 years ago · 7th in this Competition arrow_drop_up 4 more_vert Thanks for the question! The scale for features is very different between different assets and different timestamps, I decided to normalize input using only the data from the input sequence with the same asset (90 data points for each feature) to compute the mean and std. yuanzhe zhou Posted 3 years ago · 204th in this Competition arrow_drop_up 1 more_vert did the author delete some content? Patrick Yam Topic Author Posted 3 years ago · 7th in this Competition arrow_drop_up 1 more_vert No, I didn't delete any content. If you can't see the picture, it could be because Imgur is blocked by your internet provider. yuanzhe zhou Posted 3 years ago · 204th in this Competition arrow_drop_up 0 more_vert I see, thanks. Peppa Pig Posted 3 years ago · 1649th in this Competition arrow_drop_up 1 more_vert Wait, what? No added features and just solving the problem with modeling? If such things are really possible I get back to learning ;) Chirag Desai Posted 3 years ago arrow_drop_up 1 more_vert Great..🙌 lot to learn for beginner like me..Thanks for sharing this.. please share code also when you can. JM Posted 3 years ago arrow_drop_up 1 more_vert Great result for no extra features Milad A Shani Posted 3 years ago arrow_drop_up 2 more_vert congratulations and thanks for sharing the content. Is it available to access the code? Satoshi Datamoto Posted 3 years ago arrow_drop_up 2 more_vert 1stals- Thank you for the great writeup! I'm glad you enjoyed the data - I think it's some of the most interesting we've had on Kaggle in a while. It sounds like you have a really strong understanding of the data and how to model it. Do you have any advice for other Kagglers who are just starting out? Thanks again!\" This is a great writeup, thanks for sharing! Your approach sounds very robust and I'm sure it was a lot of work. Do you have any advice for other Kagglers who are just starting out? Patrick Yam Topic Author Posted 3 years ago · 7th in this Competition arrow_drop_up 7 more_vert Thanks for your question! I always start by checking the data shape (i.e. is the data tabular? 1d? or 2d?), then pick some suitable neural network layers (e.g. LSTM/ Conv/ MHA) for it. I would say Transformer is a very strong candidate for data >= 1D, definitely give it a try. I will perform feature engineering only when the model cannot 'see' the information from the input. For example, Transformer can't handle input with a very long sequence length, so we will have to create features to capture long-term information. KhanhVD Posted 3 years ago arrow_drop_up 2 more_vert Congrats on solo gold and prize my friend @wimwim Patrick Yam Topic Author Posted 3 years ago · 7th in this Competition arrow_drop_up 0 more_vert Thanks for your kind words, my friend! HKMLGroup Posted 3 years ago arrow_drop_up 0 more_vert Congrats Patrick! mavillan Posted 3 years ago arrow_drop_up 0 more_vert Congrats for your robust solution @wimwim ! Can you comment a little bit about the CV strategy you used? Patrick Yam Topic Author Posted 3 years ago · 7th in this Competition arrow_drop_up 2 more_vert I use 3-fold Grouped CV and the key is timestamps. devcon14 Posted 3 years ago · 1532nd in this Competition arrow_drop_up 0 more_vert Thank you for sharing your approach. Wish I had the same understanding of neural net models! What kind of preprocessing did you use on price features or was it as simple as MinMaxScaler on train? Patrick Yam Topic Author Posted 3 years ago · 7th in this Competition arrow_drop_up 3 more_vert Thanks for the question! I use standardization for input scaling. I find that the features scaling could be very different between different assets (e.g. ETH vs BTC) and different timestamps (2020 vs 2021), so I calculate the mean and std for each asset and only use the data from the input sequence (90 data points) to make the model more stable and robust. LG Posted 3 years ago · 1557th in this Competition arrow_drop_up 0 more_vert Ingenious! Congratulations on your score and 7th place! This comment has been deleted. This comment has been deleted. This comment has been deleted. This comment has been deleted. Appreciation (2) Ilya Posted 8 months ago arrow_drop_up 0 more_vert Great work. thank you for sharing bturan19 Posted 3 years ago · 9th in this Competition arrow_drop_up 0 more_vert Great solution, thanks for sharing Too many requests error Too many requests",
      "G-Research Crypto Forecasting  | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. G-Research · Featured Code Competition · 3 years ago Late Submission more_horiz G-Research Crypto Forecasting Use your ML expertise to predict real crypto market data G-Research Crypto Forecasting Overview Data Code Models Discussion Leaderboard Rules bturan19 · 9th in this Competition  · Posted 3 years ago arrow_drop_up 22 more_vert 9th Place Solution - Inference Hello everyone, Firstly, I want to thank you everyone in this competition, it was a great journey for me and I'm very happy to win a gold medal for the first time. I actually shared my inference notebook long ago. Now I want to give some information about it.. My most precious feature was Hull moving average: def moving_average (a, n= 3 ) :\n    ret = np. cumsum (a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return np. append (np. array ([ 1 ]*n), ret[n - 1 :] / n)[ 1 :]\n\n#@ jit (nopython=True)\ndef calcHullMA_inference (series, N= 50 ):\n    SMA1 = moving_average (series, N)\n    SMA2 = moving_average (series, int (N/ 2 ))\n    res = ( 2 * SMA2 - SMA1)\n    return np. mean (res[ -int (np. sqrt (N)):])\n\nrow[ \"hull\" ] = last_close - calcHullMA_inference (f[asset][ \"all_close\" ][- 260 :], 240 ) content_copy Other important note about creating lag feature, is the window size. I choose my windows with a Fibonacci sequence: fibo_list = [55, 210, 340, 890, 3750]. You can see that in notebook. Last important explanation about my feature is using lag target. Firstly I tried to use the official calculation but then I just gave up and use this: row[\"target_return\"] = (last_close / f[asset][\"all_close\"][-16]) -1 and for a market indicator, I collected new \"target_return\" for every batch, and get the average of last available list. Last important note for my work is models. I have 3 different LightGBM models that were trained for different market conditions. Up market, down market and relatively more stable market. Then I get the average of them. I thought about how could I improve my models. First thing is: I could do parameter optimization, I almost used default parameters. Then I could add different kind of models, I thing my submission time was around 6 hours. lastly, I had actually more feature in my mind. Well, thanks to everyone again.. See you in next competition. Please sign in to reply to this topic. comment 5 Comments Hotness 853rising Posted 3 years ago · 1499th in this Competition arrow_drop_up 1 more_vert Congratulations! Your notebook is super helpful for me to learn how to create the lagged features in the real testing period when using the api, thank you so much! bturan19 Topic Author Posted 3 years ago · 9th in this Competition arrow_drop_up 0 more_vert I'm glad if it helps Satoshi Datamoto Posted 3 years ago arrow_drop_up 1 more_vert Well done on your 9th place finish! This is a great solution that makes use of some innovative features. I'm definitely going to have to try out the Hull moving average feature in my own models. Lachlan Gillian Posted 3 years ago arrow_drop_up 0 more_vert @bturan19 congratulations on finishing in 9th place! What led you to choose your windows with a Fibonacci sequence? bturan19 Topic Author Posted 3 years ago · 9th in this Competition arrow_drop_up 1 more_vert Actually it was something like a domain knowledge. Now, I'm not a trading expert or something but I know that, out there, people use this sequence very heavily, so, when I decide window sizes, I thought it would be better to use them instead of other random values. Too many requests error Too many requests",
      "G-Research Crypto Forecasting  | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. G-Research · Featured Code Competition · 3 years ago Late Submission more_horiz G-Research Crypto Forecasting Use your ML expertise to predict real crypto market data G-Research Crypto Forecasting Overview Data Code Models Discussion Leaderboard Rules Tom Forbes · 13th in this Competition  · Posted 3 years ago arrow_drop_up 90 more_vert 13th Place (Final) 1st Place (6 Weeks In) Final Solution View the final submission notebook here. Final Update - Congratulations to all the winners! While this comp was a bit of a rollercoaster, I'm very pleased to get my first competition Gold. Disappointing to just miss out on the top 10 after being there so long, but thats always possible with this type of challenge. Original post below. Initial Comments First of all thankyou to G-Research and Cambridge Spark for hosting this competition and to the Kaggle staff for making everything run relatively smoothly. I found it so interesting and more open ended than other comps I've competed in, in my short Kaggler career. It goes without saying that while I'm in 1st Place now there is no guarantee it will stay that way in the next 6 weeks, but I'm confident I'll be there or thereabouts. Hopefully you find this solution useful regardless of the final outcome. Elephant in the room - there has been a lot of concern about the validity of the final LB results and requests to release the final test data after the competition ends so that competitors can validate their final scores. I think this would be a good idea and should put any concerns to bed. If it's possible & not too much effort for Kaggle to accommodate the request please consider it. However, as far as im concerned my current score seems reasonable based on all the scores I achieved in testing. I have no reason to believe any of the LB scores are incorrect until there is evidence to the contrary. I would encourage those who feel their score is unexpected share their solutions as the Kaggle community might be able to debug any issues/misunderstandings. It is obviously difficult to accurately predict crypto prices, but it's possible to find some signal - hopefully this notebook will shed some light on how I got mine. Disclaimers this was not my exact final submission but very similar I have removed the dataset containing my pretrained models and scalers - so some commands will error Summary 17 features with lagged and timestamp averages for all models. Ensembles of LGBM and Keras NN models. Target Engineering and prediction switching. Some training on extra cryptos outside competition scope. Test Scores (no lookahead bias): 0.0565 best score on original testing period (Jun 13 2021 - Sep 21 2021) 0.0475 best score on supplemental_train.csv update. (Sep 21 2021 - Jan 24 2022) 0.0465 best single model score (Jun 13 2021 - Sep 21 2021) External Data I downloaded free public data from multiple exchange apis (Binance, FTX, Coinbase, Kucoin… etc.) to see if this extra data would improve my models. Some of my final models were trained on extra data. Specifically, using some currencies not included in the competition (e.g. XRP, ZEC…) from Binance seemed to provide a consistent small improvement over multiple timeframes. Feature Engineering I settled on a group of 17 features for all models. There are 8 lagged features - a simple mixture of EMA's, historical returns and historical volatility over various lookback periods - and these features were averaged across timestamps to produce 8 more. Asset_ID was also included. It was tricky to find a group that performed the best consistently across different time periods, adding and removing features from this set seemed to reduce performance. Im sure this isn't the optimal feature set, but this group seemed to work well enough. It was also important to perform some kind on binning on the features, especially for training the LGBM model. The commonly used reduce_mem_usage function and some rounding functions seemed to provide a suitable amount of bins. I found binning to 500-1000 unique values worked well for any given continuous feature. Target Engineering I think a crucial part of this competition was manipulating the target. Thanks to the work of @alexfir and other Kagglers we found out how the target was being calculated . I thought it would be useful to split the target into two components: The forward 15 minute return of an asset The beta component, where we calculate the mean of forward 15 minute returns for all assets and incorporate past 15 minute returns for the previous 3750 timestamps As @gengdaiziwang illustrates in his helpful notebook , we can see that in the case where an observation is missing for a given asset, the beta component is automatically set to 0. (opens in a new tab)\"> But this makes it a completely different target! A model trained on the target given as standard will have to deal with training on a target that randomly switches between a 15 minute forward return for one asset vs what is essentially a 15 minute forward return relative to the other assets - very different things. The approach I took was to create recreate two targets to represent these two different cases. Then I could train separate models on these recreated targets which would learn the two cases much more effectively. See an implementation of this target engineering . Now, having two models trained on different targets, I can alter my predictions to match the target produced by the api. The api provides all the information i need to figure out whether the beta component will be == 0 because if an asset is missing from an iteration i know that for the next 3750 iterations the beta component == 0 for this asset. Therefore my next 3750 predictions will come from a model that was training on a target constructued without a beta component. This target engineering and dual model method added roughly 0.01 to my score on the original test period, although this did vary quite significantly for other testing periods. To summarise as simply as possible: TargetZero = A target based on 15 minute Forward Return only TargetBeta = TargetZero + Beta Component ModelZero = Model(s) trained on TargetZero ModelBeta = Model(s) trained on TargetBeta (opens in a new tab)\"> Models I found LightGBM worked well and was easy to experiment with, i used the weighted correlation evaluation metric and fairly out of the box hyperparameters, tuning didnt seem to add much and wasn't consistent across time periods. I also used a Keras NN heavily influenced by @lucasmorin's excellent notebook . This just worked really well for me out of the box and i couldnt find many ways to improve the architecture or hyperparameters. Ensembling these two models worked well and added an extra 0.005 to my score (roughly) although the improvement varies in different market regimes. I also experimented with ensembles using catboost and XGboost but the predictions from most gradient boosting models were too highly correlated to provide much ensembling benefit. I found the diversity in predictions between NN and GB were a good match. I used a fairly simple CV method for training and testing. My folds were based on timestamp values similar to the below: Training Fold 0: 1514764860 - 1570000000 Training Fold 1: 1514764860 - 1580000000 Training Fold 2: 1514764860 - 1590000000 Training Fold 3: 1514764860 - 1600000000 Training Fold 4: 1514764860 - 1610000000 Validation Fold 0: 1570000000 - 1580000000 Validation Fold 1: 1580000000 - 1590000000 Validation Fold 2: 1590000000 - 1600000000 Validation Fold 3: 1600000000 - 1610000000 Validation Fold 4: 1610000000 - 1620000000 Test Fold A: 1615000000 - 1623542400 Test Fold B (Supplemental Train Update): 1623542400 - 1643000000 Since this public lb was useless in this competition, it was essential to construct a robust CV framework to have a reliable benchmark for making improvements to models. It was also important to have test folds in several different market regimes to avoid overfitting to one period. My best single model was a NN trained on fold 0 data only (surprisingly). I was fairly selective on picking models to include in the final submission as some folds just trained much better than others and had more consistent outperformance on unseen data. Submission This is almost the exact notebook I submitted for my final predictions. I used lists to store historical data for feature calculations and did most calculations using numpy or lists, avoiding pandas at all costs. The submission completes in roughly 7 hours. What didnt work Layering extra data from different exchanges for the same asset. E.g training on BTC data from Binance + Coinbase + FTX + G-Research version. Didnt harm but didnt improve models significantly either. Framing as a classification problem - poor results Hyperparameter tuning LGBM PCA features Using all 14 assets features per row / predicting on 14 targets. Meta modelling GB and NN predictions With more time id try Extra Feature engineering Pytorch implementation of keras NN for ensembling LSTM Other model architectures Please sign in to reply to this topic. comment 24 Comments 2 appreciation  comments Hotness James Cann Posted 3 years ago · 1809th in this Competition arrow_drop_up 3 more_vert Respect. The target switching approach is very neat! Lucas Morin Posted 3 years ago · 797th in this Competition arrow_drop_up 3 more_vert Nice solution :-) Congrats on pulling the hard data sourcing / target engineering work. So in the end you only use very few features ? Did you keep the noise from my nb ? (btw thanks for citing my work - hope to get gold) Tom Forbes Topic Author Posted 3 years ago · 13th in this Competition arrow_drop_up 3 more_vert Thanks! Yeah only 17 features for all models. Yes for some models i kept the noise and some seemed to train better without. It certainly deserves a gold! Indu Posted 3 years ago arrow_drop_up 1 more_vert Great work. Congrats. Manuel Campos Posted 3 years ago · 45th in this Competition arrow_drop_up 1 more_vert Great Job @tomforbes , I hope you have good luck in the rest of the competition. I have a question, what was the longest period of time that your models worked on the validation? …10000000 from timestamps? Tom Forbes Topic Author Posted 3 years ago · 13th in this Competition arrow_drop_up 1 more_vert Thanks Manuel! Yeah i used roughly 10000000 timestamp periods.  I found using larger validation periods didn't make too much difference to training but i preferred to keep a smaller validation set - so I could keep my test sets large. JonnydosSantos Posted 3 years ago · 22nd in this Competition arrow_drop_up 1 more_vert Great work Tom Forbes!!  Looks like you might just be deserving of first place after all :). Kyle Peters Posted 3 years ago · 59th in this Competition arrow_drop_up 1 more_vert Great job Tom.  I just has a couple questions about the time period of the training set you used?  What time period did you use to train your final models on?  Did it include the supplemental training set.  Did it vary across the models in your ensemble? Tom Forbes Topic Author Posted 3 years ago · 13th in this Competition arrow_drop_up 1 more_vert Thanks Kyle! I have outlined some of the training and validation folds i used in this post. For example my best model was a NN trained on data from 1514764860 - 1570000000 (timestamp). I choose whichever folds performed best overall on my testing folds. I didn't use any supplemental_train data for training, only testing. I didn't train any models that included data after ~1610000000 Kyle Peters Posted 3 years ago · 59th in this Competition arrow_drop_up 0 more_vert That makes sense.  I saw that but didn't know if that was just used for cv or if it was also used in the final model. Justin Krieger001 Posted 3 years ago arrow_drop_up 2 more_vert Great writeup! miguel perez Posted 3 years ago · 856th in this Competition arrow_drop_up 2 more_vert Looks great @tomforbes , thanks for sharing that. I find very interesting that you have managed to make your approach robust to data quality which was a huge condition for label generation. I have given already clearly enough my opinion that the statistical nature of the labels at test time is very different to the previous one, and not because of market forces. And I still consider that huge label quality shift at test time isn't  legit for a Kaggle comp, but that doesn't take away the merit that your solution is cleverly prepared to deal with it, so congratulations for that. As for myself, I have spent way too much energy in this competition, especially addressing the \"elephant in the room\" issue but I am goint to spend no more time on it. Every competition teaches you something and this one has not been an exception, hopefully I will choose a better way to  invest my time next time. Again, thanks for sharing and good luck! Kirderf Posted 3 years ago · 37th in this Competition arrow_drop_up 2 more_vert Good work! Same here, majority of the time feature engineering, very little modeling in contrast to other competitions. Will be interesting weeks ahead following the progress :) marketneutral Posted 3 years ago · 1310th in this Competition arrow_drop_up 2 more_vert Great write up. Thank you for sharing. I am surprised you were able to fit all the feature engineering and ensembles inside the predict loop… I kept running into time limits when trying similar. As for me, after three runs…56 -> 84 -> 164 … 😬 Marco Gorelli Posted 3 years ago · 348th in this Competition arrow_drop_up 1 more_vert It goes without saying that while I'm in 1st Place now there is no guarantee it will stay that way in the next 6 weeks, but I'm confident I'll be there or thereabouts This aged well - congrats! JM Posted 3 years ago arrow_drop_up 1 more_vert Can you post a link to where you got data from those exchanges in the same format as competition? I think you are jumping the gun a bit on LB placings, but nevertheless I think you have an interesting solution, I did not consider the target engineering for mine I think that was a good idea for sure. Well done on the 6 week performance and good luck for the rest! 🙂 Tom Forbes Topic Author Posted 3 years ago · 13th in this Competition arrow_drop_up 2 more_vert I was only interested in OHLC data so didn't need to get the more hard to source features like VWAP. This is pretty easy to find if you google most exchanges have public api docs. E.g. binance Yeah as i said its 1st place 6 weeks in - I could slip a few of places by the end but thought i'd share what i'd written up Liletta Posted 3 years ago arrow_drop_up 0 more_vert Nice work. Congrats. claudiu Posted 3 years ago · 174th in this Competition arrow_drop_up 0 more_vert To take into account the beta=0 cases I just added the approximated beta calculated with the existing history as a feature to the model and hoped the model will learn from this feature. In this way I did not have to split the models for the two cases. Would have been interesting actually to check if separate models improve. Tom Forbes Topic Author Posted 3 years ago · 13th in this Competition arrow_drop_up 1 more_vert Yeah i tried using beta as a feature but i didn't get a lot out of it. Think the benefit of splitting out the models is the target is consistent and allows for better training Kyle Peters Posted 3 years ago · 59th in this Competition arrow_drop_up 0 more_vert Also, maybe the reason there are discrepancies in other people's models and not your models is because of a heightened amount of NA's in the data.  This would cause more betas to be zero which would throw off other people's models but not yours due to the feature extraction.  I noticed an uptick in NA's from the public lb to supplemental dataset but only a slight one. Tom Forbes Topic Author Posted 3 years ago · 13th in this Competition arrow_drop_up 0 more_vert Maybe, but then I imagine most people trained their models on the target as originally constructed which had plenty of beta == zero, so whatever results they got in testing should probably still apply unless there is an unusual amount of missing data. It seemed to be the trend that the most recent data had the least missing data Appreciation (2) Ishan Mehta115 Posted 3 years ago arrow_drop_up 1 more_vert This is great. Thanks for sharing gectrade Posted 3 years ago arrow_drop_up 0 more_vert Thanks for your contribution Too many requests error Too many requests",
      "G-Research Crypto Forecasting  | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. G-Research · Featured Code Competition · 3 years ago Late Submission more_horiz G-Research Crypto Forecasting Use your ML expertise to predict real crypto market data G-Research Crypto Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "G-Research Crypto Forecasting  | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. G-Research · Featured Code Competition · 3 years ago Late Submission more_horiz G-Research Crypto Forecasting Use your ML expertise to predict real crypto market data G-Research Crypto Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "G-Research Crypto Forecasting  | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. G-Research · Featured Code Competition · 3 years ago Late Submission more_horiz G-Research Crypto Forecasting Use your ML expertise to predict real crypto market data G-Research Crypto Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Use your ML expertise to predict real crypto market data This dataset contains information on historic trades for several cryptoassets, such as Bitcoin and Ethereum. Your challenge is to predict their future returns. As historic cryptocurrency prices are not confidential this will be a forecasting competition using the time series API. Furthermore the public leaderboard targets are publicly available and are provided as part of the competition dataset. Expect to see many people submitting perfect submissions for fun. Accordingly, the active phase public leaderboard for this competition was not meaningful and was only provided as a convenience for anyone who wants to test their code. The forecasting phase public leaderboard and final private leaderboard will be determined using real market data gathered after the submission period closes. train.csv - The training set example_test.csv - An example of the data that will be delivered by the time series API. example_sample_submission.csv - An example of the data that will be delivered by the time series API. The data is just copied from train.csv . asset_details.csv - Provides the real name and of the cryptoasset for each Asset_ID and the weight each cryptoasset receives in the metric. gresearch_crypto - An unoptimized version of the time series API files for offline work. You may need Python 3.7 and a Linux environment to run it without errors. supplemental_train.csv - After the submission period is over this file's data will be replaced with cryptoasset prices from the entire submission period. The current copy has been updated from the original and covers roughly 2/3 of the submission period. Refer to the time series introduction notebook for an example of how to complete a submission. The time-series API has changed somewhat from previous competitions! Expect to see roughly three months worth of data in the test set. Until the forecasting phase of the competition, the API will just deliver a slice of the training data. The API will require 0.5 GB of memory after initialization. The initialization step ( env.iter_test() ) will require meaningfully more memory than that; we recommend you do not load your model until after making that call. The API will also consume less than 30 minutes of runtime for loading and serving the data. The API loads the data using the following types: Asset_ID: int8, Count: int32, row_id: int32, Count: int32, Open: float64, High: float64, Low: float64, Close: float64, Volume: float64, VWAP: float64 7 files 3.12 GB csv, so, py Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 3.12 GB gresearch_crypto asset_details.csv example_sample_submission.csv example_test.csv supplemental_train.csv train.csv 7 files 37 columns ",
    "data_description": "G-Research Crypto Forecasting  | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. G-Research · Featured Code Competition · 3 years ago Late Submission more_horiz G-Research Crypto Forecasting Use your ML expertise to predict real crypto market data G-Research Crypto Forecasting Overview Data Code Models Discussion Leaderboard Rules Overview Start Nov 2, 2021 Close May 4, 2022 Merger & Entry Description link keyboard_arrow_up Over $40 billion worth of cryptocurrencies are traded every day. They are among the most popular assets for speculation and investment, yet have proven wildly volatile. Fast-fluctuating prices have made millionaires of a lucky few, and delivered crushing losses to others. Could some of these price movements have been predicted in advance? In this competition, you'll use your machine learning expertise to forecast short term returns in 14 popular cryptocurrencies. We have amassed a dataset of millions of rows of high-frequency market data dating back to 2018 which you can use to build your model.   Once the submission deadline has passed, your final score will be calculated over the following 3 months using live crypto data as it is collected. The simultaneous activity of thousands of traders ensures that most signals will be transitory, persistent alpha will be exceptionally difficult to find, and the danger of overfitting will be considerable. In addition, since 2018, interest in the cryptomarket has exploded, so the volatility and correlation structure in our data are likely to be highly non-stationary. The successful contestant will pay careful attention to these considerations, and in the process gain valuable insight into the art and science of financial forecasting. G-Research is Europe’s leading quantitative finance research firm. We have long explored the extent of market prediction possibilities, making use of machine learning, big data, and some of the most advanced technology available. Specializing in data science and AI education for workforces, Cambridge Spark is partnering with G-Research for this competition. Watch our introduction to the competition below: (opens in a new tab)\"> This is a Code Competition. Refer to Code Requirements for details. Evaluation link keyboard_arrow_up Submissions are evaluated on a weighted version of the Pearson correlation coefficient . You can find additional details in the 'Prediction Details and Evaluation' section of this tutorial notebook . You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow this template in Kaggle Notebooks: import gresearch_crypto env = gresearch_crypto.make_env() # initialize the environment iter_test = env .iter_test() # an iterator which loops over the test set and sample submission for (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df[ 'Target' ] = 0 # make your predictions here env .predict(sample_prediction_df) # register your predictions content_copy A more detailed introduction to the API is available here . You will get an error if you submission includes nulls or infinities. Timeline link keyboard_arrow_up This is a forecasting competition with an active training phase and a second period where models will be run against real market data. Training Timeline November 2, 2021 - Start Date January 25, 2022 - Entry deadline. You must accept the competition rules before this date in order to compete. January 25, 2022 - Team Merger deadline. This is the last day participants may join or merge teams. February 1, 2022 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Forecasting Timeline: Starting after the final submission deadline there will be periodic updates to the leaderboard to reflect market data updates that will be run against selected notebooks. Updates will take place roughly every two weeks. May 3, 2022 - Competition End Date - Winner's announcement Prizes link keyboard_arrow_up 1st Place - $50,000 2nd Place - $20,000 3rd Place - $15,000 4th Place - $10,000 5th Place - $5,000 6th Place - $5,000 7th Place - $5,000 8th Place - $5,000 9th Place - $5,000 10th Place - $5,000 Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time Internet access disabled Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv Please see the Code Competition FAQ for more information on how to submit. Review the code debugging doc if you are encountering submission errors. Citation link keyboard_arrow_up Alessandro Ticchi, Andrew Scherer, Carla McIntyre, Carlos Stein N Brito, Derek Snow, Develra, dstern, James Colless, Kieran Garvey, Maggie, Maria Perez Ortiz, Ryan Lynch, and Sohier Dane. G-Research Crypto Forecasting . https://kaggle.com/competitions/g-research-crypto-forecasting, 2021. Kaggle. Cite Competition Host G-Research Prizes & Awards $125,000 Awards Points & Medals Participation 23,216 Entrants 2,398 Participants 1,946 Teams 3,141 Submissions Tags Tabular Finance Time Series Analysis Custom Metric Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Citation Too many requests error Too many requests"
  },
  {
    "competition_slug": "optiver-realized-volatility-prediction",
    "discussion_links": [
      "/competitions/optiver-realized-volatility-prediction/discussion/274970",
      "/competitions/optiver-realized-volatility-prediction/discussion/278676",
      "/competitions/optiver-realized-volatility-prediction/discussion/276506",
      "/competitions/optiver-realized-volatility-prediction/discussion/275169",
      "/competitions/optiver-realized-volatility-prediction/discussion/300348",
      "/competitions/optiver-realized-volatility-prediction/discussion/276137"
    ],
    "discussion_texts": [
      "Optiver Realized Volatility Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Optiver · Featured Code Competition · 3 years ago Late Submission more_horiz Optiver Realized Volatility Prediction Apply your data science skills to make financial markets better Optiver Realized Volatility Prediction Overview Data Code Models Discussion Leaderboard Rules nyanp · 1st in this Competition  · Posted 4 years ago arrow_drop_up 471 more_vert 1st Place Solution - Nearest Neighbors 2022-01-11 Updated the title. First of all, I would like to say a big thank you to Optiver and Kaggle for organizing a very interesting competition. I have never analyzed financial data before, but thanks to the very helpful tutorial notebook, I was able to fully immerse myself in the competition. I don't know where I rank on the final leaderboard, but the points of my public 2nd place solution are as follows: Time-series cross-validation by reverse engineering of time-id order Nearest neighbor aggregation features (boost from 0.21 to 0.19) Blend of LightGBM, MLP, and MoA’s 1D-CNN You can also see my notebook here: https://www.kaggle.com/nyanpn/public-2nd-place-solution Now, I'd like to introduce the detailed solution. Reverse engineering of time-id order The prices in the competition data are normalized, but as someone pointed out in the discussion, you can use \"tick size\" to recover the real prices before normalization. Furthermore, by compressing the time-id x stock-id price matrix to one dimension using t-SNE, I was able to recover the order of the time-id with sufficient accuracy. import glob import numpy as np import pandas as pd from joblib import Parallel, delayed from sklearn.manifold import TSNE from sklearn.preprocessing import minmax_scale def calc_price_from_tick ( df ):\n    tick = sorted (np.diff( sorted (np.unique(df.values.flatten()))))[ 0 ] return 0.01 / tick def calc_prices ( r ):\n    df = pd.read_parquet(r.book_path,\n                         columns=[ 'time_id' , 'ask_price1' , 'ask_price2' , 'bid_price1' , 'bid_price2' ])\n    df = df.groupby( 'time_id' ) \\\n        .apply(calc_price_from_tick).to_frame( 'price' ).reset_index()\n    df[ 'stock_id' ] = r.stock_id return df def reconstruct_time_id_order ():\n    paths = glob.glob( '/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/**/*.parquet' )\n\n    df_files = pd.DataFrame(\n        { 'book_path' : paths}) \\\n        . eval ( 'stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")' ,\n              engine= 'python' ) # build price matrix using tick-size df_prices = pd.concat(\n        Parallel(n_jobs= 4 )(\n            delayed(calc_prices)(r) for _, r in df_files.iterrows()\n        )\n    )\n    df_prices = df_prices.pivot( 'time_id' , 'stock_id' , 'price' ) # t-SNE to recovering time-id order clf = TSNE(\n        n_components= 1 ,\n        perplexity= 400 ,\n        random_state= 0 ,\n        n_iter= 2000 )\n    compressed = clf.fit_transform(\n        pd.DataFrame(minmax_scale(df_prices.fillna(df_prices.mean())))\n    )\n\n    order = np.argsort(compressed[:, 0 ])\n    ordered = df_prices.reindex(order).reset_index(drop= True ) # correct direction of time-id order using known stock (id61 = AMZN) if ordered[ 61 ].iloc[ 0 ] > ordered[ 61 ].iloc[- 1 ]:\n        ordered = ordered.reindex(ordered.index[::- 1 ])\\\n            .reset_index(drop= True ) return ordered[[ 'time_id' ]] content_copy The correctness of the recovery of the time-id order for the training data can be easily verified by comparing it with the real market data. By doing so, we know that the training data is for the period between 2020/1/1~2021/3/31. Left: Stock price restored by t-SNE  Right: Actual stock price retrieved by yfinance (2020-01-01~2021-03-31) On the other hand, for the test data, I did not use the time-id information directly in the features and models because there is no guarantee that t-SNE will be able to sort the time-id order correctly. However, even if it cannot be used directly for features, the order of the time-id can be used in various ways. Time series cross-validation . Now that we know the correct order of the timestamps, we can construct the validation sets as if they were normal time-series data. Detection of covariate shifts . Using methods such as adversarial validation enables us to find features that change over time. Data exclusion . I didn't do this because it didn't work for me, but we can exclude periods of specific market events from our data, for example, the stock market crash in early 2020. Time series CV was especially important. I used a 4-fold time-series CV, with 10% of data used for validation for each fold. This allowed me to get a good enough (though not perfect) correlation between CV and LB throughout the competition. How is the data being generated? By the way, between 2020/1/1~2021/3/31, the stock market was open for 443 days, and there were 3833 time-ids during that period, which means that about 8.6 time-ids were recorded per day. If Optiver excludes (for some reason, perhaps it's hard to predict) the first and last hour of the time that the stock market is open, then the time from 10:30~15:00 can be used for competition data. If they divided this time period into 30-minute intervals, up to 9 time-ids will be recorded per day. In reality, given the case of circuit breakers and early closing days, the number of time-ids they can use for the data should be a little less than 9 per day. It's just a guess, but I figured this is how the data is generated. Feature engineering In this competition to predict the next 10 minutes of realized volatility (RV) from the previous 10 minutes of data, the most important feature would obviously be the RV of the previous 10 minutes. However, if we consider how the data was generated, just 10 minutes after the target computation period for a time-id, the training data for the next time-id starts. This means that if the order of the time-id can be fully recovered, we can expect the RV at the next time-id to be a very powerful feature. If we generalize further, we can improve the prediction accuracy by using not only the next time-id, but also the information of time-ids that are \"similar\" in some distance metric. For example, the RV of the same stock when the market had similar price, volatility, and trading volume would be useful for predicting the RV at a certain time-id. So, I used NearestNeighbor with various distance metrics to find the similar N time-ids and calculate the average of features like RV and stock size (N=2,3,5,10,20,40). In addition to the time-id, I also calculated the aggregation between similar stock-ids. Furthermore, by combining these ideas, I calculated features such as \"the average tau of 20 similar stocks with similar volatility in 5 closest time-ids\". target_feature = 'book.log_return1.realized_volatility' n_max = 40 # make neighbors pivot = df.pivot( 'time_id' , 'stock_id' , 'price' )\npivot = pivot.fillna(pivot.mean())\npivot = pd.DataFrame(minmax_scale(pivot))\n\nnn = NearestNeighbors( n_neighbors =n_max, p =1)\nnn.fit(pivot)\nneighbors = nn.kneighbors(pivot) # aggregate def make_nn_feature(df, neighbors, f_col, n =5, agg =np.mean, postfix = '' ):\n    pivot_aggs = pd.DataFrame(agg(neighbors[1:n,:,:], axis =0), columns =feature_pivot.columns, index =feature_pivot.index)\n    dst = pivot_aggs.unstack().reset_index()\n    dst.columns = [ 'stock_id' , 'time_id' , f '{f_col}_cluster{n}{postfix}_{agg.__name__}' ]\n    return dst\n\nfeature_pivot = df.pivot( 'time_id' , 'stock_id' , target_feature)\nfeature_pivot = feature_pivot.fillna(feature_pivot.mean())\n\nneighbor_features = np.zeros((n_max, *fea ture_pivot.shape)) for i in range(n):\n    neighbor_features[i, :, :] += feature_pivot.values[neighbors[:, i], :] for n in [2, 3, 5, 10, 20, 40]:\n    dst = make_nn_feature(df, neighbors, feature_pivot, n)\n    df = pd.merge(df, dst, on=[ 'stock_id' , 'time_id' ], how = 'left' ) content_copy I created about 600 features in total, 360 of which were Nearest Neighbor features, and most of my score improvement was based on these NN features. I don't think we can use future information in a real Optiver’s scenario, but the idea of using Nearest Neighbor to aggregate nearby features can probably be used in a real model. Feature processing Now that we know the order of the time-id, we can detect features that are changing over time. By performing adversarial validation, I noticed that the features aggregated from trade.order_count and book.total_volume changed remarkably over time. Therefore, instead of using their raw features, I converted these features into ranks within the same time-id. I also applied np.log1p to features that have large skew, as they may degrade the predictions if large outliers come during the 2nd stage. These have little improvement on the LB scores, but I believe they help to reduce the risk of shakedown in private LB. Modeling I used three simple blends for prediction: LightGBM, 1D-CNN, and MLP. The 1D-CNN is a simplified version of the architecture used in MoA 2nd place solution. (This CNN worked surprisingly well in both the recent MLB and Optiver competition.) 1D-CNN architecture I did not use the pretrained model because I wanted to use the test data for feature calculation, thus all the model training was done within a single notebook. My NN model was a bit unstable in training, so I trained 10 models with different seeds and cherry-picked the top 5 models with the best validation scores for prediction (oops, I forgot to use seed=3047 :) ). What didn’t work a lot of domain-specific features like beta coefficient TabNet (not bad, but too long training time) training NN on residual (target - book.log_return.realized_volatility) dimensionality reduction features What I thought might work, but didn't have time for 300-sec model (split the training data into the first and second halves, and create a model that predicts the RV of the second half from the first half and use it as meta-feature or data augmentation) Ensembles with LSTMs and RNNs that do not create features Auto-encoder 5 Please sign in to reply to this topic. comment 143 Comments 12 appreciation  comments Hotness nyanp Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 17 more_vert I forgot to write something important: the idea that t-SNE can restore the order of time-ids came from this notebook. https://www.kaggle.com/stassl/recovering-time-id-order?scriptVersionId=71499310 Thanks for the great idea! @stassl Stas Sl Posted 4 years ago · 154th in this Competition arrow_drop_up 9 more_vert I also included more details in the latest version of it - maybe will be interesting to compare with yours. I ended up using another dimensionality reduction method, because for some reason 1d TSNE didn't work for me very well. In my turn I'd like to shout out to @nquay3 (he deleted his account for some reason) for discovering the original idea of using tick size on which my notebook is heavily based. 13 more replies arrow_drop_down +3 Stas Sl Posted 4 years ago · 154th in this Competition arrow_drop_up 16 more_vert Nearest neighbor aggregation features You are brave to disclose this first 🙃 yu Posted 4 years ago · 1173rd in this Competition arrow_drop_up 3 more_vert guess you guys have one submission without leakage to hedge the risk 😏 Stas Sl Posted 4 years ago · 154th in this Competition arrow_drop_up 6 more_vert guess you guys have one submission without leakage to hedge the risk 😏 I actually have, though it probably will lost among all these 0.19xxx scores. I had big internal dilemma in last weeks what to work on and what to submit, initially I planned to improve both solutions, but as competition was nearing the end, it became apparent that I can't keep up with both and had to prioritize one over another. It was very tempting to continue improve your best model, so I couldn't resist. 4 more replies arrow_drop_down Jean-Michel Nairac Posted 4 years ago · 85th in this Competition arrow_drop_up 14 more_vert Hi. Thanks for sharing and well done! I would like to ask for further clarification regarding your NN approach if possible. You mentioned that you aggregate features from other time periods that have similar properties. It looks like you are using a future time period, say 12, to create a feature for the time period one hour earlier. On this, Do you not face the \"using the future to predict the past\" risk when doing this? What if in the forecasting phase, time periods are predicted one at a time so that you cannot use other time periods. Or what if there are significant gaps between time periods, so that you cannot establish a suitable distance metric to extract similar properties? If I understand this correctly, you are using the NN approach to predict similar time ids in the test set, which works well when you have batches of test data within a short time period. But if the forecasting requires prediction one at a time in chronological sequence, you might not be able to look ahead at each new time period. Tsai29 Posted 4 years ago · 2181st in this Competition arrow_drop_up 1 more_vert Pretty much the same questions I have after reading the solution. But thanks for sharing this. Nice Job. nyanp Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 12 more_vert @nrcjea001 Thanks for the comment! There are several questions, and very good points, which I will address in turn. Do you not face the \"using the future to predict the past\" risk when doing this? These kinds of features need to be handled with care, but I believe that aggregation using NN is safer than features that use the time-id order directly. What if in the forecasting phase, time periods are predicted one at a time so that you cannot use other time periods. If I understand correctly, this will not be the case since the private test data will be processed in batch. Or what if there are significant gaps between time periods, so that you cannot establish a suitable distance metric to extract similar properties? There will probably be a 6-month gap between train-private tests. I created the same situation between train-validation to make sure that the NN features have no negative impact on the score in that situation. I think the model is robust to such long gaps because short-term NN features below 10-neighbors are dominant in the model. However, I don't know if the test data will also be sampled 9 time-ids/day. A change in this density would certainly have an impact on my model, but based on the forecasting timeline and the number of test data, I assumed that it would not change significantly from the train data. Jean-Michel Nairac Posted 4 years ago · 85th in this Competition arrow_drop_up 1 more_vert ok thanks for clarifying and all the best for the forecasting Damian Posted 4 years ago · 318th in this Competition arrow_drop_up 10 more_vert Good job of backing out the ordering of the time_id's. However, this is useless for a forecasting exercise and is clearly not in the spirit of the contest. It would be interesting to see what the hosts + kaggle do in response to solutions that do this. It is also interesting, that even with knowledge of future time_id, this solution couldn't meet the hosts' expectation of a **good ** model at 0.15 Ming Pan Posted 4 years ago · 2258th in this Competition arrow_drop_up 9 more_vert Actually the possibility was publicly raised to the competition host and using a time series API was suggested to them, which would have blocked the ability to use future time ids and forced realistic solutions. The host explicitly stated they would not create a time series API , and the first reason given was Building a timeseries API for our competition will be time-consuming. So from my perspective, the host already made the decision to allow for winning solutions that would not be useful in real market-making applications. Personally I commend @nyanpn for this solution (and I'll be rooting for him to get GM) and others who exploited time id relationships in the test set (especially @stassl for leaving up his notebook after the original leaker deleted their post and account). Such solutions winning the competition would hopefully illustrate to Kaggle the importance of time series APIs and lead to more realistic competitions in the future. Carlo Lepelaars Posted 4 years ago · 425th in this Competition arrow_drop_up 5 more_vert Awesome solution! Wouldn't have thought of using t-SNE for such a practical purpose to recover the order of time_id. Very curious to see the result of denoising auto-encoder solutions that were so successful in the Jane Street competition. Has anyone of you considered and played around with quarticity (vol of vol) features? We integrated these features in our model, but struggled to get consistent improvements using it: https://dspyt.com/advanced-realized-volatility-and-quarticity nyanp Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 5 more_vert @carlolepelaars I tried features in that article, but unfortunately none of them worked. 4 more replies arrow_drop_down KhanhVD Posted 4 years ago · 1767th in this Competition arrow_drop_up 3 more_vert Thanks for the writeup @nyanpn , Nearest Neighbors features is all you need :) LongYin/杰少 Posted 4 years ago arrow_drop_up 3 more_vert Detection of covariate shifts is valuable in practice.  By the way,  I have tried this, 300-sec model (split the training data into the first and second halves, and create a model that predicts the RV of the second half from the first half and use it as meta-feature or data augmentation) This helps. nyanp Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 0 more_vert @longyin2 That’s fantastic! How did you integrate this 300sec model into your predictions? 3 more replies arrow_drop_down Pavel Shunkevich Posted 4 years ago · 1160th in this Competition arrow_drop_up 1 more_vert Thanks for sharing! That is very interesting solution! Jonathan Mallia Posted 4 years ago · 1922nd in this Competition arrow_drop_up 1 more_vert Thanks a lot @nyanpn for sharing this, it's an outstanding solution. The NN features are the real deal, they have improve my single model from a local CV of 0.180396 to 0.177138. Impressive CV gain! Dimora Posted 4 years ago · 60th in this Competition arrow_drop_up 1 more_vert Very well done. I had a blast reading through your notebook. Pleasantly surprised on what is possible with reverse engineering and data analysis and frightened to see how much I don't know and how much there is to learn. Dimora Posted 4 years ago · 60th in this Competition arrow_drop_up 1 more_vert @nyanpn I have a question that I hope you could elaborate on. Regarding identifying the real price of the stock. What is the logic behind 0.01 / tick giving you the price of the stock? why 0.01 and why take the smallest tick size? What would the largest tick size tell you about the stock? 3 more replies arrow_drop_down Tonghui Li Posted 4 years ago · 22nd in this Competition arrow_drop_up 1 more_vert @nyanpn Thanks so much for sharing! You said you were able to train 20 NN models in the submission notebook? How long does it take to train one NN model on average? My 1DCNN takes over 1 hour to train in the Kaggle notebook so I am curious how you made that work. Thanks in advance! nyanp Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 2 more_vert It took 15s/epoch x 50epocs = 750sec to train singe NN. Perhaps because my CNN is much smaller than the original MoA’s CNN. 5 more replies arrow_drop_down UKI Posted 4 years ago · 138th in this Competition arrow_drop_up 1 more_vert This is great! Thanks for sharing! My understand is, the leaks between time_ids have boosted the prediction performance, and I'm not sure Optiver wants this. At this time, I'm concerning Optiver could change the sampling period of private data (for example 30min to 1 hour) to reduce the effect of the leaks between time_ids. Vu Huynh Posted 4 years ago arrow_drop_up 1 more_vert Reverse engineering of time-id order Same idea but we think about it too late. Did you think it is cheating as the host don't want us create model with true ordered time ? nyanp Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 8 more_vert While this is 100% legal against the competition rules, unfortunately it is probably not what the host wanted us to do. On the other hand, it has happened repeatedly in Kaggle that in order to win the competition, we need to use methods that would probably not work in a real-world scenario. I think this mismatch should be reduced in the design of the competition (this is also why I love the time-series code competition). I believe that time/stock NN features are useful even if future information is not available (e.g. you can use information from past periods of similar volatility), so I still believe that the host can get some meaningful insights from my solution. 3 more replies arrow_drop_down Tobias Tesch Posted 4 years ago · 50th in this Competition arrow_drop_up 1 more_vert Thanks for sharing all the details of your great work and congratulations!! :) Just a question as I'm no experienced kaggler yet: is it okay to \"reverse engineer\" for example time_id order and price? I'm just wondering this because there must have been a reason for the organizers to \"hide\" this information? yu Posted 4 years ago · 1173rd in this Competition arrow_drop_up 1 more_vert https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/256725 basically they want us to use the 10 minutes window only if I understand right Michael Poluektov Posted 4 years ago · 7th in this Competition arrow_drop_up 6 more_vert I don't think this is what they wanted, but I don't think a model not using any kind of future or historic data could ever have a good placement on public or private LB Aggregating features by stock_id is a common tactic in top scoring public notebooks, but by doing that we are passing some future information to our model, same with using stock_id as a categorical feature, or doing any kind of stock_id clustering on test data Even aggregating by time_id can cause a leak, since seconds_in_bucket are not synchronised, part of the target variable of the stocks with more book updates can be influenced by an event in stocks with fewer book updates (if there are no book updates at the beginning of a 10 minute window, the whole window for that particular stock is shifted to start at the next book update) Which is not to say all of the models using these features are completely worthless. Obviously looking at the window right after the target we're trying to predict is not possible in the real world, but using some kind of historic information from the past hour/day/week is. And obviously so is all the \"normal\" feature engineering which is still needed to get a good placement. I'm not experienced either, but from what my more experienced friends told me such tricks aren't uncommon on Kaggle. I have no clue what will happen next though. Tobias Tesch Posted 4 years ago · 50th in this Competition arrow_drop_up 0 more_vert Thanks for your responses! I agree that aggregating over stock_id also includes future information in the prediction. Nevertheless, this seems to have been a conscious decision of the organizers when designing the competition in this way. On the other side, reverse engineering price or time_id order is pretty clearly against the design choices of the organizers. Anyway, I guess we will see if this is okay. Rayan-aay Posted 4 years ago · 1162nd in this Competition arrow_drop_up 2 more_vert Great work ! many thanks for the share. I already did test to add an Autoencoder to denoise the data, but it doesn't increase the score overall. khyeh Posted 4 years ago · 18th in this Competition arrow_drop_up 2 more_vert Reverse engineering of time-id order Thanks for your detailed solution! I particularly enjoy reading this part, since we've been struggled to do a proper cv and need to verify on LB directly which might lead to overfitting in the end. leo Posted 4 years ago · 1103rd in this Competition arrow_drop_up 2 more_vert You got only one piece of the puzzle from Stas Sl's notebook, then you reconstructed the whole picture. Very impressive! Also, glad to see someone else also tried the 300-sec model idea. RDizzl3 Posted 4 years ago · 104th in this Competition arrow_drop_up 2 more_vert @nyanpn @stassl - I have a question for you. If you had not done work to recover time ids was breaking that 0.19XX plane possible still? Trushant Kalyanpur Posted 4 years ago · 18th in this Competition arrow_drop_up 7 more_vert I can confirm that 0.19x can be broken without recovering time id. Our single model was public LB 0.18951 without time id recovery. 10 more replies arrow_drop_down +4 RDizzl3 Posted 4 years ago · 104th in this Competition arrow_drop_up 2 more_vert @nyanpn - Thank you for your detailed explanation and code snippets that is extremely useful! So, I used NearestNeighbor with various distance metrics to find the similar N time-ids and calculate the average of features like RV and stock size (N=2,3,5,10,20,40). In addition to the time-id, I also calculated the aggregation between similar stock-ids. Furthermore, by combining these ideas, I calculated features such as \"the average tau of 20 similar stocks with similar volatility in 5 closest time-ids\". I tried features like this and know I am very confused as to why I could not get them to work properly. I will be studying this today and trying to figure out where my execution for this idea went wrong. AmbrosM Posted 4 years ago · 2199th in this Competition arrow_drop_up 2 more_vert Thank you for the detailed description of your solution! What didn’t work training NN on residual (target - book.log_return.realized_volatility) I trained my LightGBM model on a multiplicative residual (target / book.log_return.realized_volatility). It gave a slight (i.e. insignificant) performance improvement over training on the target directly. Max2020 Posted 4 years ago arrow_drop_up 2 more_vert I don't think we can use future information in a real Optiver’s scenario, but the idea of using Nearest Neighbor to aggregate nearby features can probably be used in a real model. I thought of that line today, too 🙃 Lucas Morin Posted 4 years ago · 2832nd in this Competition arrow_drop_up 1 more_vert This is working because the test set is presented at once, including data from the future. It probably won’t work so well in a real life setting. Yufeng Ma Posted 2 years ago arrow_drop_up 0 more_vert Hi nyanp, congrats and great work ! I am curious about how the 1-D CNN's inner blocks' (e.g., conv, average pooling , and max pooling) ordering are decided. Is that searched by NAS methods / grid search or designed by some intuitions ? Andrew K Posted 2 years ago arrow_drop_up 0 more_vert Thnx a lot for sharing)) Year later but your solution is still helpful) Aijing Xing Posted 3 years ago arrow_drop_up 0 more_vert @nyanpn Sorry for my poor English and Python recognization. Does the \"make neighbors\" means, in terms of the identical stock_id, borrow the features from the historical time-ids(which have similar feature values) for current time-id? And \"aggregate\" means calculating the mean values of choosen neighbors' features ? Alfred Motladile Posted 3 years ago arrow_drop_up 0 more_vert I leant a lot from your work. Thank you and congratulations. Too many requests error Too many requests",
      "Optiver Realized Volatility Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Optiver · Featured Code Competition · 3 years ago Late Submission more_horiz Optiver Realized Volatility Prediction Apply your data science skills to make financial markets better Optiver Realized Volatility Prediction Overview Data Code Models Discussion Leaderboard Rules A.Sato · 4th in this Competition  · Posted 4 years ago arrow_drop_up 45 more_vert Tentative 3rd Place Solution (6th in Public) - life is volatile Before I start, I would like to thank kaggle and Optiver for hosting this phenomenal competition. And, of course, my great teammate, @tomotomo5 , who drafted this post. We have ended off 6th place in the Public Leaderboard and are 3rd place currently (as of October 15th) in the Private Leaderboard. In this discussion, we would like to introduce the core ideas we have implemented that boosted our score. Some of them are not yet discussed much in the community.  Model-wise, our final model is a simple stacking model of LightGBM x ANN (less noteworthy). time_id Nearest Neighbor The largest gamechanger of our solution was features based on time_id nearest neighbors, which was a similar idea of feature creation @nyanp has explained in his solution below. https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/274970 I confess that our philosophy was not as sophisticated as @nyanp 's. We just tried to find the time ids that have similar market conditions. In fact, we were not quite aware that we could recover time-id order, although we ended up creating quite similar features to @nyanp 's. Differences are, No usage of dimension reduction (TSNE in @nyanps solution) when calculating the nearest neighbors Adding the distance ratio of the 1st nearest time_id and k-th nearest time_id No time-series cross-validation as we did not come up with the idea of time-series reverse engineering:( Target Transformation Target Transformation was a crucial part of our model that increased the robustness. Stock volatility could behave as a non-stationary series, meaning that the level of volatility might be unstable over time but the change of volatility is less likely so. With this in mind, we decided to change the task of the competition from directly predicting the level of realized volatility to predicting the ratio of the target to the realized volatility of 0~600 seconds. Specifically, our transformed target is formulated as follows. (transformed_target) = (target) / (realized volatility of 0~600 seconds) 300 seconds Model Since a large time interval between the training dataset and the final test dataset set was expected, we wanted to give our model as much information on the test dataset as possible. Therefore, we (1) concatenated the train + test set, (2) sliced the 600 seconds in half, (3) used the first half (0-300 seconds) to create features (4) to predict the realized volatility in the latter 300-600 seconds. We then (5) created features based on 300-600 seconds of the dataset, and (6) predicted the realized volatility of 600-900 seconds. The predicted results were used as features of our main model. The best part of this idea was that we were able to train our model based on test data. Macro Estimation We believed that individual stock volatility depends largely on the macro environment. Therefore, we trained a model to predict the average volatility of all stock_ids in each time_id. The results were used as features of our main model. Hope the best that the private leaderboard thereafter is not that volatile even if life is volatile . Thank you so much for reading, HAPPY KAGGLING! Please sign in to reply to this topic. comment 5 Comments 1 appreciation  comment Hotness nyanp Posted 4 years ago · 1st in this Competition arrow_drop_up 4 more_vert Great solution, thanks for sharing! 300sec model might be one of the reasons for your team’s shakeup. The 300sec model is more advantageous with more private test data, so I might be overtaken by your team in the next update ;) BTW I also did not use tSNE for the NN feature, but simply calculated the nearest time-ids for each time-id (I tried dimensionally reduction for NN but it did not improve CV). I think our NN features are almost the same. Tomomu Iwai Posted 4 years ago · 4th in this Competition arrow_drop_up 1 more_vert Great pleasure teaming with you! Thank you so much! This comment has been deleted. This comment has been deleted. Appreciation (1) Yam Peleg Posted 4 years ago · 391st in this Competition arrow_drop_up 2 more_vert Great solution! thanks for sharing! Too many requests error Too many requests",
      "Optiver Realized Volatility Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Optiver · Featured Code Competition · 3 years ago Late Submission more_horiz Optiver Realized Volatility Prediction Apply your data science skills to make financial markets better Optiver Realized Volatility Prediction Overview Data Code Models Discussion Leaderboard Rules Michael Poluektov · 7th in this Competition  · Posted 4 years ago arrow_drop_up 28 more_vert 7th place main ideas Firstly, many thanks to Optiver for sponsoring this competition. It allowed me to gain a lot of knowledge about finance and data science in general, and it ended up being very interesting. About time_id aggregations It's probably not a surprise to anyone that my public LB placement can be mostly attributed to \"The Leak\", so credit to whoever posted this (and then deleted their account). I used a similar approach to the one described in @nyanpn 's post , with a few minor differences such as [this] (basically I also re-ordered the NN time-ids and derived some features from there)( https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/274970#1531808) . It goes without saying that such a solution could not actually be implemented in production, so this post will focus on all the other features & techniques I've learned about and that I haven't seen mentioned elsewhere. I'd also like to say that I'm quite inexperienced in both data science and quantitative finance, and I don't know what my LB placement would be without time_id aggregations, so don't take this for a top solution. These ideas improved my CV and LB but I'm not sure if they would, by themselves, get me anywhere in the top LB spots. Baseline & models My submission started as a fork of the \"lgbm baseline\" , thanks to @alexislyon for providing such a helpful learning resource. Although I changed most the the feature engineering, my final model is still based on a 50/50 LGBM and FFNN ensemble. Trend Most public solutions are using aggregations by seconds_in_bucket over 100 second time periods. I've found out that instead of passing those features as is, it is more efficient to fit a linear regression model on those features, and pass the mean, slope and error: Function to get mean, slope, error of input array def get_dir_stats (arr):\n    mat = np. array ([np. ones ( len (arr)), np. arange ( len (arr))]). transpose ()\n    coefs, err, _, _ = lstsq (mat, arr)\n    return np. array ([coefs[ 0 ] + ( len (arr)/ 2 )*coefs[ 1 ], coefs[ 1 ], err]) content_copy Whenever we would like to analyse the way a feature behaves over time, we would calculate that feature over 100 second time periods, and pass the array of those features (for example [rv_0-100, rv_100-200, …]) to the above function. This ended up improving my CV and LB by about 0.00100, and I believe this also helped improve readability, which in turn helped me for feature selection. Regarding stock_id Stock_id as a categorical feature wasn't used in my final submissions because I believe it wouldn't scale well with future data. As an extreme example, imagine what would happen if one of the stocks in our dataset was GameStop and the dataset was taken before 2021. I did use stock_id aggregations, but only over the N closest time_ids. This does require price denormalisation but I still think that would be the way to go even if we only had access to past time_ids for each prediction. This probably isn't such an original concept but I haven't seen it discussed elsewhere. I'm interested to see what would be the most successful approach without time_id aggregations. Feature selection I used BorutaShap with an XGBRegressor model for feature selection. After running it on a kaggle kernel overnight I was still left with about 60 features I was unsure about. I tried re-running it again with the unimportant features removed, but I kept getting diminishing returns. If I had more time I could try to save the state of the program and resume it in a new session every 8 hours. Alternatively, I could try to get access to a decent CPU and run it locally. Interestingly, most of the features deemed unimportant were domain specific (most quarticity features, a lot of trade features from the \"lgbm baseline\" notebook etc.) and not part of the increasing amount of aggregated features. Please sign in to reply to this topic. comment 13 Comments 1 appreciation  comment Hotness Regi Public Posted 4 years ago · 2577th in this Competition arrow_drop_up 1 more_vert \"I used BorutaShap with an XGBRegressor model for feature selection. \" Would this give better results than just using e.g. model.feature_importance(importance_type='gain') then picking top e.g. 75 features.. Does BorutaShap work in a better way? Michael Poluektov Topic Author Posted 4 years ago · 7th in this Competition arrow_drop_up 1 more_vert Picking features by feature importance didn't work for me but perhaps with a different importance type I could've had more success. I used the default option which I've now learned was 'split' so perhaps with 'gain' it could've worked. 3 more replies arrow_drop_down Regi Public Posted 4 years ago · 2577th in this Competition arrow_drop_up 1 more_vert \" I've found out that instead of passing those features as is, it is more efficient to fit a linear regression model on those features, and pass the mean, slope and error:\" This is fucking genius - really.. I'm going to try this out now and see.. The amount of time it will save to  create features, and as you say, code readability.. and keep models simpler.. great insight Regi Public Posted 4 years ago · 2577th in this Competition arrow_drop_up 1 more_vert So I played around with Micheal's code and went through line by line to understand. I then re-wrote my own function.. I figured instead of calculating the mean, we can just calc c, the intercept and pass along with m (gradient) and error. Still 3 values like Micheal's code. 'c' gives a similar but slightly different value than mean, but my feeling is, an ML model will give almost identical output regardless if one is use instead of the other import numpy as np import np.linalg def lineOfBestFit ( y ):\n\n    l= len (y)\n    x = range (l) # create a list of [0,1,2, .., n] A = np.vstack([x, np.ones(l)]).T # create matrix suitable for lstsq() # Return the least-squares solution to a linear matrix equation.. # rcond=None stops warning message for outdated function.. calc = np.linalg.lstsq(A, y, rcond= None ) # Note: c, is highly correlated to 'mean', # It's not quite the same as Michael Poluektov mean, # but I believe will do equally well.. m, c = calc[ 0 ] # m = gradient, c = intercept # Error returned as a list, hence the [0] to 'delist' sumSqrErr = calc[ 1 ][ 0 ] return (c, m, sumSqrErr) content_copy My next step to to replace my feature code with this..and check results are similar.. Then after that.. why not a quadratic, with 4 or 5 parameters.. this will fit even better, so hopefully give some improvements.. did you consider this also? 5 more replies arrow_drop_down Appreciation (1) Jonathan Mallia Posted 4 years ago · 1922nd in this Competition arrow_drop_up 1 more_vert Thanks for sharing @michaelpoluektov Too many requests error Too many requests",
      "Optiver Realized Volatility Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Optiver · Featured Code Competition · 3 years ago Late Submission more_horiz Optiver Realized Volatility Prediction Apply your data science skills to make financial markets better Optiver Realized Volatility Prediction Overview Data Code Models Discussion Leaderboard Rules khyeh · 18th in this Competition  · Posted 4 years ago arrow_drop_up 42 more_vert Public 12 Place Short Solution Thanks to Kaggle, Optiver for hosting this competition, and thanks to a lot of public sharing from which I learned a lot. Finally thanks to my teammates for their efforts during the competition period. Validation Strategy Since the public\\private LB represent the future market, the best way is to do a time-based validation split. However, since it is declared that we don't have an actual sequence relationship between different time_ids,  the best way to do validation is to do time_id based group k-fold validation. The rule of thumb for validation becomes: Check every set of changes if the cv holds or improves, we test on LB. If LB is improving, we keep those features. The idea is that what features\\models we created\\tuned should generalize to the different buckets of time at least. Also, we treat LB as an extra validation fold that should represent the private test set better. Modeling [Ideal NN Design] Use RNN to extract book\\trade features for each stock_id at each time_id Use Transformer to capture inter-stock relationship [Evolution 1] Issue: The training was too slow Solution: Replace RNN parts with handmade features - training is much faster - almost no difference in performance Performance: cv: 0.211, lb: ~0.200 [Evolution 2] Issue: As hint by the top team, we did not consider the relationship between time_ids. Solution: Use Nearest Neighbors to find top-N closest time_ids, and average the features for each stock from the time_id neighbors as new features. Performance: cv: ~0.198, lb: ~0.189 [Blending] Transformer + Tabnet + NN with different feature sets. Please sign in to reply to this topic. comment 25 Comments Hotness Ertuğrul Demir Posted 4 years ago · 242nd in this Competition arrow_drop_up 3 more_vert Congrats! Looks like nearest neighbor features were the killers for the gold zone… Stas Sl Posted 4 years ago · 154th in this Competition arrow_drop_up 3 more_vert Thanks for sharing, definitely there is a lot of common with my solution. But a lot of differences too. I wonder if you visualized attention weights for stocks, how different they were for different stocks? I found that for many stocks they were quite similar, often paying attention to same stocks. Trushant Kalyanpur Posted 4 years ago · 18th in this Competition arrow_drop_up 1 more_vert Unfortunately we didn't get too much time to analyze the attention weights. With one week left in the competition, we were still below bronze zone , so our focus was primarily figuring out how to get better models and exploit the inter stock/time id relations. We are very interested in looking at your approach. White Memories Posted 4 years ago · 1159th in this Competition arrow_drop_up 3 more_vert Nice approach. However wonder if any team could break 0.19 without using future information. Trushant Kalyanpur Posted 4 years ago · 18th in this Competition arrow_drop_up 1 more_vert With the intentionally designed structure of the data, it is hard to know what time id groups are in the future or not, so you really cannot escape using information from the future unless you reverse engineer original time id sequence like some teams mentioned. The best we can do is try out some feature engineering and see if CV and LB have correlation and improvement. jchen950 Posted 4 years ago · 2159th in this Competition arrow_drop_up 1 more_vert Agree. KNN by performance is not using future information (at least we don't know without recovering the time order). Intentionally use of future information will be - create train/val based on current and future time_ids and aggregate features by nearest time_id. Trushant Kalyanpur Posted 4 years ago · 18th in this Competition arrow_drop_up 4 more_vert I would like to add that it was a very interesting journey to the public gold zone. One week before the submission deadline, we had found out which set of features were good for different architectures like transformers, tabnet, LGB, NN and we were below the bronze zone. Our best single model at that time was a 0.198x. The addition of nearest neighbor features gave us a single 0.195x model with significant improvement so we knew we had something. Adding the features which we had found to be best on specific archs took our score from 0.198x to 0.189x for a single transformer model. The additional gains to 0.187x were the results of blending of multiple transformer models + NN + tabnet.  All in all a very exciting finish at least to public LB for us.. what happens on private LB is all a toss up :) . I would also like to thank Kaggle and Optiver for this unique competition due to the nature of how the data was framed and also to my amazing teammates for a strong finish. leo Posted 4 years ago · 1103rd in this Competition arrow_drop_up 2 more_vert Hi TK, congrats on your great result! This step-by-step progression is very informative! When tuning parameters, did you find smaller models or simpler structures perform better on this dataset? yuanzhe zhou Posted 4 years ago · 164th in this Competition arrow_drop_up 1 more_vert Did you aggregate features from test dataset into your training data? For a time series competition, it's usually not allowed to use the test dataset's information (I saw many rules explicitly banned it) during training process … but if you calculated the closest time_id only from training dataset, then I find it legit 👀 khyeh Topic Author Posted 4 years ago · 18th in this Competition arrow_drop_up 2 more_vert for train set, I only aggregate features from train. for test set, I only aggregate features from test: since private test set will be in more far away future than in public test set (that reflects on LB)… yuanzhe zhou Posted 4 years ago · 164th in this Competition arrow_drop_up 0 more_vert Thank you for your reply! From your result, it seems that even without potential leak between datasets, these aggregating features can still be helpful. 👍 RandomWalker Posted 4 years ago · 2305th in this Competition arrow_drop_up 1 more_vert Congrats and thank you for sharing your solution. May I ask a question about your neural net design? As I understand from your picture, the model processes each stock's feature time series separately and then combines the extracted features as embedding to the main transformer layer, and finally makes separate predictions for the 112 stocks. So the model indeed is multi-input multi-output and has 112 inputs layers and 112 output layers? khyeh Topic Author Posted 4 years ago · 18th in this Competition arrow_drop_up 1 more_vert yes, you are right. I'm not actually creating 112 input layers but combined them into tensor of shape (batch size, 112, feature dimension). It also involves some coding effort to do the masked output and masked loss, since not every stock_id appears in the same time_id. ge jin Posted 3 years ago arrow_drop_up 0 more_vert Thank you for your detailed explaination of the solutions. Sorry but I still have one thing unclear: when you say you put tensor (batch size, 112, feature dimension) into the main transformer layer, is the feature dimension a concatenate of stock id embedding and non-categorical features (features associated with price and volume, etc)? Thanks for your kind reply. LongYin/杰少 Posted 4 years ago arrow_drop_up 2 more_vert Thanks for sharing your ideas. I also tried 1dcnn and rnn/lstm  but get bad performance by using raw features, can you share what is your input of rnn parts? khyeh Topic Author Posted 4 years ago · 18th in this Competition arrow_drop_up 1 more_vert @longyin2 They are features from book\\trade parquet similar to the good work from https://www.kaggle.com/gunesevitan/optiver-realized-volatility-prediction-1d-cnn Using rnn\\cnn itself only reaches 0.22-23x in cv, which is not good for me as well. What made the difference is to further consider inter-stock relationships in the same bucket of time by using yet another transformer LongYin/杰少 Posted 4 years ago arrow_drop_up 1 more_vert Thanks for your reply good good study Posted 4 years ago · 1343rd in this Competition arrow_drop_up 0 more_vert Hello, thx for you ideas, May I ask how did you define \"top-N closest time_ids\"? Kazumitsu Sakurai Posted 4 years ago · 1257th in this Competition arrow_drop_up 0 more_vert Congrats and thanks for sharing your amazing work! I also tried to make RNN but training speed was very slow. Would you like to explain more details of Evolution 1? I'm interested in how did you cope with this problem. Does it mean you stored RNN output in your storage, and then you used it as features to feed transformers? khyeh Topic Author Posted 4 years ago · 18th in this Competition arrow_drop_up 2 more_vert sorry for not being clear above. I meant instead of using RNN, I used handmade time-series features 3 more replies arrow_drop_down Denis Vodchyts Posted 4 years ago · 1945th in this Competition arrow_drop_up 0 more_vert Interesting, Could you please share you transformer layer/network? Did you use raw inputs to Lstm? Thanks. This comment has been deleted. Too many requests error Too many requests",
      "Optiver Realized Volatility Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Optiver · Featured Code Competition · 3 years ago Late Submission more_horiz Optiver Realized Volatility Prediction Apply your data science skills to make financial markets better Optiver Realized Volatility Prediction Overview Data Code Models Discussion Leaderboard Rules shigeria · 24th in this Competition  · Posted 3 years ago arrow_drop_up 15 more_vert Public 1398th → Private 24th Solution As the title says, We experienced a great shakeup. Although it might be a total fluke, I would like to sum up a brief solution summary. Final submission code is here . 1. features We added some features used in the following Notebooks. https://www.kaggle.com/tommy1028/lightgbm-starter-with-feature-engineering-idea https://www.kaggle.com/denisvodchyts/lgbm-slightly-different-features https://www.kaggle.com/alexioslyon/lgbm-baseline Also, We added original price discussed in some discussion(Original Discussion is now deleted.). Because We could not get good Public LB score, We finally betted on features that seemed to be largely affected by the leak and have an effect on Private LB, though they did not work on Public LB. Furthermore, We chose different ways in filling in missing values(fill in 0 or np.mean depending on the features) 2. validation and models We used group kfold by stock id, but in NN, group kfold was based on knn algorithms and in Tabnet and LGBM, We just used sklearn group kfold. NN model architecture is like below. def base_model():\n\n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = keras.Input(shape=(1,), name = 'stock_id' )\n    num_input = keras.Input(shape=(222,), name = 'num_data' )\n\n    #embedding, flatenning and concatenating\n    x = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, input_length =1, name = 'stock_embedding' )(stock_id_input)\n    y = tf.expand_dims(num_input, axis =1)\n    x = keras.layers.Concatenate( axis =-1)([x, y])\n    x = keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(256, return_sequences = True ))(x)\n    x = keras.layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNGRU(128, return_sequences = True ))(x)\n    avg_pool = keras.layers.GlobalAveragePooling1D()(x)\n    max_pool = keras.layers.GlobalMaxPooling1D()(x)\n    conc = keras.layers.concatenate([avg_pool, max_pool])\n    out = keras.layers.Flatten()(conc)\n\n    # Add one or more hidden layers for n_hidden in hidden_units:\n\n        out = keras.layers.Dense(n_hidden, activation = 'swish' )(out)\n\n\n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation = 'linear' , name = 'prediction' )(out)\n\n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n\n    return model content_copy We ensembled NN * 0.4 + Tabnet * 0.4 + LGBM * 0.2 Finally We thanked for competitors in this Competition and the host! Sorry for poor English, thanks for reading! Please sign in to reply to this topic. comment 1 Comment Hotness liuyihan2000 Posted 3 years ago · 846th in this Competition arrow_drop_up 0 more_vert great work! Too many requests error Too many requests",
      "Optiver Realized Volatility Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Optiver · Featured Code Competition · 3 years ago Late Submission more_horiz Optiver Realized Volatility Prediction Apply your data science skills to make financial markets better Optiver Realized Volatility Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Apply your data science skills to make financial markets better This dataset contains stock market data relevant to the practical execution of trades in the financial markets. In particular, it includes order book snapshots and executed trades. With one second resolution, it provides a uniquely fine grained look at the micro-structure of modern financial markets. This is a code competition where only the first few rows of the test set are available for download. The rows that are visible are intended to illustrate the hidden test set format and folder structure. The remainder will only be available to your notebook when it is submitted. The hidden test set contains data that can be used to construct features to predict roughly 150,000 target values. Loading the entire dataset will take slightly more than 3 GB of memory, by our estimation. This is also a forecasting competition, where the final private leaderboard will be determined using data gathered after the training period closes, which means that the public and private leaderboards will have zero overlap . During the active training stage of the competition a large fraction of the test data will be filler, intended only to ensure the hidden dataset has approximately the same size as the actual test data. The filler data will be removed entirely during the forecasting phase of the competition and replaced with real market data. book_[train/test].parquet A parquet file partitioned by stock_id . Provides order book data on the most competitive buy and sell orders entered into the market. The top two levels of the book are shared. The first level of the book will be more competitive in price terms, it will then receive execution priority over the second level. trade_[train/test].parquet A parquet file partitioned by stock_id . Contains data on trades that actually executed. Usually, in the market, there are more passive buy/sell intention updates (book updates) than actual trades, therefore one may expect this file to be more sparse than the order book. train.csv The ground truth values for the training set. test.csv Provides the mapping between the other data files and the submission file. As with other test files, most of the data is only available to your notebook upon submission with just the first few rows available for download. sample_submission.csv - A sample submission file in the correct format. 229 files 2.73 GB parquet, csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 2.73 GB book_test.parquet book_train.parquet trade_test.parquet trade_train.parquet sample_submission.csv test.csv train.csv 229 files 8 columns  Too many requests",
    "data_description": "Optiver Realized Volatility Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Optiver · Featured Code Competition · 3 years ago Late Submission more_horiz Optiver Realized Volatility Prediction Apply your data science skills to make financial markets better Optiver Realized Volatility Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Jun 28, 2021 Close Jan 10, 2022 Merger & Entry Description link keyboard_arrow_up Volatility is one of the most prominent terms you’ll hear on any trading floor – and for good reason. In financial markets, volatility captures the amount of fluctuation in prices. High volatility is associated to periods of market turbulence and to large price swings, while low volatility describes more calm and quiet markets. For trading firms like Optiver, accurately predicting volatility is essential for the trading of options, whose price is directly related to the volatility of the underlying product. As a leading global electronic market maker, Optiver is dedicated to continuously improving financial markets, creating better access and prices for options, ETFs, cash equities, bonds and foreign currencies on numerous exchanges around the world. Optiver’s teams have spent countless hours building sophisticated models that predict volatility and continuously generate fairer options prices for end investors. However, an industry-leading pricing algorithm can never stop evolving, and there is no better place than Kaggle to help Optiver take its model to the next level. In the first three months of this competition, you’ll build models that predict short-term volatility for hundreds of stocks across different sectors. You will have hundreds of millions of rows of highly granular financial data at your fingertips, with which you'll design your model forecasting volatility over 10-minute periods. Your models will be evaluated against real market data collected in the three-month evaluation period after training. Through this competition, you'll gain invaluable insight into volatility and financial market structure. You'll also get a better understanding of the sort of data science problems Optiver has faced for decades. We look forward to seeing the creative approaches the Kaggle community will apply to this ever complex but exciting trading challenge. Getting started In order to make Kagglers better prepared for this competition, Optiver's data scientists have created a tutorial notebook debriefing competition data and relevant financial concepts of this trading challenge. Also, Optiver's online course can tell you more about financial market and market making. (opens in a new tab)\"> For more information about exciting data science opportunities at Optiver, check out their data science landing page here or e-mail their recruiting team directly at datascience@optiver.com. This is a Code Competition. Refer to Code Requirements for details. Evaluation link keyboard_arrow_up Submissions are evaluated using the root mean square percentage error, defined as: RMSPE = √ 1 n n ∑ i = 1 ( ( y i − ˆ y i ) / y i ) 2 Submission File For each row_id in the test set, you must predict the target variable. The file should contain a header and have the following format: row_id,target 0 - 0,0.003 0 - 1,0.002 0 - 2,0.001 ... content_copy Timeline link keyboard_arrow_up This is a forecasting competition with an active training phase and a second period where models will be run against real market data. Training Timeline June 28, 2021 - Start Date September 20, 2021 - Entry deadline. You must accept the competition rules before this date in order to compete. September 20, 2021 - Team Merger deadline. This is the last day participants may join or merge teams. September 27, 2021 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Forecasting Timeline: Starting after the final submission deadline there will be periodic updates to the leaderboard to reflect market data updates that will be run against selected notebooks. Updates will take place roughly every two weeks, with an adjustment to avoid the winter holidays. January 10, 2022 - Competition End Date Prizes link keyboard_arrow_up 1st Place - $25,000 2nd Place - $20,000 3rd Place - $15,000 4th Place - $10,000 5th - 10th Place - $5,000 Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. In order for the \"Submit\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time Internet access disabled Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv Please see the Code Competition FAQ for more information on how to submit. And review the code debugging doc if you are encountering submission errors. Citation link keyboard_arrow_up Andrew Meyer, BerniceOptiver, CameronOptiver, IXAGPOPU, Jiashen Liu, Matteo Pietrobon (Optiver), OptiverMerle, Sohier Dane, and Stefan Vallentine. Optiver Realized Volatility Prediction. https://kaggle.com/competitions/optiver-realized-volatility-prediction, 2021. Kaggle. Cite Competition Host Optiver Prizes & Awards $100,000 Awards Points & Medals Participation 21,295 Entrants 4,395 Participants 3,852 Teams 6,670 Submissions Tags Tabular Finance Root Mean Square Percentage Error Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "jane-street-market-prediction",
    "discussion_links": [
      "/competitions/jane-street-market-prediction/discussion/224348",
      "/competitions/jane-street-market-prediction/discussion/224713",
      "/competitions/jane-street-market-prediction/discussion/226837",
      "/competitions/jane-street-market-prediction/discussion/269181",
      "/competitions/jane-street-market-prediction/discussion/224079",
      "/competitions/jane-street-market-prediction/discussion/224029",
      "/competitions/jane-street-market-prediction/discussion/227167"
    ],
    "discussion_texts": [
      "Jane Street Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Jane Street Group · Featured Code Competition · 4 years ago Late Submission more_horiz Jane Street Market Prediction Test your model against future real market data Jane Street Market Prediction Overview Data Code Models Discussion Leaderboard Rules Yirun Zhang · 1st in this Competition  · Posted 4 years ago arrow_drop_up 339 more_vert Yirun's Solution (1st place): Training Supervised Autoencoder with MLP Update: The single-model AE-MLP scores 6022.202 on the private leaderboard, which is still 1st place! Notebook (Training): Jane Street: Supervised Autoencoder MLP In this competition, I have used a supervised autoencoder MLP approach and my teammates use XGBoost. Our final submission is a simple blend of these two models. Here, I would like to explain my approach in detail. The supervised autoencoder approach was initially proposed in Bottleneck encoder + MLP + Keras Tuner 8601c5 , where one supervised autoencoder is trained separately before cross-validation (CV) split. I have realised that this training may cause label leakage because the autoencoder has seen part of the data in the validation set in each CV split and it can generate label-leakage features to overfit. So, my approach is to train the supervised autoencoder along with MLP in one model in each CV split. The training processes and explanations are given in the notebook and the following statements. Cross-Validation (CV) Strategy and Feature Engineering: 5-fold 31-gap purged group time-series split Remove first 85 days for training since they have different feature variance Forward-fill the missing values Transfer all resp targets (resp, resp_1, resp_2, resp_3, resp_4) to action for multi-label classification Use the mean of the absolute values of all resp targets as sample weights for training so that the model can focus on capturing samples with large absolute resp. During inference, the mean of all predicted actions is taken as the final probability Deep Learning Model: Use autoencoder to create new features, concatenating with the original features as  the input to the downstream MLP model Train autoencoder and MLP together in each CV split to prevent data leakage Add target information to autoencoder (supervised learning) to force it to generate more relevant features, and to create a shortcut for backpropagation of gradient Add Gaussian noise layer before encoder for data augmentation and to prevent overfitting Use swish activation function instead of ReLU to prevent ‘dead neuron’ and smooth the gradient Batch Normalisation and Dropout are used for MLP Train the model with 3 different random seeds and take the average to reduce prediction variance Only use the models (with different seeds) trained in the last two CV splits since they have seen more data Only monitor the BCE loss of MLP instead of the overall loss for early stopping Use Hyperopt to find the optimal hyperparameter set Deep Learning Neural Networks 30 Please sign in to reply to this topic. comment 47 Comments 5 appreciation  comments Hotness 朴大福 Posted 4 years ago arrow_drop_up 6 more_vert 🐂 Jiajun Huang Posted 8 months ago arrow_drop_up 1 more_vert 🖊️ Andy Posted 4 years ago · 15th in this Competition arrow_drop_up 6 more_vert Wow, did not expect your solution was this simple. In my case, a swap noise DAE gave me a better result and added varieties to my ensemble. Hope you stay on top in the future reruns! Yirun Zhang Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 10 more_vert I have various more complex solutions but not submitted. Not sure how they perform. Carl McBride Ellis Posted 4 years ago · 240th in this Competition arrow_drop_up 6 more_vert Usually we see the Private score for all of our submissions, selected or not. Given the computational overhead in this competition I understand that it is totally impractical. However I too would really like to have seen how our many different and varied experiments performed on the Private data… All the best Tensor Girl Posted 4 years ago · 1312th in this Competition arrow_drop_up 6 more_vert @carlmcbrideellis Even I am curious to know how all of my team solutions performed . I had a gut feeling that simplest solution will win Gold in this competition from the beginning . Shuhao Cao Posted 4 years ago · 240th in this Competition arrow_drop_up 5 more_vert 😂 Coincidentally, we are writing an academic paper that uses this idea (supervised training for an encoder) to do operator learning between infinite dimensional Banach spaces (different network structure though). Yirun Zhang Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 4 more_vert Sounds very interesting. Please let me know if it is published someday. 👍 Mingjie Wang Posted 4 years ago · 1st in this Competition arrow_drop_up 4 more_vert shake shake！🙈🙈🙈 marketneutral Posted 4 years ago · 2239th in this Competition arrow_drop_up 3 more_vert Can you share some details on the XGBoost model, e.g., max tree depth, number of trees? Also how you weighted/blended these two models? Yirun Zhang Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 3 more_vert Please see more details in https://www.kaggle.com/c/jane-street-market-prediction/discussion/229623 https://www.kaggle.com/xiaowangiiiii/current-1th-jane-street-ae-mlp-xgb daforge Posted 4 years ago · 226th in this Competition arrow_drop_up 1 more_vert Good stuff thanks for sharing and congrats! François BOYER Posted 4 years ago · 561st in this Competition arrow_drop_up 1 more_vert Thanks for sharing What do you mean by \"Add target information to autoencoder (supervised learning) \" Where do you add this target information ? I guess not as an input, since you don't have the target at inference time. Also when you say you trained autoencoder and MLP together in the same model : did you also train them on exactly the same data ? And using the autoencoder embedding as input. I was afraid to train them on the same data because I didn't want the autoencoder to provide too \"optimistic\" / overfitted embedding during training Yirun Zhang Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 0 more_vert Please check the summary figure in my notebook, which should answer your question about target information. Arjun B Posted 4 years ago · 3085th in this Competition arrow_drop_up 1 more_vert Great Work @gogo827jz ilovepotatoes Posted 4 years ago · 1363rd in this Competition arrow_drop_up 1 more_vert Keep up the good work!. This time cat isn't shaking down ;) Bs004 Posted 4 years ago · 676th in this Competition arrow_drop_up 1 more_vert I'm confused on how you will use autoencoder while prediction on unseen data as you won't have target variable there? Yirun Zhang Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 0 more_vert You only need the encoder part of the autoencoder when predicting. Kaggoo Posted 8 months ago · 447th in this Competition arrow_drop_up 0 more_vert Thanks @gogo827jz for the notebook. Can you explain the part where you said \"autoencoder when predicting\"? Can you please share the inference notebook? MXK Posted 4 years ago · 1088th in this Competition arrow_drop_up 1 more_vert @gogo827jz I have followed most of your work and discussions, and it's actually your Linkedin post that brought me here, so once again I'm gonna say, congrats and well deserved as well as thank you for sharing these insights with us =) marketneutral Posted 4 years ago · 2239th in this Competition arrow_drop_up 1 more_vert Thanks very much for sharing. I'm excited to see you so high in the LB as you have been so helpful in sharing ideas and notebooks along the way. I'm not trolling here -- this is an honestly curious question -- what happened to the guy who so aggressive with you in the forums saying a deep learning approach was nonsense overfitting and a rules-based approach was the way to go? Where is he on the leaderboard thus far? Yirun Zhang Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 4 more_vert I only remember that I was criticised for publishing the neural network starter notebook which uses the vanilla group CV split (not purged time-series CV) and it potentially contains time-leakage and overfitting problems. Actually, he was right on this, so I added some explanation later. DSs404 Posted 4 years ago arrow_drop_up 1 more_vert nice sharing！ Thrinesh Duvvuru Posted 4 years ago arrow_drop_up 1 more_vert Awesome @gogo827jz .keep rocking Olusesi Adebisi Posted 4 years ago arrow_drop_up 1 more_vert The detail about the submission is quite intuitive but I need insight into the coding orientation. I think the stated label leakages may be prevented if the constituent units of the code are put into respective functions. What do you think? My own contribution of function training: https://www.kaggle.com/olusesiadebisi/jane-street-market-function-trained provided an 80% accuracy. Please advise am still learning. Lucas Morin Posted 4 years ago · 626th in this Competition arrow_drop_up 1 more_vert Thanks for sharing. This one hurts a bit as it doesn't really seems too far from my NN solution taht completely broke down. Mind to elaborate a bit on how you choose your architecture ? What was your hyperoptimisation strategy (finding optimal parameters globally / on each fold) ? Yirun Zhang Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 6 more_vert I use hyperopt to search the global hyperparameter set. It improves the score a lot. Pratik Poudel Posted 4 years ago · 1304th in this Competition arrow_drop_up 2 more_vert I believe doing regression and then feeding the predictions to a good classification model would do good. Besides that some feature engineering using combinations of features would do a great job that i was unable to do. I did try some combinations but the code exceeded time limit. I did submit 1 overfit classification model and 1 regression-classification model. I got the results classification = 3718.025(6842.609 Previous Leaderboard) and regression-classification=4678.781(6109.097 previous) Yirun Zhang Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 1 more_vert classification = 3718.025(6842.609 Previous Leaderboard) Is this one your overfitting classification model? Its score looks not that 'overfitting' because lots of overfitting models score 10000+ in the previous LB. 3 more replies arrow_drop_down Olusesi Adebisi Posted 4 years ago arrow_drop_up 0 more_vert Thank you @gogo827jz for sharing.Congratulation on your leadership position. Even in front of criticism you showed high humility and learned. You see criticism as feedback for improvement. I like your strategy and cannot help but try to follow it. Congratulations once again. nanpolend Posted 2 months ago arrow_drop_up 0 more_vert 讚非常好的模型我自己找時間跑看看 Ali_Haider_Ahmad Posted 4 months ago arrow_drop_up 0 more_vert A lot to learn from you 😁 HarrisonPan Posted 6 months ago arrow_drop_up 0 more_vert this solution is awesome! devil_monarch Posted 7 months ago arrow_drop_up 0 more_vert goated Abhishek Kumar Posted 8 months ago arrow_drop_up 0 more_vert Wow excellent C4rl05/V Posted 3 years ago arrow_drop_up 0 more_vert Hello @gogo827jz , how do you find these strategies? Super interesting reading Hao Chen Posted 3 years ago arrow_drop_up 0 more_vert Simple but unexpectedly effective method. Thanks! Too many requests error Too many requests",
      "Jane Street Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Jane Street Group · Featured Code Competition · 4 years ago Late Submission more_horiz Jane Street Market Prediction Test your model against future real market data Jane Street Market Prediction Overview Data Code Models Discussion Leaderboard Rules Martin BB · 3rd in this Competition  · Posted 4 years ago arrow_drop_up 40 more_vert 3rd place solution: Ensembles of deep (49 layer) MLPs After many initial experiments with more or less „complicated“ approaches I ended up with almost no feature engineering, no CV, but an esemble of 15 deep MLPs, each of them consisting of 49 layers. Data: Remove first 85 days Rows with weight=0 were included NaN filling: median conditioned on f0 (I think this was actually my best idea throughout this competition :D ) Model: input block: batchnorm + logarithmic feature extension (→ 130 std features, 130 log features) block 0: 3 dense layers with 100 units, batchnorm, dropout (0.35) and mish activation block 1-23: 2 dense layers with 100 units, batchnorm, dropout (0.35) and mish activation skip connections between Block 0 and Block i (i=[1,..,23] output: Dense layer with 5 units (all 5 ‚resps‘) - sigmoid activation 15 ensembles of these deep models action: mean over all 15x5 outputs; threshold = 0.5 Training: batch size = 30k Adam with lr = 1e-3 200 epochs loss: mixture of BCE (label smoothing = 0.1) and a „utility-like“ loss Inference: The models are converted to tf-lite interpreters. This really speeds up submission. Otherwise it never would have been possible to evaluate so many deep models. Validation: The basic model shape and the hyperparameters where chosen from a simple train/ validation split on my local machine. In between I also used a GKF CV but I wasnt really happy about it, so I went back to this very simple train / validation split (train: 85-300, val: 350-500). To reduce the impact of different seeds, I rerun my tests up to 8 times. This gave me quite a good estimate of how new implementations really perform. For the final model, the number of epochs for training were found by retrain the models and submit them. So one could say I used the public LB data for early stopping. As we have 1 million rows in the public set, this seems legit to me. Ensembles: It is often said that ensembles only improve the performance if the models learn something different, i.e. if they are trained with different data. I think this is true in general, but here the situation is a little different. We saw an extreme dependence of seeds for different public models. I also found this for my single models. Using many models trained from different seeds kinda solves this issue. Like fighting fire with fire, fight randomness with randomness. Please sign in to reply to this topic. comment 10 Comments 1 appreciation  comment Hotness walter Posted 4 years ago · 1135th in this Competition arrow_drop_up 2 more_vert Thanks for your reply! But I am wondering, if you skip the connect between Block 0 and Block 1-23, how does Block 1-23 connect to the input layer? Martin BB Topic Author Posted 4 years ago · 3rd in this Competition arrow_drop_up 0 more_vert No problem. Block 0 is connected directly to the input block, block x fox x={1,..,23} is connected to residual of block x-1 and block 0. I hope its clear, if not, I will plot the model :) walter Posted 4 years ago · 1135th in this Competition arrow_drop_up 2 more_vert Thanks for sharing! For the following layers, did you just stack them together so we get 49 layers in total? I wonder why you skip the connections between Block 0 and Block i? So each of them are connected to the input layer directly? block 0: 3 dense layers with 100 units, batchnorm, dropout (0.35) and mish activation block 1-23: 2 dense layers with 100 units, batchnorm, dropout (0.35) and mish activation skip connections between Block 0 and Block i (i=[1,..,23] Martin BB Topic Author Posted 4 years ago · 3rd in this Competition arrow_drop_up 2 more_vert Yes, correct. Each block is connected to the previous block, so there are 49 layers in total per model. Regarding the skip connections, I was wondering when somebody would ask this question . Actually, it was by accident, but I only realized it after I run the training and I found it does a quite OK job. I think this kind of skip connections, where the residual between layer output and low level features is used, can serve as additional regulariziation, assuming the low level features are still noisy. Unfortunatelly, there was no time left to check how standard skip connections perform. Paul Fornia Posted 4 years ago · 23rd in this Competition arrow_drop_up 2 more_vert Interesting stuff! I'm extremely fascinated by the diversity of approaches that seemed to do well. E.g., I was shocked to hear that some high-ranking teams used some non-DL approaches. And it's interesting to hear that you got the very deep networks to work, I could not. Did you do anything fancy like pre-training layers? I notice you used many epochs (I usually early-stopped after ~10), maybe that's what I was missing. I also used a very similar, simple Train/Validate set, which I was nervous about (I split at day 400, but essentially the same). I didn't like that k-fold CV looked backwards in time, and I didn't like that the purged, out-of-time thing required significantly smaller time windows. I felt the simple approach, which could get a full 400 days into training, and a bigger validation window, was the closest thing to what was getting deployed. There really were no perfect options for cross validation. I agree that the fillnas by feat_0 was a good idea. It's basically a very simple version of training an auxiliary model to fillnas based on other columns. I tried a bunch of stuff with feat_0, like two entirely separate models for each feat_0, and flipping signs of some features for short trades, but none of that panned out. Nice work! Martin BB Topic Author Posted 4 years ago · 3rd in this Competition arrow_drop_up 3 more_vert No, I didn't used pretrained layers, just random init. The number of epochs is quite dependent on the batchsize. When I used ~5k samples per batch, then 10-20 epochs yield the best results. But then I found this nice notebook notebook , which motivated me to test larger batchsizes. I totally felt the same about the k-fold and purged CV. And actually I was also quite \"nervous\" about using such a simple approach, but I am happy that I am not the only who did so. Well, I also tried many different things for feat_0,  but nothing really worked out. Even the models that did not use f0 worked pretty well. I am curious if anybody find some way to make real advantage of feat_0. Thanks :) vvm Posted 4 years ago · 2829th in this Competition arrow_drop_up 0 more_vert Really interesting. What were your train utility scores if I may ask? Martin BB Topic Author Posted 4 years ago · 3rd in this Competition arrow_drop_up 1 more_vert between 13k and 14.5k vvm Posted 4 years ago · 2829th in this Competition arrow_drop_up 0 more_vert Thank you.  With 170<200 epocs and ensemble of 12-14 models. I got 42% of the total training utility score on a 60/40 CV split. Albeit the 40% part had only 20% of the total score. It wasn't a contiguous split. Didn't use that model for the final submission thinking it was an overfit. It had 4400+ on a public lb. I would be curious to see how that model would do against the final data set though. I hope they will make it public after the competition is over. For the final submission, I used the models that had a 19%-20% (~44K) in train utility score and had higher lb scores. They took between 50-90 epocs to train (different folds and seeds). Appreciation (1) walter Posted 4 years ago · 1135th in this Competition arrow_drop_up 0 more_vert oh I understand, thank you! Too many requests error Too many requests",
      "Jane Street Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Jane Street Group · Featured Code Competition · 4 years ago Late Submission more_horiz Jane Street Market Prediction Test your model against future real market data Jane Street Market Prediction Overview Data Code Models Discussion Leaderboard Rules Neil Hazra · 10th in this Competition  · Posted 4 years ago arrow_drop_up 89 more_vert 10th Place Solution: Geometric Brownian Motion and Mixture Density Networks Given that the our solution is still number 2 after the second rerun, we have slightly more confidence to write a post about the methodology. we also use regression which is, I think, unique in this competition. Key Points of the Method: Geometric Brownian Motion with Drift Fitting Mixture Density Network with negative log likelihood loss function These two are the only significant key differences I have seen between our solution and the other posted notebooks. Geometric Brownian Motion with Drift Fitting: One of the most basic models used when modeling financial assets is geometric brownian motion with drift. Assuming that our data here roughly follows such a distribution, sampling the value of the price at a given time gives us very little information on this financial asset because of the stochastic nature of the process.  The drift in price however is much more fundamental to the particular stock than the return at any particular time so this is a great feature to engineer. We use this as a target feature when training the neural network. The following code computes the drift factor for each asset in the training through MLE on resp_1 to resp_4. This notebook really influenced the code: https://www.kaggle.com/pcarta/jane-street-time-horizons-and-volatilities Here is my code: I am happy to answer questions in comments but the above notebook really explains everything, all I did was use a more complex model (with drift) and do the same maximum likelihood estimation, basically converting math to pytorch. import torch from tqdm import tqdm mus = None sigmas = None horizons = None def train_gbm(): device = torch.device( \"cuda:0\" if torch.cuda.is_available() else \"cpu\" ) dT_init = np.ones(len(columns)- 1 ).astype(float) sigma_init = np.random.randn(len(resps))*. 01 + 1 mu_param = torch.tensor(np.random.randn(len(resps))*. 01 , device=device).requires_grad_(True) with torch.no_grad(): dT_param_log = torch.log(torch.tensor(dT_init)).to(device).requires_grad_(True) sigma_param_log = torch.log(torch.tensor(sigma_init)).to(device).requires_grad_(True) W = torch.tensor(resps.values + 1 , device=device).float() W_init = torch.cat((torch.ones((W.size( 0 ), 1 )).to(device), W[:,:- 1 ]), dim = 1 ) opt = torch.optim.Adam([dT_param_log, sigma_param_log, mu_param], lr = 0 . 001 ) iteration = 0 for iteration in tqdm(range( 15000 )): opt .zero_grad() # add the fixed 1 at the beginning of the time deltas dT_log = torch.cat([torch.zeros( 1 , device=device) , dT_param_log]) dT = torch.exp(dT_log).view(( 1 ,- 1 )) sigmas = torch.exp(sigma_param_log).view((- 1 , 1 )) mus = mu_param.view((- 1 , 1 )) ln_W = torch.log(W) ln_W_init = torch.log(W_init) log_prob = -ln_W - torch.log(sigmas) - 1 / 2 * torch.log(dT) - (ln_W - ln_W_init - (mus - 0 . 5 *sigmas** 2 )@dT)** 2 /( 2 *sigmas** 2 @ dT) row_liklihood = log_prob.sum(dim = 1 ) total_liklihood = row_liklihood.mean(dim = 0 ) negative_total_liklihood = -total_liklihood if (iteration % 1000 == 0 ): print (negative_total_liklihood) negative_total_liklihood .backward() opt .step() times = torch.exp(torch.cat([torch.zeros( 1 , device=device) , dT_param_log])).cumsum(dim = 0 ) mus = mu_param.detach().cpu().numpy() sigmas = torch.exp(sigma_param_log).detach().cpu().numpy() horizons = times.detach().cpu().numpy() return mus, sigmas, horizons mus , sigmas, horizons = train_gbm() import scipy from scipy import special expected_return = np.exp(mus * horizons[ 3 ]) - 1 lam = sigmas * np.sqrt(horizons[ 3 ]) row = (mus - 0 . 5 *sigmas** 2 )*horizons[ 3 ] probability_greater_zero = 0 . 5 - 0 . 5 * special.erf(-row/(np.sqrt( 2 )*lam)) content_copy This gives me the expected value of resp assuming that the asset follows a geometric brownian motion distribution as well as the drift parameter and the probability that resp is greater than 0. All of which could be used to fine tune the training of the network. I played around with all three as targets and also thought of label smoothing techniques for binary classification using the sigma parameter. Mixture Density Network: A mixture density network is just a fancy name for a neural network that predicts a conditional probability distribution as an output. I assumed that the distribution of my target variables (resp_1, resp_2, resp_3, resp, resp_4, drift, expected_return) of  financial asset conditioned on the 130 features provided is approximately normal. I don't have a very good justification for this assumption but considering that the unpredictable inherent randomness of financial asset is most likely a combination of a large number of smaller random variables we can use a central limit theorem argument to justify this. The MDN predicts the parameters to a multivariate normal distribution and it does so by returning a lower triangular matrix of shape 6x6 and a mean vector of length 6. The six components are (resp_1, resp_2, resp_3, resp, resp_4, drift) respectively. And the loss function is negative log likelihood. Here is the code for anyone interested: class MDNDecisionMaker (nn.Module): def __init__ ( self, in_dim, out_dim, hidden_dim, dropout_prob ): super ().__init__() self .hidden = hidden_dim self .out_dim = out_dim self .dropout = dropout_prob self .feed_forward_network = nn.Sequential(\n            nn.BatchNorm1d(in_dim),\n            nn.Linear(in_dim, self .hidden), #1 nn.SiLU(),\n            nn.Dropout(p = 0.3 ),\n            nn.Linear( self .hidden, self .hidden), #2 nn.SiLU(),\n            nn.Dropout(p = 0.3 ),\n            nn.Linear( self .hidden, self .hidden), #3 nn.SiLU(),\n            nn.Dropout(p = 0.3 ),\n            nn.Linear( self .hidden, self .hidden), #3 nn.SiLU(),\n            nn.BatchNorm1d( self .hidden)\n        ).double() # predict mean value of multivariate gaussian distribution self .mean_network = nn.Sequential(\n            nn.Linear( self .hidden, self .hidden), #3 nn.SiLU(),\n            nn.Linear( self .hidden, out_dim)\n        ).double() # predict non diagonal lower triangular values of matrix self .cholesky_nondiag_sigmas_network = nn.Sequential(\n            nn.Linear( self .hidden, self .hidden), #3 nn.SiLU(),\n            nn.Linear( self .hidden, out_dim*out_dim), #2 ).double() # predict the diagonal elements, these must be non zero to ensure invertibility self .cholesky_diag_sigmas_network = nn.Sequential(\n            nn.Linear( self .hidden, self .hidden), #3 nn.SiLU(),\n            nn.Linear( self .hidden, out_dim)\n        ).double() self .bceloss = nn.BCELoss() def forward ( self, x, return_covariance = False ):\n        parameters = self .feed_forward_network(x.double())\n        means = self .mean_network(parameters)\n        cholesky_lower_triangular = torch.tril( self .cholesky_nondiag_sigmas_network(parameters).view(- 1 , self .out_dim, self .out_dim), diagonal = - 1 )\n        cholesky_diag = torch.diag_embed(torch.exp( self .cholesky_diag_sigmas_network(parameters)).view(- 1 , self .out_dim))\n        cholesky_sigmas =  cholesky_diag + cholesky_lower_triangular if return_covariance:\n            covariances = torch.bmm(cholesky_sigmas, torch.transpose(cholesky_sigmas, 1 , 2 )) return mean, covariances return means, cholesky_sigmas def liklihood_loss ( self, means, cholesky_covariances, samples ):\n        batch_dim = samples.size()[ 0 ] \n        means = means.double()\n        samples = samples.double()\n        cholesky_covariances = cholesky_covariances.double()\n        log_probs = - 1 / 2 * torch.bmm((samples-means).view(batch_dim, 1 , self .out_dim), \n                                     torch.cholesky_solve((samples - means).view(batch_dim, self .out_dim, 1 ), cholesky_covariances)).view(- 1 , 1 ) \\\n                    -torch.log(torch.diagonal(cholesky_covariances, dim1=- 2 , dim2=- 1 )). sum (dim = 1 , keepdim = True ) \\\n                    - self .out_dim/ 2 * 1.83787706641 log_liklihood = log_probs. sum (dim = 0 )/batch_dim return -log_liklihood content_copy A few key points of the network include: Outputting a valid positive definite gaussian kernel required me to output a lower triangular matrix with a positive diagonal. Then I can multiply this by its adjoint to get a positive definite matrix (this is called the cholesky decomposition). Getting the right type of matrix required some reparameterization with exponentials to ensure I got positive numbers. The mixture density network is very very difficult to train. In particular NaN's pop up everywhere because of all the inverses of matrices. Dropout was carefully tuned to prevent overfitting while also not causing erratic optimizer behaviour that would drive the loss to NaNs. There probably was a lot of scope for tuning the architecture of the network, but I went with a reasonable choice of layers and breadth (SiLU also looked good from the public kernels). My partner used k-fold purged cross validation to choose the best neural network model to select. And we tried ensembling models from different folds to boost performance. Predictions We played around with this for a while, I used monte-carlo simulation where I sampled from the geometric brownian motion process I parameterized earlier. It turned out that nothing worked better for me than taking the mean of the output mean vector from the neural network and trading if that was positive and not trading if it was negative. Finance Please sign in to reply to this topic. comment 12 Comments Hotness Diganta Posted 8 months ago arrow_drop_up 1 more_vert This is a great strategy for a lot of problems! Thanks for sharing! Geralt_of_Rivia Posted 4 years ago arrow_drop_up 1 more_vert Thank you for sharing!! That's very elegant for a math student:) I am fairly a newbie on Kaggle, and I am a bit confused with how you combine these two parts (GBM and MDN) together? If I understand correctly, those estimated drift parameters are served as samples that would feed into MDN, i.e. (resp_1, resp_2, resp_3, resp_4, resp, drift). Not sure if I am correct - since it's a little weird that takes the parameter (in GBM) as a realization (sample) to estimate another parameter (in MDN)… Perhaps I'm wrong with your model? Neil Hazra Topic Author Posted 4 years ago · 10th in this Competition arrow_drop_up 2 more_vert I think of the GBM step as a version of feature engineering as I am creating a new target (drift) with the given targets (resp_1…resp_4, resp). The intuition behind this feature is that assuming that geometric brownian motion models the price of the financial asset, the drift parameter is much more fundamental to the financial asset than the price at any given time. Thus, I hypothesis is that drift is more correlated with the 130 provided features than resp_i. This means that the conditional distribution of drift given the 130 features is more peaked. While the GBM step found the optimal estimate for drift given resp_i, the MDN is used to figure out the conditional distribution of drift given the 130 features. Once I found the conditional distribution of drift I can make a probabilistic decision on my trading or passing based on that distribution. I think the confusion arises because I am estimating drift in the GBM step and then again training a neural network to estimate the drift again. But estimating drift in the GBM step I am using resp_i and I am pretty sure I am estimating drift with high accuracy. Think of this as a preprocessing step, like filtering an image for noise before doing image classification. On the other hand, the MDN predicts resp based on the 130 features. 4 more replies arrow_drop_down Mingjie Wang Posted 4 years ago · 1st in this Competition arrow_drop_up 2 more_vert COOOOOOOOOOOOOOOOOOOOOOOOOOOOOL👍👍👍 This plan is so amazing Martin BB Posted 4 years ago · 3rd in this Competition arrow_drop_up 2 more_vert Thanks for sharing, really cool stuff you implemented! Congrats! I guess the NaN's are related to the inverse of matrices which are close to singularity? Because in this case, the singular values of the matrices would be quite small, the condition number high and the system rather instable. I was wondering if you tested / used regularization techniques like Tikhonov or Singular Value Decomposition +clipping/discarding of small singular values. I would assume this helps with NaNs. I was also wondering about the 'assets' in your explanation. Did you assume that each row/trade corresponds to the same asset or did you form assets by grouping single trades by certain features (like feature_41-43)? Neil Hazra Topic Author Posted 4 years ago · 10th in this Competition arrow_drop_up 1 more_vert I wasn't able to fully pinpoint where the NaN's were coming up. I had full control over the diagonal entries in my lower triangular matrix (these are the eigenvalues) so I initially (unsuccessfully) used an ELU (exponential linear unit) activation function to parameterize the eigenvalues to have some minimum value (10^-3 for instance). I think this would have have a similar effect to clipping singular values.  My best guess is that when the neural network predicted a near singular covariance matrix, the gradient of the loss function would explode. A stable optimizer, higher precision and a small dropout made the network outputs less noisy and less likely to predict a near singular covariance matrix. Every row in the training set I treated as an individual asset. I didn't use any of the features aside from inputs to my network. Andy Posted 4 years ago · 2467th in this Competition arrow_drop_up 0 more_vert Thanks for sharing! Congrats! It is just a surprise that Black-Scholes stuff works for HFT. Great lesson to learn This comment has been deleted. Too many requests error Too many requests",
      "Jane Street Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Jane Street Group · Featured Code Competition · 4 years ago Late Submission more_horiz Jane Street Market Prediction Test your model against future real market data Jane Street Market Prediction Overview Data Code Models Discussion Leaderboard Rules Andy · 15th in this Competition  · Posted 4 years ago arrow_drop_up 42 more_vert Thanks! Becoming (Perhaps) One of the Youngest Competition Master My first gold medal! A huge thank you to the Kaggle community, without the discussion posts and public notebooks, I could not have reached this point! I posted an article writing about the story and some tips I learned along the way . A Brief Summary of My Journey Hi, I'm Andy Wang, a 14 year old high school freshman. I became interested in Data Science and Machine Learning about a year ago. I found Kaggle hoping to enhance my skills and gain more experience of the industry. With my friend Andre Ye , we started joining Kaggle competitions hoping to learn and discover new things. It was a shock to us when we received our silver medal in the first competition that we attended from a huge shakeup. Afterwards, we joined 2 more competition and was amazed by the results. I never thought that I could reach this point, the biggest thanks to the Kaggle community and my friends Andre. The knowledge I acquired from attending the competitions is more than that of the results. Finally, the biggest lesson that I've learned from competitions is that simple solutions can go a long way. With the right cross validation scheme and a stable CV, the results were never too bad (Especially in this competition). Again, I cannot thank the Kaggle community enough for teaching me and leading me through the fascinating field of Machine Learning. (Aiming for Competition GM now! 😃 ) Please sign in to reply to this topic. comment 10 Comments Hotness Sunnymoon Sultan Posted 4 years ago arrow_drop_up 1 more_vert Congrats on this amazing achievement!!! keep up the good work🔥🔥 Tarlan Nazarov Posted 4 years ago · 236th in this Competition arrow_drop_up 1 more_vert Wow! So much knowledge at such a young age… With this speed you will get to top 1 in one year 😉 Amol Ambkar Posted 4 years ago arrow_drop_up 1 more_vert Great Achievement … Krish Yadav Posted 4 years ago · 1748th in this Competition arrow_drop_up 1 more_vert Wow your achievements are fabolous, all the best for GM YvonneF Posted 4 years ago arrow_drop_up 1 more_vert @andy1010 Congrats Looking forward to seeing your next achievement Hoang Pham Viet Posted 4 years ago arrow_drop_up 1 more_vert I'm inspired by your enthusiasm, thanks for sharing! fireflies Posted 4 years ago arrow_drop_up 2 more_vert Congrats =)) Youngest Competition Master! Tien Hoang Posted 4 years ago arrow_drop_up 0 more_vert Congratulation. Amazing job. Sharlto Cope Posted 4 years ago · 1972nd in this Competition arrow_drop_up 0 more_vert Congratulation! Devashree Madhugiri Posted 4 years ago arrow_drop_up 0 more_vert Congratulations on this achievement!! Too many requests error Too many requests",
      "Jane Street Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Jane Street Group · Featured Code Competition · 4 years ago Late Submission more_horiz Jane Street Market Prediction Test your model against future real market data Jane Street Market Prediction Overview Data Code Models Discussion Leaderboard Rules Paul Fornia · 23rd in this Competition  · Posted 4 years ago arrow_drop_up 17 more_vert 24th place solution: Couple more tricks, and how I kinda sorta cheated Inspired by @dmitryvyudin 's recent post, I thought I'd also comment on a couple of tricks I used that I haven't seen anyone else mention. I think the first couple are good ideas, while the last is getting into a bit of a cheating grey area, and is certainly not deployable. Curious to hear if anyone else tried any of these! 1. Dynamic Label Smoothing based on resp From very early on, I was frustrated that regression approaches were not working, and felt that classification was throwing away valuable information contained in the Resp targets. When I came across label smoothing (1 -> 0.99, 0 --> 0.01), I had the idea to use resp to control how much smoothing to do. So my classification labels were actually sigmoid(a*resp), where a is a large, tunable parameter. This way, if resp was very close to zero, the 0/1 label would actually be blunted to something a bit closer to 0.5. This was repeated for all 5 resp targets, fed into multi-target NN, and I simply averaged the final predictions. This had a measurable benefit to my CV scores. 2. Log the weights in my CV. The most challenging part of this contest IMO was measuring which models were actually doing well. I would constantly get conflicting advice on different runs, even when ensembling many random seeds together or repeating experiments. In addition to focusing on metrics like AUC, something else that helped was log-transforming my weights for my internal CV utility function. This seemed to help mitigate the effects of large-weighted trades without ignoring them entirely. (and as many others have mentioned, I almost entirely ignored the public LB. Both my submissions were in the bottom half in public LB rank.) 3. Use weights to triage to faster/slower models in deployment. I thought this was pretty clever, but it's unclear how much it actually helped me. If you look at the weight distribution and the utility score, it turns out that a few observations make up the vast amount of the total weight. Because of this, you can actually deploy much better/slower models on just a few obs, and get massive benefit. For example, obs with a weight of 30 or more only make up 3 % of the observations, but a third of the totalweight. I threw an ensemble of 20 MLPs, 5 Densenets, and 5 resnets at these observations. The lowest weighted obs barely mattered, so they got 1 MLP, and ran in 1/30th the time. In the end, my deployment run time was FAR below the max allowed, whereas probably would have timed out if I had thrown the full 30-model ensemble at every observation. 4. Head I win, tails you lose. From early on, I had decide that I was going to use my two submission for opposing thresholds. (An aside: I was never convinced that the utility score actually penalized risk-taking like the hosts intended. The sharpe ratio balances risk and return, but because it is multiplied by total return, the return term is actually in the utility equation twice, and thus outweighs the volatility penalty in the denominator in practice. I found no evidence that a \"conservative\" approach in my submissions ever helped. But maybe I'm missing something…) All that is to say: I think 0.5 is the best/correct threshold if you have to pick one. However, because we have two submissions, I submitted one that was \"bearish\" and one that was \"bullish\". So basically one submission with a threshold of 0.49, and one with a threshold of 0.51. But I took this one step further: If feat_0 is in fact long/short or buy/sell, then shorts should be more aggressive in a bad market, and longs should be more aggressive in a good one. So in the end, my \"bearish\" submission uses a 0.49 threshold for feat_0 == 0 obs, and a 0.51 thresh for feat_0 == 1 obs, and my other submission does the opposite. If the market booms relative to the training data, one of my submissions will do well. If it crashes, the other will do well. I could have gone more aggressive with something like 0.47 and 0.53, but I also wanted to do reasonably well if the market did exactly as well as it did during the training data. If Jane Street can find a way to trade in two parallel universes, and only suffer the consequences of the one that works out better for them, I'd highly recommend this trading strategy. Other than these tricks, everything I did was pretty similar to stuff found in published notebooks. I ensembled MLPs, densenets, and resnets, with the most weight on MLPs. 3-4 layers, dropout, swish, very few epochs. First 85 days INCLUDED, and weight == 0 observations included. I'm not sure why so many folks took this data out. Curious to hear feedback, and looking forward to the next 6 months of suspense! Please sign in to reply to this topic. comment 4 Comments Hotness Lucas Morin Posted 4 years ago · 626th in this Competition arrow_drop_up 0 more_vert Regarding #3 : I went along the same path but at some point I figured that (an esemble of) model so big it would need a small model frist would likely be overfit. I ended up reducing my models size and dropping the first small one from my main submission. As a joke I submitted the small model too. As you can guess the big model crumbled down and now I am left with a very simple model to navigate the worst economical crisis in a century… Regarding #4 : regarding threshold my main idea was to try to make them 'adaptative' i.e. building macro variables (long short indicators, Risk-On Risk Off indicators…) or using micro variable (mainly volatility / inverse of the weights) and see how it impacted the threshold. After some reflexion it occured to me that I could just put those variables in the model, make sure to calibrate the model in probability and use a 0.5 threshold. Not sure that your idea is good but it might work for the wrong reason : I suspect our training data set has been rebalanced and that the test might be more unbalanced in terms of feature 0. A small bias towards long position might be the key to win this competition. Paul Fornia Topic Author Posted 4 years ago · 23rd in this Competition arrow_drop_up 1 more_vert I'm not totally sure what you mean by \"model so big it would need a small model first\", but yes, I also couldn't find any big/complex model that performed particularly well. Everything fancy I tried basically overfit. That's why my \"big\" model was actually just an ensemble of a ton of relatively simple networks with different seeds. Sorry you got stuck with a simple model, but if it makes you feel better, the stock market seems basically unaware of the current crisis (at least since August), so maybe a simple model will do well over the next 6 months… very curious to see if the shakeup continues. CharlieSCC Posted 4 years ago · 352nd in this Competition arrow_drop_up 0 more_vert Quite interesting ideas, especially the third one. Paul Fornia Topic Author Posted 4 years ago · 23rd in this Competition arrow_drop_up 0 more_vert Thanks! I didn't have any super slow models that were performing dramatically better, so I don't know if it really helped me that much. But it did help me sleep easier knowing that a timeout error was far less likely. Too many requests error Too many requests",
      "Jane Street Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Jane Street Group · Featured Code Competition · 4 years ago Late Submission more_horiz Jane Street Market Prediction Test your model against future real market data Jane Street Market Prediction Overview Data Code Models Discussion Leaderboard Rules Dmitry Yudin · 39th in this Competition  · Posted 4 years ago arrow_drop_up 40 more_vert 39th Place - Solution Overview & Code I thought I’d post an overview of my solution while I am relatively high on the leaderboard which might not last for long. My solution is based on a simple MLP with some elements that I have not seen posted before including a bit of feature and target engineering and weight-dependent voting in post-processing. 1.    CV Strategy For me this is by far the most important part – with the amount of noise in the data everything else would not be possible to evaluate without a decent CV strategy. I’ve spent a huge chunk of time trying to find a validation approach that gave at least somewhat consistent and logical results. I eventually landed on using mean OOF utility scores similar to what I posted here . From the very start I decided not to use the public LB other than to check that the code runs and does not time out – otherwise the temptation to overfit the public LB would be too difficult to resist. I used all the data for both training and validation based on GroupKFold split using dates as groups – this was a risky decision as I assumed there was no leakage between training and validation dates. I have a few reasons to think that this leakage indeed was not a problem: •    First, this seems to be an HFT competition – as @gregorycalvez noticed in this excellent notebook , the NaN patterns suggest that the data contains some rolling intraday features. This does not mean that there are no other rolling features covering many days, but from my point of view, it does make it less likely/important. •    I tried splitting data into groups of consecutive days – from 1 to 100 days in one group. In case of leakage you could expect to get better results with 1-day groups compared to 100-day groups because there is potentially much more interaction between training and validation data. But after tons of experiments I just could not see this effect. For the actual training I used groups of 50 consecutive days. •    At the beginning I tried splitting data completely randomly, i.e. using trades from the same date both for training and validation. In this setup the intraday leakage was noticeable but even then my model had to run for a lot of epochs to pick up this information with validation scores continuing to improve even after 100’s of epochs. I have not observed this behavior with GroupKFold . 2.    Feature Engineering There were a number of discussions about the meaning of feature_0 (buy/sell, long/short?). I have no idea what the correct answer is – my hypothesis is that it is produced by a separate JS model that selects the trading opportunities. And if this is true then its recent history might be indicative of some sort of market condition. This gave me an idea to build a series of rolling 'lag' features based on feature_0 and this resulted in a modest but noticeable improvement in the CV scores. I have also added a few other features based on the ‘clock’ feature_64 that together also improved the CV score: •    Binary feature representing part of the trading day (before/after lunch) •    Number of trades suggested by JS algorithm earlier today (for the first part of the day) or after lunch (for the second part of the day) - the intuition here that together with 'clock' this feature could also represent a market condition (e.g. more trade opportunities = more volatility) •    'Gradient' of feature_64 with respect to timestamp - similar intuition to the previous point 3.    Target Engineering Like many others, I have noticed that treating this task as multi-label classification leads to better results compared to trying to predict just one label - resp . I have made a couple of adjustments compared to most public notebooks: •    I did not use resp_4 - my CV always went down when I tried to add it. This could be to some extent explained by the fact (conjecture?) that the time horizon of resp_4 is longer than that of resp . •    Instead, I added the mean value of resp , resp_1 , resp_2 and resp_3 as a separate target which did improve the CV score. This can be thought of as a proxy for the general direction of returns over the whole resp time horizon. 4.    Model & Training Nothing interesting architecture-wise – very basic 3-layer MLP with batch normalization and dropout. For optimization I used LAMB with Lookahead – according to the paper, LAMB is supposed to work well with large batch sizes and it seemed to outperform other optimizers I tried. 5.    Inference & Post-Processing I did not like the idea of playing with the threshold – values other than 0.5 seem artificial and lack any intuitive meaning. Instead, for most of the competition, I used a ‘qualified’ majority voting. I.e. only accepting the trade opportunity if 66% of models ‘voted’ for it. Later on, I started taking the weight of the trading opportunity into account - the more the weight of the opportunity, the more confident I have to be to act on it. This results in slightly better utility scores since the utility score formula punishes high variance. Overall I used 50 models (5 folds & 10 seeds) with each model having 5 votes – one vote per target. 6.    Code •    Training - https://www.kaggle.com/dmitryvyudin/jane-street-tf-lamb •    Inference - https://www.kaggle.com/dmitryvyudin/jane-street-weighted-voting-inference 7. Other Some of the things I tried but could not make work based on my CV strategy: •    Treating the problem as regression as opposed to classification – much better results with L1 loss than MSE but still worse than classification •    Using weight or log(weight) in the loss function •    Adding noise to targets •    Knowledge distillation as a target de-noising technique – this looked very promising but I found a leakage literally hours before the submission deadline and had to revert to earlier, simpler models • AdaHessian optimizer (was very excited about the results initially but in the end LAMB performed better) •    TabNet – found it too slow for this competition Please sign in to reply to this topic. comment 9 Comments Hotness Lucas Morin Posted 4 years ago · 626th in this Competition arrow_drop_up 5 more_vert Thanks ! Posts like yours is how we might get something from this competition. Andy Posted 4 years ago · 2467th in this Competition arrow_drop_up 2 more_vert Thanks a lot for sharing your insights! Tons are learned from your post but I was just surprised how easily I was convinced to use the group-purged-time-series CV strategy without many testing or experiment just because it is similar to the actual situation. Leakage is definitely a problem but it should be proved by CV results instead of \"belief\" of existence. Thanks a lot for your sharing about CV strategy and I guess I will rerun my CV and be more careful on how valid a CV strategy is in the future. Thanks again! Wilmer E. Henao Posted 4 years ago · 1110th in this Competition arrow_drop_up 2 more_vert You didn't remove any variables? Dmitry Yudin Topic Author Posted 4 years ago · 39th in this Competition arrow_drop_up 3 more_vert I used all the features. I played with feature selection for a bit but did not see any meaningful performance gains. 5 more replies arrow_drop_down Too many requests error Too many requests",
      "Jane Street Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Jane Street Group · Featured Code Competition · 4 years ago Late Submission more_horiz Jane Street Market Prediction Test your model against future real market data Jane Street Market Prediction Overview Data Code Models Discussion Leaderboard Rules hedwig100 · 44th in this Competition  · Posted 4 years ago arrow_drop_up 9 more_vert Current 29th place approach I'm relatively new to kaggle, so I'm surprised to be high in the leaderboard (maybe because of much luck). I learned a lot from this competition. I would like to share my approach. I published inference notebook and training notebook . Later in this competition, I noticed ensembling models boosts my local CV. I think this is caused by stable prediction. So, I trained various models, and ensembled these models.  Details are shown below. Training Strategy PurgedGroupTimeSeriesSplit(5fold and 20gap) but maybe leakage was caused when training encoder… watched AUC and UtilityScore in each fold. used Early Stopping by valid-auc Preprocess fill NaN by 0 and FeatureNeutralization(p=0.25) fill NaN by mean denoising autoencoder Model simple NN, CNN and DenseNet. I also used this famous notebook 's model architecture. I trained this model with my CV strategy. In first submission, I used my 3model. In second submission, I used my 3model + 1pytorch model For each model, I used weight trained in last fold, and weight trained with all data(except for weight = 0 and first 85days) second submission achieves higher score. Ensemble weight average weight was decided by CV. Thank you for reading! Please sign in to reply to this topic. comment 2 Comments Hotness Mateus Coutinho Marim Posted 4 years ago arrow_drop_up 2 more_vert Nice work! I don't know if your results ensembling these methods have something to do with stable predictions, but I think that it has more to do with the rules learned by each model. Ensemble methods work well when their inner models are more diverse as possible, meaning that they learn different decision boundaries and are capable of predicting differently from each other. More importantly, is that your ensemble method achieved higher accuracy than its individual components. I think that you can follow this way of thinking for analyzing your ensemble success.  🙂 hedwig100 Topic Author Posted 4 years ago · 44th in this Competition arrow_drop_up 1 more_vert Thanks a lot for your reply! I'll try to understand the effect of ensemble more clearly. Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Test your model against future real market data Note 2021-10-08: the dataset for this competition is no longer available. This dataset contains an anonymized set of features, feature_{0...129} , representing real stock market data. Each row in the dataset represents a trading opportunity, for which you will be predicting an action value: 1 to make the trade and 0 to pass on it. Each trade has an associated weight and resp , which together represents a return on the trade. The date column is an integer which represents the day of the trade, while ts_id represents a time ordering. In addition to anonymized feature values, you are provided with metadata about the features in features.csv . In the training set, train.csv , you are provided a resp value, as well as several other resp_{1,2,3,4} values that represent returns over different time horizons. These variables are not included in the test set. Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation. This is a code competition that relies on a time-series API to ensure models do not peek forward in time. To use the API, follow the instructions on the Evaluation page . When you submit your notebook, it will be rerun on an unseen test: Note that during the second (forecasting) phase of the competition, the notebook time limits will scale with the number of trades presented in the test set. Refer to the Code Requirements for details. Too many requests",
    "data_description": "Jane Street Market Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Jane Street Group · Featured Code Competition · 4 years ago Late Submission more_horiz Jane Street Market Prediction Test your model against future real market data Jane Street Market Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Nov 23, 2020 Close Aug 24, 2021 Merger & Entry Description link keyboard_arrow_up “Buy low, sell high.” It sounds so easy…. In reality, trading for profit has always been a difficult problem to solve, even more so in today’s fast-moving and complex financial markets. Electronic trading allows for thousands of transactions to occur within a fraction of a second, resulting in nearly unlimited opportunities to potentially find and take advantage of price differences in real time. In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their “fair values” and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world. Developing trading strategies to identify and take advantage of inefficiencies is challenging. Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. As a result, it can be hard to distinguish good luck from having made a good trading decision. In the first three months of this challenge, you will build your own quantitative trading model to maximize returns using market data from a major global stock exchange. Next, you’ll test the predictiveness of your models against future market returns and receive feedback on the leaderboard. Your challenge will be to use the historical data, mathematical tools, and technological tools at your disposal to create a model that gets as close to certainty as possible. You will be presented with a number of potential trading opportunities, which your model must choose whether to accept or reject. In general, if one is able to generate a highly predictive model which selects the right trades to execute, they would also be playing an important role in sending the market signals that push prices closer to “fair” values.  That is, a better model will mean the market will be more efficient going forward. However, developing good models will be challenging for many reasons, including a very low signal-to-noise ratio, potential redundancy, strong feature correlation, and difficulty of coming up with a proper mathematical formulation. Jane Street has spent decades developing their own trading models and machine learning solutions to identify profitable opportunities and quickly decide whether to execute trades. These models help Jane Street trade thousands of financial products each day across 200 trading venues around the world. Admittedly, this challenge far oversimplifies the depth of the quantitative problems Jane Streeters work on daily, and Jane Street is happy with the performance of its existing trading model for this particular question. However, there’s nothing like a good puzzle, and this challenge will hopefully serve as a fun introduction to a type of data science problem that a Jane Streeter might tackle on a daily basis. Jane Street looks forward to seeing the new and creative approaches the Kaggle community will take to solve this trading challenge. This is a Code Competition. Refer to Code Requirements for details. Evaluation link keyboard_arrow_up This competition is evaluated on a utility score. Each row in the test set represents a trading opportunity for which you will be predicting an action value, 1 to make the trade and 0 to pass on it. Each trade j has an associated weight and resp , which represents a return. For each date i, we define: p i = ∑ j ( w e i g h t i j ∗ r e s p i j ∗ a c t i o n i j ) , t = ∑ p i √ ∑ p 2 i ∗ √ 250 | i | , where | i | is the number of unique dates in the test set. The utility is then defined as: u = m i n ( m a x ( t , 0 ) , 6 ) ∑ p i . Submission File You must submit to this competition using the provided python time-series API, which ensures that models do not peek forward in time. To use the API, follow the following template in Kaggle Notebooks: import janestreet env = janestreet.make_env() # initialize the environment iter_test = env .iter_test() # an iterator which loops over the test set for (test_df, sample_prediction_df) in iter_test:\n    sample_prediction_df.action = 0 #make your 0/1 prediction here env .predict(sample_prediction_df) content_copy Timeline link keyboard_arrow_up This is a forecasting competition with an active training phase and a second period where models will be run against real market returns. Training Timeline: February 15, 2021 - Entry deadline. You must accept the competition rules before this date in order to compete. February 15, 2021 - Team Merger deadline. This is the last day participants may join or merge teams. February 22, 2021 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Forecasting Timeline: Starting after the final submission deadline there will be periodic updates to the leaderboard to reflect market data updates that will be run against selected notebooks. August 23, 2021 - Competition End Date Prizes link keyboard_arrow_up 1st Place - $40,000 2nd Place - $20,000 3rd Place - $10,000 4th - 9th Place - $5,000 Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. For this competition, training is not required in Notebooks. In order for to be eligible for submission, the following conditions must be met: Training Phase Your notebook must use the time-series module to make predictions CPU Notebook <= 5 hours run-time GPU Notebook <= 5 hours run-time TPU Notebooks are not supported Freely & publicly available external data is allowed, including pre-trained models Forecasting Phase Because the size of the test set will change during the live forecasting phase, the time limits will be adjusted in proportion to the test set size , with a 10% added time allowance. As a hypothetical example, if there are 1,000,000 test rows and a 5 hour runtime limit during the training phase and the forecasting phase has 2,000,000 rows, your notebook will be allowed 10 hours + 10% = 11 hours during the forecasting phase. Please see the Code Competition FAQ for more information on how to submit. Citation link keyboard_arrow_up cnyberg, dd_engi, janestreet-jjia, Maggie, and Will Cukierski. Jane Street Market Prediction. https://kaggle.com/competitions/jane-street-market-prediction, 2020. Kaggle. Cite Competition Host Jane Street Group Prizes & Awards $100,000 Awards Points & Medals Participation 29,182 Entrants 4,884 Participants 4,245 Teams 7,211 Submissions Tags Finance Tabular Custom Metric Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "riiid-test-answer-prediction",
    "discussion_links": [
      "/competitions/riiid-test-answer-prediction/discussion/218318",
      "/competitions/riiid-test-answer-prediction/discussion/210113",
      "/competitions/riiid-test-answer-prediction/discussion/209585",
      "/competitions/riiid-test-answer-prediction/discussion/210171",
      "/competitions/riiid-test-answer-prediction/discussion/209581",
      "/competitions/riiid-test-answer-prediction/discussion/210552",
      "/competitions/riiid-test-answer-prediction/discussion/210354",
      "/competitions/riiid-test-answer-prediction/discussion/209694",
      "/competitions/riiid-test-answer-prediction/discussion/209635",
      "/competitions/riiid-test-answer-prediction/discussion/209622",
      "/competitions/riiid-test-answer-prediction/discussion/209798",
      "/competitions/riiid-test-answer-prediction/discussion/209713",
      "/competitions/riiid-test-answer-prediction/discussion/209597",
      "/competitions/riiid-test-answer-prediction/discussion/209587",
      "/competitions/riiid-test-answer-prediction/discussion/210025",
      "/competitions/riiid-test-answer-prediction/discussion/209711",
      "/competitions/riiid-test-answer-prediction/discussion/209659",
      "/competitions/riiid-test-answer-prediction/discussion/210041",
      "/competitions/riiid-test-answer-prediction/discussion/209624",
      "/competitions/riiid-test-answer-prediction/discussion/209689",
      "/competitions/riiid-test-answer-prediction/discussion/210276",
      "/competitions/riiid-test-answer-prediction/discussion/209596"
    ],
    "discussion_texts": [
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules keetar · 1st in this Competition  · Posted 4 years ago arrow_drop_up 170 more_vert [1st place solution] Last Query Transformer RNN Hi Kagglers, here is the paper link. Paper Link : https://arxiv.org/abs/2102.05038 Please refer the paper for details. Summary I wanted to use Transformer but I could not input long history to the model because QK matrix multiplication in Transformer has O(L^2) time complexity when input length is L. My approach is to use only last input as Query, because I only predict last question's answer correctness per history inputs. It means I will only compare between last question(query) and other questions(key), and not between other questions. It makes QK matrix multiplication in Transformer to have O(L) time complexity(because len(Q)=1, len(K)=L), which allows me to input much longer history. In final submission, I ensemble 5 models with 1728 length history inputs. I didn't do feature engineering much, since I can use extremely long history, I wanted the model to learn it by itself. 5 input features I used are question id, question part, answer correctness, current question elapsed time, and timestamp difference. Acknowledgement Thank you @limerobot , to share amazing transformer model to approach table data problem in the 3rd place solution of 2019 Data Science Bowl. It was a good motivation to work on transformer encoder. https://www.kaggle.com/c/data-science-bowl-2019/discussion/127891 Thanks to competition sponsers and organizers for hosting this fun competition. Thank you for reading. Please sign in to reply to this topic. comment 25 Comments 1 appreciation  comment Hotness Arshad Shaikh Posted 4 years ago · 2475th in this Competition arrow_drop_up 1 more_vert Pytorch Implementation of 1st place solution - https://github.com/arshadshk/Last_Query_Transformer_RNN-PyTorch ( Let me know if any changes are needed or any bugs present ) Darragh Posted 4 years ago · 11th in this Competition arrow_drop_up 3 more_vert Thank you @keetar for sharing this approach. Very very nice, and great to see such long histories used. If you have published code somewhere, it would be great to see the encoder layer set up - if it is not public, no worries. Well done again, well deserved. keetar Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 9 more_vert Hi @darraghdog , thank you. Actually, it's really simple to implement the model. I only changed the one line from normal transformer encoder. In encoder layer, I changed attn_output, _ = self.mha(x, x, x[:,:,:], mask) #mha : multi head attention to attn_output, _ = self.mha(x, x, x[:,-1:,:], mask) Hope it helps! Arshad Shaikh Posted 4 years ago · 2475th in this Competition arrow_drop_up 4 more_vert Hey @keetar Congrats. I had a couple of doubts, can you please answer them- Assume that I represent Query, Key and Value as Q, K, and V respectively, and let's assume that I have an input sequence with- D - Dimension of model S - Sequence Length of input According to the paper first we need to multiply the Q and K transpose so - result1 = Q * K    --->      (S , D) * ( D ,S)  ---> (S,S) dimension Now, we divide by sq root of a scaling factor and then apply softmax and get result2 with dim (S,S). result2 = softmax(  result1  /  sqRoot(ScalingFactor) ) Now we multiply the result2 with our Values V--> FinalResult  =   result2 *  V       --->  i.e.  (S,S) * (S,D) ---> (S,D) dimension My doubts- 1. If you keep seq len = 1  in the Query Q, then the final output will be having dim (1, D). \nThis is just a single feature with dim D, then how are you passing it through the LSTM? \n( Or you are assuming the single feature with dim D as a sequence and passing it through the LSTM ? If so then can you tell the intuition behind this please ) 2. In one of the comments you mentioned the following - attn_output, _ = self.mha(x, x, x[:,-1:,:], mask) Can you please elaborate more on the code, What is x and what is x[:,-1:,:] ?\nWhich one of those 3 is Query Q ?\n( If the code above is used from torch.nn.MultiheadAttention then in that case you are passing the values V as x[:,-1:,:], which will raise an error. ) Thanks This comment has been deleted. 6 more replies arrow_drop_down +1 mamas Posted 4 years ago · 2nd in this Competition arrow_drop_up 2 more_vert Thank you keetar for your great solution, it was really fun to compete with you. Finally I lost, though. It is not surprising to use only the last input as query, because I heard @its7171 , @takoihiraokazu and @nadare did the same thing and I suppose many people did this, as it's easy to implement. However, they said using the last input as query is very time-consuming (I remember @its7171 said training takes 1 week). How long is your training time, why is it so fast and what is your trick? Did you use TPU? My another question is about the sequence length and the input features. I was very surprised that your sequence length is 1728 and you used only 5 features. How did you notice sequence length is so important? Did you throw away other useful features to make the sequence length longer? I would like to know your journey to L = 1728 :) keetar Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 2 more_vert Hi @mamasinkgs , thank you. I'm surprised to hear many people tried this, as I saw none. Could you provide some links? 4 more replies arrow_drop_down hlf Posted 4 years ago arrow_drop_up 1 more_vert Hi, I had some doubts about your inputs, can you please anwer it. Categorical embedding is used for first three features, and continuous embedding is used for last two continuous features. but how you make continuous embedding? 'Timestamp difference' feature indicates the difference from the past question timestamp to the current question timestamp, and it is clipped by maximum value, 3 day. how you fill the first 'Timestamp difference' in a sequence? Thanks! HenryHZY Posted 4 years ago · 268th in this Competition arrow_drop_up 1 more_vert Cong! What a simple but sweet solution:) TitiTest Posted 4 years ago · 238th in this Competition arrow_drop_up 1 more_vert Congrats for this really elegant and original solution ! keetar Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thank you :) Pierre Posted 4 years ago · 2690th in this Competition arrow_drop_up 0 more_vert Hi, does the LSTM use a mask as well? It looks like Tensorflow's LSTM accepts mask. the residual addition in the transformer includes the padding tokens which we may want to ignore in the lstm. Neo Zhao Posted 4 years ago arrow_drop_up 0 more_vert great point This comment has been deleted. Appreciation (1) ZavodRobotov Posted 4 years ago arrow_drop_up 0 more_vert Good work! Thanks for sharing. Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules mamas · 2nd in this Competition  · Posted 4 years ago arrow_drop_up 206 more_vert 2nd Place Solution (LSTM-Encoded SAKT-like TransformerEncoder) Thank you all teams who competed with me, all the people who participated in this competition, and the organizer that hosts such a great competition with the well-designed API! Congrats @keetar , who defeats me and becomes the winner in this competition. I'm happy because it's my first time I get solo prize! It's my 4th kaggle competition and it was fun to compete with my past teammates ( @nyanpn , @pocketsuteado ) and people who I competed with past competitions (e.g. @aerdem4 , @its7171 ). Here, I will explain the summary of my model and my features. I uploaded 2 kaggle notebooks for the explanation. As the ensemble is not so important in my solution, I will only explain my single model. 6 similar models weighted average : 0.817 public/0.818 private single model : 0.814 public/0.816 private Models Overview my model is similar to SAKT, with 400 sequence length, 512 dimension, 4 nheads. I don't use lecture information for the input of transformer model, which I guess is why I lost in this competition. For the query and key/value of SAKT-like model, I used LSTM-encoded features, whose input is as follows. \"Query\" features content_id part tags normalized timedelta normalized log timestamp correct answer task_container_id delta content_type_id delta normalized absolute position \"Memory\" features explanation correctness normalized elapsed time user_answer Detailed Explanation of Training/Inference Process I tried a very precise indexing/masking technique to avoid data leakage in training process, I guess which is partly why I became 2nd place. OK, suppose a very simple model of the sequence length = 5, and a task_container_id history of a specific user (without lecture) is like this. [0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 5, 8, 7, 7, 6] As there is 15 measurements in this user, I made 15/5 = 3 training samples and loss mask. (1) input task_container_id: [pad, pad, pad, pad, pad, 0, 0, 0, 1, 1] (1) loss_mask: [False, False, False, False, False, True, True, True, True, True] (2) input task_container_id: [0, 0, 0, 1, 1, 2, 2, 3, 3, 4] (2) loss_mask: [False, False, False, False, False, True, True, True, True, True] (3) input task_container_id: [2, 2, 3, 3, 4, 5, 8, 7, 7, 6] (3) loss_mask: [False, False, False, False, False, True, True, True, True, True] In this competition, the handling of the task_container_id is very important, as it is not allowed to use \"memory\" features of the same task container id to avoid leakage. So, after applying LSTM to the features, such fancy indexing is required to avoid the leakage for 3 training samples, where -1 means this position can't attend any position. (1) indices: [-1, -1, -1, -1, -1, 4, 4, 4, 7, 7] (2) indices: [-1, -1, -1, 2, 2, 4, 4, 6, 6, 8] (3) indices: [-1, -1, 1, 1, 3, 4, 5, 6, 6, 8] To get this indices very fast in the training/inference phase, I wrote such cython (#1) function (vectorized implementation): %%cython import numpy as np\ncimport numpy as np\ncpdef np.ndarray[int] cget_memory_indices(np.ndarray task):\n    cdef Py_ssize_t n = task.shape[ 1 ]\n    cdef np.ndarray[int, ndim = 2 ] res = np.zeros_like(task, dtype = np.int32)\n    cdef np.ndarray[int] tmp_counter = np.full(task.shape[ 0 ], -1 , dtype = np.int32)\n    cdef np.ndarray[int] u_counter = np.full(task.shape[ 0 ], task.shape[ 1 ] - 1 , dtype = np.int32)\n    for i in range(n):\n        res[:, i] = u_counter\n        tmp_counter += 1 if i != n - 1 :\n            mask = (task[:, i] != task[:, i + 1 ])\n            u_counter[mask] = tmp_counter[mask]\n    return res content_copy After applying such fancy indexing to the output of LSTM features, I concatenated them with \"Query\" features and apply MLP, then we can get query for SAKT model. Then, I obtained key/value for SAKT model by applying MLP to query concatenated with \"Memory\" features. To train SAKT-like model, precise memory masking is also required to avoid leakage. I used 3D attention mask of torch.nn.MultiheadAttention. It should be noted torch.repeat_interleave must be leveraged to make 3D mask (batchsize * nhead, sequence length, sequence length). The memory mask for these 3 samples are like this. Here, 1 means True and 0 means False. Please remember the documentation says attn_mask ensure that position i is allowed to attend the unmasked positions. If a BoolTensor is provided, positions with True is not allowed to attend while False values will be unchanged. content_copy (1) memory_mask: array( [ [1, 1, 1, 1, 1, 1, 1, 1, 1, 0] , [1, 1, 1, 1, 1, 1, 1, 1, 1, 0] , [1, 1, 1, 1, 1, 1, 1, 1, 1, 0] , [1, 1, 1, 1, 1, 1, 1, 1, 1, 0] , [1, 1, 1, 1, 1, 1, 1, 1, 1, 0] , [0, 0, 0, 0, 0, 1, 1, 1, 1, 1] , [0, 0, 0, 0, 0, 1, 1, 1, 1, 1] , [0, 0, 0, 0, 0, 1, 1, 1, 1, 1] , [1, 1, 1, 0, 0, 0, 0, 0, 1, 1] , [1, 1, 1, 0, 0, 0, 0, 0, 1, 1] ] ) content_copy (2) memory_mask: array( [ [1, 1, 1, 1, 1, 1, 1, 1, 1, 0] , [1, 1, 1, 1, 1, 1, 1, 1, 1, 0] , [1, 1, 1, 1, 1, 1, 1, 1, 1, 0] , [0, 0, 0, 1, 1, 1, 1, 1, 1, 1] , [0, 0, 0, 1, 1, 1, 1, 1, 1, 1] , [0, 0, 0, 0, 0, 1, 1, 1, 1, 1] , [0, 0, 0, 0, 0, 1, 1, 1, 1, 1] , [1, 1, 0, 0, 0, 0, 0, 1, 1, 1] , [1, 1, 0, 0, 0, 0, 0, 1, 1, 1] , [1, 1, 1, 1, 0, 0, 0, 0, 0, 1] ] ) content_copy (3)  memory_mask: array( [ [1, 1, 1, 1, 1, 1, 1, 1, 1, 0] , [1, 1, 1, 1, 1, 1, 1, 1, 1, 0] , [0, 0, 1, 1, 1, 1, 1, 1, 1, 1] , [0, 0, 1, 1, 1, 1, 1, 1, 1, 1] , [0, 0, 0, 0, 1, 1, 1, 1, 1, 1] , [0, 0, 0, 0, 0, 1, 1, 1, 1, 1] , [1, 0, 0, 0, 0, 0, 1, 1, 1, 1] , [1, 1, 0, 0, 0, 0, 0, 1, 1, 1] , [1, 1, 0, 0, 0, 0, 0, 1, 1, 1] , [1, 1, 1, 1, 0, 0, 0, 0, 0, 1] ] ) content_copy To make this memory mask very fast in the training phase, I wrote such cython (#2) function (not vectorized implementation): %%cython\nimport numpy as np\ncimport numpy as np\ncpdef np.ndarray[ int ] cget_memory_mask(np.ndarray task, int n_length):\n    cdef Py_ssize_t n = task.shape[ 0 ]\n    cdef np.ndarray[ int , ndim = 2 ] res = np.full((n, n), 1 , dtype = np.int32)\n    cdef int tmp_counter = 0 cdef int u_counter = 0 for i in range (n):\n        tmp_counter += 1 if i == n - 1 or task[i] != task[i + 1 ]: res [i - tmp_counter + 1 : i + 1 , :u_counter] = 0 if u_counter == 0 : res [i - tmp_counter + 1 : i + 1 , n - 1 ] = 0 if u_counter > n_length: res [i - tmp_counter + 1 : i + 1 , :(u_counter - n_length)] = 1 u_counter += tmp_counter\n            tmp_counter = 0 return res content_copy In the inference phase, memory mask is not required. For more details, please look at my single model notebook . My Features Transformer is great, but it suffers from a problem that the sequence length cannot be infinite. I mean, it cannot consider the information of very old samples. To tackle this problem and to leverage the lecture information, I made simple features and concatenated them with the output features of SAKT-like model, and applied MLP and sigmoid. I uploaded the names of 90 features as attachments. For the implementation of these features, I didn't use any pd.merge or df.join and most of the implementation are done using numpy. To make user-content features, I used scipy.sparse.lil_matrix to spare memory usage. I think my feature is not so good as other competitors (about 0.795 when using GBDT), but still improved score 0.001 ~ 0.002. I made some tricky features (e.g. obtained by SVD), but it did not improve the score of NN model (improved GBDT model, though.), probably because the information of NN features includes that of such tricky features. CV Strategy My CV strategy is completely different from tito( @its7171 )'s one. First, I probed the number of new users in test set and I found there are about 7000 new users. as we know there is 2.5M rows in test set and we know the average length of the history of all users, we can estimate the number of rows of the new users and the number of rows of the existing users. After the calculation, I found the number of rows of new users (i.e. user split ): the number of rows of existing users (i.e. timeseries split ) = 2 : 1. content_copy So, For validation, I decided to use 1M rows for timeseries split and 2M rows for user split. When making validation set of timeseries split, I was so careful that leakage can't happen. I mean, my training dataset and validation dataset never shares same task_container_id for a given user. What worked increase length from 100 to 400 worked. using normalized timedelta is better than digitized timedelta (mentioned in SAINT+ paper). concatenating embeddings is better than adding embedding, as mentioned in https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/201798 . dropout = 0.2 is very important. StepLR with Adam is good. I trained my model for about 35 epoch with lr = 2e-3, then trained it for 1 epoch with lr = 2e-4. What didn't work Random masking of sequences didn't improve the score. bundle_id and normalized task_container_id is not needed for the input of LSTM. As I didn't make diverse models, weighted averaging is enough and blending using GBDT didn't work well. Full data training (without any validation data) didn't improve the score. In my implementation, SAINT didn't work well. In my opinion, it's logically very hard to implement SAINT without data leakage in training due to task_container_id. Comments I was very sad to see the private score bug problem of kaggle. Of course this competition is very great, but I think this competition could have been one of the most successful competition in kaggle without this problem. Anyway, I really enjoyed my 4th kaggle competition. I'll continue kaggle and will surely become the winner in the next competition! Thanks all, see you again! P.S. The word memory_mask may be confusing because it is different from pytorch Transformer 's memory_mask . The word memory_mask is more like mask in pytorch's TransformerEncoder . The reason my word is confusing is, I used Encoder-Decoder model at first, then gave up it and started using Encoder-only model, but I didn't change the function names. :( feature_list.txt Please sign in to reply to this topic. comment 50 Comments Hotness Andrés Miguel Torrubia Sáez Posted 4 years ago · 3rd in this Competition arrow_drop_up 4 more_vert Good luck WINNING your next competition. Theo Viel Posted 4 years ago arrow_drop_up 1 more_vert Thanks for the nicely detailed write-up and congratz for the impressive finish! Nikola Bacic Posted 4 years ago · 17th in this Competition arrow_drop_up 1 more_vert In my implementation, SAINT didn't work well. In my opinion, it's logically very hard to implement SAINT without data leakage in training due to task_container_id. Couldn't you control that with tgt_mask from PyTorch's implementation ? mamas Topic Author Posted 4 years ago · 2nd in this Competition arrow_drop_up 2 more_vert Yes we can control tgt_mask in decoder. but if we use the same input of the decoder as SAINT+ paper in this image, I think the problem cannot be fixed just by controlling tgt_mask , because the query of decoder produces leakage (i.e. uses the information of the sample in the same task_container_id). In my understanding, the data leakage caused by query is unavoidable by controlling masking, while the data leakage caused by key/value is avoidable by masking. 15 more replies arrow_drop_down KhanhVD Posted 4 years ago · 146th in this Competition arrow_drop_up 2 more_vert Big congrats to you on 2nd solo prize. Thanks for the detail writeup, very intelligence solution @mamasinkgs Yih-Dar SHIEH Posted 4 years ago · 70th in this Competition arrow_drop_up 2 more_vert @mamasinkgs , thank you for sharing. See you in the next one! About your CPython implementation for getting the masks, it is amazing, you must be an advanced Python programmer! For me, I use TensorFlow, and it is quite straightforward and fast to get these mask by using tf operations, like def get_causal_attention_mask ( nd, ns, dtype, only_before ): \"\"\"\n    1's in the lower triangle, counting from the lower right corner. Same as tf.matrix_band_part(tf.ones([nd, ns]),\n    -1, ns-nd), but doesn't produce garbage on TPUs.\n    \"\"\" # Remark: Think `nd` as the number of queries and `ns` as the number of keys. # In encoder-decoder case, the queries are the decoder features and the keys are the encoder features. i = tf. range (nd)[:, tf.newaxis] # repeat along dim 1 j = tf. range (ns) # repeat along dim 0 m = i >= (j - ns + nd) + tf.cast(only_before, dtype=tf.int32) return tf.cast(m, dtype) def get_attention_mask_from_timestamp_batch ( timestamp_tensors, dtype, only_before ): \"\"\"\n    Args:\n        timestamp_tensors: 2-D tf.int32 tensor, representing a batch of sequences of non-decreasing timestamps.\n\n    Returns:\n        attention_mask: 3-D tf.int32 tensor of shape = [batch_size, query_len, key_len], consisting of 0 and 1.\n            Here `query_len` and `key_len` are actually `seq_len`. It should be reshpaed, when used to calculate \n            attention scores, to [batch_size, nb_attn_head, query_len, key_len].\n    \"\"\" t = timestamp_tensors\n\n    batch_size = tf.math.reduce_sum(tf.ones_like(t[:, : 1 ], dtype=tf.int32))\n    seq_len = tf.math.reduce_sum(tf.ones_like(t[: 1 , :], dtype=tf.int32))\n\n    x = tf.broadcast_to(t[:, :, tf.newaxis], shape=[batch_size, seq_len, seq_len]) # repeat along dim 2 y = tf.broadcast_to(t[:, tf.newaxis, :], shape=[batch_size, seq_len, seq_len]) + tf.cast(only_before, dtype=tf.int64) # repeat along dim 1 m =  x >= y return tf.cast(m, dtype) content_copy mamas Topic Author Posted 4 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Thanks! I'm not familiar with tensorflow, but I'll try it in future competitions. Nimra Tassawar Posted 4 years ago arrow_drop_up 0 more_vert Hi, I have few questions Can you guide me how to run the code you provided? and how to get results from it. Did you published any research paper on this implementation. I want to understand your implementation in detail. At kaggle, I have seen your 3 submissions related to this competition. i. https://www.kaggle.com/mamasinkgs/2nd-place-solution-for-hosts ii. https://www.kaggle.com/mamasinkgs/public-private-2nd-place-solution iii. https://www.kaggle.com/mamasinkgs/public-private-2nd-place-solution-single-fold I want to know which one is the actual and correct submission? mamas Topic Author Posted 4 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Hi Nimra, Please just run my notebook. I'm sorry I didn't provide the training code. Yes, please check https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/218148 I remember ii is the actual submission, which is the ensemble of 6 models. iii is the submission with single model. Nimra Tassawar Posted 4 years ago arrow_drop_up 0 more_vert @mamasinkgs thanks for your reply. Actually I have run your notebook. Everything seems good, no errors. But from where can I see results? Do submission.csv file is getting updated every time I run the notebook? Also one more question, I am a newbie in machine learning and python. Can you suggest me what courses should I take or any thing else that can help me understand code? Athar Sayed Posted 4 years ago · 518th in this Competition arrow_drop_up 0 more_vert @mamasinkgs , Very Nice Approach of Using Transformer . Few Questions , I wanted to ask , if you don't mind sharing 1) How did you estimate New Users in Private Test set by probing ? 2) Is memory masking neccessary while training Model , what happens if We train Model without masking ? mamas Topic Author Posted 4 years ago · 2nd in this Competition arrow_drop_up 0 more_vert 1) I uploaded https://www.kaggle.com/mamasinkgs/submission-with-7000-limit?scriptVersionId=45368594 , so please check it. I tried binary search by changing the limit. 2) I thought models will learn some useless patterns. But, judging from others' solutions, precise masking is not necessarily needed. I think it depends on architectures, though. Athar Sayed Posted 4 years ago · 518th in this Competition arrow_drop_up 0 more_vert Okay Thanks . Wisso Posted 4 years ago · 1046th in this Competition arrow_drop_up 0 more_vert Very nice approach, I was looking forward to this. A side question though, in this comment it was inferred that you were not using a transformer. When did you shift to this? or was it a transformer all along? mamas Topic Author Posted 4 years ago · 2nd in this Competition arrow_drop_up 4 more_vert Nice question! In the beginning of the competition, I used LightGBM as usual and got to 0.792 (1st place) soon. Then, I started using Transformer by reading this blog , soon I got to 0.801 using Transformer and 0.803 by ensembling GBDT and Transformer. I remember I wrote this comment then. (so, The fact is I was using both GBDT and Transformer). After that, my Transformer improved a lot and I decided to throw away GBDT and use my hand-crafted features for the additional input of Transformer. \"Is Attention All You Need?\" means I was wondering whether Transformer is better than GBDT when I wrote this comment. yoyoyo Posted 4 years ago · 3391st in this Competition arrow_drop_up 0 more_vert a dump question from a newbie. Which software do you use to draw that model structure graph? thank you ! mamas Topic Author Posted 4 years ago · 2nd in this Competition arrow_drop_up 1 more_vert I simply used powerpoint. Nya 🚀 Posted 4 years ago · 830th in this Competition arrow_drop_up 0 more_vert I uploaded the names of 90 features as attachments. Where did you upload the attachments? mamas Topic Author Posted 4 years ago · 2nd in this Competition arrow_drop_up 0 more_vert Thank you, I uploaded it now! Dean Posted 4 years ago · 37th in this Competition arrow_drop_up 0 more_vert Congratulations and thank for detailed explanation! May I ask a question about how to deal with the tags features? Thank you. mamas Topic Author Posted 4 years ago · 2nd in this Competition arrow_drop_up 2 more_vert Thank you! I used one-hot encoding (like multilabel problem) for tags and concatenate them with other features, then applied MLP. tomoo inubushi Posted 4 years ago · 62nd in this Competition arrow_drop_up 0 more_vert Congrats and thank you for your detailed explanation. I have two questions about your solution. What is your cross-validation and model evaluating strategy? Did your task_container-aware masking improved CV score (and how much)? I think if we did not carefully split train/valid datasets and evaluate the model, CV score would be decreased when using task_container-aware masking, as the model cannot utilize the intra-container leakage you mentioned. One of the major CV strategies in this competition by @its7171 did not care task_container_id, though it split train/valid datasets by timestamp + random int. This CV strategy might split some task_containers into train/valid datasets, and might cause some intra-container leakage. (I should note that I used these datasets with good CV-LB relationship) [update] I confirmed that there are no such data at all. mamas Topic Author Posted 4 years ago · 2nd in this Competition arrow_drop_up 3 more_vert What is your cross-validation and model evaluating strategy? My CV strategy is completely different from tito's one. First, I probed the number of new users in test set and I found there is about 7000 new users. as we know there is 2.5M rows in test set and we know the average length of the history of all users, we can estimate the number of rows of the new users and the number of rows of the existing users. After the calculation, I found # rows for user split (i.e. new users): # rows for timeseries split (i.e. existing users) = 2 : 1 . So, For validation, I decided to use 1M rows for timeseries split and 2M rows for user split. When making validation set of timeseries split, I was so careful that inter-container leakage problem can't happen. I mean, my training dataset and validation dataset never shares same task_container_id for a specific user. Did your task_container-aware masking improved CV score (and how much)? I don't know how much it improves CV/LB score, because I was really careful about inter-container leakage problem from the beginning and have never tried task_container-unaware masking and indexing (I hate leakage). So, there is no evidence that shows task_container-aware masking is important. That's just my guess from LB score. 4 more replies arrow_drop_down 2981 Posted 4 years ago · 431st in this Competition arrow_drop_up 0 more_vert thanks for sharing! a great job!  this is a big project! kobi2000 Posted 4 years ago · 1487th in this Competition arrow_drop_up 0 more_vert @mamasisking , congratulations! Can you please also share info about the resources you put into the competition? hardware, amount of effort, time etc. mamas Topic Author Posted 4 years ago · 2nd in this Competition arrow_drop_up 9 more_vert I use V100 * 4 (VRAM 64GB) and training takes about 30h. I think I spent more than 500 hours for this competition. I did nothing other than this competition since 10/16. 3 more replies arrow_drop_down Yih-Dar SHIEH Posted 4 years ago · 70th in this Competition arrow_drop_up 0 more_vert @mamasinkgs , sorry to bother, but could you share your insight about this statement In my implementation, SAINT didn't work well. In my opinion, it 's logically very hard to implement SAINT without data leakage in training due to task_container_id. content_copy Why you think task_container_id cause data leakage during training, and what kind of leakage? mamas Topic Author Posted 4 years ago · 2nd in this Competition arrow_drop_up 1 more_vert I mean, in my opinion, the input of the decoder of SAINT+ architecture cannot be implemented without any information loss or any data leakage, due to task_container_id. Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Javier Martín · 3rd in this Competition  · Posted 4 years ago arrow_drop_up 162 more_vert 3rd place solution (0.818 LB): The Transformer 2021-01-16 edit 2 : added missing linear layers after categorical embeddings + tag features to diagram 2021-01-16 edit 1 : the source code is up. Wow, what a ride this has been. First off, thanks @sohier @hoonpyotimjeon Kaggle, Riiid and everyone involved in setting up this challenging competition. Congratulations to #1 and #2 @keetar and @mamasinkgs !!! We truly look forward to reading about your solutions! Also huge thanks to my teammate @antorsae who joined me in the last stretch and without whose ideas, intuition and hardware I wouldn’t have gotten this far. I was attracted to this competition by the relatively small dataset footprint compared to my other two previous competitions (deep fakes and RSNA pulmonary embolisms) but this ended up being much more resource intensive than I anticipated. Our solution is a mixture of two Transformer models with carefully crafted attention, engineered features and a time-aware adaptive ensembling mechanism we nick-named “The Blindfolded Gunslinger”. (“Blindfolded gunslinger” hand-drawn by @antorsae inspired by Red Dead Redemption 2) The Transformer We use two transformers trained separately with 2.5% of the users held out for validation and sequences of 500 interactions. At train we simply split user stories in 500 non-overlapping interaction chunks and sample the chunks randomly. Transformer 1: 3+3 layers (encoder+decoder), no LayerNorms, T-Fixup init see paper for reasons why we used it, ReLU activations, d_model=512 Transformer 2: 4+4 layers, no LayerNorms, T-Fixup init, GELU activations, d_model=512 We feed both the encoder and the decoder ALL the features. We use learned features for continuous variables (simply projecting them to d_model) and for categorical variables we first map it to embeddings with low dimensionality and then project it also to d_model=512 (to avoid potential overfitting). We use an embedding bag for question tags. To prevent the transformer from looking into the future we shift the encoder input including both questions + answers to the right, hide all the answer-specific features from the decoder (user_answer, answered_correctly, qhe, qet), ie. those that are not immediately available upon inferring the interaction and use the appropriate attention masks in all 3 attentions. To our surprise the 3+3 model outperformed its bigger 4+4 brother even if we tried to finetune the latter at the final hours of the competition. Engineered features This was a very rich dataset  but we found the following derived features helped the transformer converge faster and reach a higher AUROC score. A lot of them have been discussed in the forum: qet , qhe : these are the prior_qet and prior_qhe counterparts shifted upwards one container. This was probably discussed first by @doctorkael here: https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/194184 tsli : time since list interaction, AKA timestamp delta. Discussed in many threads. clipped_tsli : tsli clipped to 20 minutes. @claverru hinted at this in the Saint benchmark mega-thread: https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/195632 ts_mod_1day : timestamp modulus 1 day. This may reveal daily patterns such as the user being more / less attentive / tired in the mornings / after work, etc. ts_mod_1week : timestamp modulus 1 week. We similarly hope this will reveal weekly patterns (are “mondays” a bad day? etc.) attempt_num , attempts_correct , attempts_correct_avg : about 11% of the questions were repeated questions, so it made a lot of sense to keep a record of which question had been answered by whom and how many times it was answered correctly. This was revealed by @aravindpadman in his great thread: https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/194266 and it was an extremely demanding feature to code since it takes a total of 11.2 GB of space at inference all by itself. Attention Our architecture follows the auto-regressive application of sequence to sequence transformers, so a causal attention mask is needed to make sure a given interaction cannot attend to interactions in the future; however there is an exception to this which we believe is critical: interaction grouped by the same task_container_id. We compute separate attention masks for the encoder and decoder, preventing the encoder self-attention from attending past interactions if they belong to the same task_container_id, and conversely modifying the decoder self-attention to allow to attend to all interaction belonging to the same task_container_id, we further restrain the output of the encoder to the decoder with the encoder attention to prevent a leakage of information from the residual connections in the encoder. causal_mask = ~torch.tril(torch.ones( 1 ,sl, sl,dtype=torch.bool,device=x_cat.device)).expand(b,- 1 ,- 1 ) x_tci = x_cat[...,Cats.task_container_id] x_tci_s = torch.zeros_like(x_tci) x_tci_s [..., 1 :] = x_tci[...,:- 1 ] enc_container_aware_mask =  (x_tci.unsqueeze(- 1 ) == x_tci_s.unsqueeze(- 1 ).permute( 0 , 2 , 1 )) | causal_mask dec_container_aware_mask = ~(x_tci.unsqueeze(- 1 ) == x_tci.unsqueeze(- 1 ).permute( 0 , 2 , 1 ))   & causal_mask content_copy The Blindfolded Gunslinger We made a joke in the meme thread about us wanting to ensemble multiple models, but the competition having only 9 hours to run full inference… It was already reported that the public test set was sitting on the first 20% of the test set so it is possible to maximize the allotted time for the private test set by skipping model inference in the first 20% and predicting only the last 80%. We implemented dynamic ensembling that attempts to perform as much ensembling as it is possible in the allotted time for the last 80%. We dubbed this idea “The Blindfolded Gunslinger” because it fires two guns (models) as much as it can (after a while it will only fire one) but it is blindfolded in the sense that the public LB will be ~0.5 so we cannot be sure if it worked or not until now… Hardware 1 computer with Ryzen 3950x (16c32t) + 64 Gb RAM + 1x3090 1 computer with Threadripper 1950x (16c32t) + 256 Gb RAM + 6x3090 We set up the big computer during the competition which was a project on its own: Also in the last 8 hours of the competition we rented a 190 Gb RAM + 6x3090 but it did not help us much. Software We used pytorch 1.7.1, fastai and we trained using distributed training and mixed precision (both as implemented in fastai). For inference we included the last 500 interactions and summaries in both pickle files and memory-mapped numpy matrices. Source code Available at: https://github.com/jamarju/riiid-acp-pub Please sign in to reply to this topic. comment 36 Comments Hotness Javier Martín Topic Author Posted 4 years ago · 3rd in this Competition arrow_drop_up 7 more_vert The source code is up. @imeintanis kobi2000 Posted 4 years ago · 1487th in this Competition arrow_drop_up 4 more_vert The code is so clean that it makes my eyes wet. Learning a lot in many aspects. Thanks a lot! Javier Martín Topic Author Posted 4 years ago · 3rd in this Competition arrow_drop_up 2 more_vert LOL I've never had anyone say anything this beautiful about something I've written, thanks 😄 Claudio Verdú Ruiz Posted 4 years ago · 58th in this Competition arrow_drop_up 5 more_vert Congrats! No doubt why you got your well deserved 3rd position. Keep it up! Ioannis M Posted 4 years ago · 220th in this Competition arrow_drop_up 5 more_vert Wow very impressive indeed!! Congratulations - are you planning to share the code or at least your Blindfolded Guns approach ? Javier Martín Topic Author Posted 4 years ago · 3rd in this Competition arrow_drop_up 4 more_vert Yes, we will. Allow me a couple of days to recover mentally and clean up. Iván de Prado Posted 4 years ago arrow_drop_up 3 more_vert Congratulations! TIL the T-fixup trick. I think the right link to the paper is http://www.cs.toronto.edu/~mvolkovs/ICML2020_tfixup.pdf Javier Martín Topic Author Posted 4 years ago · 3rd in this Competition arrow_drop_up 0 more_vert You are right, I fixed the link. Thanks for noticing! Theo Viel Posted 4 years ago arrow_drop_up 3 more_vert Thanks for the very interesting read, and congratulations on the 3rd place ! Yu Kang Posted 4 years ago · 3032nd in this Competition arrow_drop_up 3 more_vert Congratulations and amazing sulotion ! One quick question: why you choose an encoder-decoder framework? Since the inputs and outputs are always equal length, is sequence-tagging method more suitable? Javier Martín Topic Author Posted 4 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thank you! We tried a variety of encoder+decoder layouts (0+4, 4+0, 2+4, 4+2, etc.) and those two (3+3, 4+4) turned out the best. I believe we are actually doing sequence tagging, I just didn't know it had such name. KhanhVD Posted 4 years ago · 146th in this Competition arrow_drop_up 3 more_vert Amazing solution. Congratulations on results @bacterio and team. LeiWang66808 Posted 4 years ago · 161st in this Competition arrow_drop_up 2 more_vert Congratulations and thank you for sharing this computer is amazing. Yassine Alouini Posted 4 years ago arrow_drop_up 2 more_vert That computer is impressive. 😄 Congratulations on the third place and thanks for sharing your solution. 👌 Neil Gibbons Posted 4 years ago · 1196th in this Competition arrow_drop_up 2 more_vert Thanks for sharing and congratulations! I have a question about the hardware: was this used only to train a model? Or was it used in the submission process somehow (this would confuse me since I thought submission had to be made through the kernel)? Andrés Miguel Torrubia Sáez Posted 4 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Train only. Neil Gibbons Posted 4 years ago · 1196th in this Competition arrow_drop_up 0 more_vert Thanks very much! Minh Tri Phan Posted 4 years ago · 136th in this Competition arrow_drop_up 2 more_vert Congrats you and your team for the 3rd position!!! Thank you for your detailed solutions, so much to learn cswwp Posted 4 years ago · 24th in this Competition arrow_drop_up 2 more_vert Congrats, learn a lot  from your team. Naresh Jagadeesan Posted 4 years ago · 158th in this Competition arrow_drop_up 2 more_vert Congratulations! Amazing work! 💥🎉 AmorfEvo Posted 4 years ago · 1505th in this Competition arrow_drop_up 2 more_vert Such graphics :D Maher el Ouahabi Posted 4 years ago · 279th in this Competition arrow_drop_up 2 more_vert Wow impresionante. Enhorabuena por ese tercer puesto @bacterio @antorsae !  ¿Cuánto habéis tardado en montar esa bestia de ordenador? Andrés Miguel Torrubia Sáez Posted 4 years ago · 3rd in this Competition arrow_drop_up 5 more_vert Question was how long did it took to set up the computer. Well, I had to build my own chassis using 20x20 aluminum profiles, dual 1600W PSU, some 3d-printed supports, PCIe switches (there's 4 PCIe slots only), etc. I had a lot of issues POSTing w/ 5+ GPUs which I fixed by disabling non-critical stuff in BIOS (e.g. HD audio, etc.). Xataka (Spanish website) will soon release a small post about the build. Stay tuned. 4 more replies arrow_drop_down u++ Posted 4 years ago · 2845th in this Competition arrow_drop_up 2 more_vert Congrats for the 3rd place and thank you for sharing. I was very impressed by the elaborate NN structure. Shujun Posted 4 years ago · 20th in this Competition arrow_drop_up 2 more_vert Wow very nice solution! And insane compute, this comp is very compute hungry! YL Posted 4 years ago arrow_drop_up 0 more_vert Very late congratulations! and thank you very much for the solution write-up. I have a question and just wondering if you could kindly clarify: \"To prevent the transformer from looking into the future we shift the encoder input including both questions + answers to the right\" Since you have already used a causal mask to prevent attention from the future to the past, why is the above operation (shifting the encode input to the right) still needed? many thanks! Javier Martín Topic Author Posted 4 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Let's say the encoder input is: $$QA_1, QA_2,  QA_3, …$$ and the decoder input is: $$Q_1, Q_2, Q_3, …$$ where \\(QA_x\\) is question+answer \\(x\\)'s data and \\(Q_x\\) is question \\(x\\)'s data. Now suppose we don't shift the encoder input: For question \\(Q_x\\) we MUST mask all QA from \\(QA_1\\) up to (and including) \\(QA_x\\), otherwise question 1 would be able to see its own answer and cheat. This means that for \\(Q_1\\) all QAs should be masked but we can't do that because in the attention mechanism, masking means we set the masked values to -inf after calculating \\(QK^T\\). In the case of \\(Q_1\\), the first row would be entirely set to -inf, and that's a problem because the softmax of all -inf is nan: a = torch .triu (torch .full (( 5 , 5 ), -float ( 'inf' )))\ntorch .nn .functional .softmax ( a , 1 ) tensor ( [[   nan,    nan,    nan,    nan,    nan] , [1.0000, 0.0000, 0.0000, 0.0000, 0.0000] , [0.5000, 0.5000, 0.0000, 0.0000, 0.0000] , [0.3333, 0.3333, 0.3333, 0.0000, 0.0000] , [0.2500, 0.2500, 0.2500, 0.2500, 0.0000] ]) content_copy And that's why we always need to shift the encoder to the right in causal applications: there always has to be at least one token to pay attention to so that softmax can be calculated. Julia Posted 4 years ago · 2108th in this Competition arrow_drop_up 0 more_vert Great solution!  Tried to submit the 05_inference.ipynb to Kaggle, but the notebook ran out of the memory.  So I ran this notebook and generated submission.csv locally, and copied submission.csv to the directory \"/kaggle/working/\" and submitted to Kaggle again, and the notebook was stopped after a few minutes with error, but without any score. Have anyone tried to submit the 05_inference.ipynb to Kaggle, and successfully got good score? Javier Martín Topic Author Posted 4 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Hello, @julia5 and thanks for the kind words. You can't run 05 off-line because 80% of the test data is hidden. Try to convert the 05.ipynb nb to .py and submit that instead, it should reduce RAM footprint and that's actually how we did it. The first cell in 05 explains how to do such conversion with the ipynb-py-convert tool. Julia Posted 4 years ago · 2108th in this Competition arrow_drop_up 0 more_vert Thank you for your kind reply, it's really helpful.  I did what you said, and submitted the notebook to Kaggle successfully. Jonathan Bowden Posted 4 years ago arrow_drop_up 0 more_vert Haha, truly embodying the \"python should be fun\" ethos, nice work :) This comment has been deleted. Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Duc-Kinh Le Tran · 4th in this Competition  · Posted 4 years ago arrow_drop_up 124 more_vert 4th place solution : Single Transformer Model Hi everyone, It has been a real pleasure competing in this wonderful challenge.  Thank you Kaggle and the Competition Host for making it possible. I'm happy to share here my solution which got me to the 4th place. It is a single transformer model inspired from previous works (like SAINT, SAKT ) very much discussed in this competition. The model I hope my figure is straightforward. Below are key features of the model I'd like to explain more: Input sequences I tried to include all data available from the train table and metadata tables. I also add time lag , which is the delta time from the previous interaction (questions in the same container have the same timestamp so they share the same time lag ). Also, on the question table, I added 2 features : difficulty level ( correct response rate of each questions), popularity ( number of appearances ), which are computed from the whole training data. Input embeddings Same size of embeddings for all inputs, the embeddings are then concatenated and go through linear transform to feed to the first encoder and decoder of the transformer. Embeddings of continuous features ( time lag , question elapsed time , question difficulty , question popularity ) are computed using a ContinuousEmbedding layer.  The idea of ContinuousEmbedding is to sum up ( weighted sum ) a number of consecutive embedding vectors (from the embedding weight matrix ). This way we have a \"smooth\" version for the embeddings of the continuous variable: 2 values very close together should have similar embeddings. The transformer Input of the encoder are embeddings of all input elements. Input of the decoder doesn't not contain user answer related elements. Encoder and decoder layers are almost the same as in the original paper ( Attention is all you need ). One key difference is of course the causal masks to prevent the current position from seeing the future. The other is a feature that I add to improve the performance and convergence speed : a kind of time aware weighted attention. The idea is to decay the attention coefficient by a factor of d t − w where d t is the difference in timestamp of a position and the position it attends to and w is a trainable parameter constrained to be non-negative ( one parameter per attention head ) . This is pretty easy to implement: compute the timestamp difference matrix in log scale, multiply it with the parameters w and subtract it from the attention logits ( scaled dot product output of the attention layer). Training I use the cv method https://www.kaggle.com/its7171/cv-strategy (thanks @its7171 ). The model was implemented in Tensorflow and trained on TPU with Colab Pro. Sequences are randomly cut and padded to have the same length and all parts are kept for training. The final version of my model has embeddings size of 128, model size of 512, 4 encoder layers and 4 decoder layers. It was trained with the sequence length of 1024 for about 36000 steps ( warmup 4000 steps then cosine decay) and with batch size 64. Training took about 4-5 hours. On the submission kernel I had to reduce sequence length to 512 due to resource limit. Some observations Input embeddings: concatenation is better than sum Longer sequence ( for both training and inference ) improves the performance Model size also matters: bigger model size generally improves the performance but going beyond size of 512 and 4 layers does not improve much. Why not ensemble I didn't have much time toward the end of the competition. When I still made improvement on my single model I made the choice of staying on that rather than spending time on making ensemble of smaller models. I'm not sure it was a good choice,  but it got me this far so I'm still happy. Code Here is the submission kernel that I made public. You should find all my code source training log in the kernel. As you can see it scores 0.8180 on valid set, 0.815 on public LB and 0.817 on private LB. My top scored submission has the same model and training configuration but was trained on the whole training set, which did not improve much. Here is the source code on github including all steps to reproduce the solution. Best regards to all Please sign in to reply to this topic. comment 39 Comments 1 appreciation  comment Hotness KhanhVD Posted 4 years ago · 146th in this Competition arrow_drop_up 3 more_vert From Viet nam, big congratulations to you on 4th place @letranduckinh , amazing solution. Thanks for sharing! mamas Posted 4 years ago · 2nd in this Competition arrow_drop_up 4 more_vert Congrats 4th place! I was very surprised that your model is trained using only Colab TPU. The idea of time-weighted multihead attention is great! I tried similar idea, but I gave up it because of too long training time. Yih-Dar SHIEH Posted 4 years ago · 70th in this Competition arrow_drop_up 3 more_vert @mamasinkgs , why you are surprised training using only Colab TPU. 😄 It is very powerful, in my case, 1M sequences of length 128 takes 5 minutes. A model is trained will be finished in 4-5 hours. Phil Dhingra Posted 4 years ago · 931st in this Competition arrow_drop_up 1 more_vert Thanks for the share! Sorry if this is a newb question, but is it possible to make the TF Records dataset public or provide a link to the one you used? Duc-Kinh Le Tran Topic Author Posted 4 years ago · 4th in this Competition arrow_drop_up 3 more_vert Here is the github link of my solution: https://github.com/dkletran/riiid-challenge-4th-place . You should find all scripts and instruction to regenerate all files you need to train & submit the model. Sorry for the late response. Phil Dhingra Posted 4 years ago · 931st in this Competition arrow_drop_up 0 more_vert Thank you so much, Duc-Kinh, this is a life-saver. Phil Dhingra Posted 4 years ago · 931st in this Competition arrow_drop_up 0 more_vert I noticed that the submission notebook linked to in the GitHub README doesn't work with the weights file generated from the training. The notebook gives a dimension-mismatch error. However, the weights work with your second notebook: https://www.kaggle.com/letranduckinh/riiid-model-submission-4th-solution I've made a \"Late Submission\" with this notebook, and it matches your 0.817 private. Thanks again. _CA℟L_ Posted 4 years ago · 297th in this Competition arrow_drop_up 1 more_vert Congrats ! and its a great solution.. Question: How did you sample the dataset? I see that you have a batch size of 64 and only 2048 steps per epoch.. is that just 64*2048 samples per epoch ? Duc-Kinh Le Tran Topic Author Posted 4 years ago · 4th in this Competition arrow_drop_up 1 more_vert 2048 is not the number of steps for a full traversal of the train dataset. Really sorry the naming in my code is a little misleading. Go through the whole dataset needs around 5000-6000 steps so the model was trained for around 6-7 epochs in the good sense of this word. _CA℟L_ Posted 4 years ago · 297th in this Competition arrow_drop_up 0 more_vert Ahh got it! So the training set has approx 6000*64 =  380K samples (I'm guessing one per user). Why not train the model on each point in the training data (100 Mi) …. use the lookback from that point backwards as input? Duc-Kinh Le Tran Topic Author Posted 4 years ago · 4th in this Competition arrow_drop_up 1 more_vert Basically one sample per user except for users having more than sequence_length (1024) interactions. Training the model on each point in the training data would need a lot more time because in this case the gradient is propagated from only one position of a long sequence. I did try partial label masking as a kind of regularization (compute loss function only for some final positions of the sequence) but the model converges slowly without any improvement on performance. Yih-Dar SHIEH Posted 4 years ago · 70th in this Competition arrow_drop_up 1 more_vert @letranduckinh Congratulations! I have a question about your inputs: It seems to me that put the content and lag time into both encoder / decoder, and user answer/ user correctness / had_explanation into the encoder only. Could you explain a bit your choice? Usually, the content is put in the encoder, and the properties of the answering questions are put into the decoder. Of course, in your case, it works. But I am still wondering the reason behind your design. Maybe you use questions as query, and the historical answer correctness as the key? Javier Martín Posted 4 years ago · 3rd in this Competition arrow_drop_up 3 more_vert I'm also looking forward to @letranduckinh 's answer. We did it the same way and that was exactly our intuition. By putting Q+A in the encoder and Q in the decoder you make the transformer's encoder-decoder attention ask itself the following question: \"Hey, I'm a question about grammar (query), let's match other grammar questions in history (key) and let me know how the user did , ie. answers (value)\" 3 more replies arrow_drop_down Rafi Hai Posted 4 years ago · 16th in this Competition arrow_drop_up 1 more_vert 4th place with a single Transformer, amazing job @letranduckinh and thanks for sharing the code! Regarding this sentence: Input of the encoder are embeddings of all input elements. Input of the decoder doesn't not contain user answer related elements Could you give insights on how you made the decision? Did it work better for CV to include features in both encoder & decoder? Andrés Miguel Torrubia Sáez Posted 4 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Congratulations on your solution and very clean code. Is ContinuousEmbedding similar to https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html where you want to compute the sum or average of multiple embeddings but you are not interested in each one individually? (We used the latter for question tags) Duc-Kinh Le Tran Topic Author Posted 4 years ago · 4th in this Competition arrow_drop_up 0 more_vert I added a schema to explain this, hope it helps. CoreyJamesLevinson Posted 4 years ago arrow_drop_up 1 more_vert What's the difference between a Continuous Embedding and a Linear layer? Ty, and congratulations. Duc-Kinh Le Tran Topic Author Posted 4 years ago · 4th in this Competition arrow_drop_up 0 more_vert I updated my post with with a schema to explain this. cswwp Posted 4 years ago · 24th in this Competition arrow_drop_up 1 more_vert In saint paper, continuous is just a linear layer without bias @returnofsputnik Allohvk Posted 4 years ago · 302nd in this Competition arrow_drop_up 2 more_vert Creating a difficult model is easy. Creating an easy model is difficult! Creating an easy model that breaks into top 5 is most difficult. Hearty congratulations! These are the kind of solutions that have strong potential for the hosts.. Old Monk Posted 4 years ago · 585th in this Competition arrow_drop_up 0 more_vert Amazing, awesome and very clean solution! Thanks for sharing! Phil Dhingra Posted 4 years ago · 931st in this Competition arrow_drop_up 0 more_vert I'm also having trouble understanding what data_map_512.pickle is. I'm able to see it being loaded, and we have the file, but I don't know how it's generated. Nya 🚀 Posted 4 years ago · 830th in this Competition arrow_drop_up 0 more_vert @letranduckinh Amongst all the notebooks I've seen in this competition, I think that yours was the most well-organized and readable. Thank you for sharing your solution 🙂 A question: Do you have a comparison of the importance of each feature used by your model? Duc-Kinh Le Tran Topic Author Posted 4 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thank you for your kind words. Unfortunately I don't have a quantitative analysis of feature importance of my model. All I can say is that I already tried to remove some features from the model, it performed worse or sometimes no significant changes on valid set.  In the early stage of the competition, when I added time lag it gave an important boost, answered correctly, user answer are also important. fortune cookie Posted 4 years ago arrow_drop_up 0 more_vert Congratulations ! I have spent some time reading all the codes of your great solution. But I'm confused why there is a first_token_embedding before each sequence for the encoder. Thanks ! Duc-Kinh Le Tran Topic Author Posted 4 years ago · 4th in this Competition arrow_drop_up 3 more_vert Thanks. Here's the deal with first_token_embedding : because of the causal mask of the encoder - decoder attention (a position only attends to future position, excluding positions with the same timestamp), no position (of the encoder output) attends to the first position of the decoder. In the multi-head attention layer, attention masks are achieved by subtracting 1e9 from the attention logits; which doesn't work when no position attends to a position (because in that case, all attentions logits at the target position are subtracted by 1e9; these subtracted terms are canceled out when computing the softmax and as a result the mask has no effect) . To avoid this situation the encoder output is pre-padded with a first_token_embedding which attends to all positions of the decoder. cswwp Posted 4 years ago · 24th in this Competition arrow_drop_up 0 more_vert Nice solution,  i also find increase the sequence length will give big boost, and also the model training time will increase by O(N^2) Philip Le Posted 4 years ago · 2314th in this Competition arrow_drop_up 0 more_vert Thanks for sharing the code, @letranduckinh . Congrats on your gold & 4th position. How was your experience with TPU in Google Colab Pro, compared to its GPU, and other Cloud platforms? Duc-Kinh Le Tran Topic Author Posted 4 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thanks. I didn't have much experience with TPU, only started using Colab TPU recently for Kaggle competitions because it is cheaper than cloud GPU. Theo Viel Posted 4 years ago arrow_drop_up 0 more_vert 4th with a single model is impressive ! Congratz ! Minh Tri Phan Posted 4 years ago · 136th in this Competition arrow_drop_up 0 more_vert The amazing solution, I love the way it is relatively straightforward to understand, also a single model and especially, trainable on the Google Colab TPU, which is totally approachable to everyone. Congratulations on your winning! Duc-Kinh Le Tran Topic Author Posted 4 years ago · 4th in this Competition arrow_drop_up 0 more_vert Just added a new figure to explain the embeddings of continuous variables. Javier Martín Posted 4 years ago · 3rd in this Competition arrow_drop_up 0 more_vert This is a very clean code an straight forward solution, thanks for sharing MPWARE Posted 4 years ago · 16th in this Competition arrow_drop_up 0 more_vert Congratulations, another single model solution! u++ Posted 4 years ago · 2845th in this Competition arrow_drop_up 0 more_vert Thank you for sharing and congrats on your solo gold & 4th place🎉 Appreciation (1) TroubleJ Posted 4 years ago · 107th in this Competition arrow_drop_up 0 more_vert Congratulations! Thank you for sharing! Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Ahmet Erdem · 6th in this Competition  · Posted 4 years ago arrow_drop_up 146 more_vert 6th Place Solution: Very Custom GRU First of all congrats to @keetar and @mamasinkgs and other top teams. I will shortly explain my solution. It is late here, so I may be missing some points. Some General Details Used questions only. Lectures improve validation score but increase train-val gap and don’t improve LB. Didn't use dataframes. Used numpy arrays partitioned by user ids. Sequence length 256 15 epochs. 1/3 of the data each time with reducing LR. 8192 batch size Ensemble of the same model with 7 different seeds trained on whole data 0.8136 single model validation score, 0.813 LB. Ensemble: 0.815. 8 hours training on 4-GPU machine Used Github and committed any improvement with a message like: Add one more GRU layer (Val: 0.8136, LB: 0.813) Inputs Engineered Features Assume current question’s correct answer is X. Logarithm of: Number of questions since last X. Length of current X streak. (can be zero) Length of current streak on any non-X answer. (can be zero) This helps with users who always pick A as answer etc. Embeddings Content Cosine Similarity A bit similar to attention with 16 heads Linear transformation and l2 norm applied on content vectors For 16 different transformation, cosine similarity between current content and history contents are calculated. Transformation is symmetric for content and history contents. U-GRU: GRU with 2 directions but not BiGRU First does reverse pass, concatenates the output and then does forward pass MLP: 2 layers of [Linear, BatchNorm, Relu] Edit: Part embedding is trainable. There is actually sigmoid x tanh layer before GRUs. Please sign in to reply to this topic. comment 23 Comments 2 appreciation  comments Hotness KhanhVD Posted 4 years ago · 146th in this Competition arrow_drop_up 5 more_vert Great solution, congrats on 6th place @aerdem4 ! Neil Gibbons Posted 4 years ago · 1196th in this Competition arrow_drop_up 1 more_vert Brilliant, thanks for sharing and congrats! Rakesh Jarupula Posted 4 years ago · 2920th in this Competition arrow_drop_up 1 more_vert Congratulations !!! 🔥🔥. I have few questions(May be not related to your solution) about getting started with competition as a beginner. What are the resources(other than hardware) you have referred during the competition. As I am new to the kaggle competitions… Do I need industry level knowledge about the kind of problems given in the competition. What are some tips to beginners like me. Ioannis M Posted 4 years ago · 220th in this Competition arrow_drop_up 1 more_vert Well done @aerdem4 !! Very innovative sol!! especially your training scheme with the U GRU and the CSS transformation.. are you planning to share any code (full or key parts) ? Ahmet Erdem Topic Author Posted 4 years ago · 6th in this Competition arrow_drop_up 1 more_vert Thank you. Normally I share my code but this time it is in notebooks rather than well-organized scripts. I don't know if it will be readable. Ioannis M Posted 4 years ago · 220th in this Competition arrow_drop_up 0 more_vert Thanks, maybe just the model/dataset definition would be helpful and nice learning task for anyone that is interested Ahmet Erdem Topic Author Posted 4 years ago · 6th in this Competition arrow_drop_up 1 more_vert I hope my inference kernel can be enough for that, therefore I made it public now: https://www.kaggle.com/aerdem4/riiid-starter mamas Posted 4 years ago · 2nd in this Competition arrow_drop_up 1 more_vert It's really terrific to get to such a high score without Transformer. Congrats! Theo Viel Posted 4 years ago arrow_drop_up 1 more_vert Congratz, and thanks for the very nice write-up ! Darragh Posted 4 years ago · 11th in this Competition arrow_drop_up 1 more_vert Well done Ahmet, I like the idea of tracking if all A's are chosen - like run length encoding. What hidden dimension did you use in the GRU to fit 8192 in batch ? Ahmet Erdem Topic Author Posted 4 years ago · 6th in this Competition arrow_drop_up 2 more_vert GRU hidden dimension was 128. Larger GRUs were overfitting. Model size is around 13 MB, which is mostly content embedding weights. SIMPLE Posted 4 years ago arrow_drop_up 1 more_vert Congrats, thank you for sharing, the U-GRU part is very interesting. I see that the dimensions of your embedding are different, how do they blend together? Ahmet Erdem Topic Author Posted 4 years ago · 6th in this Competition arrow_drop_up 0 more_vert I concatenate them. Furkan Ömerustaoğlu Posted 4 years ago · 38th in this Competition arrow_drop_up 1 more_vert Congrats, very nice approach, thanks for sharing. cswwp Posted 4 years ago · 24th in this Competition arrow_drop_up 1 more_vert Congrats, simple and efficient model👍 Daniels Posted 4 years ago · 35th in this Competition arrow_drop_up 1 more_vert Nice solution！Congratulations! Maher el Ouahabi Posted 4 years ago · 279th in this Competition arrow_drop_up 1 more_vert Brilliant approach. Congratulations! u++ Posted 4 years ago · 2845th in this Competition arrow_drop_up 1 more_vert Impressive solution. Thank you for sharing. Andrés Miguel Torrubia Sáez Posted 4 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Wow. Extremelly innovative. Congratulations. Ahmet Erdem Topic Author Posted 4 years ago · 6th in this Competition arrow_drop_up 2 more_vert Thanks @antorsae Congrats to you and your teammate with the 3rd place! MPWARE Posted 4 years ago · 16th in this Competition arrow_drop_up 1 more_vert Nice solution! AmorfEvo Posted 4 years ago · 1505th in this Competition arrow_drop_up 2 more_vert Suspect for special award ^^ Appreciation (2) levonian Posted 4 years ago · 54th in this Competition arrow_drop_up 1 more_vert Very interesting, thanks for sharing! Rodolphe Lampe Posted 4 years ago · 69th in this Competition arrow_drop_up 1 more_vert Very interesting thanks ! Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules chlxyd · 8th in this Competition  · Posted 4 years ago arrow_drop_up 40 more_vert #8 solution: Ensemble of 15 same NN models Hi all, Learned a lot from other's solutions! In this post, I would like to share some insights in my solution. In summary, my best single NN model could achieve 813/815 public/private score, by ensemble of 5 folds and 3 snapshots in each fold, finally 15 nn models achieve the 814/816 public/private score. With 15 nn models, the online inference cost less than 4 hours , thus it can ensemble at least 30 models in this pipeline. Dataset split I split the train and valid set by several steps: Calculate all unique users in dataset. Select 5% users totally in the valid set, and 45% users totally in the train set. For the least 50% users, random split the user data into train and valid set by time. Thus we have both individual user in train and valid set, and also have many users who appear in both train and valid set. This split can achieve less than 0.001 score difference compared with LB. By changing different random seed, we can get different folds. Feature engineering Since there are many detailed FE in other's post, I will just share some important points. Basic features Evaluate user ability We can evaluate user ability by his history action, including the correctness, time elapsed, lag time, and so on. These features can be calculated on not only content level, but also on same part, same tags, same content and so on. These features can also be extended based on time, for example, we can make a feature which is the user correctness in last 60 seconds. Evaluate content difficulty We can evaluate content difficulty by calculate its global accuracy, std, average time elapsed and so on. Evaluate User x Content features Even a content is difficult, the user may still skill enough and can solve it correctly. Thus we also need to describe how the user could perform on this content. user_acc_diff: For a content, if the content has low global acc, but the user answered it correctly, the user might above the average of all users. We can use the $logloss(content_global_average_acc, user_answer)$ to evaluate the difference. user_elapsed_diff: like the user_acc_diff, we can also evaluate user time elapsed difference in user history contents. We can dig many features on above three fields, for example, user's average lag time on history content/ history same part content/ history same tag content.. could also be a useful features. Other features/tricks Time related features: last_content_timestamp_diff, last_lag_time and its statistical information in history. Abnormal Users: If a user answered every content less in 4 seconds, and all his chose answer are same (such as C), then if the correct answer of next content is C, we can believe he will correctly answered next content. Learned lectures for a specific content: If there is a content-lecture-content pattern in user history, and the two content are same contents. It might the user learned a specific lecture for this content, which means in the second time, the probability he answered it correctly is high. Wrong answer ratio: There might be a pattern like \"select C as default for hard contents\".  Thus calculating the ratio of user choice on his incorrectly content can tell as weather the user could lucky guess current content even he don't know he correct answer. Also, features such as current content id/current timestamp/current part are also used. Finally, I get 120 dim features, which can get public 0.806 by a single lgb model in single fold. P.S. The categorical feature (set on content id) of lightgbm can boost my score about 0.003. There are also many useful hint which could improve the speed and save the memory: Do not use pandas to calculate features, transfer it to numpy or just python. Using a class to store user information, and save each user information in individual file. In the inference phase, we can only read the user information for who appeared in the test set, the total number of users in test set are less than 10,000. This is the key to reduce memory. If the memory still not enough, the LRU-cache could be used to remove unused users. In my experiment, using numpy array to store information cost more disk space compared with python variable. NN Since it is very late when I notice the key to the top is NN model, I don't have much time analysis the NN models, especially design a specific features or structures. To save the time, I use the lightgbm features as the NN input (Time axis is added and seq len is 128). Robust Standard Normalization There are many outlier in the features from lightgbm, thus simply utilize standard normalization can hardly get desired results. I utilized the robust standard normalization to normalize all the features: def robust_normalization( column ):\n    cur_mean = np.nanmedian( column )\n    cur_qmin, cur_qmax = np.nanpercentile(cur,[ 2.5 , 97.5 ])\n    cur_std = np.nanstd( column [( column >=cur_qmin) & ( column <=cur_qmax)])\n    column = np.clip( column , a_min = cur_qmin, a_max = cur_qmax)\n    column = ( column -cur_mean)/cur_std return column content_copy Models My NN model is very simple, just a transformer encoder with a fc classifier. The encoder has 4 transformer layers and embed dim is 128, no other modifications. The size of model file is only 7M, that's why I can ensemble many models on inference phase. The model can achieve 813/815 on single model/single fold. and 814/816 on 5 folds ensemble. There are much time left on inference, I use 3 snapshot ensemble on each fold, which can also boost some scores. Please sign in to reply to this topic. comment 10 Comments Hotness KhanhVD Posted 4 years ago · 146th in this Competition arrow_drop_up 1 more_vert Congrats @chlxyd on 8th place and thanks for sharing details solution ZZ Posted 4 years ago · 556th in this Competition arrow_drop_up 0 more_vert Congrats on the great solution and nice finish! Could you elaborate on how to save each user's information separately, with 400K pickle files or h5py? Also would like to learn more about your implementation on LRU-Cache . Besides, could you share a little bit more about your NN model structure and params, any code snippet would be helpful. Thanks~! chizhu Posted 4 years ago · 35th in this Competition arrow_drop_up 0 more_vert congrats! ddw 安静 Posted 4 years ago · 74th in this Competition arrow_drop_up 0 more_vert congrats @chlxyd Fengari Posted 4 years ago arrow_drop_up 0 more_vert congrats bro! Jiwei Liu Posted 4 years ago · 1139th in this Competition arrow_drop_up 0 more_vert big big congrats bro! HAO Posted 4 years ago · 23rd in this Competition arrow_drop_up 0 more_vert I have a question about the ensembling. Why don't you ensemble nn model with lgb model. For me, when ensembling a nn model(0.806) with a lgb model(0.798), the LB score is 0.809. I believe you must have a better lgb model than mein. chlxyd Topic Author Posted 4 years ago · 8th in this Competition arrow_drop_up 1 more_vert My LGB and NN are from the same feature group, thus I can get only 0.0002 boost after my NN score higher than 0.810, that's why I give up the LGB model in the later submission. HAO Posted 4 years ago · 23rd in this Competition arrow_drop_up 0 more_vert Great post! Learn a lot from it, thanks! MeisterMorxrc Posted 4 years ago · 74th in this Competition arrow_drop_up 0 more_vert Congratulations on winning the solo gold medal~~， hange tql👍👍👍 Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules nyanp · 9th in this Competition  · Posted 4 years ago arrow_drop_up 74 more_vert 9th place solution : 6 Transformers + 2 LightGBMs First of all, thanks to the hosts for a great competition! This was one of the toughest competitions I have ever entered, but well worth the effort. Prediction of our team (tito @its7171 and nyanp @nyanpn )consists of following models. tito's transformer: LB 0.813 nyanp's SAINT+ transformer: LB 0.808 nyanp's LightGBM: LB 0.806 Simple blending of these models scored LB 0.814 / private 0.816. Pipeline We converted the entire train.csv data into hdf5, and loaded only the user_id that appeared during inference into a np array (97% RAM savings compared to hold entire training data in RAM). We estimate that the overhead due to I/O in hdf is ~45 minutes. This some overhead allowed us to combine tito's large transformer with nyanp's feature engineering pipeline. Transformer (tito, LB 0.814) This is transformer model with encoder only, based on @claverru 's nice kernel summary only trained and predicted for answered_correctly of last question of the sequence. all features are concatenated (only position encoding is added). used lectures, in timestamp order as it is Window size 300-600 batch size 1000 drop_out 0 n_encoder_layers 3-5 augmentation to replace content_ids with dummy ids at a certain rate kept only one question in the last task to avoid leaks features embedded or dense was decided by CV. (embedded) content id (embedded) part id (embedded) same task question size (dense) answered_correctly (dense) had_explanation (dense) elapsed time (dense) lag time (dense) diff of timestamp from the last question combining models To avoid the overhead of calling model.predict() multiple times for ensemble, I made a combined model that links four models. inputs = tf.keras.Input(shape=(input_shape, n_features))\nout1 = model1(inputs[:,-window_size1:,:])\nout2 = model2(inputs[:,-window_size2:,:])\nout3 = model3(inputs[:,-window_size3:,:])\nout4 = model4(inputs[:,-window_size4:,:])\ncombo_model = tf.keras.Model(inputs, [out1,out2,out3,out4]) content_copy SAINT+ (nyanp, LB 0.808) d_model = 256 window_size = 200 n_layers = 3 attention dropout = 0.03 question, part, lag are embed to encoder response, elapsed time, has_explanation are embed to decoder To prevent leakage, in addition to upper triangular attention mask, I masked the loss in questions other than the beginning of each task_container_id. Questions with the same task_container_id were shuffled in each batch during training, and the loss weights were adjusted to reduce the effect of masks. This mask improved LB by 0.0003. (Note: I believe that indirect leaks still exist, but I've spent 80% of my time on the LightGBM implementation and data pipeline, so I couldn't improve it any further) Other than that, there is nothing special about this NN. It scored .806 in single, .808 by averaging 2 models. LightGBM (nyanp, LB 0.806) LightGBM models are trained on 264 features. To speed up inference, I fixed the number of trees to 3000 and ensembled two models with different seeds (This is better than single large LGBM in terms of both speed and accuracy). By compiling this model with treelite , inference time became 3x faster (~10ms/batch, ~10min in total). features I mapped prior_question_* rows with their respective rows by following code and utilized them in some features. df[ 'elapsed_time' ] = df.groupby( 'user_id' )[ 'prior_question_elapsed_time' ].shift(- 1 )\ndf[ 'elapsed_time' ] = df.groupby([ 'user_id' , 'timestamp' ])[ 'question_elapsed_time' ].transform( 'last' )\n\ndf[ 'has_explanation' ] = df.groupby( 'user_id' )[ 'prior_question_had_explanation' ].shift(- 1 )\ndf[ 'has_explanation' ] = df.groupby([ 'user_id' , 'timestamp' ])[ 'has_explanation' ].transform( 'last' ) content_copy Here is the list of my features. There is no magic here; no single feature boost CV more than 0.0002. I repeated feature engineering based on well-known techniques and a little bit of domain knowledge. question features count encoding target encoding number of tags one-hot encoding of tag (top-10 frequent tags) SVD, LDA, item2vec using user_id x content_id matrix LDA, item2vec using user_id x content_id matrix (filtered by answered_correctly == 0) Typical word2vec model is trained on next-word prediction task. By constructing word2vec model over incorrectly answered questions, the latent vectors extracted from the model can be used to capture which incorrect question are likely to co-occur with each other. 10%, 20%, 50%, 80% elapsed time of all users response, correct response, wrong response SAINT embedding vector + PCA user features avg/median/max/std elapsed_time avg/median/max/std elapsed_time by part avg has_explanation flag avg has_explanation flag by part nunique of question, part, lecture cumcount / timestamp avg answered_correctly with recent 10/30/100/300 questions, recent 10min/7 days avg answered_correctly by part, question, bundle, order of response, question difficulty, question difficulty x part question difficulty: discretize the avg answered_correctly of all users for each question into 10 levels cumcount by part, question, bundle, question difficulty, question difficulty x part lag from 1/2/3/4 step before correctness, lag, elapsed_time, has_explanation in the same question last time tag-level aggregation features calculate tag-level feature for each user x tag, then aggregate them by min/avg/max cumcount of wrong answer, cumcount of correct answer, avg target, lag (estimated elapsed time) - (avg/median/min/max elapsed time within same part) estimated elapsed time = (timestamp - prev timestamp) / (# of questions within the bundle) (estimated elapsed time) - (10%, 20%, 50%, 80% elapsed time of all users correct/wrong response) Because TOEIC part1-4 questions are usually answered after listening to the conversation, the correct answers tend to be concentrated immediately after the conversation ends task_container_id - previous task_container_id I'm not sure why this worked. There might be a difference in the correct rate if people answered from different devices than usual (multi-user?). part of last lecture lag from last lecture whether the question contains the same tag as the last lecture whether the part is the same as the previous question median lag - median elapsed_time part of the first problem the user solved lag - median lag inner-product of user-{correct|incorrect}-question-vector and question-vector user-correct-question-vector: average of LDA vectors for each question that the user answered correctly. rank of lag compared to the same user's past lag (filtered by answered_correctly == 0, 1 respectively) feature calculation Instead of updating the user dictionary, I calculate user features from scratch for each bacth. The numpy array of historical data was loaded from the hdf storage and then split into questions and lectures, which were then wrapped in a pandas-like API and passed to their respective feature functions. @feature( 'lag1.user_lecture' ) def lag1_user_lecture ( df: RiiidData, pool: DataPool ) -> np.ndarray: \"\"\"\n    time elapsed from last lectures for each user\n\n    :param df: data in current batch\n    :param pool: cached data storage\n    \"\"\" lag1 = {} for u, t in set ( zip (df.questions[ 'user_id' ], df.questions[ 'timestamp' ])):\n        past_lect = pool.users[u].lectures # history of lecture if len (past_lect) > 0 :\n            lag1[u] = t - past_lect[ 'timestamp' ][- 1 ] return np.array( list ( map ( lambda x: lag1.get(x, np.nan), df.questions[ 'user_id' ])), dtype=np.float32) content_copy For training, I use the same functions as inference time. This way, there were almost no restrictions on feature creation, and I did not have to worry about bugs of train-test difference. The feature functions were frequently benchmarked by a dedicated benchmark script, and functions with high overhead were optimized by various ways (numba, bottleneck and various algorithm improvement) . The question features were precomputed and made into a global numpy array of shape (13523, *) and merged into the feature data using fancy index. @feature( tuple ( f'question_item2vec_ {i} ' for i in range ( 20 ) ) ) def question_item2vec ( df: RiiidData, _: DataPool ): \"\"\"\n    question embedding vector using item2vec\n    \"\"\" qid = df.questions[ 'content_id' ]\n\n    ret = QUESTION_ITEM2VEC[qid, :] # mere fancy index, faster than pd.merge return ret content_copy Other ideas TTA on SAINT+ by np.roll (it did improve SAINT+ by 0.001+, but we couldn't include it because of submission timeout) linear blending based on public LB labels (timeout, too) Feedback on Time-Series API Competition Although the competition was well-designed, our team still found that the time-series API allowed us to obtain private score information with 1-bit probing even after the known vulnerability was fixed. env = riiideducation.make_env()\niter_test = env.iter_test()\n\nground_truth = []\nprediction = []\nthreshold = 0.810 for idx, (test_df, sample_prediction_df) in enumerate (iter_test):\n    ground_truth.extend( list (test_df[ 'answered_correctly' ].values))\n\n    predicted = model.predict(...)\n\n    prediction.extend( list (predicted)) if len (prediction) >= 2500000 :\n        private_auc = roc_auc_score(prediction[ 500000 :], ground_truth[ 500000 :]) if private_auc < threshold: raise RuntimeError() # private auc of this submission is guaranteed to exceed .810 if submission is succeeded content_copy By using this probing, we can select the final submission with the highest private score, or determine the best ensemble weight for private score by using the hill-climbing method. We contacted the Kaggle Team and asked them if this is legal, and they said it was a \"low signal matter\". We still think this is a gray-area and decided not to use this probing. We think it wasn't critical in this competition, but if there is a future competition with the same format but without AUC metrics, the hack with the \"magic coefficient\" could improve the ranking significantly. If a competition with the same format is held on Kaggle in the future, we suggest the Kaggle team to fix this problem (e.g. put a dummy value in the private label during the competition). Please sign in to reply to this topic. comment 16 Comments Hotness JT-karl Posted 4 years ago · 143rd in this Competition arrow_drop_up 1 more_vert Congratz @nyanpn , many thanks for sharing ! Can give more details about LDA on user_id x content_id matrix ? Is the LDA trained on the number of occurrences ? How did you choose a well-fitted n_components ? Thanks again @its7171 for your great kernels. It seems almost everyone relied on your cv strategy … nyanp Topic Author Posted 4 years ago · 9th in this Competition arrow_drop_up 4 more_vert Thanks, I've made my FE code public. Hope this helps ;) https://www.kaggle.com/nyanpn/lda-feature-for-riiid-part-of-9th-place-lightgbm nyanp Topic Author Posted 4 years ago · 9th in this Competition arrow_drop_up 3 more_vert How did you choose a well-fitted n_components ? The n_component for LDA is set to 10, but I don't know if this is the best because I didn't have time to explore other values. Since the number of questions is only ~15,000, setting the n_component larger than 30 will probably not have much effect. JT-karl Posted 4 years ago · 143rd in this Competition arrow_drop_up 1 more_vert Awesome, thanks !! Yih-Dar SHIEH Posted 4 years ago · 70th in this Competition arrow_drop_up 1 more_vert @nyanpn Congrats! About Instead of updating the user dictionary , I calculate user features from scratch for each bacth. content_copy Do you mean, even during the inference (submission time), once you get the prior_user_answer and prior_answer_correctness , you use them along with other standard information from test_df in the previous inference batch, and from these raw data to calculate all the features you created? nyanp Topic Author Posted 4 years ago · 9th in this Competition arrow_drop_up 1 more_vert Exactly. I kept the raw historical data for each user in a numpy array, and prior_user_answer and prior_answer_correctness were also merged into the historical data once and then used for features in the same fashion as other standard information. nyanp Topic Author Posted 4 years ago · 9th in this Competition arrow_drop_up 1 more_vert Exactly. I kept the raw historical data for each user in a numpy array, and prior_user_answer and prior_answer_correctness were also merged into the historical data once and then used for features in the same fashion as other standard information. Yih-Dar SHIEH Posted 4 years ago · 70th in this Competition arrow_drop_up 1 more_vert @nyanpn Thank you for confirmation. That's indeed a good way to avoid the inconsistency between the training and inference time, which I introduced (these errors) at the end of the competition. mamas Posted 4 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Congrats nyanp, your feature is really great :)! Wow… seems you found the security hole of Time-Series API. I think kaggle team has to deal with this problem in future competitions. Yih-Dar SHIEH Posted 4 years ago · 70th in this Competition arrow_drop_up 1 more_vert there is also an ongoing competition that relates to time series API, Jane Street Market Prediction Not sure if there is a hole there, but better for Kaggle to checkout. mamas Posted 4 years ago · 2nd in this Competition arrow_drop_up 1 more_vert I guess there is no hole in Jane, because it has Forecasting Timeline . nyanp Topic Author Posted 4 years ago · 9th in this Competition arrow_drop_up 2 more_vert Thanks @mamasinkgs and congrats too! I personally don't think this issue is a \"low signal matter\" at all. I hope it will be dealt with appropriately. I guess there is no hole in Jane, because it has Forecasting Timeline. Yes. the problem occurs when the Time-Series API is combined with the Synchronous Code Competition format ; a competition where recalculation is performed for the Private Leaderboard would not have the problem. Dean Posted 4 years ago · 37th in this Competition arrow_drop_up 2 more_vert Congrats nyanp! Could you show the sample code about how to  get item2vec? Thank you. nyanp Topic Author Posted 4 years ago · 9th in this Competition arrow_drop_up 9 more_vert item2vec is a simple idea to run word2vec with a sequence of question IDs as sentences, and turn latent expressions into features. Please note that it cannot be run in kaggle notebook due to lack of RAM. import pandas as pd import numpy as np from gensim.models import Word2Vec\n\nEMBEDDING_DIM = 20 train = pd.read_csv( 'train.csv' )\nquestions = pd.read_csv( 'questions.csv' )\n\ntrain = train[(train.content_type_id == 0 ) & (train.answered_correctly == 0 )]\n\nsentences = train.groupby( 'user_id' )[ 'content_id' ].apply( lambda x: [ str (t) for t in x]).values\n\nmodel = Word2Vec(sentences, size=EMBEDDING_DIM , window= 100 , seed= 0 , workers= 16 )\n\nresult_vector = np.zeros(( len (q), EMBEDDING_DIM )) for i in range ( len (q)): try :\n        result_vector[i, :] = model.wv[ str (i)] except : pass content_copy KhanhVD Posted 4 years ago · 146th in this Competition arrow_drop_up 2 more_vert Congrats @nyanpn and tito on 9th place and thanks for sharing your team solution. Thanks tito for cross validation strategy. nyanp Topic Author Posted 4 years ago · 9th in this Competition arrow_drop_up 2 more_vert Thanks! I also agree that @its7171 's validation is awesome. Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Darragh · 11th in this Competition  · Posted 4 years ago arrow_drop_up 54 more_vert 11th Place Solution Thanks to my teammate Akihiko for competing with me on this, there was great learning from the community for both of us. And thanks to our hosts for a wonderful challenge. Congrats all who competed. Solution has heavily inspired by Bestfitting's TalkingData solution which we noticed a lot of the people in this competition took part in :) We used around 35 features including the raw, in a 2 LSTM layer model, each layer single direction trained on sequence length 256, infer on 512. Batchsize for training 2048. Hidden layer size of 512. Example below. First layer, Used features below in embcatq , no continuous features. No label in first layer, like in the SAINT paper. Added the difference of some of the embedding to the final embedding. This gives the model info on how similar each historical question was to the question in the sample. Second layer, Outputs of first layer and included continuous features. Added embedding for interaction of question and chosen answer. Continuous features generated using @its7171 great book - this is why I would like to be able to give multiple upvotes. One important feature was the answer ratio, what percentage of students picked the same answer as the chosen answer. How to handle the histories in memory was a problem, but there was plenty of space for it on the GPU, so loaded that first to numpy then to a torch tensor on GPU (history features took around 6GB); then loaded the other objects to RAM. Attention did not work for us - looked promising on validation though - should have persisted with it.  We just took the final hidden cell from the LSTM as output. The below got ~ 0.811 public, by make some changed to the architecture and bagging four models, lifted to 0.813 public, 0.816 private. class LearnNet (nn.Module): def __init__ ( self, modcols, contcols, padvals, extracols, \n                 dropout = 0.2 , hidden = args.hidden ): super (LearnNet, self ).__init__() self .dropout = nn.Dropout(dropout) self .modcols = modcols + extracols self .contcols = contcols self .emb_content_id = nn.Embedding( 13526 , 32 ) self .emb_content_id_prior = nn.Embedding( 13526 * 3 , 32 ) self .emb_bundle_id = nn.Embedding( 13526 , 32 ) self .emb_part = nn.Embedding( 9 , 4 ) self .emb_tag= nn.Embedding( 190 , 8 ) self .emb_lpart = nn.Embedding( 9 , 4 ) self .emb_prior = nn.Embedding( 3 , 2 ) self .emb_ltag= nn.Embedding( 190 , 16 ) self .emb_lag_time = nn.Embedding( 301 , 16 ) self .emb_elapsed_time = nn.Embedding( 301 , 16 ) self .emb_cont_user_answer = nn.Embedding( 13526 * 4 , 5 ) self .tag_idx = torch.tensor([ 'tag' in i for i in self .modcols]) self .cont_wts = nn.Parameter( torch.ones( len ( self .contcols)) ) self .cont_wts.requires_grad = True self .cont_idx = [ self .modcols.index(c) for c in self .contcols] self .embedding_dropout = SpatialDropout(dropout) self .diffsize = self .emb_content_id.embedding_dim + self .emb_part.embedding_dim + \\ self .emb_bundle_id.embedding_dim + self .emb_tag.embedding_dim * 7 IN_UNITSQ = self .diffsize * 2 + \\ self .emb_lpart.embedding_dim + self .emb_ltag.embedding_dim + \\ self .emb_prior.embedding_dim + self .emb_content_id_prior.embedding_dim + \\ len ( self .cont_idxcts)\n        IN_UNITSQA = ( self .emb_lag_time.embedding_dim + self .emb_elapsed_time.embedding_dim + \\ self .emb_cont_user_answer.embedding_dim) + len ( self .contcols)\n        LSTM_UNITS = hidden self .diffsize = self .emb_content_id.embedding_dim + self .emb_part.embedding_dim + \\ self .emb_bundle_id.embedding_dim + self .emb_tag.embedding_dim * 7 self .seqnet1 = nn.LSTM(IN_UNITSQ, LSTM_UNITS, bidirectional= False , batch_first= True ) self .seqnet2 = nn.LSTM(IN_UNITSQA + LSTM_UNITS, LSTM_UNITS, bidirectional= False , batch_first= True ) self .linear1 = nn.Linear(LSTM_UNITS * 2 + len ( self .contcols), LSTM_UNITS// 2 ) self .bn0 = nn.BatchNorm1d(num_features= len ( self .contcols)) self .bn1 = nn.BatchNorm1d(num_features=LSTM_UNITS * 2 + len ( self .contcols)) self .bn2 = nn.BatchNorm1d(num_features=LSTM_UNITS// 2 ) self .linear_out = nn.Linear(LSTM_UNITS// 2 , 1 ) def forward ( self, x, m = None ): ## Continuous contmat  = x[:,:, self .cont_idx]\n        contmat = self .bn0(contmat.permute( 0 , 2 , 1 )) .permute( 0 , 2 , 1 )\n        contmat = contmat * self .cont_wts\n\n        content_id_prior = x[:,:, self .modcols.index( 'content_id' )] * 3 + \\\n                            x[:,:, self .modcols.index( 'prior_question_had_explanation' )]\n        embcatq = torch.cat([ self .emb_content_id(x[:,:, self .modcols.index( 'content_id' )].long()), self .emb_part(x[:,:, self .modcols.index( 'part' )].long()), self .emb_bundle_id(x[:,:, self .modcols.index( 'bundle_id' )].long()), self .emb_tag(x[:,:, self .tag_idx].long()).view(x.shape[ 0 ], x.shape[ 1 ], - 1 ), self .emb_prior(x[:,:, self .modcols.index( 'prior_question_had_explanation' )].long() ), self .emb_lpart(x[:,:, self .modcols.index( 'lecture_part' )].long()), self .emb_ltag(x[:,:, self .modcols.index( 'lecture_tag' )].long()) , self .emb_content_id_prior(  content_id_prior.long()),\n            ], 2 )\n        embcatqdiff = embcatq[:,:,: self .diffsize] - embcatq[:,- 1 ,: self .diffsize].unsqueeze( 1 ) # Categroical embeddings embcatqa = torch.cat([ self .emb_cont_user_answer(x[:,:, self .modcols.index( 'content_user_answer' )].long()), self .emb_lag_time(x[:,:, self .modcols.index( 'lag_time_cat' )].long()), self .emb_elapsed_time(x[:,:, self .modcols.index( 'elapsed_time_cat' )].long())\n            ] , 2 ) #embcatqadiff = embcatqa - embcatqa[:,-1].unsqueeze(1) embcatq = self .embedding_dropout(embcatq)\n        embcatqa = self .embedding_dropout(embcatqa)\n        embcatqdiff = self .embedding_dropout(embcatqdiff) # Weighted sum of tags - hopefully good weights are learnt xinpq = torch.cat([embcatq, embcatqdiff], 2 )\n        hiddenq, _ = self .seqnet1(xinpq)\n        xinpqa = torch.cat([embcatqa, contmat, hiddenq], 2 )\n        hiddenqa, _ = self .seqnet2(xinpqa) # Take last hidden unit hidden = torch.cat([hiddenqa[:,- 1 ,:], hiddenq[:,- 1 ,:], contmat[:, - 1 ]], 1 )\n        hidden = self .dropout( self .bn1( hidden) )\n        hidden  = F.relu( self .linear1(hidden))\n        hidden = self .dropout( self .bn2(hidden))\n        out = self .linear_out(hidden).flatten() return out content_copy LSTM Please sign in to reply to this topic. comment 7 Comments 2 appreciation  comments Hotness u++ Posted 4 years ago · 2845th in this Competition arrow_drop_up 1 more_vert Congrats and thank you for sharing. Smart solution! KhanhVD Posted 4 years ago · 146th in this Competition arrow_drop_up 1 more_vert Great solution. Congrats on results and thanks for the writeup solution @darraghdog and team Aditya Soni Posted 4 years ago · 125th in this Competition arrow_drop_up 2 more_vert Now this is a really classy solution! I loved this one! Thanks for sharing! Aditya Soni Posted 4 years ago · 125th in this Competition arrow_drop_up 1 more_vert Hey, Can you share what was the performance if you didn't do the below? Ty! Super clean and easy to digest code! embcatqdiff = embcatq[:, :, :self.diffsize] - embcatq[:, -1, :self.diffsize].unsqueeze(1) Darragh Topic Author Posted 4 years ago · 11th in this Competition arrow_drop_up 0 more_vert Very little, maybe ~0.001, but would have caused places in the leaderboard. Intuitively it made a lot of sense for me as I was confused how the model got information of the LSTM… when it does a forward pass on the LSTM, it did not know what the question asked at the end was (only uni-directional). Appreciation (2) Kamal Posted 4 years ago · 529th in this Competition arrow_drop_up 1 more_vert Congratulation and thanks for sharing 2981 Posted 4 years ago · 431st in this Competition arrow_drop_up 1 more_vert thanks for sharing! Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules pocket · 12th in this Competition  · Posted 4 years ago arrow_drop_up 74 more_vert 12th place Solution First of all, I want to thank both the hosts(kaggle and riiid) for hosting this competition. It was a really challenging competition in many ways, and really tested my limits. The competition was also very clean (albeit the privateLB bug unrelated to this competition). A very clean and logical train/test split/correlation is what makes this competition shine. Overall solution: For our single mode, NN scored around 0.806-810(privateLB), LGBM 0.804. We stacked these single models with NN and LGBM to achieve a score of 0.816. I suspect our single models were mediocre in terms of score, but the stacking got us to gold. Regarding our 5place drop on the privateLB, we kind of expected it. We knew that one of our models(SAINT) had a bad private score because of the private score leak, but we couldn't quite fix it. Even after the competition has ended, we still have no clue as to why the model suffers in the privateLB. Validation: We used tito-split throughout most of the competition period. We mostly experienced good Valid-LB correlation, which I believe is the same for others. The single models used a 95M/5M split, and the stack used the 5M for training/validation. We also tried user-split as our final submission for diversity, but this had no impact on our final score. LGBM details: Resource management (When generating features for submission) I used h5py for memory-hungry features like (user x content features, user x tag features). The h5py file uses user_id as the key, and read the whole user-feature when hitting new users while predicting. This creates a good balance between memory and runtime. Since there are not many unique users in the test-set (compared to train), the time to FileIO is not too much, and huge amounts of memory is saved. All the other features were pickled and loaded to memory in the beginning. Features: Question features: These were made by the full training-set(100M). Obviously this is leaky, but since every single question has >1000 counts, the leak is tolerable. Example features: mean question ac, mean user rating of who did not answer correctly. User features: How good the user is, especially related to parts, tags, contents. Example features: user x contents mean question ac Timestamp features: This was kind of a surprise to me. Not only was the timestamp diff of t and t-1 a good features, diff of t-1 and t-2 up to t-9 and t-10 improved my model. Example features: user user timestamp diff from last lecture Rating features: Elo features from this notebook (thank you very much). Trueskill did not improve my model. With only mean ac, the model cannot determine if the user is challenging hard questions or easy ones, so rating questions and users makes a lot of sense. SVD features: LGBM is bad at expressing category columns (compared to NN). So I took the question embedding layer of NN, and used the 20dimension SVD as features. Feature selection: I used about 70-80 features for my model. Since we were stacking a lot of models, I didn't want to use too much resource with my LGBM(runtime, memory), so I picked features which had a lot of impact, and made my model contribute to the stacking-model. There were a lot of features which didn't improve my model much, and all of them were thrown away. Hyper-param: I didn't change this too much, but increasing the num-leaf 127->1023 improved my score by 0.003, which was a surprise. This happened after I added lots of timestamp features, so there might be a very complicated interaction underlying in the timestamp. Machine: I used GCP, 64coreCPU 416GBmemory. Even with this monster machine, I ran out of memory a lot when generating the full features (which I avoided by processing in chunks). I believe this instance cost me around $1000 over the competition (this is all payed by my company, and we are hiring btw). A lot of this cost happened because I was lazy (never used a preemptive instance), and I believe you could still be competitive with a $100 budget, even on this HUGE data, so don't be discouraged by the cost if you are a Kaggle beginner (I would recommend a competition with smaller data though :) ) Stacking details: Stacking is always difficult. It often leads to overfitting. In this competition, there was a very good Valid-LB correlation, so I guessed(correctly) that stacking could work. I wanted to make sure this layer works, so I tried to do everything conservative. Validation: 3M/2M train/valid setup. For the final subs, I made multiple models with a time-series-split (negligible gain). Input Models: Multiple models from Sakami(SAINT based and AKT based), which had different window size. We couldn't fit in all the models, so we chose 3 as our final sub. Owruby had another SAINT model which both added diversity and improved the stack a lot. Lyaka had a SSAKT model which also helped slightly. I also added my LGBM model, which surprisingly improved the stack, even with the features added. Input Features: LGBM stack model: Hand selected 15 features from my single LGBM model. There was literally no gain from the other 65. NN stack model: Selected features + the last layers from singleNNs. This was done by Lyaka. Stacking models: LGBM stack model: num_leaves was reduced 1023->127. NN stack model: Lyaka did both MLP and a Transformer. Both had similar scores, and we used MLP as the final sub. Final output: Mean of LGBM and NN stack model Final thoughts: Although we couldn't quite achieve the goal we wanted (beating mamas), I think we tried our best and did our best. Everyone contributed to the final stack, everyone did a ton of work, and we had great teamwork to make the stacking happen. I am really happy with the team we had, like I always had throughout my kaggle history. Thank you all. Please sign in to reply to this topic. comment 12 Comments 1 appreciation  comment Hotness pocket Topic Author Posted 4 years ago · 12th in this Competition arrow_drop_up 3 more_vert Additional resources: NN solution My code (As is, and unorganized) NN code Tonghui Li Posted 4 years ago · 88th in this Competition arrow_drop_up 1 more_vert Thanks for sharing!  And huge congrats on becoming GM! Theo Viel Posted 4 years ago arrow_drop_up 1 more_vert Thanks for the nice write-up (I always enjoy pretty stacking schemes) and congratz on the nice finish ! Abdessalem Boukil Posted 4 years ago · 39th in this Competition arrow_drop_up 1 more_vert Good job! I am curious, for people that stacked multiple transformers, how come your solution didn't time out. Mine did for sequences longer than 100, and it is a single 128 d_model. This discouraged me from scaling the model. pocket Topic Author Posted 4 years ago · 12th in this Competition arrow_drop_up 1 more_vert You can check our NN solution , or ask @sakami u++ Posted 4 years ago · 2845th in this Competition arrow_drop_up 2 more_vert Thank you for the detailed explanation of your solution. I believe that a great result are derived from combined efforts of all the team members. Congrats for gold medal, and becoming competition grandmaster @pocketsuteado and @owruby ! Jaideep Posted 4 years ago · 24th in this Competition arrow_drop_up 2 more_vert @pocket i would say its a great work . We did try AKT but it dknt gave us score ,we replaced mha with akt mha ,as part of change  not sure if this was right implementation cswwp Posted 4 years ago · 24th in this Competition arrow_drop_up 1 more_vert @pocketsuteado Can you share yours AKT net code plz?  we want to learn yours implement, thank you sakami Posted 4 years ago · 12th in this Competition arrow_drop_up 1 more_vert You can check our SAINT/AKT code here . cswwp Posted 4 years ago · 24th in this Competition arrow_drop_up 0 more_vert Nice, thank you pocket Topic Author Posted 4 years ago · 12th in this Competition arrow_drop_up 0 more_vert NN details will be posted later by @sakami Appreciation (1) Neil Gibbons Posted 4 years ago · 1196th in this Competition arrow_drop_up 0 more_vert Thanks for sharing! Very inspirational Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules kurupical · 13th in this Competition  · Posted 4 years ago arrow_drop_up 65 more_vert Public/Private 13th solution (team takoi + kurupical) First of all, thanks to Hoon Pyo (Tim) Jeon and Kaggle team for such an interesting competition. And congratulates to all the winning teams! The following is the team takoi + kurupical solution. Team takoi + kurupical Overview validation @tito 's validation strategy. https://www.kaggle.com/its7171/cv-strategy kurupical side I would like to have three kaggler to thank. @takoi for inviting me to form a team. If it weren't for you, I couldn't reach this rank! @limerobot for sharing DSB 3rd solution. I'm beginner in transformer for time-series data, so I learned a lot from your solution! @wangsg for sharing notebook https://www.kaggle.com/wangsg/a-self-attentive-model-for-knowledge-tracing.! I used this notebook as a baseline and finally get 0.809 CV for single transformer. model hyper parameters 20epochs AdamW(lr=1e-3, weight_decay=0.1) linear_with_warmup(lr=1e-3, warmup_epoch=2) worked for me baseline (SAKT, https://www.kaggle.com/wangsg/a-self-attentive-model-for-knowledge-tracing ) use all data (this notebook use only last 100 history per user) embedding concat (not add) and Linear layer after cat embedding(@limerobot DSB2019 3rd solution) (+0.03) Add min(timestamp_delta//1000, 300) (+0.02) Add \"index that user answered same content_id at last\" (+0.005) Transformer Encoder n_layers 2 -> 4 (+0.002) weight_decay 0.01 -> 0.1 (+0.002) LIT structure in EncoderLayer (+0.002) not worked for me I did over 300 experiments, and only about 20 of them were successful. SAINT structure (Transformer Encoder/Decoder) Positional Encoding Consider timeseries timedelta.cumsum() / timedelta.sum() np.log10(timedelta.cumsum()).astype(int) as category feature and embedding etc… optimizer AdaBelief, LookAhead(Adam), RAdam more n_layers(4 => 6), more embedding_dimention (256 => 512) output only the end of the sequence large binning for elapsed_time/timedelta (500, 1000, etc…) treat elapsed_time and timedelta as continuous takoi side I made 1 LightGBM and 8 NN models. The model that combined Transformer and LSTM had the best CV. Here is architecture and brief description. ​ Transformer + LSTM ​ ​ features I used 17 features. 15 features were computed per user_id. 2 features were computed per content_id. main features sum of answered correctly average of answered correctly sum of answered correctly for tag_user_id average of answered correctly for tag_user_id lag time lag time of same content_id previous answered correctly for the same content_id distance between the same content_id average of answered correctly for each content_id average of lag time for each content_id ​ ​ LightGBM I used 97 features. The following are the main features. sum of answered correctly average of answered correctly sum of answered correctly for tag_user_id average of answered correctly for tag_user_id lag time lag time of same part lag time of same content_id previous answered correctly for the same content_id distance between the same content_id Word2Vec features of content_id decayed features (average of answered correctly) number of consecutive times with the same user answer flag if the user answer being answered in succession matches the correct answer average of answered correctly for each content_id average of lag time for each content_id Please sign in to reply to this topic. comment 14 Comments Hotness tatsu Posted 4 years ago · 1236th in this Competition arrow_drop_up 1 more_vert Thank you for sharing a great solution and congratulations on gold medal! It might be a dumb question but what makes you stack a LSTM layer on a Transformer layer? I am studying NN and it would be a great help if you guide me to understand how LSTM is superior to Transformer. And, how did you decide which features put in transformer or dense layer? Takoi Posted 4 years ago · 13th in this Competition arrow_drop_up 1 more_vert Thanks! It might be a dumb question but what makes you stack a LSTM layer on a Transformer layer?\nI am studying NN and it would be a great help if you guide me to understand how LSTM is superior to Transformer. content_copy I think LSTM is better at capturing nearby information than Transformer, but Transformer is better at capturing distant information. That's why I included both. And, how did you decide which features put in transformer or dense layer? content_copy I included cumulative and average features for all previous problems of the user, and features that greatly increased the score of LightGBM. 5 more replies arrow_drop_down Aditya Soni Posted 4 years ago · 125th in this Competition arrow_drop_up 1 more_vert Congratulations on a cool finish! What's same_question_index and target? distance between the same content_id -> so this is the difference b/w the sequence no when a particular content_id was last seen Vs their current sequence number assigned, right? decayed features (average of answered correctly) -> How did you decay them? Did you do something like alpha*new_value + (1-alpha)*old_value ? LIT structure in EncoderLayer. What's LIT? Thanks a lot! Takoi Posted 4 years ago · 13th in this Competition arrow_drop_up 3 more_vert Thank you! I answer question about my side. distance between the same content_id -> so this is the difference b/w the sequence no when a particular content_id was last seen Vs their current sequence number assigned, right ? content_copy Yes. decayed features (average of answered correctly) -> How did you decay them? Did you do something like alpha*new_value + ( 1 -alpha)*old_value? content_copy I decayed the value as 0.3 * new_vlaue + (1 - 0.3) * old_value . kurupical Topic Author Posted 4 years ago · 13th in this Competition arrow_drop_up 2 more_vert Hi @adityaecdrid ! What's same_question_index and target? -> same as takoi's distance between the same content_id LIT structure in EncoderLayer. What's LIT? -> Sorry, I forget to explain detail. https://arxiv.org/pdf/2012.14164.pdf . My implements is below. TransformerEncoder is copy from torch.nn.TransformerEncoder, I just replaced the linear part of src2 with a LITLayer. class LITLayer (nn.Module): \"\"\"\n    https://arxiv.org/pdf/2012.14164.pdf\n    \"\"\" def __init__ ( self, input_dim, embed_dim, dropout, activation ): super (LITLayer, self ).__init__() self .input_dim = input_dim self .embed_dim = embed_dim self .activation = activation self .linear1 = nn.Linear(input_dim, embed_dim) self .lstm = nn.LSTM(embed_dim, embed_dim) self .linear2 = nn.Linear(embed_dim, input_dim) self .norm_lstm = nn.LayerNorm(embed_dim) self .dropout = nn.Dropout(dropout) def forward ( self, x ):\n        x = self .linear1(x)\n        x = self .activation(x)\n        x = self .dropout(x)\n        x, _ = self .lstm(x)\n        x = self .norm_lstm(x)\n        x = self .dropout(x)\n        x = self .linear2(x) return x class TransformerEncoderLayer (nn.Module): r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n    in a different way during application.\n\n    Args:\n        d_model: the number of expected features in the input (required).\n        nhead: the number of heads in the multiheadattention models (required).\n        dim_feedforward: the dimension of the feedforward network model (default=2048).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n\n    Examples::\n        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n        >>> src = torch.rand(10, 32, 512)\n        >>> out = encoder_layer(src)\n    \"\"\" def __init__ ( self, d_model, nhead, dim_feedforward= 256 , dropout= 0.1 , activation= \"relu\" ): super (TransformerEncoderLayer, self ).__init__() self .self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout) # Implementation of Feedforward model self .norm1 = nn.LayerNorm(d_model) self .norm2 = nn.LayerNorm(d_model) self .dropout1 = nn.Dropout(dropout) self .dropout2 = nn.Dropout(dropout) self .activation = _get_activation_fn(activation) self .lit_layer = LITLayer(input_dim=d_model, embed_dim=dim_feedforward, dropout=dropout, activation= self .activation) def __setstate__ ( self, state ): if 'activation' not in state:\n            state[ 'activation' ] = nn.ReLU super (TransformerEncoderLayer, self ).__setstate__(state) def forward ( self, src: torch.Tensor, src_mask: Optional [torch.Tensor] = None , src_key_padding_mask: Optional [torch.Tensor] = None ) -> torch.Tensor: r\"\"\"Pass the input through the encoder layer.\n\n        Args:\n            src: the sequence to the encoder layer (required).\n            src_mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n\n        Shape:\n            see the docs in Transformer class.\n        \"\"\" src2 = self .self_attn(src, src, src, attn_mask=src_mask,\n                              key_padding_mask=src_key_padding_mask)[ 0 ]\n        src = src + self .dropout1(src2)\n        src = self .norm1(src) # src2 = self.linear2(self.dropout(self.activation(self.linear1(src)))) src2 = self .lit_layer(src)\n        src = src + self .dropout2(src2)\n        src = self .norm2(src) return src content_copy Aditya Soni Posted 4 years ago · 125th in this Competition arrow_drop_up 1 more_vert Thanks guys for the quick response! 2981 Posted 4 years ago · 431st in this Competition arrow_drop_up 1 more_vert congratulations！ Theo Viel Posted 4 years ago arrow_drop_up 1 more_vert Very nice write-up, thanks for sharing ! And congratz on the gold finish! KhanhVD Posted 4 years ago · 146th in this Competition arrow_drop_up 2 more_vert Congrats on 13th place @kurupical and @takoihiraokazu and thanks for sharing your solution Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules MPWARE · 16th in this Competition  · Posted 4 years ago arrow_drop_up 69 more_vert 16th Place - Single Model Hi all, Here is an insight of our 2 solutions that both scored public 0.812/private 0.815 and that reached 16th gold place. This competition was both ML and engineering optimization to make everything work in 9h with 13GB RAM/16GB GPU. We spent almost 30% of time on optimization to keep the last 512 interactions per users +  per content attempts in memory + required for our features. We would like to thank Kaggle and RIIID organizers for this great competition! Congratulations to the top teams and all competitors for their motivation all along the challenge. I would like to thank my teammates @rafiko1 , @cdeotte @titericz and @matthiasanderer . You've been amazing, I've learnt a lot from you. I really enjoyed this competition. Solution 1: Single transformer model The SAINT+ model is described here https://arxiv.org/pdf/2010.12042.pdf The code for our SAINT+ adaptation is available here https://github.com/rafiko1/Riiid-sharing . Our single model SAINT++ achieved, CV: 0.812 Public LB: 0.812, Private LB: 0.815 We trained with 95% of users first then fine tuned with all data using smart window technique (see diagram below for SAKT). The model is simple in terms of features. It only contains the four features of SAINT+ (pictured above), with one additional feature - the number of attempts of a user for specific content (hence SAINT++): Content id Lag time Prior question elapsed time Previous responses Number of attempts The greatest improvement in features compared to SAINT+ came from grouping lag time into seconds, unlike minutes as done in the paper. Then, we went bigger and bigger on the architecture and burned some GPU power 🔥. We increased on parameters of the model, most importantly the sequence length and number of layers. Final parameters of the model are as follows: Input Sequence length: 512 Encoding layers: 4 Decoding Layers: 4 Embedding size: 288 Dense Layer: 768 heads: 8 Dropout: 0.20 We used the Noam learning rate scheduler: with initial warmup and exponential decrease down to 2e-5. Final improvement came from our recursive trick during inference. Here, we rounded predictions that came from the same bundle to 0 or 1 - as their true response is unknown in time yet. The rounded predictions are then fed back to the model to predict the next response within the same bundle. This trick boosts CV LB +0.0025, but requires a batch size of 1, so we couldn’t ensemble multiple transformer models. Solution 2: Ensemble of transformer, modified SAKT and LGB LightGBM model scored CV=0.793, public LB=0.792 with 44 features. Our main features: Question correctness per content and per user, tags 1 and 2, part, elapsed time, had explanation, number of attempts, multiple lags, running average (answer), multiple rolling means/median (answers, lags, elapsed time) + weighted mean, mean after/before 30 interactions, multiple momentums (lag, answers), per part correctness, per session (8 hours split) running average. Only 3 categories: part, tags1, tags2. Train/valid split from Tito . Pytorch SAKT modified scored CV=0.786, LB=0.789 with additional features. Training procedure with a smart window. When a user’s sequence length is larger than model input, i.e. N>W, then using random crops gives +0.002 CV versus tiled crops. And using smart random crops gives +0.003 CV versus tiled crops. Basic random crops have a low probability of selecting the early or late questions from a user’s sequence whereas smart random crops have an equally likely probability of selecting all questions from a user. TensorFlow Transformer model alone scored CV=0.811, LB=0.811 Same as solution#1 but with sequence length = 256 What did not work: TabNet Features with lectures for LGB. It worked on CV but not on LB (might be an issue in inference). Post processing using absolute position of question aka. question sequence number. Plotting mean(answered_correctly) vs question number looked like the image below. We can see that the 30 first questions have a different distribution compared with the rest. Also looks like there are subsequently batches of 30 questions (becomes visible if we zoom in the plot below). PP using that information worked in CV improving by around 0.0009, but didn’t work on LB. What worked partially: But was not applicable for us within the 9h runtime limit: More than 3 models ensemble Level 2 model (XGB) could boost by +0.001 Lessons learnt: Start inference Kernel as soon as possible when you need to deal with an API. Try to simulate API locally to understand how data will be handled. Tito ’s simulator was perfect for that purpose. Push your inference (with the simulator) to the limits to debug it, it will avoid the frustrating “submission scoring error”. Team-up at some point, your teammates always have good ideas. One additional word to Kaggle @sohier I loved your API and the way it hides private data, it’s more realistic as in real world usage/production and it avoided chaotic blending. Congratulations for that, however, even if I guess you want to prevent probing, you should find a solution to provide better error feedback. If it is not possible (the more error codes the more probing) then you need to provide a simulator and guidelines allowing competitors to troubleshoot locally. Transformers LightGBM Please sign in to reply to this topic. comment 54 Comments Hotness CoreyJamesLevinson Posted 4 years ago arrow_drop_up 3 more_vert @MPWare Thanks for sharing the \"Simple\" Pytorch code, but why does it score so much less than Tensorflow? The Pytorch model uses 2 layers instead of Tensorflow uses 4 layers The Pytorch model uses 100 sequence length instead of Tensorflow uses 512 sequence length The Pytorch model uses 256 embedding size instead of Tensorflow uses 288 The Pytorch model uses dropout = 0.1 but Tensorflow uses dropout = 0.2 The Pytorch model only uses exercise_id  in encoder, but Tensorflow uses and response, but Tensorflow uses exercise_id, Lag time, prior question elapsed time, previous responses, number of attempts. Both models just use response_correct embedding in the decoder Are there any other differences? Would the Pytorch model be as good if you changed it to use all these features? Ty, I will really cherish that \"simple\" pytorch demo notebook you provided. MPWARE Topic Author Posted 4 years ago · 16th in this Competition arrow_drop_up 3 more_vert @returnofsputnik As you've noticed, the main difference is that I'm not using exactly the same features. I'm not using attempts at all. The configuration to get LB=0.795 with Pytorch model is below, it requires seq_len=256 . We've noticed that seq_len is a quite important to get a better model. The training procedure is also important, I'm not using the smart window described by @cdeotte . I've spent some time to fix the NaN loss issue with Pytorch MultiHeadAttention + padding masks. Root cause was right padding instead of left padding. I've stopped to try improving it once we got better results with @rafiko1 TensorFlow model which uses seq_len=512 . Other difference is sigmoid vs softmax but I don't think it's really important. class raw_conf: pad_mode = \"token\" pad_right = False flatten = True sampler = None # \"prob\" # Option to give long user's sequence higher probability seq_len = 256 # 100 embedding_dim = 256 exercices_id_size = 13523 exercices_part_size = 7 response_size = 2 elapsed_time_cat = True elapsed_time_size = 73 # Categories after binning lag_time_cat = True lag_time_size = 366 # Categories after binning explanation_size = 2 # Model nhead = 8 num_encoder_layers = 4 num_decoder_layers = 4 dim_feedforward = 2048 dropout = 0 . 1 activation = None num_classes = 1 loss = MaskedBCEWithLogitsLoss(num_classes) post_activation = \"sigmoid\" optimizer = \"Noam\" # \"Adam\" scheduler = \"Cosine\" if optimizer == \"Adam\" else None lr = 0 . 0001 min_lr = 0 . 00005 beta1 = 0 . 9 BATCH_SIZE = 128 content_copy 6 more replies arrow_drop_down u++ Posted 4 years ago · 2845th in this Competition arrow_drop_up 3 more_vert Congrats & thank you for sharing your approach and codes. Darren Lahr Posted 4 years ago · 368th in this Competition arrow_drop_up 3 more_vert Congratulations and thanks for sharing your solution, I and many others will be able to learn a lot from it. I also used Tensorflow and implemented Saint+ however my code to sample sequences/users was not robust so I intend to see how the model would have performed using the sampling code you have shared. I have read through the code in demo_riiid_train.ipynb and think I understand how you have set up training and validation sets but was hoping you could confirm my understanding? You allocate distinct users to both the training and validation set You then assign a probability to each user based on sequence length. For say user 115 in the training set their probability is their sequence length divided by the sum of all sequence lengths in training set. You then generate a one off validation set where you randomly sample users with replacement and take random training crops for each row. Based on the 'select_window_size' function For the training dataset you do the same but repeat step 3 each epoch You sample using N_SELECT_PER_EPOCH = 100000, was this just a hyperparameter you tuned? Thank you Rafi Hai Posted 4 years ago · 16th in this Competition arrow_drop_up 1 more_vert Correct. Distinct users gives a simple and reliable validation. Correct. It will be used inside select_window_size as the equivalent of WeightedRandomSampler in Pytorch Correct. This is important for training. For validation it's the convenient choice, but not the best choice. Better is to take all sequences for validation instead. Correct. N_SELECT_PER_EPOCH is defining how many samples to pass within each epoch. It's not really a hyperparameter to tune. Just how long you'd like each epoch to be by specifying number of samples. Darren Lahr Posted 4 years ago · 368th in this Competition arrow_drop_up 1 more_vert Thank you, will do a late submission and see how my score would have changed. mamas Posted 4 years ago · 2nd in this Competition arrow_drop_up 4 more_vert Congrats, single 0.812/0.815 with SAINT is really great! Rafi Hai Posted 4 years ago · 16th in this Competition arrow_drop_up 1 more_vert Congrats on your amazing 2nd place @mamas , well-deserved! Abdur Rehman Posted 4 years ago · 873rd in this Competition arrow_drop_up 1 more_vert @mpware congrats on gold medal and thanks for sharing your approach. Would you like to share your code ? MPWARE Topic Author Posted 4 years ago · 16th in this Competition arrow_drop_up 4 more_vert Thanks! Our best single model + features is with TensorFlow. @rafiko1 if you get a chance to share it … In the other thread , it was a Pytorch model (with simple features) that only reached public LB 0.795. Here it is if it can help. # Model class RIIIDModel (nn.Module): def __init__ ( self, cfg, verbose= False ): super ().__init__() self .response_size = cfg.response_size self .lag_time_size = cfg.lag_time_size self .elapsed_time_size = cfg.elapsed_time_size self .explanation_size = cfg.explanation_size self .attempt_size = cfg.attempt_size self .seq_len = cfg.seq_len self .embedding_dim = cfg.embedding_dim self .elapsed_time_cat = cfg.elapsed_time_cat self .lag_time_cat = cfg.lag_time_cat self .num_classes = cfg.num_classes self .verbose = verbose self .pad_mode = cfg.pad_mode self .pos_encoder1 = None self .pos_encoder2 = None additional_token_dim = 1 if self .pad_mode == \"token\" else 0 # Exercices embeddings self .exercices_id_embedding = nn.Embedding(cfg.exercices_id_size + additional_token_dim, self .embedding_dim) self .exercices_part_embedding = nn.Embedding(cfg.exercices_part_size + additional_token_dim, self .embedding_dim) if cfg.exercices_part_size is not None else None # Response embeddings self .response_embedding = nn.Embedding(cfg.response_size + 1 + additional_token_dim, self .embedding_dim) # +1 to include start token if self .elapsed_time_cat is True : self .elapsed_time_embedding = nn.Embedding(cfg.elapsed_time_size + 1 + additional_token_dim, self .embedding_dim) if cfg.elapsed_time_size is not None else None # +1 to include start token else : self .elapsed_time_embedding = nn.Linear( 1 , self .embedding_dim, bias= False ) if cfg.elapsed_time_size is not None else None # Continuous embedding if self .lag_time_cat is True : self .lag_time_embedding = nn.Embedding(cfg.lag_time_size + 1 + additional_token_dim, self .embedding_dim) if cfg.lag_time_size is not None else None # +1 to include start token else : self .lag_time_embedding = nn.Linear( 1 , self .embedding_dim, bias= False ) if cfg.lag_time_size is not None else None # Continuous embedding self .explanation_embedding = nn.Embedding(cfg.explanation_size + 1 + additional_token_dim, self .embedding_dim) if cfg.explanation_size is not None else None # +1 to include start token input_features_dim = self .embedding_dim # Position encoder (relative or absolute position of the tokens in the sequence) if cfg.position_encoding_enabled is True : self .pos_encoder1 = PositionalEncoding(input_features_dim, cfg.dropout) self .pos_encoder2 = self .pos_encoder1 # Transformer with default encoder/decoder self .transformer = nn.Transformer(d_model=input_features_dim, \n                                          nhead=cfg.nhead, \n                                          num_encoder_layers=cfg.num_encoder_layers,\n                                          num_decoder_layers=cfg.num_decoder_layers, \n                                          dim_feedforward=cfg.dim_feedforward, \n                                          dropout=cfg.dropout, \n                                          activation= 'relu' , \n                                          custom_encoder = None , \n                                          custom_decoder = None ) # Decoder self .fc = nn.Linear(input_features_dim, self .num_classes) # If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged. # tensor([[False,  True,  True,  True], #         [False, False,  True,  True], #         [False, False, False,  True], #         [False, False, False, False]]) def generate_mask ( self, size, diagonal= 1 ): return torch.triu(torch.ones(size, size)== 1 , diagonal=diagonal) def forward ( self, data, src_mask= None , tgt_mask= None , mem_mask= None , src_key_padding_mask= None , tgt_key_padding_mask= None , memory_key_padding_mask= None ): # Each input is (BS, seq_len) # Content data_content_id = data[CONTENT_ID].long() # Answers data_response = data[TARGET].long() # Optional features data_part = data[PART].long() if self .exercices_part_embedding is not None else None if self .elapsed_time_cat is True :\n            data_elapsed_time = data[PRIOR_QUESTION_ELAPSED_TIME].long() if self .elapsed_time_embedding is not None else None else :\n            data_elapsed_time = data[PRIOR_QUESTION_ELAPSED_TIME]. float ().unsqueeze( 2 ) if self .elapsed_time_embedding is not None else None if self .lag_time_cat is True :\n            data_lag_time = data[LAG].long() if self .lag_time_embedding is not None else None else :\n            data_lag_time = data[LAG]. float ().unsqueeze( 2 ) if self .lag_time_embedding is not None else None data_explanation = data[PRIOR_QUESTION_HAD_EXPLANATION].long() if self .explanation_embedding is not None else None # Start token(s) # Add start token to correctness data_response = torch.roll(data_response, shifts=( 0 , 1 ), dims=( 0 , 1 )) # Shift right the sequence data_response[:, 0 ] = self .response_size # Start token (2) # Add start token to lag time if data_lag_time is not None :\n            data_lag_time = torch.roll(data_lag_time, shifts=( 0 , 1 ), dims=( 0 , 1 )) # Shift right the sequence data_lag_time[:, 0 ] = self .lag_time_size # Start token # Add start token to elapsed time if data_elapsed_time is not None :\n            data_elapsed_time = torch.roll(data_elapsed_time, shifts=( 0 , 1 ), dims=( 0 , 1 )) # Shift right the sequence if self .elapsed_time_cat is True :\n                data_elapsed_time[:, 0 ] = self .elapsed_time_size # Start token else :\n                data_elapsed_time[:, 0 ] = 0.0 # Add start token to explanation if data_explanation is not None :\n            data_explanation = torch.roll(data_explanation, shifts=( 0 , 1 ), dims=( 0 , 1 )) # Shift right the sequence data_explanation[:, 0 ] = self .explanation_size # Start token # Questions, Part, Elapsed time, Lag embeddings x_content_id = self .exercices_id_embedding(data_content_id) # (BS, seq_len, embedding_dim) x_exercices_part = self .exercices_part_embedding(data_part) if self .exercices_part_embedding is not None else None # (BS, seq_len, embedding_dim) x_elapsed_time = self .elapsed_time_embedding(data_elapsed_time) if self .elapsed_time_embedding is not None else None # (BS, seq_len, embedding_dim) x_lag_time = self .lag_time_embedding(data_lag_time) if self .lag_time_embedding is not None else None # (BS, seq_len, embedding_dim) x_explanation = self .explanation_embedding(data_explanation) if self .explanation_embedding is not None else None # (BS, seq_len, embedding_dim) # Response embeddings x_correctness = self .response_embedding(data_response) # (BS, seq_len, embedding_dim) x_position = None # Ei (sum of embeddings) x_exercices = x_content_id if x_exercices_part is not None :\n            x_exercices = x_exercices + x_exercices_part # (BS, seq_len, embedding_dim) x_position_exercices = self .pos_encoder1(x_exercices) if self .pos_encoder1 is not None else x_exercices # (BS, seq_len, embedding_dim) # Ri (sum of embeddings) [S, R1, Rk-1], S is start token x_responses = x_correctness if x_lag_time is not None :\n            x_responses = x_responses + x_lag_time if x_elapsed_time is not None :\n            x_responses = x_responses + x_elapsed_time # (BS, seq_len, embedding_dim) if x_explanation is not None :\n            x_responses = x_responses + x_explanation # (BS, seq_len, embedding_dim) if x_attempt is not None :\n            x_responses = x_responses + x_attempt # (BS, seq_len, embedding_dim) if x_exercices_task is not None :\n            x_responses = x_responses + x_exercices_task # (BS, seq_len, embedding_dim) x_position_responses = self .pos_encoder2(x_responses) if self .pos_encoder2 is not None else x_responses # (BS, seq_len, embedding_dim) # Transformer src: (S,N,E), tgt:(T,N,E), src_mask:(S,S), tgt_mask:(T,T) # where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number # output: (T,N,E) # src_key_padding_mask: (N,S), tgt_key_padding_mask: (N,T), memory_key_padding_mask: (N,S) x_position_exercices = x_position_exercices.transpose( 1 , 0 ) # (seq_len, BS, embedding_dim) x_position_responses = x_position_responses.transpose( 1 , 0 ) # (seq_len, BS, embedding_dim) x_transformer = self .transformer(src=x_position_exercices, tgt=x_position_responses, src_mask=src_mask, tgt_mask=tgt_mask, memory_mask=mem_mask, \n                                         src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask) # (seq_len, BS, embedding_dim) x_transformer = x_transformer.transpose( 1 , 0 ) # (BS, seq_len, embedding_dim) output = self .fc(x_transformer)\n        output = output.squeeze(dim= 2 ) return output content_copy 13 more replies arrow_drop_down Yih-Dar SHIEH Posted 4 years ago · 70th in this Competition arrow_drop_up 2 more_vert @mpware @cdeotte About And @mpware has a brilliant way of storing each user's attempt history using only 3 bits per time step to save memory. (3 bits can represent 0 thru 7 in binary). content_copy Would it be possible to share how you perform this? At the very end of this competition, I needed to deal with the time/memory issue once I introduced the performance features. And it took time, and I didn't have much time to fix the other inference bugs due to the new features. It would be great to learn from your great management of the memory, hope I could hear from you, at least a bit. MPWARE Topic Author Posted 4 years ago · 16th in this Competition arrow_drop_up 4 more_vert @yihdarshieh I'm maintaining the following stucture in memory. I've been inspired from different threads in forum and benchmarked multiple solutions before keeping this one. Per user and per content attempts dictionary costs too much memory if stored as integers (even np.int8 ) because it could be up to 393k x 13k.  To lower memory usage I'm using 3xbitarray (index is content_id) to address 2x2x2=8 attempts. As we're using attempts are categories we have 0-7 values and all attempts beyond 7 fall into last category which is indeed 7 or higher. We also need to store per user history for: Questions Answers Lags Elapsed time Attempts Part (optional) Had explanation (optional) Attempts history per user is computed on-fly based on the cseen values containing only the total (that's the trick). Our different models (LGBM, Transfomer) can share this dictionary. per_user_dict = defaultdict(CustomDictNP) def bit_array ():\n    b = bitarray( 13530 , endian= 'little' ) # Higher than totals question if any new question in test set b.setall( False ) return b class CustomDictNP : def __init__ ( self ): # Count/sum for further average self .qc = 0 # answers count self .qs = 0 # answers sum (correct) self .s = 0 # session id self .sc = 1 # session count self .ss = 0 # total sessions self .qes = 0.0 # total elapsed time # History self .ha = np.array([], dtype=np. bool ) # answers self .hp = np.array([], dtype=np.int8) # parts self .hq = np.array([], dtype=np.int16) # questions self .he = np.array([], dtype=np.float32) # elapsed time self .hx = np.array([], dtype=np. bool ) # prior question had explanation self .hl = np.array([], dtype=np.float32) # lag self .hat = np.array([], dtype=np.int8) # attempts # Last timestamps self .q = 0 # Questions self .timestamp_u = [] self .timestamp_u_correct = [] # content_id seen self .cseen0 = bit_array() self .cseen1 = bit_array() self .cseen2 = bit_array() def add_attempt ( custom, cid ):\n    new_attempt = get_attempts(custom, cid) + 1 if new_attempt < 8 : if new_attempt == 1 : \n            custom.cseen0[cid] = True custom.cseen1[cid] = False custom.cseen2[cid] = False elif new_attempt == 2 : \n            custom.cseen0[cid] = False custom.cseen1[cid] = True custom.cseen2[cid] = False elif new_attempt == 3 : \n            custom.cseen0[cid] = True custom.cseen1[cid] = True custom.cseen2[cid] = False elif new_attempt == 4 : \n            custom.cseen0[cid] = False custom.cseen1[cid] = False custom.cseen2[cid] = True elif new_attempt == 5 : \n            custom.cseen0[cid] = True custom.cseen1[cid] = False custom.cseen2[cid] = True elif new_attempt == 6 : \n            custom.cseen0[cid] = False custom.cseen1[cid] = True custom.cseen2[cid] = True else : \n            custom.cseen0[cid] = True custom.cseen1[cid] = True custom.cseen2[cid] = True def get_attempts ( custom, cid ): return custom.cseen0[cid] + 2 *custom.cseen1[cid] + 4 *custom.cseen2[cid] content_copy 6 more replies arrow_drop_down Theo Viel Posted 4 years ago arrow_drop_up 2 more_vert Congratz to you all ! 2981 Posted 4 years ago · 431st in this Competition arrow_drop_up 2 more_vert thanks for sharing! I am a beginer to transformer model. I think I need to learn the model before seeing your share. Dean Posted 4 years ago · 37th in this Competition arrow_drop_up 2 more_vert Thanks for your sharing and congratulations to your team. Would you like to explain how to add the \"Number of attempts\" feature into encoder? Is it convert to embedding? Thank you!! Chris Deotte Posted 4 years ago · 16th in this Competition arrow_drop_up 3 more_vert Yes as an embedding and add it to other encoder input embeddings. The encoder input begins as x = tf.keras.layers.Embedding(number of unique content id, 288)(CONTENT ID SEQUENCE) . Then you add positional encoding, x += positional_encoding(window size, 288) . And last you add \"Number of attempts\" encoding. x += tf.keras.layers.Embedding(max number of attempts + 1, 288)(NUMBER OF ATTEMPTS SEQUENCE) . The maximum number of attempts is clipped with NUMBER OF ATTEMPTS SEQUENCE = np.clip(NUMBER OF ATTEMPTS SEQUENCE, 0, 7) . And @mpware has a brilliant way of storing each user's attempt history using only 3 bits per time step to save memory. (3 bits can represent 0 thru 7 in binary). 4 more replies arrow_drop_down Tom Posted 4 years ago arrow_drop_up 2 more_vert @mpware Congrats on the gold medal and nice single model solution ! MPWARE Topic Author Posted 4 years ago · 16th in this Competition arrow_drop_up 0 more_vert @tikutiku Thank you! william.wu Posted 4 years ago · 42nd in this Competition arrow_drop_up 2 more_vert @mpware Congratulations to your team, very impressed that single RAINT+ with these 5 features can reach such a high score. My best RAINT+ model is only 0.802 on private Would like to know more details about which of these features are on the encoder side and which are on the decoder side? Content id Lag time Prior question elapsed time Previous responses\nNumber of attempts content_copy There's no doubt on the content id and response. For other features, could you please explain a little bit about which parts did you put the feature in? Chris Deotte Posted 4 years ago · 16th in this Competition arrow_drop_up 2 more_vert Thanks. Encoder had Content id and Number of attempts. Decoder had Lag time, Prior question elapsed time, and Previous responses. william.wu Posted 4 years ago · 42nd in this Competition arrow_drop_up 2 more_vert Got it, thanks! KhanhVD Posted 4 years ago · 146th in this Competition arrow_drop_up 2 more_vert Congratulations on 16th place and gold medal @mpware @cdeotte and team. Good work and strongly transformer model william.wu Posted 4 years ago · 42nd in this Competition arrow_drop_up 0 more_vert How about the question's tags, it's a VarLen feature. Do you have any idea of adding VarLen feature in the RAINT+? What I did is sorting the tags in each question then combine them and map to an integer. But it's not an optimal way, since the information of each individual tag is lost. Yih-Dar SHIEH Posted 4 years ago · 70th in this Competition arrow_drop_up 0 more_vert I use a tag embedding, i.e. map each integer in [-1, 188) to vectors of a fixed dim (in my case 256). Then these embedding are averaged (the embedding of -1 is for padding for  the tags, as you said, it is varlen, and it is ignored during the averaging). I don't know how the team of @mpware did it though. This comment has been deleted. Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Nikola Bacic · 17th in this Competition  · Posted 4 years ago arrow_drop_up 63 more_vert 17th place solution - 4 features model My solution is based on a stack of the 3 models: Edit: Code Transformer(Encoder) - Validation: 0.8100 LSTM - Validation: 0.8062 GRU - Validation: 0.8060 LightGBM for stack: Validation: 0.8119 LB: 0.814x I validated on new users (2.5M rows). This one is great, but it was computationally expensive for me. All three models used 4 features (embeddings): question id embedding response of the previous question (1-correct 0-incorrect) ln(lag+1) * minute_embedding : taking a ln(x+1) of the lag initialy improved my score by ~0.01. I'd be very interested to hear whether it'd improve your models too ln(prior_elapsed_time+1) * minute_embedding Input is sum of these 4 embeddings. Parameters Shareable parameters (all three models): max_quest = 300 (window size) slide = 150 Adam optimizer cosine lr scheduler BCE loss xavier_uniform_ weight initialization (0.004 improvement over PyTorch's default one) I used Optuna for hyperparameter search on 20% of the data. It was my first time using it, and it's a great tool! Transformer hyperparameters: nhead = 8 head_dim = 60 dim_feedforward = 2048 num_encoder_layers = 8 epochs = 6 batch_size = 64 lr = 0.00019809259513409007 warmup_steps = 150*5 LSTM hyperparameters: input_size_lstm = 384 hidden_size_lstm = 768 num_layers_lstm = 4 epochs = 4 batch_size = 64 lr = 0.0007019926812886481 warmup_steps = 100 GRU hyperparameters: input_size_gru = 320 hidden_size_gru = 512 num_layers_gru = 3 epochs = 4 batch_size = 64 lr = 0.0008419253431185227 warmup_steps = 80 What didn't work: -lectures -questions metadata -position encoding: both learnable and hard-coded -predicting user_answer instead of correctness -gradient clipping -label smoothing -dropout -… gg PyTorch Please sign in to reply to this topic. comment 20 Comments 3 appreciation  comments Hotness Stephen Lau Posted 4 years ago arrow_drop_up 1 more_vert Wait. Did you say you only use 4 features? That's impressive! Nikola Bacic Topic Author Posted 4 years ago · 17th in this Competition arrow_drop_up 0 more_vert Well who is learning, me or the model? :) TonyLee Posted 4 years ago arrow_drop_up 1 more_vert Congrats~~~ Your work is really impressive! I am a rookie in data science and I curiosity about how to generate 'question id embedding' features. Is it one-hot-encoding vector of question id? Nikola Bacic Topic Author Posted 4 years ago · 17th in this Competition arrow_drop_up 0 more_vert No, it's a vector repsesentation: Embedding mamas Posted 4 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Great, it was really fun to compete with you! Nikola Bacic Topic Author Posted 4 years ago · 17th in this Competition arrow_drop_up 0 more_vert Thanks, you were pushing me to limit, sorry I couldn't keep up :)  I was really tired last 2-3 weeks  (not that it's an excuse) and couldn't make almost no progress. Ioannis M Posted 4 years ago · 220th in this Competition arrow_drop_up 1 more_vert Simple and clever nice!! Regarding the inferenece did you strugle or had to optimize the code? Edit: any tips on this are welcome Ps: I tried also last day to stack 5 weak models but didn't pass the inference stage and didn't have time to optimize it further (cv meta-model 0.805) - It was more as proof of concept and for learning rather than hitting the LB.. I should have tried with 3 though Nikola Bacic Topic Author Posted 4 years ago · 17th in this Competition arrow_drop_up 1 more_vert Ofcourse I struggled, out of first 13 submission, 4 were successful :) All three of my models preprocess the data in the same way, so I had to do it only once! I preprocessed the data locally into python dictionary with the following function, and uploaded it as a kaggle dataset. During inference, I was simply updating the inner dictionaries. def csv_to_dict ( df ): \"\"\"maps the training data from csv to dictionary\"\"\" # separate question events questions_df = df[df[ \"content_type_id\" ] == False ] # fill nans questions_df[ \"prior_question_elapsed_time\" ] = questions_df[ \"prior_question_elapsed_time\" ].fillna( 301000 ) # questions history container questions_container = dict ( tuple (questions_df.groupby([ \"user_id\" ]))) # df -> tensors for user_id in questions_container.keys():\n        tmp_df = questions_container[user_id]\n        questions_container[user_id] = { \"content_id\" : torch.LongTensor(tmp_df[ \"content_id\" ].values), \"timestamp\" : torch.LongTensor(tmp_df[ \"timestamp\" ].values), \"prior_question_elapsed_time\" : torch.LongTensor(tmp_df[ \"prior_question_elapsed_time\" ].values), \"answered_correctly\" : torch.ShortTensor(tmp_df[ \"answered_correctly\" ].values),\n            } return questions_container content_copy kobi2000 Posted 4 years ago · 1487th in this Competition arrow_drop_up 1 more_vert Hi, congrats! Can you please elaborate on max_quest and slide ? Nikola Bacic Topic Author Posted 4 years ago · 17th in this Competition arrow_drop_up 1 more_vert This is related to the sampling strategy. max_quest is maximum number of questions processed at a time i.e. maximum input sequence length. Now, some users might have > max_quest questions in their histories so you need to make multiple samples from one user. This is when slide comes into play. The simplest way to handle this is to set slide = max_quest and get samples from 0-300, 300-600, 600-900… But this is suboptimal because questions close to the left hand side of the sample, like 300:350, or 600-650 can't see recent history. So I set slide = 150 and make samples like this: sample1: 0-300 -> calculate loss on all positions sample2: 150-450 -> calculate loss on 300-450 positions sample3: 300-600 -> calculate loss on 450-600 positions etc. Hopefully this is clear enough. kobi2000 Posted 4 years ago · 1487th in this Competition arrow_drop_up 0 more_vert Thanks a lot for the clear explanation. One more question related to minute_embedding , please. Is it lag and prior_elapsed converted to minutes, then multiplied by a shared learnable embedding? Nikola Bacic Topic Author Posted 4 years ago · 17th in this Competition arrow_drop_up 0 more_vert Yes, but embedding is not shared. Not that it makes too much of a difference;  I remember trying that and it giving me negligible worse result. Theo Viel Posted 4 years ago arrow_drop_up 1 more_vert Simple yet effective ! Congratz on the strong finish ! Alex Posted 4 years ago · 125th in this Competition arrow_drop_up 1 more_vert Congrats !! Only 4 features is pretty amazing -Response of the previous question is for quesiton_id or for task_container_id ? -Did you try to sum mutiple previous responses ? Like a rolling window Nikola Bacic Topic Author Posted 4 years ago · 17th in this Competition arrow_drop_up 1 more_vert Part and tags improved my score on 10% of the data, but with 100% I found that transformer learns them :) Thanks! Nikola Bacic Topic Author Posted 4 years ago · 17th in this Competition arrow_drop_up 2 more_vert -Response of the previous question is for quesiton_id or for task_container_id ? It's for quesiton_id. Yes, there is some leakage, but when I tried to fix that I got some funny results. Maybe my implementation wasn't right :? -Did you try to sum mutiple previous responses ? Like a rolling window I didn't. I think that transformer can pick that up, right? KhanhVD Posted 4 years ago · 146th in this Competition arrow_drop_up 2 more_vert Congrats on 17th place and thanks for sharing details solution @bacicnikola Appreciation (3) 2981 Posted 4 years ago · 431st in this Competition arrow_drop_up 1 more_vert thanks for sharing! a great job! u++ Posted 4 years ago · 2845th in this Competition arrow_drop_up 1 more_vert Congrats and thank you for sharing! Chuxian Mo Posted 4 years ago · 234th in this Competition arrow_drop_up 1 more_vert Cool, thanks for sharing! Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules senkin13 · 18th in this Competition  · Posted 4 years ago arrow_drop_up 98 more_vert 18th place solution Congrats to all medal teams and new Grandmasters,Masters,Experts.Thanks to Organizers and kaggle for such a good competition,it shows that kaggle competition is not just a game but also can be a useful machine learning project. Team At first we have three  teams individually, tomoyo and me, ethan and qyxs , wrb0312.We focus on feature engineering and optimization before wrb0312 joined us,I think we made many good features but neural network dominated this competition.wrb0312 did a great job even he use transformer for the first time.After wrb0312 joined us there are only ten days left,we focus on ensembling our models for inference,and also improved transformer very much.Our team members are from china and japan, it's very interesting to see we use chinese,japanese,english mixed-language to communicate.Greate job everyone! Optimization For GBM features, rather than using many dictionary to save features' data, we developed a nubma-based framework to speed up feature engineering process and online calculation. Firstly, the data are sorted by ['user_id', 'timestamp', 'content_id'] and split into different arrays. Then we created features in different array via self-designed rolling function or self-designed cumlative function. Actually, it provides us a very flexible way to create features and test it. In 10m data, the feature engineering process needs only 5 minutes to finish it. Some examples are listed as below. from tqdm import tqdm from numba import jit,njit from joblib import Parallel, delayed from tqdm import tqdm import gc from multiprocessing import Process, Manager,Pool from functools import partial from numba import prange import numpy as np import pandas as pd from numba import types from numba.typed import Dict import functools, time from numba.typed import List def timeit ( f ): def wrap ( *args, **kwargs ):\n        time1 = time.time()\n        ret = f(*args, **kwargs)\n        time2 = time.time() print ( '{:s} function took {:.3f} s' . format (f.__name__, np. round (time2-time1, 2 ))) return ret return wrap def rolling_feat_group ( train, col_used ):\n    a = train[col_used].values\n    ind = np.lexsort((a[:, 2 ],a[:, 1 ],a[:, 0 ]))\n    a = a[ind]\n    g = np.split(a, np.unique(a[:, 0 ], return_index= True )[ 1 ][ 1 :]) return g, ind, col_used @jit( nopython = True , fastmath = True ) def rolling_cal ( arr, step, window = 5 , shift_ = 1 ):\n    m = 2 arr_ = np.concatenate((np.full((window, ), np.nan), arr))\n    ret = np.zeros((arr.shape[ 0 ], m))\n    beg = window for i in step: \n        tmp = arr_[beg-window:beg]\n        ret[beg - window:(beg - window + i), 0 ] = np.nanmean(tmp)\n        ret[beg - window:(beg - window + i), 1 ] = np.nansum(tmp)\n        beg += i return ret @jit( nopython = True , fastmath = True ) def rolling_time_cal ( arr, window = 5 , shift_ = 1 ):\n    m = 1 arr_ = np.concatenate((np.full((window, ), np.nan), arr))\n    ret = np.zeros((arr.shape[ 0 ], m)) for i in range ( 0 ,arr.shape[ 0 ], 1 ): \n        tmp = arr_[i:i+window+ 1 ]\n        ret[i, 0 ] = np.nanmean(tmp) return ret def rolling_cal_wrap ( tmp_g, shift_period ):\n    m = 2 tmp_res = []\n    step = np.unique(tmp_g[:, 1 ], return_counts= True )[ 1 ] for window_size in shift_period:\n        tmp = rolling_cal(tmp_g[:, 2 ], step, window_size)\n        tmp_res.append(tmp)\n    tmp_res = np.concatenate(tmp_res, axis = 1 ) return tmp_res def rolling_time_cal_wrap ( tmp_g, shift_period ):\n    m = 2 tmp_res = [] for window_size in shift_period:\n        tmp = rolling_time_cal(tmp_g[:, 2 ], window_size)\n        tmp_res.append(tmp)\n    tmp_res = np.concatenate(tmp_res, axis = 1 ) return tmp_res def rolling_feat_cal ( tmp_g, name_dict, global_period ):\n    answer_idx = name_dict.index( 'answered_correctly' )\n    prior_idx = name_dict.index( 'prior_question_elapsed_time' )\n    item_mean_idx = name_dict.index( 'item_mean' )\n    task_set_idx = name_dict.index( 'task_set_distance' )\n    tmp_res1 = rolling_cal_wrap(tmp_g[:,[ 0 , 1 , answer_idx]], global_period)\n    tmp_res2 = rolling_time_cal_wrap(tmp_g[:,[ 0 , 1 , prior_idx]], global_period)\n    tmp_res3 = rolling_time_cal_wrap(tmp_g[:,[ 0 , 1 , item_mean_idx]], global_period)\n    tmp_res4 = rolling_time_cal_wrap(tmp_g[:,[ 0 , 1 , task_set_idx]], global_period)\n    tmp_res = np.concatenate([tmp_res1, tmp_res2, tmp_res3, tmp_res4], axis = 1 ) return tmp_res content_copy If anyone interested in how to create features via numba-framework, Tomoyo publiced his full GBM pipeline in github( https://github.com/ZiwenYeee/Riiid-numba-framework ) Catboost(LB 0.807) summary We created 183 features for final catboost model,including some original features,global statistics(item base),cumulative and rolling statistics(user base),tfidf-svd(base on question's user list),word2vec(base on user's question list, wrong and correct tag list ),timedelta from many perspective,last same part groups features. gbm benchmark We compared lightgbm ,xgboost,catboost,catboost is the best for the training and inference speed,and memory consuming.When train the full data,lightgbm need over 100 hours with my AMD Ryzen ThreadRipper 3970X,xgboost always have out of memory error even using dask with 4 RTX 3090. strong features and interesting finding by qyxs 1.  the history difficulty statistics features of user who had correct/wrong answers, boost almost 0.003 tmp_df = for_question_df .groupby ( 'content_id' ) [ 'answered_correctly' ] .agg ( [[ 'corr_ratio' , 'mean' ] ]) .reset_index ()\ntmp_fe = for_question_df [for_question_df[ 'answered_correctly' ] == 0 ] .merge (tmp_df, on= 'content_id' ) .groupby ( 'user_id' ) [ 'corr_ratio' ] .agg ( [ 'min' , 'max' , 'mean' , 'std' ] ) .reset_index ()\nfor_train = for_train .merge (tmp_fe, on= 'user_id' , how= 'left' ) content_copy 2.  focus on the records about the current part of user connect with last same part, generate the features include answer correct ratio, time diff, frequency etc, boost almost 0.002 for_question_df [ 'rank_part' ] = for_question_df .groupby ( [ 'user_id' , 'part' ] ) [ 'timestamp' ] .rank (method= 'first' )\nfor_question_df [ 'rank_user' ] = for_question_df .groupby ( [ 'user_id' ] ) [ 'timestamp' ] .rank (method= 'first' )\nfor_question_df [ 'rank_diff' ] = for_question_df [ 'rank_user' ] - for_question_df [ 'rank_part' ] for_question_df [ 'part_times' ] = for_question_df .groupby ( [ 'user_id' , 'part' ] ) [ 'rank_diff' ] .rank (method= 'dense' )\nfor_question_df [ 'rank_diff' ] = for_question_df .groupby ( [ 'user_id' , 'part' ] ) [ 'rank_diff' ] .rank (method= 'dense' , ascending=False)\n\nlast_part = for_question_df [for_question_df[ 'rank_diff' ] == 1 ]\npart_times = for_question_df .groupby ( [ 'user_id' , 'part' ] ) [ 'part_times' ] .agg ( [[ 'part_times' , 'max' ] ]) .reset_index ()\n\nlast_part_df = last_part .groupby ( [ 'user_id' , 'part' ] ) [ 'answered_correctly' ] .agg ( [[ 'last_continue_part_ratio' , 'mean' ] , [ 'last_continue_part_cnt' , 'count' ] ]) .reset_index ()\nlast_part_time = last_part .groupby ( [ 'user_id' , 'part' ] ) [ 'timestamp' ] .agg ( [[ 'last_continue_part_time_start' , 'min' ] , [ 'last_continue_part_time_end' , 'max' ] ]) .reset_index ()\nlast_part_df = last_part_df .merge (last_part_time, on= [ 'user_id' , 'part' ] , how= 'left' )\nlast_part_df = last_part_df .merge (part_times, on= [ 'user_id' , 'part' ] , how= 'left' )\nlast_part_df [ 'part_time_diff' ] = last_part_df [ 'last_continue_part_time_end' ] - last_part_df [ 'last_continue_part_time_start' ] last_part_df [ 'part_time_freq' ] = last_part_df [ 'last_continue_part_cnt' ] /last_part_df [ 'part_time_diff' ] for_train = for_train .merge (last_part_df, on= [ 'user_id' , 'part' ] , how= 'left' )\nfor_train [ 'last_continue_part_time_start' ] = for_train [ 'timestamp' ] - for_train [ 'last_continue_part_time_start' ] for_train [ 'last_continue_part_time_end' ] = for_train [ 'timestamp' ] - for_train [ 'last_continue_part_time_end' ] content_copy 3.  the answer correctly ratio of each question under differenct user abilititys (split for 11 bins), boost almost 0.001 for_question_df[ 'user_ability' ] = for_question_df.groupby( 'user_id' )[ 'answered_correctly' ].transform( 'mean' ).round(1)\ntmp_df = for_question_df.pivot_table( index = 'content_id' , columns = 'user_ability' , values = 'answered_correctly' , aggfunc = 'mean' ).reset_index()\ntmp_df.columns = [ 'content_id' ] + [f 'c_mean_{i}_ratio' for i in range(11)]\nfor_train = for_train.merge(tmp_df, on = 'content_id' , how = 'left' ) content_copy Some interseting points: 1.they would watch lecture after users had wrong answers, so we could generated some features from this. LB is not improved caused by the lectures info in next group maybe. 2.the content_id such as 0-195， 7851-7984 etc, then are all same in one part and continuous with each other，we could build a new bundle to generate features strong features and interesting finding by ethan 1.  user's behavior in last 1,5,…,60 minutes, 0.001 boost for w in [1, 5, 10, 15, 30, 45, 60] : print (w)\n    tmp = q_logs [q_logs[ 'timestamp' ] >=(q_logs [ 'end_time' ] -w* 60 * 1000 )] .copy ()\n    group_df = tmp .groupby ( [ 'user_id' ] ) [ 'content_id' ] .agg ( [[ 'user_content_nunique_in_last{}mins' .format(w), 'nunique' ] ]) .reset_index ()\n    train = train .merge (group_df, on= [ 'user_id' ] , how= 'left' )\n    group_df = tmp .groupby ( [ 'user_id' ] ) [ 'part' ] .agg ( [[ 'user_part_nunique_in_last{}mins' .format(w), 'nunique' ] ]) .reset_index ()\n    train = train .merge (group_df, on= [ 'user_id' ] , how= 'left' )\n    group_df = tmp .groupby ( [ 'user_id' ] ) [ 'answered_correctly' ] .agg ( [[ 'user_correct_raito_in_last{}mins' .format(w), 'mean' ] ]) .reset_index ()\n    train = train .merge (group_df, on= [ 'user_id' ] , how= 'left' ) content_copy 2. \"users' ablility\" statistics in each question, seperately by \"answered_correctly\"(0/1), 0.002 boost cc = q_logs .groupby ( [ 'user_id' ] ) [ 'answered_correctly' ] .agg ( [[ 'corr_ratio' , 'mean' ] ]) .reset_index ()\ngg = q_logs [[ 'user_id' , 'content_id' , 'answered_correctly' ] ] .merge (cc, on= [ 'user_id' ] , how= 'left' )\n\ngroup_df1 = gg [gg[ 'answered_correctly' ] == 1 ] .groupby ( [ 'content_id' ] ) [ 'corr_ratio' ] .agg ( [[ 'question_correct_user_ablility_min' , 'min' ] , [ 'question_correct_user_ablility_max' , 'max' ] , [ 'question_correct_user_ablility_mean' , 'mean' ] , [ 'question_correct_user_ablility_skew' , 'skew' ] , [ 'question_correct_user_ablility_med' , 'median' ] , [ 'question_correct_user_ablility_std' , 'std' ] ]) .reset_index ()\ngroup_df2 = gg [gg[ 'answered_correctly' ] == 0 ] .groupby ( [ 'content_id' ] ) [ 'corr_ratio' ] .agg ( [[ 'question_wrong_user_ablility_min' , 'min' ] , [ 'question_wrong_user_ablility_max' , 'max' ] , [ 'question_wrong_user_ablility_mean' , 'mean' ] , [ 'question_wrong_user_ablility_skew' , 'skew' ] , [ 'question_wrong_user_ablility_med' , 'median' ] , [ 'question_wrong_user_ablility_std' , 'std' ] ]) .reset_index () content_copy 3. \"lagtime\" statistics in each question, seperately by \"answered_correctly\"(0/1), means the distribution of users' preprare time for answering this question correctly, about 0.001 boost user_task_timestamp = q_logs [[ 'user_id' , 'task_container_id' , 'timestamp' ] ] .drop_duplicates ()\nuser_task_timestamp [ 'lag_time' ] = user_task_timestamp [ 'timestamp' ] - user_task_timestamp .groupby ( [ 'user_id' ] ) [ 'timestamp' ] .shift ( 1 )\ntmp = q_logs [[ 'user_id' , 'task_container_id' , 'content_id' , 'answered_correctly' ] ] .merge (user_task_timestamp .drop ( [ 'timestamp' ] , axis= 1 ), on= [ 'user_id' , 'task_container_id' ] , how= 'left' )\ngroup_df = tmp [tmp[ 'answered_correctly' ] == 1 ] .groupby ( [ 'content_id' ] ) [ 'lag_time' ] .agg ( [[ 'c_lag_time_mean' , 'mean' ] , [ 'c_lag_time_std' , 'std' ] , [ 'c_lag_time_max' , 'max' ] , [ 'c_lag_time_min' , 'min' ] , [ 'c_lag_time_median' , 'median' ] ]) .reset_index ()\ntrain = train .merge (group_df, on= [ 'content_id' ] , how= 'left' )\n\ngroup_df = tmp [tmp[ 'answered_correctly' ] == 0 ] .groupby ( [ 'content_id' ] ) [ 'lag_time' ] .agg ( [[ 'w_lag_time_mean' , 'mean' ] , [ 'w_lag_time_std' , 'std' ] , [ 'w_lag_time_max' , 'max' ] , [ 'w_lag_time_min' , 'min' ] , [ 'w_lag_time_median' , 'median' ] ]) .reset_index ()\ntrain = train .merge (group_df, on= [ 'content_id' ] , how= 'left' ) content_copy feature list [ 'content_id ', 'prior_question_elapsed_time ', 'prior_question_had_explanation ', 'correct_answer ', 'user_count ', 'user_sum ', 'user_mean ', 'item_count ', 'item_sum ', 'item_mean ', 'answer_ratio_0 ', 'answer_ratio_1 ', 'answer_ratio_2 ', 'bundle_id ', 'part ', 'le_tag ', 'question_correct_user_ablility_mean ', 'question_correct_user_ablility_median ', 'question_wrong_user_ablility_mean ', 'question_wrong_user_ablility_median ', 'word2vec_0 ', 'word2vec_1 ', 'word2vec_2 ', 'word2vec_3 ', 'word2vec_4 ', 'svd_0 ', 'svd_1 ', 'svd_2 ', 'svd_3 ', 'svd_4 ', 'tags_w2v_correct_mean_0 ', 'tags_w2v_wrong_mean_0 ', 'tags_w2v_correct_mean_1 ', 'tags_w2v_wrong_mean_1 ', 'tags_w2v_correct_mean_2 ', 'tags_w2v_wrong_mean_2 ', 'tags_w2v_correct_mean_3 ', 'tags_w2v_wrong_mean_3 ', 'tags_w2v_correct_mean_4 ', 'tags_w2v_wrong_mean_4 ', 'real_time_wrong_mean ', 'real_time_wrong_median ', 'real_time_correct_mean ', 'real_time_correct_median ', 'task_set_distance_wrong_mean ', 'task_set_distance_wrong_median ', 'task_set_distance_correct_mean ', 'task_set_distance_correct_median ', 'mean_0_ratio ', 'mean_1_ratio ', 'mean_3_ratio ', 'mean_4_ratio ', 'mean_5_ratio ', 'mean_6_ratio ', 'mean_7_ratio ', 'mean_8_ratio ', 'mean_9_ratio ', 'mean_10_ratio ', 'user_d1 ', 'user_d2 ', 'task_set_distance ', 'user_diff_mean ', 'user_diff_std ', 'user_diff_min ', 'user_diff_max ', 'task_set_item_mean ', 'task_set_item_min ', 'task_set_item_max ', 'task_set_distance2 ', 'task_distance_shift ', 'task_set_distance_diff ', 'task_distance_diff_shift ', 'container_mean_1 ', 'container_mean_5 ', 'container_std_5 ', 'container_mean_10 ', 'container_std_10 ', 'container_mean_20 ', 'container_std_20 ', 'container_mean_30 ', 'container_std_30 ', 'container_mean_40 ', 'container_std_40 ', 'prior_question_elapsed_time_mean_1 ', 'prior_question_elapsed_time_mean_5 ', 'prior_question_elapsed_time_mean_10 ', 'prior_question_elapsed_time_mean_20 ', 'prior_question_elapsed_time_mean_30 ', 'prior_question_elapsed_time_mean_40 ', 'item_mean_mean_30 ', 'item_mean_mean_40 ', 'task_set_distance_mean_1 ', 'task_set_distance_mean_5 ', 'task_set_distance_mean_10 ', 'task_set_distance_mean_20 ', 'task_set_distance_mean_30 ', 'begin_time_diff ', 'end_time_diff ', 'part_time_diff_mean ', 'part_session_mean ', 'part_session_sum ', 'part_session_count ', 'full_group0_item_mean_mean ', 'full_group0_item_mean_median ', 'full_group0_task_set_distance_median ', 'full_group0_timestamp_mean ', 'full_group0_timestamp_median ', 'full_group1_item_mean_mean ', 'full_group1_item_mean_median ', 'full_group1_task_set_distance_median ', 'full_group1_timestamp_median ', 'part_sum ', 'part_count ', 'part_mean ', 'part_sum_global_ratio ', 'part_sum_1 ', 'part_sum_5 ', 'part_mean_5 ', 'part_sum_10 ', 'part_mean_10 ', 'cum_answer0_mean_item_mean ', 'cum_answer0_median_item_mean ', 'cum_answer0_median_task_set_distance ', 'cum_answer1_mean_item_mean ', 'cum_answer1_median_item_mean ', 'cum_answer1_mean_task_set_distance ', 'cum_answer1_median_task_set_distance ', 'cum_answer0_time_diff ', 'cum_answer1_time_diff ', 'global_task_set_shift1 ', 'global_task_set_shift2 ', 'global_task_set_shift4 ', 'global_task_set_shift5 ', 'cum_answer0_mean_wrong_time_diff ', 'cum_answer0_median_wrong_time_diff ', 'cum_answer1_mean_right_time_diff ', 'content_correct_mean ', 'content_correct_sum ', 'content_correct_count ', 'hard_answer0_time ', 'hard_answer1_time ', 'full_bundle_item_mean_mean ', 'full_bundle_item_mean_median ', 'full_bundle_task_set_distance_mean ', 'full_bundle_task_set_distance_median ', 'full_bundle_timestamp_mean ', 'full_bundle_timestamp_median ', 'bundle_sum ', 'bundle_mean ', 'bundle_count ', 'user_trend_mean ', 'user_trend_median ', 'user_trend_roll_user_ans_sum ', 'user_trend_roll_user_ans_mean ', 'user_trend_roll_user_ans_count ', 'user_trend_roll_item_ans_mean ', 'user_trend_roll_item_ans_count ', 'div_ratio1 ', 'div_ratio2 ', 'div_ratio3 ', 'new_Feat0 ', 'new_Feat1 ', 'new_Feat2 ', 'new_Feat3 ', 'part_time_wrong_div ', 'part_time_right_div ', 'diff_lag_median_div ', 'diff_item_median_div ', 'diff_time_median_div ', 'diff_item_mean_div ', 'diff_task_set_mean_div ', 'diff_timestamp_mean_div ', 'last_20_frequent_answer ', 'last_20_frequent_answer_count ', 'last_20_frequent_answer_mean ', 'last_20_frequent_answer_sum ', 'last_user_same_answer_tf ', 'last_item_same_answer_tf ', 'last_right_time_diff ', 'last_wrong_time_diff ', 'last_5_part_time_div ', 'last_10_part_time_div ', 'last_20_part_time_div '] content_copy Transformer(LB 0.808) Used questions only. Lectures did't improve our validation score. In training window size 800 (the bigger,the better), in inference window size 300(the bigger,the better,but time consuming). Optimizer : Adam  with lr = 8e-4, beta1 = 0.9, beta2 = 0.999 with warmup steps to 4000. Without warmup steps, didn't converge. Number of layers = 4, dimension of the model = 256, dimension of the FFN = 2048. Batch size=80 With smaller size,  didn't converge. Dropout = 0 Postion encoding : Axial Positional Embedding https://arxiv.org/abs/1912.12180 https://github.com/lucidrains/axial-positional-embedding This encoding improved score significantly. Augmentation : Mixup-Transformer https://arxiv.org/abs/2010.02394 Inputs question_id part prior_question_elapsed_time / 1000 lagtime log1p((timestamp_t - timestamp_(t-1)) /1000 / 60) This feature improved the score significantly answered_correctly GBT feats (imortance top N) model image Ensemble(LB 0.812) Finally we use one catboost and two transformers for ensemble due to the inference time limitation.Unfortunately our inference notebook has bug although we improved our model about 0.0006 at last day. Please sign in to reply to this topic. comment 24 Comments Hotness Tomoyo Posted 4 years ago · 18th in this Competition arrow_drop_up 14 more_vert Thanks for your work and effort, bro. Looking forward to future cooperation! If anyone interested in my numba-based framework for GBM, leave a message below. I'm considering write a detailed explanation about it. u++ Posted 4 years ago · 2845th in this Competition arrow_drop_up 0 more_vert I have a strong interest on it 👍 Shuhao Cao Posted 4 years ago · 243rd in this Competition arrow_drop_up 0 more_vert Interested. Thanks. Aditya Soni Posted 4 years ago · 125th in this Competition arrow_drop_up 3 more_vert I also interested in looking at it's src; Thanks a lot for sharing. I also used numba for computing the rolling FE's, the time dropped to <3 secs for 100M rows as compared to more than 30 mins of Pandas! So very excited to explore numba! # <3 secs for 100M rows on my lapi from numba import njit # no python basically window_width = 5 dummy_window_5 = [np.nan]* 4 # window_width-1 @njit def running_window_stats_window_width_5 ( vector ): # vector must be pre-padded with a zero cumsum_vec = np.cumsum(vector, ) return (cumsum_vec[ window_width: ] - cumsum_vec[ :-window_width ]) / window_width content_copy Ethan Posted 4 years ago · 18th in this Competition arrow_drop_up 3 more_vert Congrats to all winners! Thanks for my teamates @senkin13 @guziye @juzqyxs @wrb0312 ! Great teamwork! Daniels Posted 4 years ago · 35th in this Competition arrow_drop_up 1 more_vert Congratulations! RDizzl3 Posted 4 years ago · 1431st in this Competition arrow_drop_up 1 more_vert @senkin13 question for you - looks like catboost had a pretty strong performance based on features. Were you and your team not able to get a similar score with lightgbm? Also, does catboost have a similar type of feature importance plot? Just curious to know which of these features gave the greatest gains senkin13 Topic Author Posted 4 years ago · 18th in this Competition arrow_drop_up 4 more_vert the reason we use catboost is just speed,I think lightgbm can get a similar score and feature gains Ethan Posted 4 years ago · 18th in this Competition arrow_drop_up 2 more_vert @rdizzl3 Instead of \"lagtime\", our good features focus on \"user's ablilty\" and \"question's difficulty\". User's correctness in history represents \"user's ablility\" and Question's correctness(global) represents \"question's difficulty\". For example, \"quesions' difficulty\" statics(mean, max,min…) in each user's history and \"user's ablilty\" statics in each quesion. Especially create these features seperately by answered_correctly(0 or 1) will get great gains. Alyona Pasevieva Posted 4 years ago · 144th in this Competition arrow_drop_up 1 more_vert Very interesting! KhanhVD Posted 4 years ago · 146th in this Competition arrow_drop_up 2 more_vert Great feature engineering work. Congrats on the results @senkin13 and team u++ Posted 4 years ago · 2845th in this Competition arrow_drop_up 2 more_vert I've never used nubma. Really inspiring. Thank you for sharing. JT-karl Posted 4 years ago · 143rd in this Competition arrow_drop_up 0 more_vert Congratulation for your amazing work ! Thanks for sharing your great ideas so much. Can you give additional details about w2v & svd features ? 1) While tags_w2v_correct_mean_0 ... tags_w2v_wrong_mean_4 are self-explanatory and clear, I don't understand what word2vec_0 ... word2vec_4 stand for. You said \"based on user's question list\" : is this feat the w2v of the current q_id ? Or the mean of w2v of each q_id in user's question list, just like tags ? 2) What about svd_0 ... svd_4 ? qyxs Posted 4 years ago · 18th in this Competition arrow_drop_up 2 more_vert word2vec features are the w2v of current q_id, svd features get from the tfidf of users' history q_id. JT-karl Posted 4 years ago · 143rd in this Competition arrow_drop_up 0 more_vert Makes sense. Many thanks, and congratz again ! ZZ Posted 4 years ago · 556th in this Competition arrow_drop_up 0 more_vert Congratulations for the great solution and nice finish! I have two questions: -1 In the example code, It seems like the corr_ratio are calculated by each user_id with forward looking. Model still had stable improvement between CV and LB? tmp_df = for_question_df .groupby ( 'content_id' ) [ 'answered_correctly' ] .agg ( [[ 'corr_ratio' , 'mean' ] ]) .reset_index ()\ntmp_fe = for_question_df [for_question_df[ 'answered_correctly' ] == 0 ] .merge (tmp_df, on= 'content_id' ) .groupby ( 'user_id' ) [ 'corr_ratio' ] .agg ( [ 'min' , 'max' , 'mean' , 'std' ] ) .reset_index ()\nfor_train = for_train .merge (tmp_fe, on= 'user_id' , how= 'left' ) content_copy -2 How did you update features with numba functions in the test api? (didn't find numba functions for test updating in the gihub) Tomoyo Posted 4 years ago · 18th in this Competition arrow_drop_up 1 more_vert for question 2, We only update saving data and calculate with numba function rather than updating features. I loaded up one of our submit notebook and our test script in my github. https://github.com/ZiwenYeee/Riiid-numba-framework qyxs Posted 4 years ago · 18th in this Competition arrow_drop_up 0 more_vert yeah,the improvement was stable. 2981 Posted 4 years ago · 431st in this Competition arrow_drop_up 0 more_vert congratulations, a great innovative job! Theo Viel Posted 4 years ago arrow_drop_up 0 more_vert Congratz to all your team for the nice finish ! Unfortunately our inference notebook has bug although we improved our model about 0.0006 at last day. That's heartbreaking, however do let people know if you manage to fix the bug & improve your score senkin13 Topic Author Posted 4 years ago · 18th in this Competition arrow_drop_up 1 more_vert thanks for your care,we found the bug is from changing some features from float64 to float32 by mistake in inference notebook,if it is normal we still can not get gold medal,no regret! Theo Viel Posted 4 years ago arrow_drop_up 0 more_vert Good to hear, thanks ! Chris Deotte Posted 4 years ago · 16th in this Competition arrow_drop_up 0 more_vert Congratulations. Great models and strong finish! Your transformer is interesting. Did it do better than SAINT+? For reference, SAINT+ is pictured below! DARE Posted 4 years ago · 18th in this Competition arrow_drop_up 1 more_vert Thanks! I didn't try your picture's model architecture. But maybe we can get same score by SAINT+ architecture, with shorter inference time. In this competition I tried transformer first time, so I think it is possible to optimize the model structure more (it was hard for me). Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Shujun · 20th in this Competition  · Posted 4 years ago arrow_drop_up 47 more_vert 20th solution, transformer encoder only model (SERT) with code and notebook Thanks to the organizers for hosting a comp with very little shakeup. This is always nice to see. I wanna thank @wangsg and @leadbest for the starter kernels. Without those I would not have known what to do in this difficult comp. Architecture I use the transformer encoder only SERT (SIngle-directional Encoder Representation from Transformers), to make predictions just one linear layer after the last encoder layer. This is probably a mistake Embeddings Question_id Prior question correctness Timestamp difference between bundles Prior question elapsed time Prior question explanation Tag cluster thanks to @spacelx Tag vector Fixed pos encoding, same as in Attention is All You Need Key modifications encoder only, this is kind of stupid and a mistake. I think this cost me a few places fixed pos encoding, which allows retraining the model with longer sequences layer norm and dropout after embedding layers Loss weight favoring later positions, np.arange(0,1,1/seq_length)*loss Mistakes I couldn't resolve Intra-bundle leakage, I made a nice task mask implementation, but it only made my score worse, probably because of the lack of a decoder. I ended using just an autoregressive mask First few positions during inference and training may have wrong timestamp difference since i simply do t[1:]-t[:-1] Code and submission kernel https://github.com/Shujun-He/Riiid-Answer-Correctness-Prediction-20th-solution https://www.kaggle.com/shujun717/fork-of-tag-encoding-with-loss-weight?scriptVersionId=51272583 Feel free to ask questions. This is a very brief write-up Please sign in to reply to this topic. comment 18 Comments 1 appreciation  comment Hotness higepon Posted 4 years ago · 177th in this Competition arrow_drop_up 1 more_vert Thank you for sharing and congratulations! I'm learning a lot from your code at github :) Shujun Topic Author Posted 4 years ago · 20th in this Competition arrow_drop_up 0 more_vert You're welcome! Glad it is useful KhanhVD Posted 4 years ago · 146th in this Competition arrow_drop_up 1 more_vert Strongly model. Congrats and thanks for sharing solution and code @shujun717 Shujun Topic Author Posted 4 years ago · 20th in this Competition arrow_drop_up 0 more_vert Thanks gold was within reach but alas, mistakes were made Phil Dhingra Posted 4 years ago · 931st in this Competition arrow_drop_up 0 more_vert This is great. What hardware was it run on? Shujun Topic Author Posted 4 years ago · 20th in this Competition arrow_drop_up 1 more_vert Thanks. Mostly a 2x3090 machine with a 24 core threadripper Neil Gibbons Posted 4 years ago · 1196th in this Competition arrow_drop_up 0 more_vert @shujun717 thanks for sharing, I enjoyed reading this and it was super helpful to look through your code Shujun Topic Author Posted 4 years ago · 20th in this Competition arrow_drop_up 0 more_vert @neilgibbons no problem! Glad it was of help. Abdur Rehman Posted 4 years ago · 873rd in this Competition arrow_drop_up 0 more_vert @shujun717 congrats and thanks for sharing. Loss weight favoring later positions, np.arange(0,1,1/seq_length)*loss Why you are giving more weight to loss for later position ? Are they more difficult to predict for your model and how did you figure out this ? Also, in your code, you have question_cmnts.csv . Can you tell a bit about it or can you provide some reference? Shujun Topic Author Posted 4 years ago · 20th in this Competition arrow_drop_up 1 more_vert That's a good question @abdurrehman245 . Because of the autoregressive mask, the items in the sequence can only see items that come before. For instance, the 1st question can only see itself, and the 2nd can only see itself and the 1st. Therefore, the early questions have less context compared to the later questions. If I let all positions have equal weight, the model will overfit to earlier positions with little context. Another consideration is that during inference, I'm always using the last position to make predictions, so it doesnt make sense to give equal weighting to everything. Using this loss weight gave me 0.002 boost. As for question_cmnts.csv, here's the notebook https://www.kaggle.com/spacelx/2020-r3id-clustering-question-tags cswwp Posted 4 years ago · 24th in this Competition arrow_drop_up 0 more_vert Congrats @shujun717 , nice solution! Dean Posted 4 years ago · 37th in this Competition arrow_drop_up 0 more_vert Thanks for your sharing. By the way, why did you use encoder only? Shujun Topic Author Posted 4 years ago · 20th in this Competition arrow_drop_up 0 more_vert I always prefer less complexity. To me I though the decoder was not necessary, so I got rid of it. Most of my changes was also just to reduce complexity (e.g. learnable pos encoding -> fixed) Ioannis M Posted 4 years ago · 220th in this Competition arrow_drop_up 0 more_vert Excellent work @shujun717 !! Thank you for the write up and the code.. I will study it more carefully in the weekend Edit: it would be nice to add the milestones wrt to LB scores, e.g. starting from the public nb: LB 07xx, add lag info: LB 078xx, …, up to 0.810 - if you have and you like to share such info Shujun Topic Author Posted 4 years ago · 20th in this Competition arrow_drop_up 0 more_vert Thx and nice suggestion. I will try to add that information Aditya Soni Posted 4 years ago · 125th in this Competition arrow_drop_up 0 more_vert Wow! This is cool work! Can you share more details about that tag vector of yours? Shujun Topic Author Posted 4 years ago · 20th in this Competition arrow_drop_up 1 more_vert Yes, so there are 188 tags in total, so for each question I use a tag vector of size 188 of 0's and 1's. For instance, it a question has tags 1 and 140, the 1th and 140th element would be 1's and the rest would be 0's, and then a linear layer to transform it to embedding dim Appreciation (1) Gopi Durgaprasad Posted 4 years ago · 1021st in this Competition arrow_drop_up 0 more_vert Great ….. Thank you for sharing 💯 Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Gautier · 21st in this Competition  · Posted 4 years ago arrow_drop_up 26 more_vert 21st Place Solution - Ensemble of 3 Saint++ Models Hi all! After seeing everyone share their solution we have decided to also share ours in the same spirit. We ran a modified SAINT+ that included lectures, tags and a couple of aggregate features. All our code is available on github ( https://github.com/gautierdag/riiid ). We used Pytorch/Pytorch Lightning and Hydra. Our single model public LB was 0.808, and we were able to push that to LB 0.810 (Final LB 0.813) through ensembling. Modifications to original SAINT: Lectures : lectures were included and had a special trainable embedding vector instead of the answer embedding. The loss was not masked for lecture steps. We tested both with and without, and the difference wasn't crazy. The big benefit they offered was that they made everything much simpler to handle since we did not need to filter anything. Tags : Since questions had a maximum of six tags, we passed sequences of length 6 to a tag embedding layer and summed the output. Therefore our tag embeddings were learned for each tag but then sum across to obtain an embedding for all the tags in a question/lecture. The padding simply returned a 0 vector. Agg Feats : This is what gave us a healthy boost of 0.002 on public LB. By including 12 aggregate features in the decoder we were able to use some of our learnings in previous LGBM implementations. Agg feats We used the following aggregate features: attempts: number of previous attempts on current question (clamped to 5 and normalized to 1) mean_content_mean: average of all the average question accuracy (population wise) seen until this step by the user mean_user_mean: average of user's accuracy until this step parts_mean: seven dimensional (the average of the user's accuracy on each part) m_user_session_mean: the average accuracy in the current user's session session_number: the current session number (how many sessions up till now) Note: session is just a way of dividing up the questions if a difference in time between questions is greater than two hours. Float vs Categorical We switched to use a float representation of answers in the last few days. This seemed to have little effect on our LB auc unfortunately, but had an effect locally. The idea was that since we auto-regress on answers we would propagate the uncertainty that the model displayed into future predictions. All aggs and time features are floats that are ran through a linear layer (without bias). They were all normalized to either [-1,1] or [0, 1]. All other embeddings are categorical. Training: What made a big difference early on was our sampling methodology. Unlike most approaches in public kernels, we did not take a user-centric approach to sample our training examples. Instead, we simply sampled based on row_id and would load in the previous window_size history for every row. So for instance if we sampled row 55. Then we would find the user id that row 55 corresponds to, and load in their history up until that row. The size of the window would then be the min(window_size, len(history_until_row_55)). We used Adam with 0.001 learning rate, early stopping and lr decay based on val_auc during training. Validation For validation, we kept a holdout set of 2.5mil (like the test set) and generated randomly with a certain proportion guaranteed of new users. We used only 250,000 rows to actually validate on during training. For every row in validation, we would pick a random number of inference steps between 1 and 10. We would then infer over these steps, not allowing the model to see the true answers and having to use its own. Inference During inference, when predicting multiple questions for a single user, we fed back the previous prediction of our model as answers in the model. This auto-regression was helpful in propagating uncertainty and helped the auc. I saw a writeup that said that this was not possible to do because it constrains you to batch size = 1. That is wrong, you can actually do this for a whole batch in parallel and it's a little tricky with indexes but it is doable. You simply have to keep track of each sequence lengths and how many steps you are predicting for each. Unfortunately since some of our aggs are also based on user answers, these do not get updated until the next batch because they are calculated on CPU and not updated in that inference loop. Parameters We ensemble three models. First model: 64 emb_dim 4/6 encoder/decoder layers 256 feed foward in Transformer 4 Heads 100 window size Second model: 64 emb_dim 4/6 encoder/decoder layers 256 feed foward in Transformer 4 Heads 200 window size (expanded by finetuning on 200) Third model: 256 emb_dim 2/2 encoder/decoder layers 512 feed foward in Transformer 4 Heads 256 window size Hardware We rented three machines but we could have probably gotten farther with using a larger single machine: 3 X  1 Tesla V100(16gb VRAM) Other things we tried Noam lr schedule (super slow to converge) Linear attention / Reformer / Linformer (spent way to much time on this) Local Attention Additional Aggs in output layer and a myriad of other aggs Concatenating Embeds instead of summing A lot of different configurations of num_layers / dims / .. etc Custom attention based on bundle Ensembling with LGBM Predicting time taken to answer question as well as answer in Loss K Beam decoding (predicting in parallel K possible paths of answers and taking the one that maximized joint probability of sequence) Running average of agg features (different windows or averaging over time) Causal 1D convolutions before the Transformer Increasing window size during training … Finally, thanks to @eeeedev for the adventure! Transformers PyTorch Please sign in to reply to this topic. comment 14 Comments Hotness Jagadish Sivakumar Posted 4 years ago · 958th in this Competition arrow_drop_up 1 more_vert Congrats @brotye 👍 Nice solution Shujun Posted 4 years ago · 20th in this Competition arrow_drop_up 1 more_vert Nice solution, I also had linear transformers in mind although due to time constraint did not try it. Can you give more details (lb/cv) on the linear transformers? Also, did you happen to try performers? Gautier Topic Author Posted 4 years ago · 21st in this Competition arrow_drop_up 0 more_vert Thanks! Well the problem is that I never got any of the alternative attentions to even converge. I used public implementations of them so I'm quite certain it wasn't from a bug. I spent about a week on testing a lot of them (I didn't try the Performer architecture). I think they are much harder to train for some reason on this dataset, this could be due to me not being patient enough in training or having some wrong hyperparameter/initialization scheme. u++ Posted 4 years ago · 2845th in this Competition arrow_drop_up 1 more_vert Congrats and thank you for sharing. I felt that all of the efforts are excellent, considering the characteristic of this competition! Abdur Rehman Posted 4 years ago · 873rd in this Competition arrow_drop_up 1 more_vert @brotye github link is not working. Gautier Topic Author Posted 4 years ago · 21st in this Competition arrow_drop_up 1 more_vert Thanks for noticing! The parenthesis was wrongly included in the url - fixed it! 5 more replies arrow_drop_down Darren Lahr Posted 4 years ago · 368th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for sharing, interesting sampling method. Was your ensemble just an average of the 3 models? When you compare Adam to Noam was the difference like half the epochs required to converge? Gautier Topic Author Posted 4 years ago · 21st in this Competition arrow_drop_up 1 more_vert Thank you! Yes the ensemble was a simple average of all models, all models were all too close individually to weight one over another. The problem with Noam was that because it's a linear increase that needs to be parametrised (you have to decide number of warmup steps) - the network really learns nothing at the beginning. Then the exponential decay hits immediately after reaching your target lr. While this might be good to warmup very deep Transformers, we found that using a simple lr and starting directly at the target lr was much faster. I don't remember the exact difference, but probably something like 1.5x faster to not use Noam and no loss/auc difference. It would obviously depend on how you parametrize the Noam scheme. The default and advertised Noam scheme is 4k warm up steps but that should also depend on your batch size as well. Darren Lahr Posted 4 years ago · 368th in this Competition arrow_drop_up 1 more_vert Thanks for the detailed response much appreciated Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules cswwp · 24th in this Competition  · Posted 4 years ago arrow_drop_up 28 more_vert 24th Place Solution saintpp + lgb private0.810 Firstly, thanks to Kaggle hold such a good competition, almost no shake. Congrats tops and very thanks to my teammates @jaideepvalani @ekffar . Finally our team finally public score 0.808, private 0.810, and final rank 24. we missed gold, it's ok, now i want to share our method because of jet lag, i have no read discuss in detail, so i just see my teammate @jaideepvalani had shared our solution right now. As a complementary, not repeat, i just expand the detail based on jaideepvalani's solution Architecture SaintPlusPlus, lgb content_copy Embeddings Exercise: Question_id\nPart\nCommunity id (tried, but no boosting, maybe need tuning)\nTask container id (tried, but no boosting, maybe need tuning) content_copy Interaction: lag time (same container id, keep same with first one) (discrete embedding)\nprior question elapsed time (continuous embedding)\nprior had explain (discrete embedding)\nPrior lecture (discrete embedding) content_copy Response: Prior question correctness ( discrete embedding ) content_copy Data sample optimization Data sampling is important in our experiment optimization ## SAINTPP 1 skip connection, we add skip connection for multi encoder layers inner, and multi decoder layers inner, the origin implement i use from github have no this skip connection 2 add dropout for encoder, decoder, FFN 3 try TCN causal conv after embedding, no boost, but now, i think it should tune and will work 4 big seq len for saintpp give our team boosting, i think it maybe caused by too short seq make transformer link model struggle into local minest point LGB updating Conclusion Our best saintpp(seqlen 320, embdim224, 2 x encder+ 2 x decoder) get cv 0.8016 lb 0.806, and lgb near get 0.796. Finally submission is merge lgb(lb 796) + saintpp cv1(cv 8016) + saintpp cv3(cv 0.7992)  = public lb 0.808, and private 0.810. should do things: 1 should continue optimize our net with casual conv(because transformer like is a global attention, some local correlation maybe ignored) and gru, but finally have no time 2 should try big net continue Thanks again to my teammates, they give me a lots of help and let me learn a lot. Going  for gold next Compete, keep moving. Please sign in to reply to this topic. comment 4 Comments Hotness Jaideep Posted 4 years ago · 24th in this Competition arrow_drop_up 3 more_vert There may be some common mistakes in calculation of below features for saint plus 1) Lagtime needs to be same for all question of Container. While calculating it using shift  ,we get 0 lagtime for all questions except the first one ,so one needs to replace those zero using simple group by user/task container and transform first.  This gave us boost of atleast 0.0015 to 0.003. Note: Paper mentions prior question lag time but this info seem to be getting too stale for model to intepret some thing for current question,so some how just ts2-ts1 only worked best for us. 2) Second most important mistake some people could be making in calculating lagtime during inference. Users Task containers are spreaded across multiple iterations unlike train where we have got all interactions at one place ,so using transformation as in step 1 works only partially ,as it misses on lagtime calculation for first question of same user in the other iteration, so one must maintain dict of last timestamp for each user across iterations. This correction gave us boost of 0.005 . 3) Same goes Prior interaction as lecture, calculating it for train was similar to lagtime but for inference it was even more tricky. One had to take into account last interaction of user in the train if it was Lecture, similarly do same for users last interaction in every test iteration if its a lecture. We regret we couldnt do much experiment over using of prior lecture elapsed time,prios lecture part. Shu Posted 4 years ago · 288th in this Competition arrow_drop_up 1 more_vert Congrats!!! Really impressive cswwp Topic Author Posted 4 years ago · 24th in this Competition arrow_drop_up 1 more_vert Thank you @tangshuyun sorry for late reply This comment has been deleted. Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Track knowledge states of 1M+ students in the wild Tailoring education to a student's ability level is one of the many valuable things an AI tutor can do. Your challenge in this competition is a version of that overall task; you will predict whether students are able to answer their next questions correctly. You'll be provided with the same sorts of information a complete education app would have: that student's historic performance, the performance of other students on the same question, metadata about the question itself, and more. This is a time-series code competition, you will receive test set data and make predictions with Kaggle's time-series API. Please be sure to review the Time-series API Details section closely. train.csv row_id : (int64) ID code for the row. timestamp : (int64) the time in milliseconds between this user interaction and the first event completion from that user. user_id : (int32) ID code for the user. content_id : (int16) ID code for the user interaction content_type_id : (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture. task_container_id : (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id . user_answer : (int8) the user's answer to the question, if any. Read -1 as null, for lectures. answered_correctly : (int8) if the user responded correctly. Read -1 as null, for lectures. prior_question_elapsed_time : (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle. prior_question_had_explanation : (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback. questions.csv : metadata for the questions posed to users. question_id : foreign key for the train/test content_id column, when the content type is question (0). bundle_id : code for which questions are served together. correct_answer : the answer to the question. Can be compared with the train user_answer column to check if the user was right. part : the relevant section of the TOEIC test . tags : one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together. lectures.csv : metadata for the lectures watched by users as they progress in their education. lecture_id :  foreign key for the train/test content_id column, when the content type is lecture (1). part : top level category code for the lecture. tag : one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together. type_of : brief description of the core purpose of the lecture example_test_rows.csv Three sample groups of the test set data as it will be delivered by the time-series API. The format is largely the same as train.csv . There are two different columns that mirror what information the AI tutor actually has available at any given time, but with the user interactions grouped together for the sake of API performance rather than strictly showing information for a single user at a time. Some users will appear in the hidden test set that have NOT been presented in the train set , emulating the challenge of quickly adapting to modeling new arrivals to a website. prior_group_responses (string) provides all of the user_answer entries for previous group in a string representation of a list in the first row of the group. All other rows in each group are null. If you are using Python, you will likely want to call eval on the non-null rows. Some rows may be null, or empty lists. prior_group_answers_correct (string) provides all the answered_correctly field for previous group, with the same format and caveats as prior_group_responses . Some rows may be null, or empty lists. Refer to the starter notebook for an example of how to complete a submission. The time-series API has changed somewhat from previous competitions! You should not try to submit anything for the rows that contain lectures. The API provides user interactions groups in the order in which they occurred. Each group will contain interactions from many different users, but no more than one task_container_id of questions from any single user. Each group has between 1 and 1000 users. Expect to see roughly 2.5 million questions in the hidden test set. The API will load up to 1 GB of the test set data in memory after initialization. The initialization step ( env.iter_test() ) will require meaningfully more memory than that; we recommend you do not load your model until after making that call. The API will also consume roughly 15 minutes of runtime for loading and serving the data, but will also obfuscate the true runtime for all submissions. The API loads the data using the types specified above (int32 for user_id , int8 for content_type_id , etc). 7 files 5.85 GB csv, so, py Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 5.85 GB riiideducation example_sample_submission.csv example_test.csv lectures.csv questions.csv train.csv 7 files 33 columns  Too many requests",
    "data_description": "Riiid Answer Correctness Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Riiid AIEd Challenge · Featured Code Competition · 4 years ago Late Submission more_horiz Riiid Answer Correctness Prediction Track knowledge states of 1M+ students in the wild Riiid Answer Correctness Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Oct 5, 2020 Close Jan 8, 2021 Merger & Entry Description link keyboard_arrow_up Riiid AIEd Challenge 2020 Challenge Website Thank you for all those who attended the AAAI-2021 workshop on AI Education! Prize-winning teams presented their models at the AAAI-2021 Workshop on AI Education - Imagining Post-COVID Education with AI - on February 9, 2021. You can find the model write-ups on the workshop website. Think back to your favorite teacher. They motivated and inspired you to learn. And they knew your strengths and weaknesses. The lessons they taught were based on your ability. For example, teachers would make sure you understood algebra before advancing to calculus. Yet, many students don’t have access to personalized learning. In a world full of information, data scientists like you can help. Machine learning can offer a path to success for young people around the world, and you are invited to be part of this mission. In 2018, 260 million children weren't attending school. At the same time, more than half of these young students didn't meet minimum reading and math standards. Education was already in a tough place when COVID-19 forced most countries to temporarily close schools. This further delayed learning opportunities and intellectual development. The equity gaps in every country could grow  wider. We need to re-think the current education system in terms of attendance, engagement, and individualized attention. Riiid Labs, an AI solutions provider delivering creative disruption to the education market, empowers global education players to rethink traditional ways of learning leveraging AI. With a strong belief in equal opportunity in education, Riiid launched an AI tutor based on deep-learning algorithms in 2017 that attracted more than one million South Korean students. This year, the company released EdNet, the world’s largest open database for AI education containing more than 100 million student interactions. In this competition, your challenge is to create algorithms for \"Knowledge Tracing,\" the modeling of student knowledge over time. The goal is to accurately predict how students will perform on future interactions. You will pair your machine learning skills using Riiid’s EdNet data. Your innovative algorithms will help tackle global challenges in education. If successful, it’s possible that any student with an Internet connection can enjoy the benefits of a personalized learning experience, regardless of where they live. With your participation, we can build a better and more equitable model for education in a post-COVID-19 world. Acknowledgements Academic Advisors Paul Kim , Stanford Graduate School of Education Neil Heffernan , WPI & ASSISTments Partners Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File You must make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them. The kernels environment automatically formats and creates your submission files in this competition. There is no need to manually create your submissions. Timeline link keyboard_arrow_up December 31 , 2020 - Entry deadline. You must accept the competition rules before this date in order to compete. December 31, 2020 - Team Merger deadline. This is the last day participants may join or merge teams. January 7, 2021 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - $50,000 2nd Place - $30,000 3rd Place - $10,000 4th Place - $5,000 5th Place - $5,000 Prize-winning teams will be invited to present their models at the AAAI-2021 Workshop on AI Education ( Imagining Post-COVID Education with AI ). Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. Please note that for this competition training is not required in Notebooks. In order for the \"Submit to Competition\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time TPU Notebook <= 3 hours run-time Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv Please see the Code Competition FAQ for more information on how to submit. Citation link keyboard_arrow_up Addison Howard, bskim90, Cheehyun Lee, DongminShin(Min), Hoon Pyo (Tim) Jeon, Jineon (Jin) Baek, Karis Chang, kiyoonkay, NHeffernan, seonwooko, Sohier Dane, and Yohan Lee. Riiid Answer Correctness Prediction. https://kaggle.com/competitions/riiid-test-answer-prediction, 2020. Kaggle. Cite Competition Host Riiid AIEd Challenge Prizes & Awards $100,000 Awards Points & Medals Participation 23,237 Entrants 4,412 Participants 3,406 Teams 64,378 Submissions Tags Education Tabular Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "lyft-motion-prediction-autonomous-vehicles",
    "discussion_links": [
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/201493",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/205376",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/199657",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/199588",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/199649",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/199636",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/199711",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/199541",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/201143",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/199719",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/199755",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/199494",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/200035",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/199540",
      "/competitions/lyft-motion-prediction-autonomous-vehicles/discussion/199617"
    ],
    "discussion_texts": [
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules Pascal Pfeiffer · 1st in this Competition  · Posted 5 years ago arrow_drop_up 82 more_vert 1st Place Solution & L5Kit Speedup 1st Place Solution & L5Kit Speedup Introduction First of all, I would like to thank Lyft for this great challenge and of course for their huge dataset. I must say I was a bit overwhelmed by the sheer amount of agents in the training set, but it was a fun experience to be able to train without ever using training data twice. Well, we did, when we used pre-trained models, but more on that later. I also want to thank my Teammates @philippsinger , @christofhenkel and @nvnnghia for the great collaborative teamwork during the competition. We worked well together with a pipeline consisting of two repositories (version controlled custom l5kit and a training repo) and used logging in neptune.ai to keep track of the experiments. TL;DR Our solution is an ensemble of 4 efficientnet models trained with different rasterization parameters indivdually on train_full.zarr and stacked on validation.zarr. Key to training that many models on this huge dataset were several key improvements to the original l5kit repository to remove CPU bottleneck and enable efficient multi-GPU training using pytorch. Improvements/Changes to the L5Kit One of the major challenges in this competition was the slow rasterizer. We profiled the l5kit a lot and pinpointed the bottlenecks to speed it up by a factor of 4+. After that, our experiments were mostly GPU limited and multi-GPU experiments allowed us for \"fast\" iterations. Running through train_full (191_177_863+ samples) was possible within 2 days with medium sized models. Speedups Speed before changes with semantic view only (i5-3570K single thread, so ignore the actual time): Raster-time per sample: 0.124 s Hits         Time  Per Hit   % Time  Line Contents\n================================================== 9   11147507.0 1238611.9     93.6    rasterizer.rasterize(history_frames, history_agents, history_tl_faces, selected_agent) content_copy All the speedups were achieved with plain python, no C++. The main speedups came from: Batched transformation of boxes with modifications from https://github.com/lyft/l5kit/pull/167 . It has been said that the transformation is the bottleneck in the rasterizer as it is called a lot. We used the vectorized transformation whereever possible. In box_rasterizer.py , using python lists of small numpy arrays instead of one two large numpy arrays ( agent_images, ego_images ). Python lists are surprisingly fast! np.concatenate on large numpy arrays is slow. Pre-allocation and writing to a single large array is even slower. The lists can be cast to a numpy array by out_im = np.asarray(agents_images + ego_images, dtype=np.uint8) This way, boxes will stay in uint8 Moving concatenate to the GPU As np.concatenate is slow on large arrays, we do that on the GPU. Replace \"image\": image, with \"image_box\": image_box , \"image_sat\": image_sat , \"image_sem\": image_sem Speedups from https://github.com/lyft/l5kit/pull/140 With the changes above, they now made a larger impact than the 7-8% stated in the PR. Speed after changes with semantic and satellite view (i5-3570K single thread, so ignore the actual time): Raster-time per sample: 0.032 s Hits         Time  Per Hit   % Time  Line Contents\n================================================== 9    2847054.0 316339.3     84.2  rasterizer.rasterize(history_frames, history_agents, history_tl_faces, selected_agent) content_copy Additions/Changes New rasterizer which combines satellite and semantic view ( py_sem_sat ). Flag: AgentID used as a value for drawing (instead of just boolean no agent = 0 , agent = 1 ) Flag: AgentLabelProba used as a value for drawing (instead of just boolean no agent = 0 , agent = 1 ) Flag: Velocity used as a value for drawing (instead of just boolean no agent = 0 , agent = 1 ) Option to draw all objects on the semantic layer, combinable with the label probas from above. Filter options for agents: th_extent_ratio, th_area, th_distance_av to give a larger training dataset (or harder as you want to phrase it) Multiple parallel rasterizers for ensembling models that use different raster sizes Satellite view fix for non square shapes (had some weird transformation in it before) We will make our L5kit code changes public in the next few days and after cleanup. Most likely with PRs to the original repo. Journey & Experiments With all the speed improvements from our custom L5kit we were able to eliminate the previous CPU bottleneck and run experiments on an efficient multi-GPU setup. We mostly trained on 8xV100 nodes using pytorchs awesome distributed data parallel (DDP), which enables to effectivly scale a training script to multiple GPUs. This efficient training setup was key to run quite a few experiments. (See list of stuff that did not work below) In all our experiments, we used the chopped validation set for local validation as discussed here . This yielded us very good prediction of the LB score (in the sub 13 range, the score was usually ~0.2 higher than local validation with a standard deviation of around 0.3). Originating from the provided baseline and the multi-mode prediction from here , we experimented with raster size, pooling layers, head-modifications, usage of different backbones, learning rate scheduler, subsampling and oversampling to name a few. It was a major challenge in this competition, that experiments had to be run for at least 50 % of a full epoch of train_full.zarr or when comparing different sample sizes or learning rate scheduler even until the end to be really comparable. Subsampling the train set or stepwise decrease of the LR quickly yielded a low training and validation loss, but in the end, the performance was worse. Quite quickly, we decided to always use all data, and settled to a setup using shuffled train_full.zarr (with a fixed seed to be able to resume training) with a linear decay learning rate scheduler. To keep the training sample fixed, we also settled to an AgentDataset with min_frame_history=1 and min_frame_future=10.0 to best resemble the test dataset. Opposed to the many discussions about a much lower training loss, we never experienced that. Without any augmentation, the training loss is of course slightly lower, especially in the end of the epoch, but usually within 80% of the validation score. We found that larger backbones help to reach lower loss levels, but starting at the size of an EfficientNetB7, the nets seem to be overconfident with their predictions and the LB score was sometimes shaky. The first half of test is public, second half is private test. We couldn't identify any major differences in the two sets. With that knowledge it was possible to hide the true score (setting some public rows to zeros), we wonder how many teams did that and were afraid of surprises in the private LB due to this. Best Single Model Our best single model is actually one that not fully finished on the last day. It's just an EfficientNetB3 with a Linear Layer Head and dropout attached trained on an extended train_full.zarr Dataset ( min_frame_history=1, min_frame_future=5.0, th_distance_av=80, th_extent_ratio=1.6, th_area=0.45 ) history_num_frames: 30.0 raster_size: [448, 224] pixel_size: [0.1875, 0.1875] rasterizer: py_sem_sat batch_size: 64 dropout: 0.3 The model was pre-trained for 4 epochs on different lower resolution images (starting with 112, 64 and pixel size 0.75, 0.75) , so you can argue this is also an ensemble of some kind. The 5th epoch was trained with the parameters above and a very customized learning rate schedule that starts with a consine anneal and then transitions to a stepwise lr scheduler as seen in the graph below. The last evaluation score is 9.697. The model achieves a public LB score of 9.776 and a private LB score of 9.070. With that, it would also have ranked 1st place this competition on it's own. Pretraining is VERY important! We discovered it by a mistake where we loaded an EfficientNet without pretrained weights from imagenet. It performed significantly worse by ~1.00. Ensemble Ensembling proved to be challenging in this competition, as traditional methods are not working well with the metric in use. We tried a lot of manual blending and form of post-processing without success. We then started building stacking models taking the raw predictions of the models as input, but this was also surprisingly not working well. At about the same time as @hengck23 posted it here , we had the idea of using the features/embeddings from each model as an input to a ensemble head. This worked much better and we could improve upon our single models specifically as we could introduce some diversity into the blend (different image size, pixel sizes, etc.) We also went with the dropout + single linear layer approach here, and all modification we tried with the head performed worse. To prevent overfitting, the ensembling was done on the chopped validation set (without using the chopped off actual validation part), in some experiments extended to validation + test set without noticeable change in the metric but with about twice the runtime. Ensembles of two similar models (even just two different checkpoints) already gave a boost of about 0.2 in the metric. Naturally, with more diverse models the ensemble performed even better and we were able to achieve our best public LB score of 9.319 and private LB score of 8.579 with an ensemble of 4 models (B3, B5, B6, B6) on two different raster sizes. Bootstrapped Validation We evaluated model and ensemble performance with bootstrap method on the chopped dataset. For more diverse ensembles we also got the lowest standard deviation from the bootstrap method. For our final submission it was 0.22754. The best single models had a standard deviation of ~0.27. We also identified our B7 and B8 models to have a slightly larger standard deviation in the bootstrap method of ~0.36-0.39 which may also explain their partial performance degredations on the public LB. Despite their good validation score, for that reason we excluded them in our final ensembles and final submissions. We plotted CV vs public LB for our submissions and found a spread of +-0.3 for most of our single models (green corridor) with just the B7 and B8 models outside of that corridor (not shown in the picture). Our ensembles are in an even smaller corridor with a spread of +-0.1 (orange corridor), proving their superior robustness and generalization capabilities over single models. Given this robust CV & LB correlation, we were very confident that improvements in validation also lead to improvements on the leaderboard, so we did not have to sub it. So the sudden jump on public leaderboard only happened after we decided to sub our better models and was not some sudden magic we discovered. What Didn't Work Flag: AgentID used as a value for drawing (instead of just boolean no agent = 0 , agent = 1 ) Flag: AgentLabelProba used as a value for drawing (instead of just boolean no agent = 0 , agent = 1 ) Flag: AgentLabelProba used as a value for drawing (instead of just boolean no agent = 0 , agent = 1 ) Option to draw all objects on the semantic layer, combinable with the label probas from above. Kalman filtering the output Any non NN prediction method Prediction of only the difference to a constant velocity model Using additional informations, such as velocity, history positions, label probas or anything else we could find in the head of the models. Heavier heads 3D CNNs RNN based approaches on history frames for backbone input or outbut Using additional features from shallower layers of the backbone Ensembling with kmeans, or other analytic clustering method that we tried. Some success at scores down to around 14, but no improvements below that. Subsampling the training set. Huge speedups, but accuracy was always slightly lower TTA with modified yaw angles Unfreezing the backbone after ensembling and continue training with a second epoch What We Didn't Try Augmentation Multi backbone, multi rasterizer training PyTorch Please sign in to reply to this topic. comment 31 Comments 1 appreciation  comment Hotness nosound Posted 5 years ago · 9th in this Competition arrow_drop_up 10 more_vert Very nice and clean work, kudos. It should be in textbooks on how to approach a DS problem correctly. I am very interested to compare your stacking vs my approach . Can it be possible for you to provide me with submission.csv files for single model predictions of your 4 models? I will make all the results public. Pascal Pfeiffer Topic Author Posted 5 years ago · 1st in this Competition arrow_drop_up 4 more_vert Thank you for the praise. I uploaded 4 submissions that are very close to the checkpoints we have used in the final ensemble. The difference should be marginal. You can find them here Note: first 20 rows in some submissions files are set to zero, so ignore their public LB score. 3 more replies arrow_drop_down Dieter Posted 5 years ago · 1st in this Competition arrow_drop_up 10 more_vert Great team effort :D Pascal Pfeiffer Topic Author Posted 4 years ago · 1st in this Competition arrow_drop_up 4 more_vert As we were asked to share our ensemble model, please see the snippet below: import torch from torch import nn from typing import Dict import timm from timm.models.layers.conv2d_same import Conv2dSame from utils.loss import pytorch_neg_multi_log_likelihood_batch from utils.poolings import GeM class LMM (nn.Module): def __init__ ( self, model_architecture, H= 30 , gem= False ): super ().__init__() self .H = H\n        num_history_channels = ( self .H + 1 ) * 2 rgb_channels = 6 num_in_channels = rgb_channels + num_history_channels self .future_len = 50 num_targets = 2 * self .future_len self .num_modes = 3 self .num_preds = num_targets * self .num_modes self .backbone = timm.create_model(model_architecture, pretrained= False ) if gem: self .backbone.global_pool = GeM() self .backbone.conv_stem = Conv2dSame(\n            num_in_channels, self .backbone.conv_stem.out_channels,\n            kernel_size= self .backbone.conv_stem.kernel_size,\n            stride= self .backbone.conv_stem.stride,\n            padding= self .backbone.conv_stem.padding,\n            bias= False ,\n        ) self .backbone_out_features = self .backbone.classifier.in_features self .backbone.classifier = nn.Sequential(\n            nn.Identity(),\n            nn.Linear(\n                in_features= self .backbone.classifier.in_features,\n                out_features= self .backbone_out_features,\n            ),\n        ) self .lin_head = nn.Sequential(\n            nn.ReLU(),\n            nn.Linear(\n                in_features= self .backbone_out_features,\n                out_features= self .num_preds + self .num_modes,\n            ),\n        ) for param in self .parameters():\n            param.requires_grad = False def forward ( self, image_box, image_sat, image_sem, history, history_availabilities ):\n        x = torch.cat(\n            (\n                image_box[:, : self .H + 1 ]. float (),\n                image_box[:, 31 : 31 + self .H + 1 ]. float (),\n                image_sat,\n                image_sem,\n            ), 1 ,\n        )\n        x = self .backbone(x)\n        x = self .lin_head(x) return x class LyftMultiModel (nn.Module): def __init__ ( self, cfg: Dict , PARAMS, num_modes= 3 ): super ().__init__() self .future_len = cfg[ \"model_params\" ][ \"future_num_frames\" ]\n        num_targets = 2 * self .future_len self .num_preds = num_targets * num_modes self .num_modes = num_modes self .PARAMS = PARAMS self .model1 = LMM( \"tf_efficientnet_b6_ns\" )\n        sd = torch.load( \"output/preds/config_ch_aws_2_checkpoint.pth\" )[ \"model\" ] self .model1.load_state_dict(sd, strict= True ) self .model1.lin_head = nn.Identity() self .model3 = LMM( \"tf_efficientnet_b6_ns\" )\n        sd = torch.load( \"output/preds/config_ch_aws_1_checkpoint.pth\" )[ \"model\" ] self .model3.load_state_dict(sd, strict= True )\n        backbone_out_features = self .model3.backbone_out_features self .model3.lin_head = nn.Identity() self .model4 = LMM( \"tf_efficientnet_b3_ns\" , gem= True )\n        sd = torch.load( \"output/preds/philipp_config_17_ep5_checkpoint_lr5e6.pth\" )[ \"model\" ] self .model4.load_state_dict(sd, strict= True )\n        backbone_out_features_2 = self .model4.backbone_out_features self .model4.lin_head = nn.Identity() self .model5 = LMM( \"tf_efficientnet_b5_ns\" , H= 8 )\n        sd = torch.load( \"output/preds/config_ch_ddp_16_checkpoint.pth\" )[ \"model\" ] self .model5.load_state_dict(sd)\n        backbone_out_features3 = self .model5.backbone_out_features self .model5.lin_head = nn.Identity() self .lin_head = nn.Sequential(\n            nn.ReLU(),\n            nn.Linear(\n                in_features=backbone_out_features * 2 + backbone_out_features_2\n                + backbone_out_features3,\n                out_features= self .num_preds + num_modes,\n            ),\n        ) def forward ( self,\n        image_box,\n        image_sat,\n        image_sem,\n        image_box2,\n        image_sat2,\n        image_sem2,\n        history,\n        history_availabilities, ): self .model1 = self .model1. eval () self .model3 = self .model3. eval () self .model4 = self .model4. eval () self .model5 = self .model5. eval ()\n\n        x1 = self .model1(image_box, image_sat, image_sem, history, history_availabilities)\n        x3 = self .model3(image_box, image_sat, image_sem, history, history_availabilities)\n        x4 = self .model4(image_box2, image_sat2, image_sem2, history, history_availabilities)\n        x5 = self .model5(image_box2, image_sat2, image_sem2, history, history_availabilities)\n\n        x = self .lin_head(torch.cat([x1, x3, x4, x5], dim=- 1 ))\n\n        bs, _ = x.shape\n        pred, confidences = torch.split(x, self .num_preds, dim= 1 )\n        pred = pred.view(bs, self .num_modes, self .future_len, 2 ) assert confidences.shape == (bs, self .num_modes)\n        confidences = torch.softmax(confidences, dim= 1 ) return pred, confidences class LyftMultiRegressor (nn.Module): \"\"\"Single mode prediction\"\"\" def __init__ ( self, predictor, PARAMS ): super ().__init__() self .predictor = predictor self .PARAMS = PARAMS def forward ( self,\n        image_box,\n        image_sat,\n        image_sem,\n        image_box2,\n        image_sat2,\n        image_sem2,\n        targets,\n        target_availabilities,\n        history,\n        history_availabilities,\n        velocity, ):\n        pred, confidences = self .predictor(\n            image_box,\n            image_sat,\n            image_sem,\n            image_box2,\n            image_sat2,\n            image_sem2,\n            history,\n            history_availabilities,\n        ) if self .PARAMS.predict_diffs:\n            pred = torch.cumsum(pred, dim= 2 )\n\n        loss_nll = pytorch_neg_multi_log_likelihood_batch(\n            targets, pred, confidences, target_availabilities\n        ) return loss_nll, pred, confidences content_copy corochann Posted 5 years ago · 4th in this Competition arrow_drop_up 3 more_vert Thank you @ilu000 for the write-up! and congrats for 1st place ( @christofhenkel @philippsinger @nvnnghia ) ! How much the satellite image affect to the score? Did you compare the score difference between py_semantic and your py_sem_sat . I'm guessing that this part has less impact compared to other hyperparameters change, especially filter agent prob or image size/pixel size, min_frame_history to 5 etc. Which one was important to achieve public LB score less than 10? corochann Posted 5 years ago · 4th in this Competition arrow_drop_up 2 more_vert I guess the the main difference with other teams is that you tried more bigger data, more heavy/rich input data training. By setting min_frame_history=5 , or setting image size as raster_size: [448, 224], pixel_size: [0.1875, 0.1875] . These can be done because you improved the training speed a lot! Most of the team cannot come up training this much heavier training due to time limitation. 8 more replies arrow_drop_down Theo Viel Posted 5 years ago arrow_drop_up 4 more_vert That's when I read this kind of write-up that I know I still have a lot of to learn. Congratulations on the win, really impressive performance. Tom Aindow Posted 5 years ago · 8th in this Competition arrow_drop_up 4 more_vert Amazing writeup and congratulations! Heroseo Posted 4 years ago · 42nd in this Competition arrow_drop_up 1 more_vert A little late, congrats on 1st place and successful team effort. @nvnnghia , @philippsinger , @christofhenkel It is one of the best sharing solutions in kaggle. Thanks for sharing and good write-up! Louis Yang Posted 5 years ago · 61st in this Competition arrow_drop_up 1 more_vert Amazing speed up! Amazing work! Very interesting way to do the validation. Never see a competition that has such small noise between CV and LB! For the normal distribution-like plot that you shown, is that the distribution of loss of all the samples in the validation set from a single model? Interesting to see that you don't seem to have many outliers from your model. What was the largest single sample loss from your model? Pascal Pfeiffer Topic Author Posted 5 years ago · 1st in this Competition arrow_drop_up 1 more_vert The distribution is from a bootstrap validation method where we randomly subsample (with multi-draw) 35k rows of the validation set. It's the histogramm over the scores of all subsamples, and not the scores of a single validation set. We did that, to measure the stability of the predictions (chance of shakeup). The largest single sample loss is ~2k as far as i remember. We all saw how skewed the score distribution is ;) Louis Yang Posted 5 years ago · 61st in this Competition arrow_drop_up 0 more_vert I guess I am not very familiar with bootstrap validation. Is it like k-fold validation that you train one model on each subsample datasets? Or, it is just one model but evaluate on each subsample set? I think I saw like ~40k on my one of my ok model. ~2k is still way better than I got :) hengck23 Posted 5 years ago · 21st in this Competition arrow_drop_up 1 more_vert thanks for the writeup and good work! What Didn't Work \"Unfreezing the backbone after ensembling and continue training with a second epoch\" i experience this too. it overfits. however, just unfreeze the layer 4 (last on only) of the embedding backbone is ok for low learning rate. aCode Posted 5 years ago arrow_drop_up 1 more_vert Congratulations, nice team effort. Thanks for sharing 👍 fergusoci Posted 5 years ago · 6th in this Competition arrow_drop_up 2 more_vert Wow. Well done to all of you - clinical execution! KhanhVD Posted 5 years ago arrow_drop_up 2 more_vert Congrats @nvnnghia @ilu000 @christofhenkel and @philippsinger and thanks for sharing details solution! Alin Cijov Posted 5 years ago arrow_drop_up 0 more_vert Very well done, congratulations on the hard work ! Mohammed Rizin V K Posted 5 years ago · 151st in this Competition arrow_drop_up 0 more_vert Amazing solution. is your solution improved better than others ,only by py_sat_sem or because of effnet or anything else. What is your Hardware anyway? This comment has been deleted. Appreciation (1) akshatp Posted 5 years ago arrow_drop_up 0 more_vert Inspiring work! thanks for sharing Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules heartkilla · 3rd in this Competition  · Posted 5 years ago arrow_drop_up 27 more_vert 3rd Place Solution: Baseline + Set Transformer Update: YouTube video about our solution https://youtu.be/3Yz8_x38qbc TL;DR Baseline CNN regression Six 3-mode models based on: Xception41, Xception65, Xception71, EfficientNetB5 One 16-mode Xception41 model Set Transformer as a second level clusterizer model for ensembling Our code is available on GitHub Detailed solution 1. Data preprocessing The first level models used the same raster format as generated by l5kit, with next settings: raster_size=[224, 224] pixel_size=[0.5, 0.5] ego_center=[0.25, 0.5] With history frames rendered at frame offsets 0, 1, 2, 4, 8. We found the significant time with l5kit rasterizer is spent to prepare coordinates for opencv rendering due to the large number of operations on small numpy arrays, so we combined multiple stages like transform points and CV2 shift to single, numba optimized functions. This allowed us to improve performance approximately 1.6x. Another improvement was to uncompress zarr files, it’s especially useful with random access. Next improvement was to save the cached raster and all relevant information for each training sample to numpy compressed npz file. Especially with the full dataset, we saved each N-th frame for training, since the following frames are usually very similar. All optimization combined allowed to improve the CPU load during training around 6x and train multiple models simultaneously. The cached training samples for the full dataset used around 1.3TB of space. Fast nvme SSD drive is useful. 2. First level CNN models We have tried many approaches but could not beat the baseline solution of using the imagenet pretrained CNN, avg pooling or trainable weighted pooling and fully connected layer to directly predict positions and confidences of trajectories. What made a bigger difference was the training parameters. We used SGD with a relatively high learning rate of 0.01, gradient clipping of 2 and batch size around 64-128. We used the modified CosineAnnealingWarmRestarts scheduler, starting from period of 16 epoch (200000 samples each), increasing period by 1.41421 times each cycle. We used the following models: Xception41, avg pool, batch size 64 Xception41, avg pool, batch size 128 - similar performance to batch size of 64 Xception41, learnable weighted pooling Xception41, predicts 16 modes instead of 3 Xception65 Xception71 - seems to be the best performing model, but was trained less comparing to Xception41 due to the shortage of time EfficientNet B5 Initially the EfficientNet based model performed worse compared to Xception41, but with the batch size increased from 64 to 128, the performance improved significantly, with results on par with Xception. Models have been trained on the full training dataset for about 5-7 days on a single GPU (2080ti, 3090 for larger models) each. Training for longer would likely improve the score, for example Xception41 model reached following validation loss during the last 3 cycles: 11.31, 10.86, 10.37 3. Ensembling (second level model) Since we cannot simply average different models predictions, we used an approach where we find 3 trajectories that best approximate input trajectories. This can be achieved by utilizing the competition loss directly, but with input trajectories as ground truth and weighted by input confidences. This can be seen as a particular case of GMM. where n index relates to input trajectories and k index relates to 3 final output trajectories. At first we optimized this loss with a BFGS solver and this already worked pretty well, however, it was very sensitive to the initialization and tended to get stuck in local optima. We tried to use stacking on a hold-out dataset, i.e., train a 2nd level model that takes trajectories from 1st level models as input and predicts 3 completely new trajectories. We compared a bunch of different architectures from MLP to LSTM and luckily came across Set Transformer, which worked almost as well as the BFGS optimizer. Then we noticed that in the paper, they also utilize the model for the GMM task, so we tried to optimize the loss mentioned above with the model instead of stacking, i.e., train it from scratch on the whole val (or test) dataset. It consistently performed by around 0.2 better than the optimizer. We explain such a performance boost by the model's ability to leverage global statistics of the entire provided amount of training data, in contrast to the optimizer, which works sample-wise. Our Set Transformer architecture is pretty simple. As an encoder, we have a 2-layer Transformer without positional encoding. In the decoding stage, we have three so-called \"seed vectors\" that are trainable parameters. Those vectors attend to the encoded representations of the trajectories. All of the input trajectories are predictions from 3-mode models. However, we also had a 16-mode model that boosted the optimizer quite a bit, but didn't help the transformer. So what we did was to add the 16-mode model predictions to the loss, but remove from the input. Other interesting findings and what didn't work for us 1. Experiments with the Set Transformer When experimenting with the Set Transformer on a hold-out train set, we found these interesting results. Method Val nll loss Inference time on the entire val Optimizer 12.06 ~ 30 min Transformer: supervised on train set 12.06 < 1 sec Transformer: unsupervised on train set 12.00 < 1 sec Transformer: unsupervised→supervised fine-tune on train set 11.98 < 1 sec Transformer: unsupervised on val set 11.82 ~ 30-50 min Supervised here denotes stacking (original NLL loss with GT trajectory as a target ), and unsupervised indicates the above-mentioned ensemble loss. In the table, we can see that unsupervised training from scratch on the validation data has the best performance. However, we can also utilize a train set to train a model that generalizes well to the validation set, outperforming the optimizer. Such an approach is speedy (as only a forward is needed to predict for a new observation) and may be more suitable for production purposes. 2. Data preprocessing not used for submitted models We also added extra rasterizer outputs, related to traffic lights: in addition with the current rendering, we added additional 1/4th resolution planes with information about the current and previous traffic lights. We added the separate planes for known on and off traffic lights for forward, left and right directions for the current and previous moments in time. The intuition behind - to provide the model an extra information and separate unknown traffic light from the known off traffic light, has traffic light changed right now or some time ago and easier way to distinguish different signal directions. Lower resolution is sufficient to associate the signal with lanes but can be mixed to deeper model layers, where it does not have to be compressed so much with other inputs. When trained with the extra traffic light inputs, the training and validation loss dropped faster initially but converged to the same level after a few cycles. This approach may still be useful for models used outside of the small training area. 3. What we tried, with performance either on par or worse comparing to the simple baseline Used kalman filter to estimate less noisy initial position, velocity, acceleration and angular rate / turn radius: Slightly faster but less stable training, the same final result. Used transformers to model interaction between agents and map, no improvement Predict the occupation map. May actually be useful for planning, but not very useful for the competition metrics. Use the predicted occupancy map of other agents as an input Predict all agents recurrently, using transformers or CNN Added positions embeddings for points on the map Different heads to predict trajectory, RNN, predict acceleration/turn, separate trajectory parameters from velocity on trajectory etc. Different number of FC (0-3) layers before the final output FC layer Conlusion CNN regression baseline is very hard to beat Training for longer with right parameters on the full dataset was the key l5kit can be optimized by a lot Great competition overall: lots of high quality data, nice optimizable metric, very supportive host Though we thought it would have been better to split train/val/test by geographical location and to increase the train area. Thanks for reading and happy kaggling! Computer Vision Automobiles and Vehicles Transformers CNN Regression Please sign in to reply to this topic. comment 6 Comments Hotness Artem.Sanakoev Posted 4 years ago · 3rd in this Competition arrow_drop_up 3 more_vert Hey guys, I have created a youtube video describing our 3rd place solution in intuitive way which will be very useful for beginners or those who did not take part in this competition but want to learn about it and about our approach. Video link: https://youtu.be/3Yz8_x38qbc nosound Posted 5 years ago · 9th in this Competition arrow_drop_up 1 more_vert Thanks a lot for that summary, a much better read than the news for Sunday morning. Can it be possible for you to provide the single model test csv files, I want to test your ensembling scores vs what I can get with my optimizer. Same as the check that I did for the first place solution. Instead of BFGS, as you did, I used a custom fixed point procedure. It is interesting to know if it finds the global minimum more reliably compared to BFGS, and whether it outperforms the Set Transformer. What experiment tracking system has you used? heartkilla Topic Author Posted 5 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Hi, thanks for the comment. You can download our test predictions in npz format from here . Just ignore the sample submission csv. Our scores with these models are: public - 10.209, private - 9.404. As for the loss plot, it's Tensorboard. nosound Posted 5 years ago · 9th in this Competition arrow_drop_up 1 more_vert Thank you. With equal weights for 7 models I got a significantly worse result. What are the models validation or LB scores, to select the weights more meaningfully? image https://imgur.com/nO7CrT8.png heartkilla Topic Author Posted 5 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Not sure if we've written down the scores somewhere… You can check the LB score by yourself though :) Also note the 16-mode model. It may not fit your approach (haven't read that so not sure). nosound Posted 5 years ago · 9th in this Competition arrow_drop_up 1 more_vert Well, it is too time-consuming. I think I will stop here, thanks for the help. corochann Posted 5 years ago · 4th in this Competition arrow_drop_up 2 more_vert Thanks for sharing and congrats! It's impressive that you won 3rd place using single GPU training! Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules corochann · 4th in this Competition  · Posted 5 years ago arrow_drop_up 57 more_vert 4th place solution: Ensemble with GMM Thank you to the organizers and congratulations to all the participants. Also I would like to thank my team members @zaburo , @qhapaq49 , @charmq for our hard work, I could enjoy the competition! The LB was really stable in this competition due to the big amount of data, we could work on improving the model without caring about big shake-up. We have started with my baseline kernel Lyft: Training with multi-mode confidence . Below items are substantial changes we made: [Update 2020/12/9] We have published our code: https://github.com/pfnet-research/kaggle-lyft-motion-prediction-4th-place-solution Short Summary Published baseline training pipeline was indeed already very strong. Just modifying train_full.zarr l5kit==1.1.0 Set min_history=0, min_future=10 in AgentDataset Cosine annealing for LR decrease until 0 with training 1 epoch was already enough to win the prize. 1. Use train_full.zarr data Bigger data is almost always better for deep learning model training. We used Lyft Full Training Set . However its size is really large, containing 191M data for AgentDataset. Practically we need this modifications in order to train this big dataset in real time: Distributed training We implemented distributed training using torch.distributed . It usually takes about 5 days to finish 1 epoch when we use 8GPUs. Caching some arrays into zarr beforehand to reduce on-memory usage in AgentDataset The problem arises when we run distributed training and DataLoader with setting num_workers for multi-process data loading. In the distributed training, 8 processes run in parallel and each process invokes num_workers subprocess. Therefore 8 * num_workers subprocess is launched and AgentDataset data is copied in each subprocess. Then Out Of Memory error occurs because AgentDataset internally holds cumulative_sizes attribute whose size is very big ( code ). Instead, we pre-calculated track_id, scene_index, state_index and saved as the zarr format. So that we can load each data from disk, and reduce on-memory usage. The Public Score was around 25.742 kernel at this stage. 2. Use l5kit==1.1.0 As mentioned in the discussion We did it all wrong , image is rotated while the target value is not rotated in the previous version of l5kit==1.0.6 during the beginning of the competition. We updated l5kit version to 1.1.0 once it is released, which fixes this behavior. The Public LB score was jumped to 15.874 with this update. 3. Set min_history=0, min_future=10 As written in the “Validation Strategy” section, validation&test data is made by create_chopped_dataset method. We noticed that this validation/test data consists of the data with A. always contains more than 10 future frames, and B sometimes it does not contain any history frame. To align the training AgentDataset to this test dataset behavior, we can set min_frame_history=0 and min_frame_future=10 . I think this modification is the most important part to notice in this competition . You need a courage to intentionally ignore l5kit library warning ;)( code ). It’s very effective, the score jumped to 13.059 . 4. Training: with cosine annealing Model: We trained & used following models for final ensemble Resnet18 Resnet50 SEResNeXt50 ecaresnet18 However resnet18 baseline was strong, and enough to win the prize. Image size: tried (128, 128) and (224, 224). image size of 128 training proceeds faster, but image size 224 final score was slightly better. Optimizer: Adam with Cosine annealing Cosine annealing was better than Exponential decay. I think decreasing the learning rate until very close to 0 is important for final tuning. Batch size: 12 * 8 process = 96 We just trained only 1 epoch to train full data. We did not downsample any of the data. Public LB score for single resnet18 model is 11.341 . Augmentation Image augmentation Many of the augmentation used in natural images is not appropriate for this competition task, (for example flip augmentation flips the target value as well and not realistic since right-lane, left-lane will change). We tried Cutout Blur Downscale using albumentations library. Rasterizer-level augmentation What is different from normal image prediction is that the image is drawn by rasterizer. We can also consider applying augmentation during rasterization. I tried following augmentation by modifying BoxRasterizer ( code ). Drop agent randomly I assumed that the target agent’s movement does not change so much when the other agent far from the target agent exists or not. So we randomly skip drawing some of the agent boxes. Scale extent size randomly Even when the other agent size changes a bit, I assume that the target agent’s behavior does not change. So we scaled extent size from factor 0.9~1.1 I thought rasterizer-level augmentation was an interesting idea for this competition task. However we could not see big score improvement actually. Maybe the training dataset size is already big enough and its effect is not so big. By only adding cutout augmentation , we achieved to get public LB score of 10.846 , already enough to win the prize. Validation Strategy As discussed in Validation vs LB score , we can run very stable validation using a chopped dataset. However, it removes ground truth data, format is different from training AgentDataset and difficult to validate during training phase. What we want is agents_mask_orig_bool ( code ). We saved this agents_mask_orig_bool and set it to the agent_mask argument of AgentDataset . Then we could run validation during training. We sub-sampled 10000 dataset for fast validation during training, but it differs a lot from the score using a total 190327 validation dataset. At the end phase of the competition, we validated the trained model using a full validation dataset. Ensemble: sample trajectory and GMM fitting To improve the score further, how to ensemble is the key question in this task. No golden method is suggested in the discussion and we came up with an idea to adopt the Gaussian Mixture Model . We can sample trajectory and fit the sampled points by GMM with the 3 components. We started from the sklearn implementation . Setting n_components=3 and covariance_type=”spherical” achieved the good score. However sigma is fixed to 1 in this competition metric , so we also tried implementing own GMM model with fixing covariance to be 1. Ensemble by GMM was really effective, we finally achieved public LB score 10.272/private LB score 9.475 Thus, the ensemble pushed the public LB score from 10.846 to 10.272. But it does not change the final rank this time ;) By the way, I saw other participants used k-means clustering for ensembling coords. I think the behavior is quite similar with using GMM, since it calls k-means clustering in the initialization of EM algorithm. What we tried and not worked We noticed Baseline model & l5kit default rasterizer was already very strong in this competition. We really tried a lot, but many of the attempts failed to improve the scores. I’ll write in the reply section (since it’s already very long). Automobiles and Vehicles Image Ensembling CNN PyTorch Please sign in to reply to this topic. comment 26 Comments Hotness corochann Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 5 more_vert We have published our code: https://github.com/pfnet-research/kaggle-lyft-motion-prediction-4th-place-solution Heroseo Posted 5 years ago · 42nd in this Competition arrow_drop_up 1 more_vert Great job. Thanks for sharing! @corochann 3 more replies arrow_drop_down corochann Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 6 more_vert What we tried and not worked Sorry for the late post, these are some of the items that we tried but not worked. Change hyper parameters We tried several hyper parameters, especially for rasterizers. But none of them contributed to improve the model's accuracy. image size We tried 224x224 & 128x128. The image size=128 training is faster especially because Rasterization becomes faster, and its training accuracy is almost the same until the middle of the training. However, its validation loss is a bit (only about 0.5~1.0) worse than image size = 224. pixel_size There are many frames that the car is almost stopping now but starts in the near future. We thought that when the car starts moving, its change in the pixel is very small and CNN cannot detect it when the pixel_size is bit (resolution is rough). We tried to change pixel_size from default 0.5 into 0.25 or 0.15 but the accuracy becomes worse. num_history 1. Only short history predictor: Several agents have very few past history. So I thought when we train the model with only 0 past frames (i.e., input only current frame), this specific purpose model performs better for predicting the future with only 0 past frames. In the training phase, we can train this model using the agents with many past frames, by just input only a current frame. However, this model’s accuracy is worse than the default 10 history input model. 2. Long history predictor: Oppositely, having more past information helps to improve the score? To check that hypothesis, we tried to input longer past history frames by setting history_num_frames=7, 10 with history_step_size=2 instead of default history_num_frames=10 with history_step_size=1 . This model’s accuracy was lower than the original model even if we only chose the validation input to have more than 20 history frames. Big, deep models We tried resnet101 & res152 too, but they did not work well. Custom Rasterizer We tried implementing our own rasterizer to add more rich information to CNN input, but all of them did not work well to improve the accuracy somehow… ChannelSemanticRasterizer SemanticRasterizer draws the semantic in RGB space, using 3 channels. We thought this is not always optimal for CNN input and tried to input 6 channels with 1. road, 2. default lane, 3. green signal lane, 4. yellow signal lane, 5. red signal lane, 6. crosswalk. TLSemanticRasterizer : When we executed EDA, we thought knowing the red signal length is important . Because some frames start with the red signal as current, and start in the future when the signal changed to green. We input a signal length by changing the color value so that CNN can understand how long this signal color already continued (since Host car detected the signal). AgentTypeBoxRasterizer : There are 4 agent types: CAR, CYCLIST, PEDESTRIAN and UNKNOWN in the original dataset. But UNKNOWN is not drawn in the original BoxRasterizer . Also agent type information is also dropped when drawing boxes. We tried to draw each agent type in different channels including UNKNOWN, to input more precise information. Speed up rasterizer Use numpy batch operation as much as possible, and replacing implementations  with numba jit computations. Even though it becomes faster in single process, its computation speed-up does not contribute so much for multi-process data preparation during training. Train with Agent type Other than trying the AgentTypeBoxRasterizer , we tried inputting agent type one-hot vector explicitly, but it did not contribute to improve accuracy too. Multi-agent prediction model The baseline kernel predicts future movement of only target agent. Instead I considered to build a model which predicts all the agent's future movement within the input image. The first I thought this idea speed-ups the training since it can predict multiple agent at once. However sometimes the host car detects agent with very far place, like 400 pixels far away. So we noticed it is difficult to align the image size to fixed size. And we suspended its further trial. Yaw correction The below figure shows the biggest error in the validation dataset. The error is extremely high when the yaw in the dataset was actually opposite and the model predicted the opposite way to go. We worked hard to check if the test dataset contains this kind of case. Indeed there seems to be some frames whose yaw might be opposite, however most of the time the agent is stopping in this case in the test dataset. Even though we fixed the yaw, the LB score was almost unchanged. The biggest error in the validation dataset with error=43988.1 !! Leak check When we checked the dataset carefully, we noticed that the timestamp & the map position actually overlaps within the train/validation/test dataset. We checked if the test dataset information was leaked from another dataset. But it seems that the timestamp is not aligned in scenes (maybe physically other host car is used to collect data, and timestamp record is not calibrated). Louis Yang Posted 5 years ago · 61st in this Competition arrow_drop_up 1 more_vert Thanks for sharing! I think this clear list of things that don't work is way more important than those that work! For Lyft, I think they might want to check the \"Yaw correction\" part. Predicting a car to move in a completely opposite direction will probably be a fatal mistake in real life. Even if it only happen once. corochann Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 1 more_vert Thanks for reply @louis925 , yeah I think so too. For considering application, this \"what did not work\" is more important to consider further why. Actually \"Yaw correction\" happened many times, and I guess to correct it we need better accuracy for the previous object detection NN part where I guess this module will decide the yaw direction. While the pedestrian & cyclist they are more easier to decide direction from real image and indeed there is less mistake for yaw, but car is just \"box\" shape, determining its direction from image sometimes has mistake. Dean Kang Posted 5 years ago · 324th in this Competition arrow_drop_up 3 more_vert Thank you so much for sharing. Just curious, did you use GCP or a local machine for training? Using v100 in GCP seems quite expansive. corochann Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 3 more_vert We used local machine :) Dieter Posted 5 years ago · 1st in this Competition arrow_drop_up 3 more_vert Great write-up, thanks for sharing. I like your GMM ensembling. corochann Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thank you @christofhenkel for comment. I'm looking forward to see your team's solution. I saw that your team used stacking. I also wonder if your team applied GMM ensembling, the score has increased further or not! Dieter Posted 5 years ago · 1st in this Competition arrow_drop_up 1 more_vert thats what I am also wondering. Might give it a try today. Thanks for posting the code Heroseo Posted 5 years ago · 42nd in this Competition arrow_drop_up 3 more_vert Thanks for sharing and well explanation. I totally agree with you! Lyft: Training with multi-mode confidence is really strong baseline. And Congrats 4th @corochann and @zaburo , @qhapaq49 , @charmq . corochann Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thank you! Glad to know that :) @piantic YaGana Sheriff-Hussaini Posted 5 years ago · 6th in this Competition arrow_drop_up 1 more_vert Congrats @corochann and team. Thanks for sharing your solution. corochann Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 1 more_vert Thanks, congrats @sheriytm too! hengck23 Posted 5 years ago · 21st in this Competition arrow_drop_up 1 more_vert good work and thanks for the writeup! \"so we also tried implementing own GMM model with fixing covariance to be 1.\" do you have some code or pseudo-code for that? i would like to implement and try. thanks corochann Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 6 more_vert @hengck23 Thanks for your comment. We used this code, usage is same with sklearn's GMM. import numba as nb import numpy as np from sklearn.mixture import GaussianMixture # from sklearn.mixture._gaussian_mixture import _estimate_gaussian_parameters @nb.jit( nb.types. Tuple ( ( nb.float64[:], nb.float64[:, :] ) )( nb.float64[:, :], nb.float64[:, :] ), nopython= True , nogil= True ) def _estimate_gaussian_parameters ( X, resp ):\n    nk = resp. sum (axis= 0 ) + 10 * np.finfo(resp.dtype).eps\n    means = np.dot(np.ascontiguousarray(resp.T), X) / np.ascontiguousarray(np.expand_dims(nk, 1 )) return nk, means class GaussianMixtureIdentity ( GaussianMixture ): def _initialize ( self, X, resp ):\n        n_samples, _ = X.shape self .covariances_ = np.zeros( self .n_components)+ 1.0 self .precisions_cholesky_ = np.zeros( self .n_components)+ 1.0 weights, means = _estimate_gaussian_parameters(X, resp)\n        weights /= n_samples self .weights_ = (weights if self .weights_init is None else self .weights_init) self .means_ = means if self .means_init is None else self .means_init def _m_step ( self, X, log_resp ):\n        n_samples, _ = X.shape self .covariances_ = np.zeros( self .n_components)+ 1.0 self .precisions_cholesky_ = np.zeros( self .n_components)+ 1.0 self .weights_, self .means_ = _estimate_gaussian_parameters(X, np.exp(log_resp)) self .weights_ /= n_samples content_copy Louis Yang Posted 5 years ago · 61st in this Competition arrow_drop_up 1 more_vert Congrats! Wow! You set min_history=0 ! Very interesting idea! Does that increase the amount samples in AgentDataset by a lot? corochann Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 1 more_vert Yes, the dataset becomes 191,177,863 -> 198,474,478 . I guess min_frame_history=0 increases the number, while min_frame_history=10 reduces the number. nosound Posted 5 years ago · 9th in this Competition arrow_drop_up 1 more_vert Congrats! What hardware have you used to run such a massive experiment? I have trained my models for days and only covered 34M samples, and you did 191M. How many cores have you used, what was the bottleneck? Quite impressive! corochann Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 3 more_vert Thank you and congrats to you too! @zaharch We used 8 V100 GPUs for single model training with about 32 cpu cores. I think disk IO and rasterization process was the bottleneck compared to the normal image training. corochann Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thanks! @prokaggler This comment has been deleted. corochann Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thanks @YoungseokJoung ! Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules fergusoci · 6th in this Competition  · Posted 5 years ago arrow_drop_up 31 more_vert 6th place:  Micro-inputs, Lots of Data + Distance Order to Ensemble! First up: great competition! The data was about as stable as I’ve ever encountered on Kaggle – kudos to the organisers. Congratulations to all of the winners - and to everyone who worked hard and completed the competition. It was a pleasure to work with @rytisva88 and @sheriytm on this – thank you both! I’m sure that my teammates will post separately, so I’ll stick with some observations from my own workflow here. Very interested to hear how everyone else approached the problem, as there was a lot of scope for different approaches! I ended up with two small input sizes in the interest of speed: 128 + 5 channels, 196 + 5 channels. The 128+5 model got to 11.9 after 68.8M samples. The 196+5 model reached 11.15 after 71.4M samples. Reading others’ comments and solutions it seems that some of the items that I thought were key to doing well were not, in fact! I’m sure cherry picking ideas that worked from different teams could lead to something very interesting. Data, data, data: There seemed to be one major key to success in this competition: how much data you could access, and how you sampled it. Early on it became clear that grouping scenes while training gave a significant uplift: when a single agent from each scene was selected before moving to the next iteration, scores improved. Taking this idea and following it through to train_full.zarr gave the next breakthrough: training on a chopped version of train_full.zarr yielded further improvements. The next jump in performance came from training on multiple chops of train_full.zarr. (You can adapt the l5kit code to create lightweight chops comprising just a small number of historic frames. This allowed for big savings on RAM/disk space.) We debated why there were such performance gains from training using chopped versions of train_full.zarr rather than randomly accessing indices. From early experiments it seemed like ensuring scene diversity was important: in the same way that we might enforce class balance while training, enforcing scene diversity (and perhaps more fundamentally, driver diversity?) seemed to matter. Raster size v data coverage tradeoff: Covering as much data as possible mattered, and training for as long as possible mattered. A single chop of train_full.zarr (approx 825K samples) could be shown to the model 12 times before it stopped learning. Obviously if you could show the model different samples you would do a lot better! But this seemed to be the hard limit. Keeping the model as small as possible meant it could iterate through these samples much more quickly. The inputs for the models that I ended up using were raster size 128 and 196, history_num_frames = 5, condensed into five input channels: sum(agent_history), agent_current, sum(ego_history), ego_current, sum(semantic_map) I was originally using Resnet18, but then switched to Resnest50 on @rytisva88 's recommendation and it gave an improvement of about -1 in nll. (Incidentally, @rytisva88 had the best performing single model in our group). Summing the semantic map meant that red and green traffic signals were treated the same. This seemed to work fine, surprisingly. Perhaps because only yellow gave additional information not already contained in the traffic movement. Acceleration: Eyeballing the predicted trajectories, it became clear that the models were ultimately making a bet on acceleration: typically, modes 0, 1, 2 represented trajectories arising from different agent speeds. Once it became clear that this was key it was possible to look at sampling the data such that we balanced these cases. A scene containing lots of agents was indicative of traffic. Gridlocked traffic obviously does not move much and leads to a lot of duplication in inputs. The models implemented a sampling scheme whereby scenes with a large number of agents were undersampled. The sampling proportion was: min(1, 7/agent_count). Thus, scenes containing 14 agents had 50% of those agents selected for each training iteration, etc. Ensembling: This was initially a tricky one: averaging models based on confidence values didn’t work. However, when the importance of acceleration was taken into account, the route to ensembling made sense: order the model modes by distance covered, then average the results. When this was implemented all models could be ensembled very quickly, with positive results. We also looked at incorporating curvature here, but it didn’t make any difference: distance was the key. The final ensemble optimized weights on the validation set. The weighting scheme incorporated distance and confidence values. Code: Code for contribution to our team solution can be found here Ideas that didn’t work: many! Here are a few… Traffic lights: I couldn’t get additional traffic light information to add anything: many different angles were tried! The most promising was probably traffic light persistence: we included an additional channel in the model where instead of traffic light lane lines, we drew lines containing the number of frames since the traffic light had turned to its current colour (up to a maximum of 100). Ultimately this didn’t add anything. Day/hour: Adding channels for day/hour values proved better than concatenating them directly before the dense layers of the model, but it still didn’t help much. Interpolation: Having the penultimate model layer output 25 sets of (x, y) points, followed by an interpolation between these points to make the final 50 point trajectory did not work. Weighted loss function: Given the importance of capturing acceleration, we tried a version of the loss function that weighted  the last 10 points of the trajectory equal to the first 40. This didn’t help. Please sign in to reply to this topic. comment 18 Comments Hotness corochann Posted 5 years ago · 4th in this Competition arrow_drop_up 1 more_vert Thanks for sharing detailed write-up @fergusoci , congrats! Ensemble method is interesting. datajameson Posted 5 years ago arrow_drop_up 1 more_vert Thanks for sharing @fergusoci ! Very Insightful 👍 عثمان Posted 5 years ago · 156th in this Competition arrow_drop_up 1 more_vert Very thorough and impressive. Chung-Hsien Tsai Posted 5 years ago · 25th in this Competition arrow_drop_up 1 more_vert Congratulations! Could I ask that how much improvement you got using the ensembling scheme you has mentioned? fergusoci Topic Author Posted 5 years ago · 6th in this Competition arrow_drop_up 1 more_vert The top score was an ensemble of the two best models (slightly lower score was an ensemble of four). For that one, 11.15 + 10.52 -> 10.32 Mohammed Rizin V K Posted 5 years ago · 151st in this Competition arrow_drop_up 1 more_vert WHat was your hardware fergusoci Topic Author Posted 5 years ago · 6th in this Competition arrow_drop_up 2 more_vert 3x1080ti. 128GB RAM, 10 cores. Could have done with more!! Dmytro Poplavskiy Posted 5 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Great result with 3 1080ti ! Heroseo Posted 5 years ago · 42nd in this Competition arrow_drop_up 0 more_vert Wow. Great job and Thanks for sharing YaGana Sheriff-Hussaini Posted 5 years ago · 6th in this Competition arrow_drop_up 2 more_vert Exhausted from the competition, my day job and Thanksgiving holiday stuff this had to wait and I apologize for the delayed post. My first thanks is to the competition organizers and the Kaggle team for this really interesting contest. Congratulations @philippsinger , @christofhenkel , @ilu000 , @nvnnghia a.k.a.  team NIPD for a well deserved win. @fergusoci have pretty much presented the team efforts so well I am just going to add a few points here instead of a separate post. First I want to thank my team mates @fergusoci and @rytisva88 for making this competition experience a pleasure. Since much far useful solutions from the top teams are already posted, I will give a brief on some of my training procedures:- I have limited GPU resource hence could not use train_full.zarr before teaming up. The GCP credit was unusable as GPU is not available where I am on my travels at this time. My best model before merger was a Resnet34 (224+5) trained on train.zarr to a reasonable LB score of 21.403 before it started to overfit. Training this model on the chopped data made improvements. Once we merged looking at all the teams models and various efforts after brain storming, I turned my attention to training a different type of model to add diversity. After a few (i.e. PointNet, EfficientNet, etc) trials, settled on training a Resnext50_32x4d (224+5) with train.zarr and chopped train_full.zarr to help add diversity to our ensemble. Basically, after training on 2.5M samples of train.zarr, I alternated by training between train.zarr and chopped 100 train_full.zarr. Each cycle trains on 256000 samples of the data. That made the scores go down significantly but got slower after reaching the sub 20 LB scores. At this point, I continued training on the chopped data only, which reached sub 14 scores after going through the data 2.5 times at competition close. I have continued training this model to see if it can get to the scores of our best models and make a late submission. And thanks everyone who contributed in many ways via kernels and discussions. Pascal Pfeiffer Posted 5 years ago · 1st in this Competition arrow_drop_up 2 more_vert Thanks for the writeup! And congratulations! Some smart ideas you had like using the daytime in the model and the summing trick I see that you experienced large performance gains while using a subsampled train_full over a random sampled train_full. Have you ever trained the random shuffled train_full for a complete epoch? Was the performance still lower? It makes sense, that subsampling most diverse samples can get the validation score down quicker, but I would be very interested if it actually makes a difference how you shuffle after a full epoch. Why did you use the same subsample again for a second epoch? Why not e.g. epoch0_frame +1? fergusoci Topic Author Posted 5 years ago · 6th in this Competition arrow_drop_up 1 more_vert Thanks! Same to you! No, I never trained train_full for a complete epoch, I think I'd be here until Christmas if I did! @rytisva88 had been training on a random shuffle of train_full before switching to the chopped version, so he will have a better gauge on what the impact was. @sheriytm also went from using train.zarr to a chop of train_full.zarr so will have an idea of how much that was worth. We ended up reusing chopped datasets (i.e. showing the same sample multiple times). It would have been better if we didn't, but we were running out of time and were resource constrained. If I were starting over I would have done it differently: rather than creating complete chops I would just store off the indices in train_full.zarr that those chops refer to, thereby allowing us to create sequences of diverse samples without having to show samples multiple times or run out of resources. @rytisva88 managed resources better and he was able to use 20 chopped datasets (but could have gone higher). I was stuck using 16. YaGana Sheriff-Hussaini Posted 5 years ago · 6th in this Competition arrow_drop_up 0 more_vert Have you ever trained the random shuffled train_full for a complete epoch? Was the performance still lower? No @ilu000 , in my case that was not an option due GPU challenged. See my post in the comment section here for how I went about training my models. Tom Aindow Posted 5 years ago · 8th in this Competition arrow_drop_up 2 more_vert Great writeup, look forward to other members responses. I also chopped train_full, initially at frame 100 then slowly expanded to include additional frames, and saw consistent improvement. After chopping did you continue to use rasterization on the fly while training or did you store rasters on disk? How long did training take and what hardware did you use? Congratulations :) Mohammed Rizin V K Posted 5 years ago · 151st in this Competition arrow_drop_up 0 more_vert Here they mention about their hardware fergusoci Topic Author Posted 5 years ago · 6th in this Competition arrow_drop_up 1 more_vert Yes, I kept the rasterization throughout as I was so often playing around with different configs that it didn't make sense to freeze it. Also, I wasn't sure whether I'd have the disk space… ryches Posted 5 years ago · 35th in this Competition arrow_drop_up 0 more_vert Did you ever try something like applying k-means to ensemble your trajectories? In theory similar to your distance covered ensembling, but potentially more automatic and you can also apply sample weights directly in sklearn based on the confidence output. fergusoci Topic Author Posted 5 years ago · 6th in this Competition arrow_drop_up 0 more_vert I did try kmeans early on, but it wasn't working for me, perhaps because of how I implemented it: if there were three models, say, I was clustering 9 modes into three clusters. Then taking the max conf mode for each cluster or averaging within each cluster (I tried both). But obviously had something wrong somewhere… Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules beluga · 7th in this Competition  · Posted 5 years ago arrow_drop_up 36 more_vert 7th place solution - Peter & Beluga Acknowledgements Thanks for the organizers for this tough but certainly interesting challenge. Special thanks for Vladimir Iglovikov and Luca Bergamini for their active forum contribution during the competition. Hats off to my team mate @pestipeti by the time I joined him he already had optimized the hell out of l5kit and had a solid training framework. Then he managed to boost the training speed even further by rasterizing the images on GPU . With all those improvements we were able to run dozens of experiments with different config parameters and slightly modified encodings during the last months. Back to the future We noticed that the training dataset and chopped validation set had slightly different feature distributions. After some digging we found that the chopped datasets (valid, test) always had availability for at least 10 future frames. It was quite the opposite than the default AgentDataset settings so we used that for training too. AgentDataset(\n    cfg, dataset_zarr, gpu_rasterizer, agents_mask=dataset_mask,\n    min_frame_history= 1 , min_frame_future= 10 ) content_copy It helped both in terms of validation consistency and final score. Poor Man's Ensembling We did not hope that blending or any simple heuristic would help to combine different models. (I read clever tricks though and I hear that stacking works too…) The speed of the agent matters a lot and we saw that in our experiments. Intuitively for slower objects we used smaller raster size but more history. Our final and best submission used three models based on speed (Total distance in the last 1 sec) [0-2] Slow model 320x220 with 3 sec history (compressed by 1.5 s) trained for 7+ days on slower examples [2-5] Medium model 320x220 with 3 sec history (compressed by 1.5 s) trained for 5+ days [1.5-12] [5+] Fast model 480x320 with 1 sec history on separate channels trained for 9+ days on [2+] Things that did not work We tried to use additional meta data (speed, acceleration, position, hour of the day, day of the week etc.) but it did not really help. We did not use satellite images at all. We noticed that they could have additional info (especially for pedestrians or cyclists) but it would be too slow. Different backbone. We tried a few other networks but mostly used Effnet-B2. Please sign in to reply to this topic. comment 5 Comments Hotness Louis Yang Posted 5 years ago · 61st in this Competition arrow_drop_up 3 more_vert I wonder why adding meta data like speed, or acceleration don't work. Yassine Alouini Posted 5 years ago arrow_drop_up 1 more_vert Well done! The min_frame_future trick is clever and more generally something I see done in a lot of Kaggle competitions, i.e. making the distributions of train and test as close as possible. Also, I better understand your comment on my discussion: these are very long training times indeed so better not forget to save the model. ;) Many thanks for sharing and congratulations again! Bessenyei Szilárd Posted 5 years ago · 87th in this Competition arrow_drop_up 1 more_vert Thank you for the writeup, and congrats for the 7th place! corochann Posted 5 years ago · 4th in this Competition arrow_drop_up 2 more_vert Thank you for sharing and congrats @gaborfodor @pestipeti ! I guess many of the participants were interested in GPU rasterizer work during competitioin. olivier Posted 5 years ago · 29th in this Competition arrow_drop_up 2 more_vert Thanks for sharing and congratulations @pestipeti and @beluga ! Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Build motion prediction models for self-driving vehicles The Lyft Motion Prediction for Autonomous Vehicles competition is fairly unique, data-wise. In it, a very large amount of data is provided, which can be used in many different ways. Reading the data is also complex - please refer to Lyft's L5Kit module and sample notebooks to properly load the data and use it for training. Further Kaggle-specific sample notebooks will follow shortly. Note also that this competition requires that submissions be made from kernels, and that internet must be turned off in your submission kernels. For your convenience, Lyft's l5kit module is provided via a utility script called kaggle_l5kit . Just attach it to your kernel, and the latest version of l5kit and all dependencies will be available. You can compete with just train.zarr and test.zarr , the other files are optional but will likely be helpful. Please refer to the sample notebooks for help on how to load and iterate over the datasets. Note: for full details, please refer to the data format page in L5Kit The data is packaged in .zarr files. These are loaded using the zarr Python module, and are also loaded natively by l5kit . Each .zarr file contains a set of: We are predicting the motion of the objects in a given scene. For test, you will have 99 frames of objects moving around will be asked to predict their location in the next 50. 108580 files 23.71 GB 0, tfw, tif + 10 others Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 23.71 GB aerial_map scenes semantic_map meta.json multi_mode_sample_submission.csv single_mode_sample_submission.csv 109k files 610 columns  Too many requests",
    "data_description": "Lyft Motion Prediction for Autonomous Vehicles | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Lyft · Featured Code Competition · 5 years ago Late Submission more_horiz Lyft Motion Prediction for Autonomous Vehicles Build motion prediction models for self-driving vehicles Lyft Motion Prediction for Autonomous Vehicles Overview Data Code Models Discussion Leaderboard Rules Overview Start Aug 24, 2020 Close Nov 26, 2020 Merger & Entry Description link keyboard_arrow_up Autonomous vehicles (AVs) are expected to dramatically redefine the future of transportation. However, there are still significant engineering challenges to be solved before one can fully realize the benefits of self-driving cars. One such challenge is building models that reliably predict the movement of traffic agents around the AV, such as cars, cyclists, and pedestrians. The ridesharing company Lyft started Level 5 to take on the self-driving challenge and build a full self-driving system ( they’re hiring! ). Their previous competition tasked participants with identifying 3D objects, an important step prior to detecting their movement. Now, they’re challenging you to predict the motion of these traffic agents. In this competition, you’ll apply your data science skills to build motion prediction models for self-driving vehicles. You'll have access to the largest Prediction Dataset ever released to train and test your models. Your knowledge of machine learning will then be required to predict how cars, cyclists,and pedestrians move in the AV's environment. Lyft’s mission is to improve people’s lives with the world’s best transportation. They believe in a future where self-driving cars make transportation safer, environment-friendly and more accessible for everyone. Their goal is to accelerate development across the industry by sharing data with researchers. As a result of your participation, you can have a hand in propelling the industry forward and helping people around the world benefit from self-driving cars sooner. This is a Code Competition. Refer to Code Requirements for details. Evaluation link keyboard_arrow_up The goal of this competition is to predict the trajectories of other traffic participants. You can employ uni-modal models yielding a single prediction per sample, or multi-modal ones generating multiple hypotheses (up to 3) - further described by a confidence vector. Due to the high amount of multi-modality and ambiguity in traffic scenes, the used evaluation metric to score this competition is tailored to account for multiple predictions. Note: The following is a brief excerpt of our metrics page in the L5Kit repository We calculate the negative log-likelihood of the ground truth data given the multi-modal predictions. Let us take a closer look at this. Assume, ground truth positions of a sample trajectory are and we predict K hypotheses, represented by means In addition, we predict confidences c of these K hypotheses. We assume the ground truth positions to be modeled by a mixture of multi-dimensional independent Normal distributions over time, yielding the likelihood which results in the loss Submission File Note: if you're using L5Kit , we provide a function to directly convert your predictions (single and multi-modal) into a valid CSV. Every agent is identified by its track_id and its timestamp .  Each trajectory holds 50 2D (X,Y) predictions. You can predict up to 3 trajectories for each agent in the test set. Because the format is a CSV file, all 3 trajectories fields must have a value, even if your prediction is single-modal. However, each one of the three trajectory has its own confidence, and you can set it 0 to completely ignore one or more trajectories during evaluation. The 3 confidences must sum to 1. An example of a valid CSV header: timestamp, track_id, co nf_0 , co nf_1 , co nf_2 , coord_x 00 , coord_y_ 00 ,...,coord_x 049 , coord_y_ 049 , coord_x 10 , coord_y_ 10 ,...,coord_x 149 , coord_y_ 149 , coord_x 20 , coord_y_ 20 ,...,coord_x 249 , coord_y_ 249 content_copy Timeline link keyboard_arrow_up November 18, 2020 - Entry deadline. You must accept the competition rules before this date in order to compete. November 18, 2020 - Team Merger deadline. This is the last day participants may join or merge teams. November 25, 2020 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up Cash Prizes Participants with the best score on the private leaderboard are eligible to receive 1st Place - $12,000 2nd Place - $8,000 3rd Place - $6,000 4th Place - $4,000 Additional Opportunity Beat the benchmark and you can receive $300 in GCP credits! Competitors who score higher than the host benchmark (when available) can fill out this survey form and receive a GCP coupon code. Request deadline is October 30, 2020. Limited coupons available, one coupon per user. Code Requirements link keyboard_arrow_up This is a Code Competition Submissions to this competition must be made through Notebooks. Please note that for this competition training is not required in Notebooks. In order for the \"Submit to Competition\" button to be active after a commit, the following conditions must be met: CPU Notebook <= 9 hours run-time GPU Notebook <= 9 hours run-time TPU Notebook <= 3 hours run-time Freely & publicly available external data is allowed, including pre-trained models Submission file must be named submission.csv Please see the Code Competition FAQ for more information on how to submit. Citation link keyboard_arrow_up Amy Jang, Christy, Luca Bergamini, Maggie, Oliver Scheel, Peter Ondruska, Phil Culliton, and Vladimir Iglovikov. Lyft Motion Prediction for Autonomous Vehicles. https://kaggle.com/competitions/lyft-motion-prediction-autonomous-vehicles, 2020. Kaggle. Cite Competition Host Lyft Prizes & Awards $30,000 Awards Points & Medals Participation 8,424 Entrants 1,254 Participants 935 Teams 14,900 Submissions Tags Automobiles and Vehicles Tabular Image Transportation Custom Metric Table of Contents collapse_all Description Evaluation Timeline Prizes Code Requirements Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "google-cloud-ncaa-march-madness-2020-division-1-mens-tournament",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Apply Machine Learning to NCAA® March Madness® Each season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness®, the 68-team national championship that starts in the middle of March.  We have provided a large amount of historical data about college basketball games and teams, going back many years.  Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness® game outcomes.  You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the NCAA® tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way).  Nevertheless, you may also be able to make productive use of external data.  If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data.  The MTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure.  You will probably also need to understand exactly how dates work in our data.  Remember as well that you are required to disclose your external sources of data prior to the start of the tournament Please also note that we have standardized the spelling of column names and filenames, so if you are re-using code from previous instances of this contest, you may need to adjust for this.  All of the files that are specific to the men’s contest now have a filename prefix of M, so for instance this year the teams file is named MTeams rather than just Teams. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA® tournaments (seasons 2015-2019). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2020 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in early March while Stage 1 of the competition is running.  Many of the files are only complete through the end of last season.  At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. Right now the MSeasons and MTeamConferences files are the only two files that reference the 2020 season, since those are the only files where the final 2020 data is already finalized; the other files will be updated later on when we provide preliminary data (in early March) and final Stage 2 data (after Selection Sunday) for the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March.  For instance, this year the first men’s Division I games were played on November 5th, 2019 and the men’s national championship game will be played on April 6th, 2020.  Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season.  By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in.  So for instance, the current season will be identified in our data as the 2020 season, not the 2019 season or the 2019-20 season or the 2019-2020 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: MTeams.csv This file identifies the different college teams present in the dataset.  Each school is uniquely identified by a 4 digit id number.  You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams.  There are 353 teams currently in Division-I, and an overall total of 367 teams in our team listing.  Each year, some teams might start being Division-I programs, and others might stop being Division-I programs, and in fact this year is no exception.  Savannah State (TeamID=1366) is no longer part of Division I and so their data in our dataset stops after the 2019 season, and they are no longer part of the Mid-Eastern Athletic Conference.  To balance this out, Merrimack (TeamID=1467) has just moved to Division I this year, so they show up as a new team in the data and they are now part of the America East Conference.  Other than the teams file and team conferences file, you won't see any data for Merrimack until preliminary 2020 season data starts coming in, as we approach Stage 2 of the contest. Data Section 1 file: MSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. Data Section 1 file: MNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA® tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were.  In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week (by definition, that is DayNum=136 each season).  We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 15, 2020 (DayNum=132). Data Section 1 file: MRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA® had a 64-team tournament). For each season, the file includes all games played from DayNum 0 through 132.  It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself).  Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: MNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA® tournament results for all seasons of historical data. The data is formatted exactly like the MRegularSeasonCompactResults data. All games will show up as neutral site (so WLoc is always N). Note that this tournament game data also includes the play-in games (which always occurred on day 134/135) for those years that had play-in games.  Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were. Because of the consistent structure of the NCAA® tournament schedule, you can actually tell what round a game was, depending on the exact DayNum.  Thus: Special note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA® Tournament.  This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16.  Such games are not listed in the Regular Season or the NCAA® Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6.  Although they would not be games you would ever be predicting directly for the NCAA® tournament, and they would not be games you would have data from at the time of predicting NCAA® tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology.  The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are.  A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as reasonable proxies for NCAA® tournament games.  Data Section 1 file: MSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1.  It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years.  During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA® tournaments (seasons 2015, 2016, 2017, 2018, 2019).  In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA® tournament (season 2020). When there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*5=11,390 data rows. Example #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2017 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win.  In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2017_1112_1181,0.47 Example #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2018 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win.  In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2018_1181_1314,0.516 Also note that a single prediction row serves as a prediction for each of the two teams' winning chances.  So for instance, in Example #1, the submission row of \"2017_1112_1181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win.  However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke. This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA® tournament games since the 2002-03 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files.  However, the two files are strongly related. In a Detailed Results file, the first eight columns ( Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT ) are exactly the same as a Compact Results file.  However, in a Detailed Results file, there are many additional columns.  The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF ). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals.  And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course.  So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3).  And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: MRegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season.  All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file. Data Section 2 file: MNCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA® tournaments, starting with the 2003 season.  All games listed in the MNCAATourneyCompactResults file since the 2003 season should exactly be present in the MNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA® tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests.  Also note that if you created any supplemental data last year on cities (latitude/longitude, altitude, etc.), the CityID's match between last year and this year, so you should be able to re-use that information. Data Section 3 file: MGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in.  Games from the regular season, the NCAA® tourney, and other post-season tournaments, are all listed together.  There should be no games since the 2010 season where the CityID is not known.  Games from the 2009 season and before are not listed in this file. This section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season Data Section 4 file: MMasseyOrdinals.csv This file lists out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies.  The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page . Note that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams.  A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small.  Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams. Disclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems.  They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page.  However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday.  You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings.  To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday.  For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum.  By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday.  If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline.  In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page.  We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away. This section provides play-by-play event logs for more than 99.5% of each year's regular season, NCAA® tournament, and secondary tournament games since the 2014-15 season - including plays by individual players. This year we are transitioning to a different play-by-play source, which includes data since the 2014-2015 season rather than since the 2009-2010 season (that's what we had previously for men's data).  However, we are now able to provide play-by-play for both men's and women's data, and there is locational play-by-play detail starting with games from the 2018-2019 season.  This includes an X/Y location (ranging from 0 to 100 in each dimension) on the court for each shot attempt, turnover, and foul for many games, as well as an overall categorization of the area on the court that the shot or turnover or foul occurred in (inside left wing, outside right wing, under the basket, etc.)  Some games in these recent seasons still lack the locational detail.  The data from last year (2019 season) matches what you can expect for the current year (2020 season) as we approach the postseason.  Despite the 99.5% coverage, there are still a few games missing annually, and we will try to bring those in as well, if possible. Data Section 5 file: MEvents2015.csv, MEvents2016.csv, MEvent2017.csv, MEvents2018.csv, MEvents2019.csv Each MEvents file lists the play-by-play event logs for more than 99.5% of games from that season.  Each event is assigned to either a team or a single one of the team's players.  Thus if a basket is made by one player and an assist is credited to a second player, that would show up as two separate records.  The players are listed by PlayerID within the MPlayers.csv file. Event Types and Subtypes:   This is the diagram provided by the play-by-play source:    Here is a diagram of these regions on a typical court:  And you may also have noticed that the three-point-arc is different this year in college basketball.  The width of the court is still 15.2 m (50 ft) and the length of the court is still 28.7 m (94 feet), but the distance to the arc has changed from 6.32 m (20 feet 9 inches) to 6.75 m (22.15 feet). Here is a diagram of the old court, which would apply for play-by-play in seasons 2019 and before:  And here is a diagram of the new court, which only applies for season 2020:  Data Section 5 file: MPlayers.csv Note: there are data collection errors within the events, in that they don't necessarily add up to the final stats for the game.  In addition, the player name spellings may vary over the course of a season or career for the same player, which would lead to the same player showing up with different PlayerID values.  Nevertheless, this was the highest quality data we could manage for play-by-play with the near-complete set of games. This section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments. Data Section 6 file: MTeamCoaches.csv This file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change.  For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season.  For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season. Data Section 6 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985.  Each conference is listed with an abbreviation and a longer name.  There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time.  Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. Data Section 6 file: MTeamConferences.csv This file indicates the conference affiliations for each team during each season.  Some conferences have changed their names from year to year, and/or changed which teams are part of the conference.  This file tracks this information historically. Data Section 6 file: MConferenceTourneyGames.csv This file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season.  Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA® tournament.  Thus these games could be considered as very similar to NCAA® tournament games, and (depending on your methodology) may be of use in optimizing your predictions.  However, this is NOT a new listing of games; these games are already present within the MRegularSeasonCompactResults and MRegularSeasonDetailedResults files.  So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information. Data Section 6 file: MSecondaryTourneyTeams.csv This file identifies the teams that participated in post-season tournaments other than the NCAA® Tournament (such events would run in parallel with the NCAA® Tournament).  These are teams that were not invited to the NCAA® Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years.  Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA® Tournament results.  Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA® Tournament invitees, and so these games may be of use in model optimization for predicting NCAA® Tournament results.  Also note that this information could be determined just from inspecting the MSecondaryTourneyCompactResults file, but is presented in this file as well, for your convenience. Data Section 6 file: MSecondaryTourneyCompactResults.csv This file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16.  The detailed results (team box scores) have not been assembled for these games.  For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the MRegularSeasonCompactResults file. Data Section 6 file: MTeamSpellings.csv This file indicates alternative spellings of many team names.  It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets.  Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\").  Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school.  The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 6 file: MNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below).  Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year.  You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Data Section 6 file: MNCAATourneySeedRoundSlots.csv This file helps to represent the bracket structure in any given year.  No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round.  Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game.  This can be useful in representing or simulating the tournament bracket structure. 52 files 2.19 GB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 2.19 GB MDataFiles_Stage1 MDataFiles_Stage2 MPlayByPlay_Stage2 MEvents2015.csv MEvents2016.csv MEvents2017.csv MEvents2018.csv MEvents2019.csv MPlayers.csv MSampleSubmissionStage1_2020.csv 52 files 495 columns  Too many requests",
    "data_description": "Google Cloud & NCAA® ML Competition 2020-NCAAM | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Google Cloud · Featured Prediction Competition · 5 years ago Late Submission more_horiz Google Cloud & NCAA® ML Competition 2020-NCAAM Apply Machine Learning to NCAA® March Madness® Google Cloud & NCAA® ML Competition 2020-NCAAM Overview Data Code Models Discussion Leaderboard Rules Overview Start Feb 15, 2020 Close Mar 19, 2020 Merger & Entry Description link keyboard_arrow_up Update: this competition has been cancelled on account of the COVID-19 pandemic. As a result of the continued collaboration between Google Cloud and the NCAA®, the seventh annual Kaggle-backed March Madness® competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television. In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset . In the second stage, competitors will forecast outcomes of all possible matchups in the 2020 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2020 results. As the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on! This page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here .  If you want to extend your analysis then try out our Analytics Competition here Evaluation link keyboard_arrow_up Submissions are scored on the log loss: LogLoss = − 1 n n ∑ i = 1 [ y i log ( ˆ y i ) + ( 1 − y i ) log ( 1 − ˆ y i ) ] , where n is the number of games played \\\\( \\hat{y}_i \\\\) is the predicted probability of team 1 beating team 2 \\\\( y_i \\\\) is 1 if team 1 wins, 0 if team 2 wins \\\\( log() \\\\) is the natural (base e) logarithm The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value. Submission File The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2020 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2  = 2,278 matchups. Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, \"2015_1107_1112\" indicates team 1107 potentially played team 1112 in the year 2015. You must predict the probability that the team with the lower id beats the team with the higher id. The resulting submission format looks like the following, where \"pred\" represents the predicted probability that the first team will win: id,pred 2015_1107_1112,0.5 2015_1107_1116,0.5 2015_1107_1124,0.5 ... Timeline link keyboard_arrow_up Updated March 13, 2020 : The following supersedes the respective terms outlined below.  As a result of COVID-19, this competition will be cancelled with no Kaggle points, Kaggle medals, or monetary prizes. The end date has been updated from April 7, 2020 at 11:59pm UTC to March 19, 2020 at 3:00 am UTC. Please see this announcement post for more details. Stage 1 - Model Building Saturday, Mar 14 - Prior to this deadline, competitors build and test models on historical data. The leaderboard shows the model performance on historical tournament outcomes. Stage 2 - Championship Sunday, Mar 15 - Selection Sunday® (68 teams announced) Monday, Mar 16 - Kaggle begins to accept 2020 predictions. Release of up-to-date 2019-2020 season data. Thursday, Mar 19 3PM UTC - Final deadline to submit 2020 predictions. Mar 19 onward - Watch your tournament results play out! The Kaggle Team will refresh the leaderboard throughout the competition as games are finalized. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up Updated March 13, 2020 : The following supersedes the respective terms outlined below.  As a result of COVID-19, this competition will be cancelled with no Kaggle points, Kaggle medals, or monetary prizes. The end date has been updated from April 7, 2020 at 11:59pm UTC to March 19, 2020 at 3:00 am UTC. Please see this announcement post for more details. 1st Place - $10,000 2nd Place - $7,000 3rd Place - $5,000 4th Place - $2,000 5th Place - $1,000 Stage 1 will not count towards Kaggle rankings/points. Stage 2 will count toward Kaggle rankings/points. FAQs link keyboard_arrow_up Is one tournament enough to decide the best basketball algorithm? It's better to be lucky than good! Besides, when was the last time your office held a cross validation pool? Can I update my predictions after the tournament starts? No changes are permitted once the tournament begins. How is the leaderboard going to work when the event being scored hasn't yet happened? You won't submit after the tournament starts. We'll update the solution file as the games occur, which will cause the ranks on the leaderboard to change. Why do we have to predict every match up? Why 68 teams and not 64? This was done for timing purposes. Predicting every possible matchup for the 68 teams announced on Selection Sunday gives participants the most time to get their current year predictions ready in time. There is a small \"play-in\" round, sometimes called the first round, where the 68 are narrowed to 64. While you are asked to predict these games (and you may be predicting them after they occur), we will not be scoring them. Why don't predictions in later rounds count for more? While it is possible to weight the later games, we chose to keep scoring simple and count all games equally. Any weights we pick would be mostly arbitrary (how many first-round games is a championship game worth?). Also, weighting any game increases the role that luck plays in determining a winner. We've also structured the competition so that people can still be in the running even if there are early-round upsets. Citation link keyboard_arrow_up Addison Howard, Jeff Sonas, and Will Cukierski. Google Cloud & NCAA® ML Competition 2020-NCAAM. https://kaggle.com/competitions/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament, 2020. Kaggle. Cite Competition Host Google Cloud Prizes & Awards Kudos Awards Points & Medals Participation 5,130 Entrants 0 Participants 0 Teams 0 Submissions Tags Tabular Basketball Log Loss Table of Contents collapse_all Description Evaluation Timeline Prizes FAQs Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "google-cloud-ncaa-march-madness-2020-division-1-womens-tournament",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Apply Machine Learning to NCAA® March Madness® Each season there are thousands of NCAA® basketball games played between Division I women's teams, culminating in March Madness, the 64-team national championship that starts in the middle of March.  We have provided a large amount of historical data about college basketball games and teams, going back many years.  Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness® game outcomes.  You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past. If you are unfamiliar with the format and intricacies of the tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way).  Nevertheless, you may also be able to make productive use of external data.  If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data.  The WTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure. You will probably also need to understand exactly how dates work in our data. Remember as well that you are required to disclose your external sources of data prior to the start of the tournament. We extend our gratitude to Kenneth Massey for providing much of the historical data. Special Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA® tournaments (seasons 2015-2019). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2020 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the contest data files. The data will likely be refreshed once in early March while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season. Right now the WSeasons and WTeamConferences files are the only two files that reference the 2020 season, since those are the only files where the final 2020 data is already finalized; the other files will be updated later on when we provide preliminary data (in early March) and final Stage 2 data (after Selection Sunday) for the current season. This section provides everything you need to build a simple prediction model and submit predictions. Special note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March.  For instance, this year the first women’s Division I games were played on November 5th, 2019 and the women’s national championship game will be played on April 5th, 2020.  Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season.  By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in.  So for instance, the current season will be identified in our data as the 2020 season, not the 2019 season or the 2019-20 season or the 2019-2020 season, though you may see any of these in everyday use outside of our data. Data Section 1 file: WTeams.csv This file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams.  There are 351 teams currently in Division-I, and an overall total of 365 teams in our team listing. Each year, some teams might start being Division-I programs, and others might stop being Division-I programs, and in fact this year is no exception. Savannah State (TeamID=3366) is no longer part of Division I and so their data in our dataset stops after the 2019 season, and they are no longer part of the Mid-Eastern Athletic Conference. To balance this out, Merrimack (TeamID=3467) has just moved to Division I this year, so they show up as a new team in the data and they are now part of the America East Conference. Other than the teams file and team conferences file, you won't see any data for Merrimack until preliminary 2020 season data starts coming in, as we approach Stage 2 of the contest. Also please note that we recently determined that VMI and Citadel only have men's basketball teams, not women's teams, and so we shouldn't have those schools in the women's data at all.  This means all references to TeamID 3154 and TeamID 3440 have been removed from the women's contest data this year, and there are two fewer Division 1 teams this year because of this. Data Section 1 file: WSeasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. The game dates in this dataset are expressed in relative terms, as the number of days since the start of the regular season, and aligned for each season so that day number #133 is the Monday right before the tournament, when team selections are made.  During any given season, day number zero is defined to be exactly 19 weeks earlier than Selection Monday, so Day #0 is a Monday in late October or early November such that Day #132 is Selection Sunday (for the men's tournament) and Day #133 is Selection Monday (for the women's tournament). This doesn't necessarily mean that the regular season will always start exactly on day #0 or day #1; in fact, during the past decade, regular season games typically start being played on a Friday that is either Day #4 or Day #11, but further back there was more variety. Data Section 1 file: WNCAATourneySeeds.csv This file identifies the seeds for all teams in each NCAA® tournament, for all seasons of historical data. Thus, there are exactly 64 rows for each year, since there are no play-in teams in the women's tournament.  We will not know the seeds of the respective tournament teams, or even exactly which 64 teams it will be, until Selection Monday on March 16, 2020 (DayNum=133). Data Section 1 file: WRegularSeasonCompactResults.csv This file identifies the game-by-game results for many seasons of historical data, starting with the 1998 season. For each season, the file includes all games played from DayNum 0 through 132.  It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=133 is Selection Monday).  Thus a game played before Selection Monday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever. Data Section 1 file: WNCAATourneyCompactResults.csv This file identifies the game-by-game NCAA® tournament results for all seasons of historical data. The data is formatted exactly like the WRegularSeasonCompactResults data. Each season you will see 63 games listed, since there are no women's play-in games. Although the scheduling of the men's tournament rounds has been consistent for many years, there has been more variety in the scheduling of the women's rounds.  There have been four different schedules over the course of the past 20+ years for the women's tournament, as follows: 2017 season through 2020 season: 2015 season and 2016 season: 2003 season through 2014 season: 1998 season through 2002 season: Data Section 1 file: WSampleSubmissionStage1.csv This file illustrates the submission file format for Stage 1.  It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. A submission file lists every possible matchup between tournament teams for one or more years.  During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA® tournaments (seasons 2015, 2016, 2017, 2018, and 2019).  In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA® tournament (season 2020). Since there are 64 teams in the tournament, there are 64*63/2=2,016 predictions to make for that year, so a Stage 1 submission file will have 2,016*5=10,080 data rows. Example #1: You want to make a prediction for Duke (TeamID=3181) against Arizona (TeamID=3112) in the 2005 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win.  In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%): 2005_3112_3181,0.47 Example #2: You want to make a prediction for Duke (TeamID=3181) against North Carolina (TeamID=3314) in the 2005 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win.  In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%): 2005_3181_3314,0.516 Also note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2005_3112_3181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke. This section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA® tournament games since the 2009-10 season.  Team Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files.  However, the two files are strongly related. In a Detailed Results file, the first eight columns ( Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT ) are exactly the same as a Compact Results file.  However, in a Detailed Results file, there are many additional columns.  The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team): (and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF ). Note: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals.  And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course.  So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3).  And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM. Data Section 2 file: WRegularSeasonDetailedResults.csv This file provides team-level box scores for many regular seasons of historical data, starting with the 2010 season.  All games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file. Data Section 2 file: WNCAATourneyDetailedResults.csv This file provides team-level box scores for many NCAA® tournaments, starting with the 2010 season.  All games listed in the WNCAATourneyCompactResults file since the 2010 season should exactly be present in the WNCAATourneyDetailedResults file. This section provides city locations of all regular season, conference tournament, and NCAA® tournament games since the 2009-10 season Data Section 3 file: Cities.csv This file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an W; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. Data Section 3 file: WGameCities.csv This file identifies all games, starting with the 2010 season, along with the city that the game was played in.  Games from the regular season and the NCAA® tourney are all listed together. The CityID is present in more than 98% of games since the 2010 season. Games from the 2009 season and before are not listed in this file. This section provides play-by-play event logs for more than 99% of each year's regular season and NCAA® tournament, and secondary tournament women's games since the 2014-15 season - including plays by individual players. This year we are transitioning to a different play-by-play source, which includes data since the 2014-2015 season rather than since the 2009-2010 season (that's what we had previously for men's data).  However, we are now able to provide play-by-play for both men's and women's data, and there is locational play-by-play detail starting with games from the 2018-2019 season.  This includes an X/Y location (ranging from 0 to 100 in each dimension) on the court for each shot attempt, turnover, and foul for many games, as well as an overall categorization of the area on the court that the shot or turnover or foul occurred in (inside left wing, outside right wing, under the basket, etc.)  Some games in these recent seasons still lack the locational detail.  The data from last year (2019 season) matches what you can expect for the current year (2020 season) as we approach the postseason.  Despite the 99.5% coverage, there are still a few games missing annually, and we will try to bring those in as well, if possible. Data Section 4 file: WEvents2015.csv, WEvents2016.csv, WEvents2017.csv, WEvents2018.csv, WEvents2019.csv Each WEvents file lists the play-by-play event logs for more than 99% of games from that season.  Each event is assigned to either a team or a single one of the team's players.  Thus if a basket is made by one player and an assist is credited to a second player, that would show up as two separate records.  The players are listed by PlayerID within the WPlayers.csv file. Event Types and Subtypes:   This is the diagram provided by the play-by-play source:    Here is a diagram of these regions on a typical court:  And you may also have noticed that the three-point-arc is different this year in college basketball.  The width of the court is still 15.2 m (50 ft) and the length of the court is still 28.7 m (94 feet), but the distance to the arc has changed from 6.32 m (20 feet 9 inches) to 6.75 m (22.15 feet). Here is a diagram of the old court, which would apply for play-by-play in seasons 2019 and before:  And here is a diagram of the new court, which only applies for season 2020:  Data Section 4 file: WPlayers.csv Note: there are data collection errors within the events, in that they don't necessarily add up to the final stats for the game.  In addition, the player name spellings may vary over the course of a season or career for the same player, which would lead to the same player showing up with different PlayerID values.  Nevertheless, this was the highest quality data we could manage for play-by-play with the near-complete set of games. This section contains additional supporting information, including alternative team name spellings and representations of bracket structure Data Section 5 file: WTeamSpellings.csv This file indicates alternative spellings of many team names.  It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets.  Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\").  Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school.  The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums. Data Section 5 file: WNCAATourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below).  You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket.  Unlike the analogous file on the men's side, it is not necessary to provide a Season within this file, because the women's tournament has never had play-in-games and so the 64-team women's bracket has always had the same structure each season. Data Section 5 file: Conferences.csv This file indicates the Division I conferences that have existed over the years since 1985.  Each conference is listed with an abbreviation and a longer name.  There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time.  Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes. Data Section 5 file: WTeamConferences.csv This file indicates the conference affiliations for each team during each season.  Some conferences have changed their names from year to year, and/or changed which teams are part of the conference.  This file tracks this information historically. 40 files 1.94 GB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 1.94 GB WDataFiles_Stage1 WDataFiles_Stage2 WPlayByPlay_Stage2 WEvents2015.csv WEvents2016.csv WEvents2017.csv WEvents2018.csv WEvents2019.csv WPlayers.csv WSampleSubmissionStage1_2020.csv 40 files 425 columns  Too many requests",
    "data_description": "Google Cloud & NCAA® ML Competition 2020-NCAAW | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Google Cloud · Featured Prediction Competition · 5 years ago Late Submission more_horiz Google Cloud & NCAA® ML Competition 2020-NCAAW Apply Machine Learning to NCAA® March Madness® Google Cloud & NCAA® ML Competition 2020-NCAAW Overview Data Code Models Discussion Leaderboard Rules Overview Start Feb 15, 2020 Close Mar 20, 2020 Merger & Entry Description link keyboard_arrow_up Update: this competition has been cancelled on account of the COVID-19 pandemic. As a result of the continued collaboration between Google Cloud and the NCAA®, the seventh annual Kaggle-backed March Madness® competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men’s and Women’s Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA’s historical data and your computing power, while the ground truth unfolds on national television. In the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset . In the second stage, competitors will forecast outcomes of all possible matchups in the 2020 NCAA Division I Men’s and Women’s Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2020 results. As the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on! This page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here . If you want to extend your analysis then try out our Analytics Competition here Evaluation link keyboard_arrow_up Submissions are scored on the log loss: LogLoss = − 1 n n ∑ i = 1 [ y i log ( ˆ y i ) + ( 1 − y i ) log ( 1 − ˆ y i ) ] , where n is the number of games played \\\\( \\hat{y}_i \\\\) is the predicted probability of team 1 beating team 2 \\\\( y_i \\\\) is 1 if team 1 wins, 0 if team 2 wins \\\\( log() \\\\) is the natural (base e) logarithm The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value. Submission File The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2020 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 64 teams, you will predict (64*63)/2  = 2,016 matchups. Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, \"2015_3106_3107\" indicates team 3106 played team 3107 in the year 2015. You must predict the probability that the team with the lower id beats the team with the higher id. The resulting submission format looks like the following, where \"pred\" represents the predicted probability that the first team will win: id,pred 2015_3106_3107,0.5 2015_3106_3110,0.5 2015_3106_3113,0.5 ... Timeline link keyboard_arrow_up Updated March 13, 2020 : The following supersedes the respective terms outlined below.  As a result of COVID-19, this competition will be cancelled with no Kaggle points, Kaggle medals, or monetary prizes. The end date has been updated from April 7, 2020 at 11:59pm UTC to March 20, 2020 at 3:00 am UTC. Please see this announcement post for more details. Stage 1 - Model Building Sunday, March 15 - Prior to this deadline, competitors build and test models on historical data. The leaderboard shows the model performance on historical tournament outcomes. Stage 2 - Championship Monday, March 16 - Selection Monday® (64 teams announced) Tuesday, March 17 - Kaggle begins to accept 2020 predictions. Release of up-to-date 2019-2020 season data. Friday, March 20 3PM UTC - Final deadline to submit 2020 predictions. March 20 onward - Watch your tournament results play out! The Kaggle Team will refresh the leaderboard throughout the competition as games are finalized. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up Updated March 13, 2020 : The following supersedes the respective terms outlined below.  As a result of COVID-19, this competition will be cancelled with no Kaggle points, Kaggle medals, or monetary prizes. The end date has been updated from April 7, 2020 at 11:59pm UTC to March 20, 2020 at 3:00 am UTC. Please see this announcement post for more details. 1st Place - $10,000 2nd Place - $7,000 3rd Place - $5,000 4th Place - $2,000 5th Place - $1,000 Stage 1 will not count towards Kaggle rankings/points. Stage 2 will count toward Kaggle rankings/points. FAQs link keyboard_arrow_up Is one tournament enough to decide the best basketball algorithm? It's better to be lucky than good! Besides, when was the last time your office held a cross validation pool? Can I update my predictions after the tournament starts? No changes are permitted once the tournament begins. How is the leaderboard going to work when the event being scored hasn't yet happened? You won't submit after the tournament starts. We'll update the solution file as the games occur, which will cause the ranks on the leaderboard to change. Why don't predictions in later rounds count for more? While it is possible to weight the later games, we chose to keep scoring simple and count all games equally. Any weights we pick would be mostly arbitrary (how many first-round games is a championship game worth?). Also, weighting any game increases the role that luck plays in determining a winner. We've also structured the competition so that people can still be in the running even if there are early-round upsets. Citation link keyboard_arrow_up Addison Howard, Jeff Sonas, and Will Cukierski. Google Cloud & NCAA® ML Competition 2020-NCAAW. https://kaggle.com/competitions/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament, 2020. Kaggle. Cite Competition Host Google Cloud Prizes & Awards Kudos Awards Points & Medals Participation 2,456 Entrants 0 Participants 0 Teams 0 Submissions Tags Tabular Basketball Log Loss Table of Contents collapse_all Description Evaluation Timeline Prizes FAQs Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "ashrae-energy-prediction",
    "discussion_links": [
      "/competitions/ashrae-energy-prediction/discussion/124709",
      "/competitions/ashrae-energy-prediction/discussion/123481",
      "/competitions/ashrae-energy-prediction/discussion/124984",
      "/competitions/ashrae-energy-prediction/discussion/124788",
      "/competitions/ashrae-energy-prediction/discussion/127086",
      "/competitions/ashrae-energy-prediction/discussion/123525",
      "/competitions/ashrae-energy-prediction/discussion/123496",
      "/competitions/ashrae-energy-prediction/discussion/129182",
      "/competitions/ashrae-energy-prediction/discussion/124226",
      "/competitions/ashrae-energy-prediction/discussion/123654",
      "/competitions/ashrae-energy-prediction/discussion/125307",
      "/competitions/ashrae-energy-prediction/discussion/123288"
    ],
    "discussion_texts": [
      "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules Matt Motoki · 1st in this Competition  · Posted 5 years ago arrow_drop_up 162 more_vert 1st Place Solution Team Isamu & Matt Thank you to Kaggle and ASHRAE for hosting this competition. The decision to use only non-leak data for the private test set helped to make this competition fair. Thank you to all those who contributed to the kernels and discussions and especially those who made the leaks public. Last but not least, I'd like to thank and congratulate my teammate Isamu Yamashita for being a great teammate and becoming a Competitions Master. During the competition, we shared ideas and discussed progress within our team, but we tested and trained separate models. This helped us maintain diversity in our final ensemble.  This is a combined summary our team's solution. Preprocessing Remove anomalies As others have noted, cleaning the data was very important in this competition.  The assumption is that there are unpredictable and hence unlearnable anomalies in the data that, if trained on, degrade the quality of the predictions. We identified and filtered out three types of anomalies: Long streaks of constant values Large positive/negative spikes Additional anomalies determined by visual inspection We noticed that some of these anomalies were consistent across multiple buildings at a site. We validated potential anomalies using all buildings in a site--if an anomaly showed up at the same time at multiple buildings, we could be reasonably certain that this was indeed a true anomaly. This allowed us to remove anomalies that were not necessarily part of a long streak of constant values or a large spike. Impute Missing Temperature Values There were a lot of missing values in temperature metadata. We found that imputing the missing data using linear interpolation helped our models. Local Time Zone Correlation As noted in the competition forum, the timezone in the train/test data was different from the timezone in the weather metadata. We used the information in this discussion post to correct the timezones. Target Transformations Like most competitors, we started by predicting log1p(meter_reading) .  We also corrected the units for site 0 as per this discussion post . Near the end of the competition, we tried standardizing meter_reading by dividing by square_feet ; i.e., we predicted log1p(meter_reading/square_feet) .  Isamu came up with the idea after reading this discussion post by Artyom Vorobyov .  The models trained with the standardized target added diversity to our final ensemble and improved our score by about 0.002.  If we had more time we would have liked to explore this idea further; for example, we could have tried to predict log1p(meter_reading)/square_feet or created features using the standardized targets. Feature Engineering and Feature Selection We took different approaches to feature engineering and feature selection in this competition.  Isamu took a conservative approach and carefully selected  features; on the other hand, Matt took a brute force approach and used most of them. Here are the features that helped: Raw features from train/test, weather metadata, and building metadata Categorical interactions such as the concatenation of building_id and meter Time series features including holiday flags and time of day features Count (frequency) features Lag temperature features similar to those found in the public kernels Smoothed and 1st, 2nd-order differentiation temperature features using Savitzky-Golay filter (see the figure below) Cyclic encoding of periodic features; e.g., hour gets mapped to hour_x = cos(2*pi*hour/24) and hour_y = sin(2*pi*hour/24) Bayesian target encoding (see this kernel ) Models We trained CatBoost, LightGBM, and MLP models on different subsets of the data: 1 model per meter 1 model per site_id 1 model per (building_id, meter) Our team tried different approaches to validation in this competition. Like other competitors, we tried K-Fold CV using consecutive months as the validation set.  The following code shows one approach to getting validation months: >>> import numpy as np\n>>> def get_validation_months (n):\n...     return [np. arange (i, i+n) % 12 + 1 for i in range ( 12 )]\n...\n>>> get_validation_months ( 6 )\n[ array ([ 1 , 2 , 3 , 4 , 5 , 6 ]), array ([ 2 , 3 , 4 , 5 , 6 , 7 ]), array ([ 3 , 4 , 5 , 6 , 7 , 8 ]), array ([ 4 , 5 , 6 , 7 , 8 , 9 ]), array ([ 5 , 6 , 7 , 8 , 9 , 10 ]), array ([ 6 , 7 , 8 , 9 , 10 , 11 ]), array ([ 7 , 8 , 9 , 10 , 11 , 12 ]), array ([ 8 , 9 , 10 , 11 , 12 , 1 ]), array ([ 9 , 10 , 11 , 12 , 1 , 2 ]), array ([ 10 , 11 , 12 , 1 , 2 , 3 ]), array ([ 11 , 12 , 1 , 2 , 3 , 4 ]), array ([ 12 , 1 , 2 , 3 , 4 , 5 ])] content_copy Trying different validation schemes allowed us to train models that added diversity to our final ensemble. Ensembling To reduce the risk of overfitting to the public LB and improve robustness, we ensembled predictions from many different models. Here  are some of the things we did: Used cleaned leak data as a holdout set to tune our second stage model Averaged log values; i.e., expm1(mean(log1p(x))) rather than averaged the raw values Used the generalized weighted mean and tuned the parameters using Optuna Hedged our bets by including leak-free public kernels on cleaned data: ASHRAE: Half and Half by Vopani ASHRAE- KFold LightGBM - without leak (1.08) by Sandeep Kumar Ashrae: simple data cleanup (LB 1.08 no leaks) by Robert Stockton This kernel shows how we ensembled our predictions.  Our final ensemble was a plain average of our top 4 submissions with respect to the public LB score. What Didn't Work Again, following this discussion post by Artyom Vorobyov , we thought of ensembling the predictions for buildings with the same meter_readings; e.g., buildings 1225 and 1226 with meter 0. However, we were not able to improve our public LB result with this approach. It is possible that we did something wrong here, but we didn't have enough time to go back and explore this idea further. Smoothing the predictions of our models.  We had initial success with smoothing the final predictions of our models, but after a certain point, we started to find that smoothing hurt our public LB score. Our guess is that smoothing helps spiky low quality predictions, but our ensemble predictions were already sufficiently smooth. Please sign in to reply to this topic. comment 47 Comments 10 appreciation  comments Hotness Clayton Miller Competition Host Posted 5 years ago arrow_drop_up 3 more_vert Congratulations to our first-place team!!!!!! 🙌 💪 👍 👊 Their solution will be organized and added to the other top five winners in a publication and open-source repository. Please stay tuned! Also, for everyone else, we are in the process of curating a directory of Notebooks for building performance engineers and analysts from the competition that will serve as a recipe book for beginners as well as experts who are curious to learn about certain topics. We highly encourage you all to share your solutions as a notebook for this purpose. We will acknowledge the authors of all notebooks in this process, so please annotate your notebook or profile with how you would like to be acknowledged. It would also be helpful if you could indicate whether you are an ASHRAE member. armbodj Posted 5 years ago arrow_drop_up 1 more_vert Hi and congrats When you say in the feature engineering section, Raw features from train/test, weather metadata, and building metadata Categorical interactions such as the concatenation of building_id and meter Count (frequency) features … Lets say X1, X2 two categorical features and X1_X2 the interaction between X1 and X2 Does this mean that in your final data set you had the following features at the same time: X1, X1_count, X2,  X2_count, X1_X2, X1_X2_count ? or only X1_count,  X2_count, X1_X2_count ? mavillan Posted 5 years ago · 1670th in this Competition arrow_drop_up 1 more_vert Congratulations, awesome work! Count (frequency) features What kind of features are these? Matt Motoki Topic Author Posted 5 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thank mavillan!  The count features are the result of mapping categorical features to their counts (frequencies); e.g., we could create an hourly_count feature like this df [ \"hourly_count\" ] = df .groupby ( \"hour\" ) [ \"hour\" ] .transform ( \"count\" ) content_copy Tim Yee Posted 5 years ago · 13th in this Competition arrow_drop_up 1 more_vert Congrats! Nice write up! Smoothing the predictions of our models. We had initial success with smoothing the final predictions of our models, but after a certain point, we started to find that smoothing hurt our public LB score. Our guess is that smoothing helps spiky low quality predictions, but our ensemble predictions were already sufficiently smooth. by \"smoothing\", you mean postprocessing right? Matt Motoki Topic Author Posted 5 years ago · 1st in this Competition arrow_drop_up 1 more_vert Hi Tim, if I understand the 2nd place team's postprocessing correctly, it is not the same as what we tried.  By smoothing I mean something like a moving average of the predictions. Volodymyr Posted 5 years ago · 20th in this Competition arrow_drop_up 2 more_vert Great job. Your feature Engineering and Blending techniques are awesome! What was your MLP architecture? Matt Motoki Topic Author Posted 5 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thanks!  The MLP architecture was pretty standard: entity emeddings + numerical features, followed by a few dense layers. I used Keras and the code was similar to these kernels Keras NN with embeddings for cat. features (1.15) same old entity embeddings Ailurophile Posted 5 years ago · 417th in this Competition arrow_drop_up 0 more_vert Congratulations Great Write-Up Thanks for sharing your Approach & Insights!! @mmotoki Dasha Tereshkina Posted 3 years ago arrow_drop_up 0 more_vert Very interesting! Takumi Ban Posted 5 years ago · 1193rd in this Competition arrow_drop_up 0 more_vert Congratulations! By the way, How much did \"1st, 2nd-order differentiation temperature features using Savitzky-Golay filter\"  make accuracy better? Xuanlong Yu Posted 5 years ago · 62nd in this Competition arrow_drop_up 0 more_vert Congratulations and thank you for sharing ! I will try your approach and use Outlier Detection from another group of gold medalist. John Nesbit Posted 5 years ago · 130th in this Competition arrow_drop_up 0 more_vert Congrats Ivan Vigorito Posted 5 years ago · 509th in this Competition arrow_drop_up 0 more_vert Thank you for sharing this post with the community 👍 Pradeep Posted 5 years ago arrow_drop_up 0 more_vert congratulation! Andy Crowe Posted 5 years ago arrow_drop_up 0 more_vert Congrats, nice work! Deep Chatterjee Posted 5 years ago · 782nd in this Competition arrow_drop_up 0 more_vert Congratulations @mmotoki and @yamsam and thanks for sharing your approach. Mohamad Hallak Posted 5 years ago arrow_drop_up 0 more_vert Congratulations! Thanks for the nice write up! Eren Ali Aslangiray Posted 5 years ago arrow_drop_up 0 more_vert Nice! Congrats Shahules Posted 5 years ago · 343rd in this Competition arrow_drop_up 0 more_vert congratulations and thanks for sharing.💯 Ziqing (Sophia) Chen Posted 5 years ago · 78th in this Competition arrow_drop_up 0 more_vert Congrats. Awesome work!! Thank you for sharing. ask9 Posted 5 years ago · 787th in this Competition arrow_drop_up 0 more_vert Thanks for sharing solutions,Great work really deserving @mmotoki !!!!!!!!!!!! Nikolay Matsievsky Posted 5 years ago · 3328th in this Competition arrow_drop_up 0 more_vert Thank you very much. Not clear what are final timezones shifts (?) for sites, and how they helped to improve the score (I thought weather data had been already given with local time). Also not clear what to do with some missed values for pressure, precip_depth, wind_direction - need to take smoothed average too? Also day-of-time categorization/cyclic model is worse that raw 24-hour model (separate model for every hour, because of non-smoothed behaviour for some buildings). Not sure this is applicable for your ensemble. Additionally normalization all features to 0-1 (or 0-1000) will help in precision. But not investigate, how exactly. 2020-01-10.energy0.hourly.png Matt Motoki Topic Author Posted 5 years ago · 1st in this Competition arrow_drop_up 0 more_vert Thanks Nikolay, the timezone shifts are originally from this discussion post and kernel .  To see how to fill in the missing values, have a look at my teammate Isamu's starter kernel .  About your last point, you might be right--we didn't try building one model for each hour. Tomas Pechacek Posted 5 years ago arrow_drop_up 0 more_vert Congratulations! And thanks for sharing. By the way, this cyclic hour encoding of yours, this is some usual trick in ML prediction? I think, this the first time I see it and I love it! Matt Motoki Topic Author Posted 5 years ago · 1st in this Competition arrow_drop_up 2 more_vert Thanks Tomas!  I think cyclic encoding is a relatively well known technique (blogs are easy to find if you google \"cyclic feature encoding\"), but I don't see it being used on Kaggle or in practice too frequently.  From my experience, these features lead to only small improvements (if any) and so maybe that's why they aren't more widely used. Tomas Pechacek Posted 5 years ago arrow_drop_up 0 more_vert Thank you, Matt! I will look at it. laksh Posted 5 years ago · 2672nd in this Competition arrow_drop_up 0 more_vert Thank you sharing the great work you did, this will help to understand lots of things. MuriloRazoli Posted 5 years ago arrow_drop_up 0 more_vert Congratulations, great job!! Ghawkuser Posted 5 years ago arrow_drop_up 0 more_vert So great!!! Congratulations, hope one day I could do half of this work!! Paul Tan Posted 5 years ago arrow_drop_up 0 more_vert Congrats @mmotoki Too many requests error Too many requests",
      "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules Vopani · 2nd in this Competition  · Posted 6 years ago arrow_drop_up 199 more_vert 2nd Place Solution Our team finished 2nd on private LB (12th on public). The private LB is finally released officially. So happy and pumped up for winning in the money for the first time on Kaggle (after 6 years). Santa has been kind this year :-) Our team will be kind too and share our complete solution. Solution Architecture: XGB: XGBoost LGBM: LightGBM CB: Catboost FFNN: Feed-forward Neural Network Short version Remove noise (Very important) Very few and basic features (For stability) Optimize models for each site+meter (For site-specific patterns) Ensemble of XGBoost, LightGBM, CatBoost, NeuralNetwork (To reduce variance) Postprocessing (Very critical) Leak insertion (Sucks, but probably doesn't matter) Final Ensemble (approximate): 30% XGB-bagging + 50% LGBM-bagging + 15% CB-bagging + 5% FFNN Many variations of XGB, LGBM, CB were bagged: at site+meter level, at building+meter level, at building-type+meter level. Bagged XGB gave the best results among the boosting methods. FFNN was used only for meter = 0. It gave very poor results for other meters and didn't add value to ensemble. Also, FFNN was very poor for site-14 so we didn't use it and hence that tile is missing from the models section in the architecture diagram :-) Our solution was built heavily on @oleg90777 's base XGB/LGBM setup (which scores 1.04 on LB without leak) and our key points were cleaning the data and post-processing the predictions (validated on leaked data and LB). Read more about it here . The final ensemble scores almost the best on public LB, on leaked data as well as private LB, so hopefully it is robust and useful. Long version Pre-Processing A lot of the low values of the target variable seem to be noise (as discussed multiple times in the forums, specifically for site-0) and removing these rows from the training data gives a good boost in score which has been done by several other competitors too. It was the most time consuming task as we visualized and wrote code to remove these rows for each of the 1449 buildings manually. We could have used a set of heuristics but that is not optimal due to some edge cases so we just decided to spend few minutes on every building and remove the outliers. Feature Engineering Due to the size of the dataset and difficulty in setting up a robust validation framework, we did not focus much on feature engineering, fearing it might not extrapolate cleanly to the test data. Instead we chose to ensemble as many different models as possible to capture more information and help the predictions to be stable across years. Our models barely use any lag features or complex features. We have less than 30 features in our best single model. This was one of the major decisions taken at the beginning of our work. From past experience it is tricky to build good features without a reliable validation framework. Modelling We bagged a bunch of boosting models XGB, LGBM, CB at various levels of data: Models for every site+meter, models for every building+meter, models for every building-type+meter and models using entire train data. It was very useful to build a separate model for each site so that the model could capture site-specific patterns and each site could be fitted with a different parameter set suitable for it. It also automatically solved for issues like timestamp alignment and feature measurement scale being different across sites so we didn't have to solve for them separately. Ensembling models at different levels were useful to improve score. Just bagging with different seeds didn't help much. Site-level FFNN was used only for meter = 0. Each site had a different NN architecture. It gave very poor results for other meters and didn't add value to ensemble. Also, FFNN was very poor for site-14 so we didn't use it and hence that tile is missing from the models section in the architecture diagram :-) For tuning of all models, we used a combination of 4-fold and 5-fold CV on month from training data as well as validation on leaked data. Post-Processing We have shared our post-processing experiments in another thread: https://www.kaggle.com/c/ashrae-energy-prediction/discussion/123528 Since we remove a lot of low value observations from training data, it artificially increases the mean of the target variable and hence the model's raw predictions on test data also has an inflated mean. Since RMSE is optimal at true mean value, reducing the mean of predictions of test data by a reducing factor helps bring it down to its true mean, thus improving score. We tried a range of post-processing values and finally ended up using 0.8 - 0.85 for most models. Ensembling Our best single type model was XGB but LGBM was very close and CB was not very bad either. All scored in the range of 1.04 - 1.06 on the public LB without leak. Since FFNN was built only for meter = 0, we ensembled differently for every site+meter combination using a weighted average where the weights were determined using a combination of CV score, LB score, Leak score and intuition. Final Ensemble (approximate) for meter = 0: 30% XGB-bagging + 50% LGBM-bagging + 15% CB-bagging + 5% FFNN Final Ensemble (approximate) for meters 1, 2, 3: 30% XGB-bagging + 50% LGBM-bagging + 20% CB-bagging The final ensemble scores almost the best on public LB, on leaked data as well as private LB, so hopefully it is robust and useful. Leak We used leak data primarily for local validation and for inserting into the test data as many competitors did. We didn't use any leaks outside of sites 0, 1, 2, 4, 15. Since our core models were at site+meter level, we didn't explore leveraging the leaked data as additional train data. Team Shout out to my cHaOs team-mates @oleg90777 (one of the best team leaders I've worked with), @berserker408 and @isanton . Code We will be happy to share our entire code if ASHRAE / Kaggle can confirm if we can. No timeline / commitment on this. Credits ASHRAE and Kaggle for hosting this competition. Competitors who scraped data and made it public. You are winners too. Kaggle admins for working hard to make the best out of the leak situation. Please sign in to reply to this topic. comment 103 Comments 16 appreciation  comments Hotness Clayton Miller Competition Host Posted 5 years ago arrow_drop_up 3 more_vert Congratulations to our second-place team!!!!!! 🙌 💪 👍 👊 Their solution will be organized and added to the other top five winners in a publication and open-source repository. Please stay tuned! Also, for everyone else, we are in the process of curating a directory of Notebooks for building performance engineers and analysts from the competition that will serve as a recipe book for beginners as well as experts who are curious to learn about certain topics. We highly encourage you all to share your solutions as a notebook for this purpose. We will acknowledge the authors of all notebooks in this process, so please annotate your notebook or profile with how you would like to be acknowledged. It would also be helpful if you could indicate whether you are an ASHRAE member. Giba Posted 6 years ago · 14th in this Competition arrow_drop_up 3 more_vert Congrats! Nice ensembling :) MPWARE Posted 6 years ago · 9th in this Competition arrow_drop_up 3 more_vert Congratulations! According to: https://www.kaggle.com/sohier/ashrae-interim-scores#final_all_scores.csv You had 6 submissions at 1.22 so your team is the best even if you've selected ones at 1.23. Only Btbpanda had one submission better (but they don't selected it). Sanyam Bhutani Posted 6 years ago · 1065th in this Competition arrow_drop_up 4 more_vert Congratulations! :D Remove noise (Very important) This time removing the noise got you to the prize zone! 😁 A little context for everyone, from H2O.ai's GrandMaster Panel Q: Any mistakes or Regrets from a competition? @rohanrao advises looking at outliers, based on a competition where removing just ONE outlier would have landed him 1st position. LittleShiny Posted 5 years ago · 2803rd in this Competition arrow_drop_up 1 more_vert Congratulations! Many competitors mentioned leak data. I  am new for machine learning. Can you explain what's the meaning of  'leak data' in this competition？ Oleg Knaub Posted 5 years ago · 2nd in this Competition arrow_drop_up 0 more_vert Hello, that means part of public and private labels were in the internet. So if you take that labels you would have for a part of data accuracy of 100% Narendra Singh Parihar Posted 5 years ago arrow_drop_up 0 more_vert Congratulations! 😊 AnaLyze Posted 5 years ago arrow_drop_up 1 more_vert Thank you for the well explained solution and congratulations with the second place! How did you define outliers for each building? Was it an individual and mostly visual approach or a more standardised one like e.g. based on IQR or similar? Oleg Knaub Posted 5 years ago · 2nd in this Competition arrow_drop_up 0 more_vert It was mostly an individual and mostly visual approach. A standardised approach is also good, but because of some edge cases not that perfekt like individual. Muhammet Ikbal Elek Posted 5 years ago arrow_drop_up 1 more_vert Congrats,Great! Taner Sekmen Posted 5 years ago arrow_drop_up 1 more_vert Congratulations! Stefania Rana Posted 5 years ago arrow_drop_up 1 more_vert Congratulations! And thanks for sharing! lemonlemon1 Posted 5 years ago arrow_drop_up 1 more_vert Congratulations! Yixinchen Posted 5 years ago · 87th in this Competition arrow_drop_up 1 more_vert Your idea of thousands of models is incredibly inspiring! Risab Biswas Posted 5 years ago arrow_drop_up 1 more_vert Thanks a lot @rohanrao for Sharing this :) Kurian Benoy Posted 5 years ago arrow_drop_up 1 more_vert Congratulation Ved Posted 5 years ago · 638th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for the write-up. liuze Posted 5 years ago · 1126th in this Competition arrow_drop_up 1 more_vert Congratulations! Awesome solution WangYong Posted 5 years ago · 2071st in this Competition arrow_drop_up 1 more_vert Congratulations! Ben Fan Posted 5 years ago · 259th in this Competition arrow_drop_up 1 more_vert Congratulations! rackson3861 Posted 5 years ago · 2782nd in this Competition arrow_drop_up 1 more_vert please let me know how are you stacking the models. I mean can we use the outputs of XGB with CATBOOST and followed by NN. I new to this concept. Please elaborate Vopani Topic Author Posted 5 years ago · 2nd in this Competition arrow_drop_up 1 more_vert We just took a weighted average of different models. The approximate weights are shared in the post. Vasileios Tsakalos Posted 6 years ago arrow_drop_up 1 more_vert Congrats, really good job! Arjun Kathuria Posted 6 years ago arrow_drop_up 1 more_vert sweet Anwar Alam Posted 6 years ago arrow_drop_up 1 more_vert congrats Vishwanath Poojari Posted 6 years ago arrow_drop_up 1 more_vert nice Soumyajit Karmakar Posted 6 years ago arrow_drop_up 1 more_vert nice Nono Posted 6 years ago · 1452nd in this Competition arrow_drop_up 1 more_vert Hi @oleg90777 , did you also optimize \"num_leaves\" for lightgbm in your baysian optimization? you said max_depth was one of the most important parameters. I am wondering whether number of leaves is also important for lightgbm models. Oleg Knaub Posted 6 years ago · 2nd in this Competition arrow_drop_up 2 more_vert Yes, we also optimized numleaves, feature_fraction and subsample. Nono Posted 6 years ago · 1452nd in this Competition arrow_drop_up 1 more_vert Thanks for your reply, that is very helpful. I don't know why the upvote button does not work properly, when I click on upvote button, it returns back to 0.. Written upvote then 👍 Too many requests error Too many requests",
      "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules eagle4 · 3rd in this Competition  · Posted 5 years ago arrow_drop_up 31 more_vert [3rd Place] Solution Thank you to Kaggle and ASHRAE for hosting this competition My solution is likely to disappoint some of you given the lack of sophistication. I spent most of my time (too much) on trying feature engineering, trying to look a cv split correlated to public LB (didn’t happen), trying to find a neural network architecture that will give me a boost in local cv and public LB (didn’t happen) and browsing research papers related energy prediction in the subway. I am quite clueless about web scraping but thanks to @gunesevitan for showing the way and teaching me a lot in the process nevertheless. Given diverse experiments with different CV schemes I did over the period of the competition, I decided to simply combine all the results (over 30), I got into a single submission using a simple average after selection by pearson correlation (6th on private LB). In two instances over the last 3 weeks, I used a subset without leak of these experiments and use the leak to ensemble them but I gave up because the public LB was quite poor despite a better local CV and I thought it was overfitting. However, as I selected this method as my alternative solution, it appears this was the best on the final private LB. Preprocessing: By lack of time, I only used the ideas and code of some excellent public kernels. I also wrote a script that run for a few hours where I eliminated all 0s in the same period when these 0s occurs in the same site, at the same period and across all meters. @ganfear wrote an excellent visualization here . My goal was to eliminate a maximum of \"vertical\" lines especially if they were simultaneous. I believe that this is the trick that gave me an advantage. Using this preprocessed data in the top public kernels always gave me a better LB, so I was on the right track. Feature Engineering: Row feature from train/test, weather metadata, and building metadata Count features and combination of features (see excellent write up on IEEE competition) Lag on row features but only the temperature feature lags seemed to be actually useful. Features mentioned in public kernel such as RH, feel_likes and presence of other meters seems to have a marginal upside. A feature that I only included in the other submission in the last days found in a research paper was working well with meter==1 was : latitude_dict = { 0 : 28 . 5383 , 1 : 50 . 9097 , 2 : 33 . 4255 , 3 : 38 . 9072 , 4 : 37 . 8715 , 5 : 50 . 9097 , 6 : 40 . 7128 , 7 : 45 . 4215 , 8 : 28 . 5383 , 9 : 30 . 2672 , 10 : 40 . 10677 , 11 : 45 . 4215 , 12 : 53 . 3498 , 13 : 44 . 9375 , 14 : 38 . 0293 , 15 : 40 . 7128 ,} train_df ['latitude'] = train_df['site_id'].map(latitude_dict) train_df ['solarHour'] = (train_df['hour']- 12 )* 15 # to be removed train_df ['solarDec'] = - 23 . 45 *np.cos(np.deg2rad( 360 *(train_df['doy']+ 10 )/ 365 )) # to be removed train_df ['horizsolar'] = np.cos(np.deg2rad(train_df['solarHour']))*np.cos(np.deg2rad(train_df['solarDec']))*np.cos(np.deg2rad(train_df['latitude'])) + np.sin(np.deg2rad(train_df['solarDec']))*np.sin(np.deg2rad(train_df['latitude'])) train_df ['horizsolar'] = train_df['horizsolar'].apply(lambda x: 0 if x < 0 else x) content_copy it is supposed to calculate the solar horizontal radiation coming into the building. Models: I trained Keras CNN (@aerdem4 style), LightGBM and Catboost on diverse version of cleaned data, various feature selection (including removal of building_id) - without ever beating the best kernel without leak to my disappointment. I had better success on local CV with meter level model using decision trees rather than using all the meter in the same decision trees model. Catboost and Lightgbm are clearly using different selection of features for the splits, so they are quite complementary. Only towards the end of the competition, I realized that the NN was giving approximately the same results on both local cv and public LB than decision trees models and that most of the performance was due to the cleaning more than my feature engineering. However, the results were quite uncorrelated, so it was good for diversity. Ensembling: My best submission is an ensemble of a small samples of leak free (~10) predictions among all my experiments. Using lightgbm, I used these different predictions as feature with the addition of the original feature “meter” in order to reduce overfitting. The leaky rows are used as training set in the ensembling model. My idea was that it is likely that the prediction of first 11% of the test set (included in the public LB) are going to be much better than the last 11% of the test set (private LB), so if I were to use the leak on the test set, I may be able to correct the last 11%. I was expecting a small hit on public LB but my public LB took such a larger hit than expected (+.025) that I thought I was overfitting and didn’t pursue that path. I kept the submission as my second final submission which became my best on private LB. Didn’t work for me or didn’t give any significative improvement: Pseudo labeling (read @hengck23 posts for more information) NN decoder-encoder using the leak in @mjahrer style (reniew seems to have succeed in this direction: see here ( https://www.kaggle.com/c/ashrae-energy-prediction/discussion/122203#699375 ) Thermometer encoding on cloud coverage Treat temperature features as categorical variables (given limited number of values) and use target encoding. Adaptation of @anokas tool for time series feature … Otherwise, I would like to thank all the people who taught me a lot on Kaggle among whom @Raddar , @cpmpml and @titericz during the years. Update: github code available here Time Series Analysis Neural Networks LightGBM Ensembling Please sign in to reply to this topic. comment 16 Comments 2 appreciation  comments Hotness Clayton Miller Competition Host Posted 5 years ago arrow_drop_up 1 more_vert Congratulations to our third-place winner! 🙏 👍  The third-place solution will be organized and added to the other top five winners in a publication and open-source repository. Please stay tuned! Also, for everyone else, we are in the process of curating a directory of Notebooks for building performance engineers and analysts from the competition that will serve as a recipe book for beginners as well as experts who are curious to learn about certain topics. We highly encourage you all to share your solutions as a notebook for this purpose. We will acknowledge the authors of all notebooks in this process, so please annotate your notebook or profile with how you would like to be acknowledged. It would also be helpful if you could indicate whether you are an ASHRAE member. Risab Biswas Posted 5 years ago arrow_drop_up 1 more_vert Congratulations @chabir ! Thanks for Sharing your Approach. Looking forward to have more awesome Kernals from you! 😄 Shahules Posted 5 years ago · 343rd in this Competition arrow_drop_up 1 more_vert Congrats @chabir and thanks for sharing. anokas Posted 5 years ago arrow_drop_up 1 more_vert Congrats @chabir :) Thanks for the credit, was it FTIM that you tried using? eagle4 Topic Author Posted 5 years ago · 3rd in this Competition arrow_drop_up 0 more_vert Yes Pradeep Muniasamy Posted 5 years ago arrow_drop_up 1 more_vert @chabir Congratulations Ahmet Erdem Posted 5 years ago · 378th in this Competition arrow_drop_up 1 more_vert Congrats on your solo 3rd place! I admit that I do a lot of Keras CNN nowadays but I have nothing special to be called as my style 😀 CPMP Posted 5 years ago arrow_drop_up 1 more_vert Congrats on the result!  And thanks for citing me ;) eagle4 Topic Author Posted 5 years ago · 3rd in this Competition arrow_drop_up 1 more_vert De rien, Oncle Cpmp! Your posts are really appreciated for their technicality or for their diplomacy. mavillan Posted 5 years ago · 1670th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for sharing @chabir ! What was the CV scheme you use? eagle4 Topic Author Posted 5 years ago · 3rd in this Competition arrow_drop_up 1 more_vert The great things because of the leaks is that you could afford to have use several CV scheme to diversify your models and use the leaky rows for ensembling. At the end, I settled with 3 folds unshuffled. rahul yadav Posted 5 years ago · 1574th in this Competition arrow_drop_up 1 more_vert Thanks for sharing. I think unsophisticated solution is more useful from learning perspective. Ailurophile Posted 5 years ago · 417th in this Competition arrow_drop_up 1 more_vert Congratulations!! Grear Write-Up Thanks for sharing your Approach & Insights!! @chabir YaGana Sheriff-Hussaini Posted 5 years ago · 72nd in this Competition arrow_drop_up 1 more_vert Congrats @chabir and thanks for sharing. Appreciation (2) Erkan Hatipoğlu Posted 5 years ago arrow_drop_up 1 more_vert Thanks for sharing @chabir Abhinand Posted 5 years ago · 2802nd in this Competition arrow_drop_up 1 more_vert Thanks for sharing, learned a lot. Too many requests error Too many requests",
      "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules 清水河小栗旬 · 4th in this Competition  · Posted 5 years ago arrow_drop_up 3 more_vert 4th Place Question According to the ranking published here , I got the 4th place. This is the first time that I have won a gold medal or even a bonus. I am very happy.But I'm a newbie and I don't know much about the following rules of PRIZE page ： `Because this competition is being hosted in coordination with the ASHRAE Organization and its Winter or Annual meetings in 2020, winners will be invited and strongly encouraged to attend the conference, contingent on review of solution and fulfillment of winners' obligations. Note that in addition to the standard Kaggle Winners' Obligations (open-source licensing requirements, solution packaging/delivery, presentation to host), the host team also asks that you create a short video (under 5 minutes) summarizing your solution.` Would you mind telling me what obligations I should fulfill and what should I do? Thank you very mych！ And 4th Place Solution is being prepared Please sign in to reply to this topic. comment 12 Comments Hotness Yiheng Wang Posted 5 years ago arrow_drop_up 1 more_vert Don't worry, a Kaggle staff will contact you directly via email and arrange the following things you need to do. 清水河小栗旬 Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 1 more_vert Yes， I got their email, thanks Orig1n Posted 5 years ago · 501st in this Competition arrow_drop_up 1 more_vert 看到你的ID就进来了，果然是同校大佬！恭喜恭喜~ 清水河小栗旬 Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 0 more_vert 运气好点而已😁 ，校友好啊 corochann Posted 5 years ago · 1758th in this Competition arrow_drop_up 1 more_vert Congrats! 清水河小栗旬 Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 0 more_vert Thank you very much, my best single model is based on your kernel JM100 Posted 5 years ago · 282nd in this Competition arrow_drop_up 2 more_vert I think you should wait for the official Private Leader Board. mezoganet Posted 5 years ago · 920th in this Competition arrow_drop_up 0 more_vert And where thies official Private Leader Board will be realeased ?? and moreover : how many decimals digits will be taken in account ? 清水河小栗旬 Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 0 more_vert thanks mezoganet Posted 5 years ago · 920th in this Competition arrow_drop_up -1 more_vert …winners will be invited and strongly encouraged to attend the conference, contingent on review of solution and fulfillment of winners' obligations… And do you mean you have to pay for the travel ??? from China ??? Beijing / Los angeles costs a lot of money, No ? 清水河小栗旬 Topic Author Posted 5 years ago · 4th in this Competition arrow_drop_up 0 more_vert Maybe it's voluntary Clayton Miller Competition Host Posted 5 years ago arrow_drop_up 0 more_vert Congratulations to our fourth-place winner!!!!!! 🙌 💪 👍 👊 The fourth-place solution will be organized and added to the other top five winners in a publication and open-source repository. Please stay tuned! Also, for everyone else, we are in the process of curating a directory of Notebooks for building performance engineers and analysts from the competition that will serve as a recipe book for beginners as well as experts who are curious to learn about certain topics. We highly encourage you all to share your solutions as a notebook for this purpose. We will acknowledge the authors of all notebooks in this process, so please annotate your notebook or profile with how you would like to be acknowledged. It would also be helpful if you could indicate whether you are an ASHRAE member. Too many requests error Too many requests",
      "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules TASSAN · 5th in this Competition  · Posted 5 years ago arrow_drop_up 22 more_vert 5th Place Solution First of all, we thank the kaggle and ASHRAE teams for holding the competition! I'm sorry my post was delayed due to my laziness. Our solution is not sophisticated and I think there is much room for improvement. (I want to apologize in advance, I don't use English in everyday life, so I'm not good at English) Below is an overview of our solution. Pre-proceeding We dropped rows such as Long streaks of constant values Zero target values (only electricity) By removing these data, the score was greatly improved. Feature Engineering We try two kinds of target encoding 1. percentile for each building_id, meter As shown in the figure, the 5th and 95th percentile of the target value was calculated for each building_id and meter, and we used these features. In our case, these features improved the score. 2. propotion For each building_id, we apply these process. Calculate median of target value per day of week. Calculate its proportion (see figure). This is an example of day of week. We also apply this technique to hour, day, and so on. Modeling Using only LightGBM(train for each meter) We apply two-step modeling Step1: Determine num_boost_round for each building_id Define training data(2016/01/15 ~ 2016/05/31) and validation data(2016/09/01 ~ 2016/12/31) Training with LightGBM and find early stopping round for each building_id(n1 ~ n1448). Step2: Train with all train (year 2016) data and predict test data Training with all train data and predict test data. The number of trees used for prediction were changed for each building_id (using n1~n1448 obtained in step1). This approach improved the public score, but the private score did not improve much. Ensemble Used leaked data(site 0,1,2,4,15). Weighted average for each meter and year(2017,2018). We also used other competitor's submission files. sub1: https://www.kaggle.com/purist1024/ashrae-simple-data-cleanup-lb-1-08-no-leaks sub2: https://www.kaggle.com/rohanrao/ashrae-half-and-half Submission After ensemble, we achieve 1.047 on public LB) / 1.236 on private LB (1.058 on public LB / 1.272 on private LB in our single model) If you have any questions, feel free to ask. Thank you for reading. Please sign in to reply to this topic. comment 12 Comments Hotness Muhammet İkbal Elek Posted 5 years ago arrow_drop_up 5 more_vert congrats,Thanks for solution method.Inspireful! TASSAN Topic Author Posted 5 years ago · 5th in this Competition arrow_drop_up 1 more_vert Thanks! Mr Loke Posted 5 years ago · 120th in this Competition arrow_drop_up 0 more_vert Any reason why you did not use June, July, August 2016 for training or validation data ? For step1, you have 1448 x 4 meter = 5792 models ? And for step2, you use back 5792 models from step1 but train using all 2016 data? TASSAN Topic Author Posted 5 years ago · 5th in this Competition arrow_drop_up 1 more_vert Thanks for your questions! Any reason why you did not use June, July, August 2016 for training or validation data ? → In this competition case, We must predict 2018 target values using only 2016 data(can't use 2017 data for train). In order to reflect such a situation in step 1, We didn't use June, July, and August data. For step1, you have 1448 x 4 meter = 5792 models ? And for step2, you use back 5792 models from step1 but train using all 2016 data? → No, I trained one model per meter (4 models in total in step1). Miyabon Posted 5 years ago · 336th in this Competition arrow_drop_up 0 more_vert Congrats & Thanks for sharing!!🎉 😄 👍 TASSAN Topic Author Posted 5 years ago · 5th in this Competition arrow_drop_up 0 more_vert Thanks! MPEG Posted 5 years ago arrow_drop_up 0 more_vert Congrats!! TASSAN Topic Author Posted 5 years ago · 5th in this Competition arrow_drop_up 0 more_vert Thanks!! YUYUTA Posted 5 years ago · 5th in this Competition arrow_drop_up 0 more_vert Congrats! This comment has been deleted. This comment has been deleted. TASSAN Topic Author Posted 5 years ago · 5th in this Competition arrow_drop_up 0 more_vert Thanks! Too many requests error Too many requests",
      "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules MPWARE · 9th in this Competition  · Posted 6 years ago arrow_drop_up 25 more_vert 9th place solution Finally nice competition! Not easy due to really noisy data but new lessons learnt again. Some insights of my solution that reached place #9 and gold. One of my objectives was to survive to shake-up. As soon as leaked data were discovered I decided to use it mainly for hold-out validation. I started training without any leak data, playing with features engineering, different models, different time CV folds and drove my work only with correlated CV + hold-out + LB results . It was a bit frustrating to have very low ranking with this strategy but I knew it should help at the end. On the 2 last week I included 2017 leak data in training and kept 2018 leak data for hold-out. My solution is ensemble (ridge regression) of several models : LightGBM (x7) CatBoost (x4) Neural Network with categories embedding and features standard normalization (x4) LiteMORT (x1) Features are quite simple , no model above 15 features, 12 as average: building_id , meter , site_id , primary_use , week_day , is_holiday square_feet , cloud coverage , precip_depth_1_h feels like temperature, building age, square_feet * floor_count air_temperature roll mean 24h, sea_level_pressure trend roll mean 24h air_temperature cooling degree per day, meter_reading median per building per meter per year Notice that some of my models are not using building_id to try to generalize better. A few models are per meter, others not. For each model CV I applied different time split, x3, x4 and x6 . Cleansing and imputation was important too . For weather data, I tried to find different external  sources to fill gaps but it did not give any boost mainly because we're not 100% sure of location of site_id and some data such as cloud coverage was not consistent with the ones in training data. Finally, I trained an additional simple LightGBM model to impute missing data based on provided data (similar to this kernel ). For meter reading, cleansing was not obvious, removing zero patterns (electricity, hot water in summer …) looked a good idea but one can notice that such pattern also appear in 2017/2018 leaked data. Some buildings could also have some renovations slots that would explain zero patterns. So each of my model had a zero-pattern drop strategy different ( site_id = 0 before May 2016 only, full drop, partial drop based on duration and/or on season). Post-processing : None and it was a mistake when I see top solutions. For final submission I selected the ones with best CV/Hold-Out/LB correlation and it was a good choice as they're the best score in Private LB! What did not work: Too many features lead to overfit especially with target encoding. Tree-based second level model for ensembling (overfit again) External data for weather (not in inline with provided data) Non time-split CV (auto-correlation) Thanks to organizers, Kaggle and competitors for this challenge! Please sign in to reply to this topic. comment 10 Comments Hotness eagle4 Posted 5 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Felicitations! MPWARE Topic Author Posted 5 years ago · 9th in this Competition arrow_drop_up 1 more_vert Merci, t'as fait mieux que moi 😃 Clayton Miller Competition Host Posted 5 years ago arrow_drop_up 1 more_vert Thank you very much for sharing! We would like to add your solution to our analysis of the top processes and models. Please let us know how you would like to be acknowledged. The default will be what you have on your profile. Thanks again! Also, for everyone else, we are in the process of curating a directory of Notebooks for building performance engineers and analysts from the competition that will serve as a recipe book for beginners as well as experts who are curious to learn about certain topics. We highly encourage you all to share your solutions as a notebook for this purpose. We will acknowledge the authors of all notebooks in this process, so please annotate your notebook or profile with how you would like to be acknowledged. It would also be helpful if you could indicate whether you are an ASHRAE member. MPWARE Topic Author Posted 5 years ago · 9th in this Competition arrow_drop_up 1 more_vert @claytonmiller Sure, you can add it and you can use info on my profile. I'm quite busy with another competition (deepfakes) so I won't add notebooks immediately. Clayton Miller Competition Host Posted 5 years ago arrow_drop_up 0 more_vert No worries -- add the kernels when you have a chance. Good luck in the Deep Fakes competition! YaGana Sheriff-Hussaini Posted 5 years ago · 72nd in this Competition arrow_drop_up 1 more_vert Congrats @mpware and thanks for sharing. corochann Posted 5 years ago · 1758th in this Competition arrow_drop_up 1 more_vert Congrats, thank you for sharing. MPWARE Topic Author Posted 5 years ago · 9th in this Competition arrow_drop_up 0 more_vert Thanks! And the private LB is official now! Ailurophile Posted 6 years ago · 417th in this Competition arrow_drop_up 1 more_vert Congratulations Thanks for sharing your  Approach & Insights!! @mpware Tim Yee Posted 6 years ago · 13th in this Competition arrow_drop_up 1 more_vert Nice work! Forgot all about trying model based imputation. Too many requests error Too many requests",
      "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules Tim Yee · 13th in this Competition  · Posted 6 years ago arrow_drop_up 25 more_vert 13th place gold solution Thanks to ASHRAE for providing this unclean dataset! Thanks to Kaggle for continuing and keeping the competition alive. Congrats to everyone who managed to survive the LB shakeup! Thanks to @rohanrao , @kailex , @nz0722 , @aitude , @purist1024 for your excellent notebooks which had direct impact on helping me achieve this outcome. Summary Final 2 submissions: equal weighted blend of the following plus some regularization. https://www.kaggle.com/purist1024/ashrae-simple-data-cleanup-lb-1-08-no-leaks https://www.kaggle.com/rohanrao/ashrae-half-and-half (credit for original work https://www.kaggle.com/kailex/ac-dc ) https://www.kaggle.com/aitude/ashrae-kfold-lightgbm-without-leak-1-08 https://www.kaggle.com/nz0722/aligned-timestamp-lgbm-by-meter-type public LB: 0.935 (1.032 w/o leak estimated) with aggressive regularization 0.80 and 0.91 public LB: 0.944 (1.039 w/o leak estimated) with conservative regularization 0.91 for all public LB: 0.950 (1.045 w/o leak) no tricks - I did not select for submission What worked Data Cleaning - garbage in, garbage out. This is probably the single most important aspect of the the competition. I did this manually by plotting heatmaps and also going into each building's meter to inspect the target meter readings if they looked reasonable. I also reverse engineered the heat map to show only zeros by added the following line: train_df = train_df.query('not (meter_reading != 0)') Regularization - (this is what I'm calling it, maybe some will call it postprocessing, coefficients, tricks, etc.) - multiplying by some value < 1.0. For aggressive regularization, I used two different values 0.80 for responsive meters and 0.91 for less responsive meters. I probed each site's meter individually by multiplying by 0.95 to start then as I went through all the meters, I noted which meter or meters dropped the public LB by 0.001, which I noted to be responsive to regularization). I got the idea from LANL Earthquake prediction . Fortunately, I was confident that the LB would not shake me down if I tried to overfit the LB this way. I ran some tests on the leaked sites 0,1,2,4,15 to test each site's meters effect to various values. 0.90-0.95 seemed fairly safe values to start out with when I began probing. I gained about 0.015 on LB through a series of 29 submissions, which took about 2 weeks. As my remaining submissions diminished, I ramped up the overall aggressiveness of regularization and began probing the responsive site's meters as a collective submission because there wasn't enough submissions to try all possible values. This gave between 0.004 - 0.005 reduction on Private LB score. Without any regularization, I would have ended up placing 35 on private LB, so this trick definitely gave my scores the extra kick to finish in gold. Feature Engineering - In the half-and-half model, I added a feature that grouped building_id , meter , weekday , hour and mean target encoded it using full train (after data cleaning). I got the idea from this unassuming kernel . In Kfold-lightgbm-without-leak-1-08 model, I added a feature that combined site and meter as a categorical with no mean encoding. I noticed that for some sites, the mean encoded bm_week_hour feature performed worse while others performed better, but overall, it seemed favorable. Validating using sites 0,1,2,4,15 - using actual test ground truths for various sites individually and together helped to monitor whether my experiments improved out of sample test data. Addition by Subtraction - removing certain features for certain models helped improve ground truth (GT) test validation as well as local cross validation (CV). For half-and-half model variant, I dropped site_id , sea_level_pressure , and precip_depth_1_hr . For kfold-lightgbm-without-leak-1-08 I dropped site_id , sea_level_pressure , wind_direction , wind_speed , year_built , floor_count . I removed holiday features for all existing models because they made CV and GT validation worse. For aligned-timestamp-lgbm-by-meter-type, I dropped all the lag3 features. Notes: very little hyper-parameter optimization was performed. Just very limited basic tuning. What didn't work Training using leaked test GT labels - training with GT from 2017-2018 did not improve out of sample site_id . For ex: training with site_id 0 did not improve validation scores for sites 1,2,4,15 dramatically. I only performed that one test and realized that adding a site_id to testing doesn't improve LB scores for out of site validation because each site_id is it's own microcosm and behaves different from other sites. A lot of feature engineering did not work including weather features. CatBoost/XGBoost/NN Embeddings/Linear Regression/Kernel Ridge Regression(KRR)/KNN all performed worse. I tried building 1449 (1 for each building) and 2380 (1 for each building's meter) Linear Regression, KRR, and LGBM models. I tried blending with Catboost and NN Embeddings models separately, but GT validation didn't seem to improve. Training with full year of training data and validating using sites 0,1,2,4,15 didn't seem to help much. Correcting site 0 meter 0 didn't help - discussion Some experiments I tried Using half-and-half methodology to split train into 2 separate halves - middle months (4-9)/ending months (1-3, 10-12) → performed worse. First half of the day 0-11, later half of the day 12-23. This split by hour in the day performed surprisingly well and GT validation had shown this, however I didn't select it for final submission blend. notebook L2 model - I stacked a model on top of individual model predictions (for the same folds), which only worked for half-and-half model. I left this out of final blend. I also tried ensembling through stacking final models using LGBM but there didn't seem to be any improvement in GT validation. What I didn't try Divide and Conquer notebook - I didn't bother with this notebook at all so I can't tell whether it was any good. Looking back now, I should have at least tried playing around with it. It is possible it could have helped reduce variance like @rohanrao mentioned. Different blending methods, different weights, etc. - I just kept it simple (introduced no additional complexity or bias) Making submission with certain models in final blend due to limited submissions. Namely I wanted to submit my blend with NN Embeddings, but based on GT validation, I filtered out a lot of potentially better performing blends. It is possible that sites 0,1,2,4,15 did not represent other sites well enough, but that was the trade off I had to make. notes: I mention these, because it may have been the difference for finishing closer in-the-money. What I learned I took my mean target encoding game to a new level with this competition. Before this, I was stuck at basic single categorical feature mean target encoding. Below is the code I used to do multi categorical feature mean encoding: bm_cols = ['building_id', 'meter', 'weekday', 'hour',] df_train['hour'] = df_train['timestamp'].dt.hour df_train['weekday'] = df_train['timestamp'].dt.weekday bm = df_train.groupby(bm_cols)['meter_reading'].mean().rename('bm_week_hour').to_frame() df_train = df_train.merge(bm, right_index=True, left_on=bm_cols, how='left') Final submissions used for blending half-and-half variant kfold-lightgbm-without-leak-1-08 variant aligned-timestamp-lgbm-by-meter-type variant Final thoughts It took me a year to finally reach a gold model. I was an absolute beginner when I started and I've learned a lot since joining Kaggle. I hope to continue competing and learning from all the bright and inspiring people on here. Although the kernels I blended with are all public, there were a lot of details that led to obtaining a gold model. Most notably, stable validation and logging each experiment meticulously so that after a few weeks or near the end, I wouldn't forget what led to improvements and what did not. Please sign in to reply to this topic. comment 15 Comments Hotness Clayton Miller Competition Host Posted 5 years ago arrow_drop_up 1 more_vert Thank you very much @teeyee314 for sharing! We would like to add your solution to our analysis of the top processes and models. Please let us know how you would like to be acknowledged. The default will be what you have on your profile. Thanks again! Also, for everyone else, we are in the process of curating a directory of Notebooks for building performance engineers and analysts from the competition that will serve as a recipe book for beginners as well as experts who are curious to learn about certain topics. We highly encourage you all to share your solutions (or any part of your process) as a notebook for this purpose. We will acknowledge the authors of all notebooks in this process, so please annotate your notebook or profile with how you would like to be acknowledged. We will also include general solutions and process information that is not in notebooks as well. It would also be helpful if you could indicate whether you are an ASHRAE member. Tim Yee Topic Author Posted 5 years ago · 13th in this Competition arrow_drop_up 1 more_vert Hi @claytonmiller My solutions have been made public under the Final submissions used for blending section. There's also a notebook under Some experiments I tried section that you may want to checkout at as well. Feel free to use my solutions and reference them any way you wish. I will make my blending/post processing notebook public when I have more time - I need to clean it up first. Clayton Miller Competition Host Posted 5 years ago arrow_drop_up 0 more_vert Hi Tim - thanks so much! Just get the others added when you get a chance Vopani Posted 6 years ago · 2nd in this Competition arrow_drop_up 2 more_vert Congrats @teeyee314 for your first gold! Divide and Conquer notebook - I didn't bother with this notebook at all so I can't tell whether it was any good. Looking back now, I should have at least tried playing around with it. It is possible it could have helped reduce variance like @rohanrao mentioned. Fun fact: Our best single model is a tuned version of the Divide and Conquer concept :-) Tim Yee Topic Author Posted 6 years ago · 13th in this Competition arrow_drop_up 0 more_vert @rohanrao Thanks! It was an oversight I regret :/. I came within striking distance of finishing in the money. Had I not made the oversight, I may well have squeezed a little more out of my scores. corochann Posted 5 years ago · 1758th in this Competition arrow_drop_up 0 more_vert Congrats! Robert Stockton Posted 6 years ago · 124th in this Competition arrow_drop_up 0 more_vert Thanks for the shout-out, and congratulations. I'm glad my kernel was able to be some help. Sandeep Kumar Posted 6 years ago · 216th in this Competition arrow_drop_up 0 more_vert Congrats @teeyee314 YaGana Sheriff-Hussaini Posted 6 years ago · 72nd in this Competition arrow_drop_up 0 more_vert Congrats @teeyee314 and thanks for sharing. LJH Posted 6 years ago · 45th in this Competition arrow_drop_up 0 more_vert @teeyee314 hi, I want to know your target encoding is as a new feature ? Or replace some features ？ Tim Yee Topic Author Posted 6 years ago · 13th in this Competition arrow_drop_up 0 more_vert New feature, however, always good idea to try all possibilities (sometimes its better to replace, sometimes its better to as new feature). You can play around with the feature in my notebook and watch the validation. LJH Posted 6 years ago · 45th in this Competition arrow_drop_up 0 more_vert ok,thx Hanjoon Choe Posted 6 years ago · 274th in this Competition arrow_drop_up 0 more_vert Congratulations! This comment has been deleted. This comment has been deleted. Too many requests error Too many requests",
      "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules I am deeply learning · 15th in this Competition  · Posted 5 years ago arrow_drop_up 10 more_vert 15th Place Solution Main difficulties in this competition： 1） Less submissions 2） Large amount of data - not memory friendly 3） Data leak dropped the results of the competition. Step 1：Self Examination 1） dividing the training set According to the time series relationships between the test set and the training set in the competition --- train set: 2016, test set: 2017, 2018 --- I divided the training set twice: train：first 6 months， val: last 6 months train：first 8 months， val: last 4 months Step 2：Fast data import and data size reduction 1) In this competition, participants often use ‘feather’ format to read data, getting noticeable fast importations. 2) The effort to reduce memory exists throughout the code. The two main ideas are: a) Allocate memory according to data size requirements. Do not use 16 bits if you can use 8 bits. b) Try to ensure that only the data required for the current calculation is retained in the memory. Do not load unnecessary data into the memory. After the calculation is completed, immediately release the memory. Step 3：Using leak data The leaked data is essentially equivalent to a part of the test set, so it can be used as a test set. 1) Direct training: increasing the amount of training data 2) Statistics, discovering uncontrollable situations such as power outages, abnormal weather (because it is time series data) 3) Used as a test set to evaluate the current model 4) Used to determine some hyperparameters for model fusion 5) Submit as a result Technique Summary： We used 19 features in this competition： Categorical features： 'weekend', 'hour', 'meter', \"site_id\" Numerical features： \"building_id\", \"primary_use\"，\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\", \"dew_temperature\", 'building_mean', 'site_mean', \"wind_direction\", \"wind_speed\", \"precip_depth_1_hr\", 'building_square_mean', 'floor_count', 'site_hour' building_mean: Count the total number of each group after grouping according to building_id (not sum). Site_mean: Concating site_id, meter, primary_use and then counting the total count of each group after grouping. wind_direction: Convert wind direction to ‘cos’, otherwise it will have an adverse effect on the result, because 0 and 360 coexist. Building_square_mean: After grouping by building_id, we get the log of the energy consumption of each building (the number is too large). Then we divided that by square_feet to get a ranking of the index of similar buildings' energy consumption capabilities. Site_hour：concat site_id and hour. With such grouping, we get the ‘log’ for the sum of energy consumption (number too large). We get ranking of the index of regional temperature’s impact on energy consumption. From all given features, remove sea_level_pressure.Based on experimental results, make np.log1p transformations on some features, mainly square_feet. Model：lightGBM Prameter： params = { 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'learning_rate': 0.1, 'num_leaves': 2**8, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'subsample_freq': 5, \"reg_lambda\": 2, 'reg_alpha': 1, 'seed': 0, 'early_stopping_rounds': 20 } Kfold：group - first / second half of year；explanation：The temperature distribution is similar and relatively balanced. Later, someone tested the stratified k-fold and got good results: The key of using k-fold is to ensure the similarity of the data. The two are too similar and boring. If they are too different, it does not matter at all, so try to grasp Data： Training set used all the leaked data 1）Discard site15 for its low data accuracy 2)  selectively discards some data from site4 based on calculated distance from 2016 Discard anomalies： 1） We delete all data of site0 under meter = 0 before 2016-05-20 (from a data analysis perspective, these are all noise) 2） We delete all cases where the meter reads 0 because we don't think a building will have the meter moving. 3） We delete the case where meter is huge in building_id = 1099. At the end, we still feel that there are a lot of noise points in the data, because removing many zero-value points will get very good results <0.8, but overfitting is more serious because there are more zero-value points note： Due to the unit inconsistency problem at site0, we converted units. The fact is that its effect is extremely limited (difficult to observe), but it has a limited effect when using kfold, so we keep it. Please sign in to reply to this topic. comment 0 Comments Hotness Too many requests error Too many requests",
      "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules Thomas Yokota · 17th in this Competition  · Posted 6 years ago arrow_drop_up 12 more_vert Sharing what helped It was unfortunate that this competition was plagued by leaks, but it was pretty much a straight-forward energy forecasting problem. I learned a bit about time-series CV, which was my reason for joining. With that said, here are things that I’ve added to my approach. In retrospect, I could have data cleaned better with the non-electricity meters, but live and learn. weather Weather is known to be one of the strongest drivers of energy consumption. With that said, typical approaches have been to lag and/or shift weather. Recent literature on energy forecasting have explored exponential weighted moving averages; this helped my model extremely. trend Adding a trend feature in forecasting tends to usually help than not. In this case, I added one that mimicked the winning solutions from GEFCOM. Had there been more than one year of data, I would have included an intra-year trend as well. 2-phase CV I replicated CPMP’s 2-phase for training and predicting. Some things that I wished I tried: partially-escaping seasonality by developing models at different subsets of time; in my work, I develop models by hour. This helps to escape auto-correlation issues. data-clean: it was obvious the distributions for non-electricity were skewed due to 0 readings. I saw on the last night of competition that certain meters were 0 imputed while the corresponding day/time were missing in the electricity profile. hedge leaks better. I was torn with leveraging leaks because it took away the task of forecasting energy in a real-world setting; imagine trying to explain this method to a public utilities commission. My intent was to learn something to bring back to my job. With that said, I was still happy with my approach in that most of the methods I used would have passed the PUC sniff test and it placed quite high on the leader board. Given the close spread of the RMSE towards the top, it was good to know that I didn't have to rely much on gimmicky methods to obtain a decent forecast. In this case, domain knowledge came in handy. I also want to give a huge shout-out to @cpmpml . I learned a lot from him over the years in regards to forecasting, and my successes in my job were stemmed from him. Also, @mmotoki for pushing me to do better. I never appreciated what it meant to have a rival, but my successes on Kaggle wouldn’t have happened it it weren’t for him. he deserves number 1 given his crazy work ethics; thanks for teaching me to push myself. UPDATE (for DietHard): Nothing special: # holidays from datetime import datetime, date, timedelta from pandas.tseries.holiday import Holiday, AbstractHolidayCalendar, nearest_workday, MO, TU, WE, TH, FR from pandas.tseries.holiday import USMartinLutherKingJr, USPresidentsDay, GoodFriday, USMemorialDay, USLaborDay, USThanksgivingDay from pandas import DateOffset, Series, Timestamp, date_range\n\ndef daterange(date1, date2): for n in range(int ((date2 - date1).days)+1):\n        yield date1 + timedelta(n)\n\n\nclass Holiday(AbstractHolidayCalendar):\n    rules = [\n        Holiday( \"New Year's Day -1\" , month =12, day =31),\n        Holiday( \"New Year's Day\" , month =1, day =1), #         USMartinLutherKingJr, #         USPresidentsDay, #         USMemorialDay, Holiday( 'Independence Day' , month =7, day =4, observance =nearest_workday),\n        USLaborDay, #         Holiday(\"Veteran's Day\", month=11, day=11), USThanksgivingDay,    \n        Holiday( \"Christmas Eve\" , month =12, day =24),\n        Holiday( 'Christmas' , month =12, day =25, observance =nearest_workday),\n        Holiday( \"Boxing Day\" , month =12, day =26),\n    ]\n\ndef create_holidays(year_start, year_end):    \n    cal = Holiday()\n    holidays = [] for i, year in enumerate(range(year_start, year_end+1)):\n        # default holidays\n        holiday_dates = cal.holidays( start =date(year,1,1), end =date(year,12,31)) for holiday in holiday_dates:\n            holidays.append(holiday.date())\n    return holidays\n\nholidays = create_holidays(2016, 2018)\ndf[ 'is_holiday' ] = df[ 'timestamp' ].isin(holidays) * 1 # exponential weighted moving average ewma = pd.Series.ewm\nweather[ 'air_temperature_ewma_day' ] = ewma(weather[ 'air_temperature' ], span =24, adjust = True ).mean()\nweather[ 'air_temperature_ewma_week' ] = ewma(weather[ 'air_temperature' ], span =24*7, adjust = True ).mean()\nweather[ 'air_temperature_ewma_month' ] = ewma(weather[ 'air_temperature' ], span =24*30, adjust = True ).mean() # time trend time_trend = pd.DataFrame({ 'dt' : pd.date_range(config[ 'timeframe' ][ 'start' ], config[ 'timeframe' ][ 'end' ], freq = 'H' )})\ntime_trend[ 'time_trend' ] = time_trend[ \"time_trend\" ] = range(0, 0+len(time_trend))\ntime_trend_dict = create_dictionary(time_trend[ \"dt\" ], time_trend[ 'time_trend' ])\ndf[ 'time_trend' ] = df[ 'timestamp' ].map(time_trend_dict)\ndf[ 'time_trend' ] = df[ 'time_trend' ].astype(np.float16) content_copy Please sign in to reply to this topic. comment 7 Comments Hotness Clayton Miller Competition Host Posted 5 years ago arrow_drop_up 0 more_vert Thank you very much for sharing! We would like to add your solution to our analysis of the top processes and models. Please let us know how you would like to be acknowledged. The default will be what you have on your profile. Thanks again! Also, for everyone else, we are in the process of curating a directory of Notebooks for building performance engineers and analysts from the competition that will serve as a recipe book for beginners as well as experts who are curious to learn about certain topics. We highly encourage you all to share your solutions (or any part of your process) as a notebook for this purpose. We will acknowledge the authors of all notebooks in this process, so please annotate your notebook or profile with how you would like to be acknowledged. We will also include general solutions and process information that is not in notebooks as well. It would also be helpful if you could indicate whether you are an ASHRAE member. Gunes Evitan Posted 6 years ago · 378th in this Competition arrow_drop_up 0 more_vert Thanks for sharing. What was your trend features? Thomas Yokota Topic Author Posted 6 years ago · 17th in this Competition arrow_drop_up 0 more_vert Basically, an incremental of 1 over the train+test span, over the total time points. This was a method that was revealed in the GEFCOM 2017 winning solution. In addition to this approach, there are ways to extract trend and seasonality prior to model fitting and adding it back in such as Dr. Smyl's approach in M4, which seems promising, and is something else I plan to explore further in my real work. DietHard Posted 6 years ago · 1662nd in this Competition arrow_drop_up 0 more_vert Hi! I also did exponential weighted moving averages (something that I grabbed from stock-market forecasting) and results were not very good. If this worked for you I'd say that it does in combination of another strategy… Could you elaborate more on your first item there? models by hour? escape auto-correlation? Thanks! Thomas Yokota Topic Author Posted 6 years ago · 17th in this Competition arrow_drop_up 0 more_vert I performed EMWA across different windows of typical seasonality seen in load forecasting such as 24 hour, week, etc. I selected the \"best\" using the train-test feature selection shared in the IEEE competition. There were a lot of good ideas shared in that competition including timeseries  CV. In regards to escaping auto-correlation, this idea was actually proposed by Itron's forecasters. This approach tends to do better because the target distribution is tighter; especially, when your daily load profiles start to swing more with photovoltaic. This was also something that was used by Dr. Smyl in GEFCOM 2017. The only issue personally was the amount of time needed and the company I'm working at is currently going through an intense planning cycle so I didn't have much time to explore much ideas. I basically spent about a week on this competition, which made me hesitant to even participate. Energy forecasting literature is pretty rich thanks to data science. I wouldn't be surprised if better approaches are introduced within the next couple of years. With that said, the company I'm working for is going to be looking for 2 forecasters if anyone is interested in this work. Just look for jobs at Hawaii's largest utilities company ;) DietHard Posted 6 years ago · 1662nd in this Competition arrow_drop_up 0 more_vert Well, this is my first competition and pretty much used it to learn something, and boy I did. Took me some googling but I found some of the references you are talking about (had no clue about GEFCOM nor any of those people you mentioned) and it looks like I'm gonna have something to read now. Thanks for sharing! By the way, would be very interested in looking at your full solution if you are ever willing to publish it. I work in the field and despite competition results, all of this helps my job as well. Thomas Yokota Topic Author Posted 5 years ago · 17th in this Competition arrow_drop_up 1 more_vert I've posted the code I used to create said features. BTW, if you don't mind, where do you work? I'm always interested in meeting people in the industry. Too many requests error Too many requests",
      "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules Volodymyr · 20th in this Competition  · Posted 6 years ago arrow_drop_up 18 more_vert 20 Private LB rank solution It was a long run and finally it is completed and we can share some tricks and ideas that we have used. According to this official discussion and this Kernel [ods.ai]PowerRangers has taken 20 Private LB rank. So let's start :) Sorry for all misspelling and not really good code style in Kernels and here too ( LGBT-> LGBM, Bland-> Blend, …). We did not have enough time to write really good code base :( Our team had 2 main obstacles: Time (The exams were coming) Computing resources As for computing resources - we carried out all our experiments and development in Kaggle Kernels. So we had to deal with RAM and execution time limitations. Our main scheme: First of all, we created some Baseline models: We started from statistics - simple mean or median over each meter . Then same statistics over several categorical features ( meter , day_of_week , building_id , month ). You can find it in this here NaiveMeanModel . Of course, they were performing really poor (1.39-1.4 on public LB). But we used these feature for more sophisticated models. Then we tried RandomForestRegressor . But it was pretty slow and perform not really good. Finally we realized that So we made our first experiments with LGBM and passed into silver zone And now it was time for PREPROCESSING! Preprocessing: First off all - detecting outliers in nearly the same way, as in this discussion , but less sophisticated. Weather data preprocessing - interpolation on NaNs and creating features is_nan for these columns Adding features: day , day_of_week , month ,  rolling features ( {feature}_mean_lag{window} ), max/min features by categorical features ( air_temperature_max ) Finally we tried to add Leak data from 2017-2018 but it was not a good idea for us. But adding leak data from 2016 gave us some more data for training You can have more detailed look at our Data Preprocessing and Feature engineering: Preprocessing Preprocessing + Feature Engineering + Leaks from 2016 Preprocessing + Leaks from 2016-2018 ) . Here we created several datasets, taking leaks from different sites in order to train uncorrelated models. Also most ideas ( and code :) ) were taken from these kernels: first The second one was deleted Now was time for modelling! Models: 1. Of course, LGBM: We are nor really professionals in LGBM hyperparams optimization + it was training really long, so we did not spend a lot of time for hyperparams optimization. But what we find out is that it was not overfitting much and increasing the number of leaves mostly helped the model (we tried 145 and 82) Even with high LR our Boost has not converged even for 7k iterations. But Leak scores and Public LB scores did not really improve for Boost trained for 7k, comparing with boost on 5k. Also it was a great difference for Boost trained for 3k. We could not try more iterations, cause Kernel has time limitations :( Also we had 5 kernels for one Booost in order to train it in CV style and then blend results. One more interesting fact is that - all DataFrames that passed to LGBM model are converted to float64 and if you have real BIG DATA, you will RUN OUT OF MEMORY, so you need to convert it into np.array . Small tip but it helped us a lot :) You can take a look at our Boost models here: with leaked data from train(2016) with leaked data from train(2016) and test(2017-2018) without leaked data 2. Neural Net ( going out from forest!!! ) Kernel Inspired by abazdyrev and his Kernel First of all, usual preprocessing for NN: scaling and label encoding for Embeddings. You can find it in PreprocessingUnit So we have chosen Embeddings for categorical features, because we have really a lot of data and they could train ( I hope ) Also we tried several optimizers (Nadam, Adam, Adamax) and as for final activations ( no-activation and softplus). Best results were for softplus and Adamax. We did not try to submit NN predictions alone but on Leak Validation it even outperformed LGBM ( WOW!!! ). And finallly we are ready for the most interesting part - BLEEEEEEEEEND We were inspired by this kernel , but it was too simple for us :) Kernel Firstly we gathered a lot of submissions  ( our and some public ones ), finally we had 32 .csv files Some public kernels and some our submissions were created with models trained on Leak data , so we can not use them for Leak Validation and they were excluded. Also we exclude some bad submissions. You can find out them in EXCLUDE_LIST Then we have to choose several submissions for BLENDING: Firstly, we tried Hyperopt on indexes for median blending. We have taken 10 submissions. Also we penalized Hyperopt for taking same files for blending Secondly, we have created some kind of Genetic Algorithm for the same purpose. Mostly it was taken from our practical work from the university. Thirdly, we tried to stack submissions with one layer Perceptron in CV style but it was hardly overfitting, so we did not try it more. Finally all that Leaked data was added to our final submission. Kernel Also add some less valuable Kernels to build the complete scheme: Blending NN trained on different Folds Blend LGBM (PP + Leaks Train + FE) on different folds Blend LGBM on different folds Blend LGBM (trained on train + leaked data) on different folds Leak Aggregator Great thanks to my team members: Evgeniy and Vladislav . They made a real good job!!! Good Kaggling and happy Holidays !!! Please sign in to reply to this topic. comment 1 Comment Hotness This comment has been deleted. Too many requests error Too many requests",
      "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules Georgi Pamukov · 25th in this Competition  · Posted 5 years ago arrow_drop_up 11 more_vert 25th Place Solution Hey Dear Kagglers :) First of all, congratulations to all winners and participants! And many thanks to all contributors for their amazing kernels and topics! Wanted to put my 2 cents in - so you can find a brief overview of my solution below. Motivation Started work on the competition quite late - just two weeks before the end and my main motivation was to experiment around the use of deep learning with tabular data (and for this kind of problems). Architecture Like probably most of the participants I ended up with 2 layer learning architecture: base layer and ensemble. Base layer: Feature engineering Because of my late start, I didn't really have the time for something fancy (FE is usually my main focus. Not this time though 😄 ). I ended up with 4 different datasets. On all, I applied cleansing (filtered out the bad rows) as per this great kernel (please upvote: https://www.kaggle.com/purist1024/ashrae-simple-data-cleanup-lb-1-08-no-leaks) . Used interpolation to fill in the missing values (on all), added \"is_missing\" features (to one of the sets). In addition generated some: weather-based features (aggregates/grouping, lag/rolling (moving averages/max/min etc)) time-based features (including holidays for one of the sets) target encodings (in one of the sets). I draw inspiration from many great kernels in the process - check the list below and please go and upvote the amazing work of the authors - they deserve it! Models My main focus was here and my goal was to have as diverse models as possible. I did NOT touche the leak data in any of the models of the base layer. Trained a total of 19 models. Amongst those: Deep NNs As I mentioned my goal was mostly to play around these - so I put significant effort here. My best NNs didn't disappoint - they were approaching the performance of the best public LGBMs (1.09 LB) - but of course, had a very different \"point of view\" on the problem. That made them extremely useful in the final ensemble, and to that, I mostly attribute my good final score. Some things that worked great here: entity embeddings for the categorical variables Radam optimizer (that came as a bit of surprise - Radam didn't appear to make a huge difference for me on CV competitions - but here it showed significant advantage. Maybe the size of the network, or the complexity of the problem … will dig deeper into that for sure) Adam with CyclicLR + ReduceLROnPlateau schedulers + longer training weather lag features + (few) time features Things that didn't work: more features (not surprised) \"is_missing\" features (kind of surprised) In short - very happy with my findings. Deep NNs will definitely be part of my considerations in future tabular data competitions. LGBMs (of course 😄 ): models per site, meter, half-half and on all data - trained on different datasets. my best scoring model was per site (performing significantly better than the best public models (1.072 LB)) L1, L2 regression models - just to have one more \"opinion\": per site and all data made sense for this more linear kind of a problem. Ensemble The second major thing that boosted my score. Based on my previous experience, stacking works better than blending when testing data is not cardinally different (and that can be argued of course). (NOT) very small disclaimer here: this is not valid in all cases well… how similar is that test data really… how proper your base layer is - is it diverse enough? Are you overfitting there already? Do you introduce leak in the meta model training data if use diverse validation/oof prediction schemes for the base models? there are also many other (important) variables - like what algorithm will be used for the meta learner, how it will be tuned, validated etc. very risky approach in general (overfitting is BIG concern…). Still … it worked well for me in the past. And worked this time as well. Let me give you some numbers: my best stack: 1.245 (PL) my best blend: 1.270 (PL) What worked well here: simple stuff (GLM, Gaussian, 5-fold CV, some regularization, adding meter to the training set). Performed very well for me also in the past (Elo Merchant Category Recommendation competition for example). What didn't work that great: more sophisticated stuff - Random Forest and even XGB, LGBM. All tend to overfit on my base data… These are pretty much the points, I want to make. One more time - please spend a minute to upvote the amazing work of the contributors below. And upvote this, in case you find it useful 😄 Cheers folks and \"see\" you on the next competition. Great kernels that deserve your vote: https://www.kaggle.com/rohanrao/ashrae-divide-and-conquer https://www.kaggle.com/corochann/ashrae-training-lgbm-by-meter-type https://www.kaggle.com/isaienkov/keras-nn-with-embeddings-for-cat-features-1-15 https://www.kaggle.com/roydatascience/ashrae-energy-prediction-using-stratified-kfold https://www.kaggle.com/iwatatakuya/ashrae-kfold-lightgbm-without-building-id https://www.kaggle.com/kailex/ac-dc https://www.kaggle.com/rohanrao/ashrae-half-and-half https://www.kaggle.com/nz0722/aligned-timestamp-lgbm-by-meter-type https://www.kaggle.com/aitude/ashrae-kfold-lightgbm-without-leak-1-08 https://www.kaggle.com/mimoudata/ashrae-lightgbm-without-leak https://www.kaggle.com/starl1ght/ashrae-stacked-regression-lasso-ridge-lgbm https://www.kaggle.com/tunguz/ashrae-histgradientboosting https://www.kaggle.com/yamsam/ashrae-leak-data-station Please sign in to reply to this topic. comment 6 Comments Hotness Ailurophile Posted 5 years ago · 417th in this Competition arrow_drop_up 1 more_vert Congratulations Great Write-Up!! Thanks for sharing your Approach & Insights!! @gpamoukoff Clayton Miller Competition Host Posted 5 years ago arrow_drop_up 0 more_vert Thank you very much for sharing! We would like to add your solution to our analysis of the top processes and models. Please let us know how you would like to be acknowledged. The default will be what you have on your profile. Thanks again! Also, for everyone else, we are in the process of curating a directory of Notebooks for building performance engineers and analysts from the competition that will serve as a recipe book for beginners as well as experts who are curious to learn about certain topics. We highly encourage you all to share your solutions (or any part of your process) as a notebook for this purpose. We will acknowledge the authors of all notebooks in this process, so please annotate your notebook or profile with how you would like to be acknowledged. We will also include general solutions and process information that is not in notebooks as well. It would also be helpful if you could indicate whether you are an ASHRAE member. Georgi Pamukov Topic Author Posted 5 years ago · 25th in this Competition arrow_drop_up 1 more_vert Hey @claytonmiller yes I'll be happy to share the code with you - and even happier if you find some use of it (I would always support good cause like this). My entire solution is in Kaggle kernels - and it is also using output from 3-4 public kernels (like leak data station for example). Please let me know on how would you want to deliver the code (or you have access to it already?). Cheers 3 more replies arrow_drop_down Too many requests error Too many requests",
      "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules Fernando Wittmann · 46th in this Competition  · Posted 6 years ago arrow_drop_up 30 more_vert [46th Pvt LB] 18th Public LB - Very simple solution Kernel: https://www.kaggle.com/wittmannf/0-939-lb-public-blend-leak-valid-ridgecv/ Most of my time was actually spent on models that turned out to not score as good as the public kernels (NNs and bldg based models). My 18th is thanks to some side experimentation when trying to maximize the tips that have been provided in public Kernels and discussion. The solution consists of a blend of non-leaked public submissions (I enhanced some of them) using Ridge CV's coefficients when fitting against the leak validation data. For the enhanced models, I discovered that it was more effective to combine them using RidgeCV's coeffs from leak validation data than just averaging them. However, I only had time to that in the simple-data-cleanup-3-models . My public score was 0.939 with leak replacement and 1.04ish without leak replacement. And here's something interesting: RidgeCV figured out negative coefficient for some submissions, although the sum of all of them was close to 1 (as expected): ## Ridge Coefficients Sum of coefficients: 0 . 9994466580815099 half -half-drop-rows-stratify-weekday has weight 0 . 14 simple -data-cleanup- 3 -models has weight 0 . 26 ashrae -kfold-lightgbm-without-leak- 1 - 08 has weight - 0 . 23 another - 1 - 08 -lb-no-leak has weight - 0 . 46 ashrae -kfold-lightgbm-without-building-id has weight 0 . 19 ashrae -energy-prediction-using-stratified-kfold has weight 0 . 52 ashrae -lightgbm-without-leak has weight - 0 . 15 ashrae -stratified-kfold-lightgbm has weight 0 . 23 ashrae - 2 -lightgbm-without-leak-data has weight 0 . 50 content_copy All credits go to the authors of the public kernels and @yamsam 's idea of leak validation. My contribution was the idea of combining them using RidgeCV. UPDATE As mentioned in this discussion , the following kernel would've ranked between 13th and 16th position of private leaderboard: https://www.kaggle.com/wittmannf/blender-leak-validation-inception?scriptVersionId=25306647 The only difference is that I performed a second level in the blending process ( submission.csv is the output from the first blend): submission_paths = [ '/kaggle/input/ashrae-half-and-half-w-drop-rows-stratify/submission.csv' , #1.106 '/kaggle/input/ashrae-simple-data-cleanup-lb-1-08-no-leaks/submission.csv' , '/kaggle/input/ashrae-kfold-lightgbm-without-leak-1-08/submission.csv' , '/kaggle/input/another-1-08-lb-no-leak/fe2_lgbm.csv' , '/kaggle/input/ashrae-kfold-lightgbm-without-building-id/submission.csv' , #1.098 '/kaggle/input/ashrae-energy-prediction-using-stratified-kfold/fe2_lgbm.csv' , #1.074 '/kaggle/input/ashrae-lightgbm-without-leak/submission.csv' , #1.082 '/kaggle/input/ashrae-stratified-kfold-lightgbm/submission.csv' , #1.075 './submission.csv' #1.04 ] content_copy Please sign in to reply to this topic. comment 22 Comments 2 appreciation  comments Hotness Isamu Posted 6 years ago · 1st in this Competition arrow_drop_up 3 more_vert Thanks for sharing!! I didn't realize RidgeCV could work. Fernando Wittmann Topic Author Posted 5 years ago · 46th in this Competition arrow_drop_up 0 more_vert You are welcome! And congrats on the 1st place!! Mimou Posted 6 years ago · 1714th in this Competition arrow_drop_up 3 more_vert Thanks for sharing! (2 Kernels from the ones you used are mine 😋 ) Fernando Wittmann Topic Author Posted 5 years ago · 46th in this Competition arrow_drop_up 0 more_vert Thanks @mimoudata ! They were very helpful! :) Tim Yee Posted 6 years ago · 13th in this Competition arrow_drop_up 1 more_vert nice solution Vopani Posted 6 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Thanks for sharing. The simplest of ideas are often the hardest to find. Fernando Wittmann Topic Author Posted 5 years ago · 46th in this Competition arrow_drop_up 0 more_vert Thanks! and congrats on the 2nd place! Luca Massaron Posted 6 years ago · 173rd in this Competition arrow_drop_up 1 more_vert We also found out that NNs and building based models didn't work as expected :-) Your blending solution is quite interesting, we didn't try using ridgeCV because we were afraid of negative coefficients, but they could have their sense, if they work out smoothing certain predictions as your strategy demonstrates. What was your C parameters in the end? Fernando Wittmann Topic Author Posted 5 years ago · 46th in this Competition arrow_drop_up 1 more_vert Hey @lucamassaron , sorry for the delay to reply you! Yeah, turns out negative coeffs actually help. This is something I wasn't expecting. I am quite curious on checking in different datasets. Regarding the C value, next time I run the kernel I will check! But my guess is that gridsearchCV chose a small C value since the amplitude of the weights is not that high. Caesar Lupum Posted 6 years ago · 733rd in this Competition arrow_drop_up 1 more_vert Thanks for sharing @wittmannf ! Congrats ! Wu_Yiqun Posted 6 years ago · 21st in this Competition arrow_drop_up 1 more_vert My solution is very similar wtih yours,hope that we don't have over-fitted. Fernando Wittmann Topic Author Posted 5 years ago · 46th in this Competition arrow_drop_up 0 more_vert Congrats on the 21st place @decarmelo ! And we can confirm that we didn't overfit (maybe mine did a little). But I noticed that your LB position actually increased!! Manraj Singh Posted 6 years ago · 370th in this Competition arrow_drop_up 1 more_vert You used the leak data for validation which isn't in the private lb, don't you think you might have over-fitted? Ahmet Erdem Posted 6 years ago · 378th in this Competition arrow_drop_up 1 more_vert It is a tiny transfer learning. I don't think it causes any overfitting with such a simple model and big data. If we all don't shake-up due to seasonality, I don't think he will shake-up. Good idea @wittmannf ! I was going to do the same but didn't expect public kernels to be that much valuable, therefore didn't invest time on it. 3 more replies arrow_drop_down Hanjoon Choe Posted 6 years ago · 274th in this Competition arrow_drop_up -1 more_vert I think this competition is about blending of blendIng of blending of … in my impression. XD. Clayton Miller Competition Host Posted 5 years ago arrow_drop_up 0 more_vert Thank you very much @wittmannf for sharing! We would like to add your solution to our analysis of the top processes and models. Please let us know how you would like to be acknowledged. The default will be what you have on your profile. Thanks again! Also, for everyone else, we are in the process of curating a directory of Notebooks for building performance engineers and analysts from the competition that will serve as a recipe book for beginners as well as experts who are curious to learn about certain topics. We highly encourage you all to share your solutions (or any part of your process) as a notebook for this purpose. We will acknowledge the authors of all notebooks in this process, so please annotate your notebook or profile with how you would like to be acknowledged. We will also include general solutions and process information that is not in notebooks as well. It would also be helpful if you could indicate whether you are an ASHRAE member. Fernando Wittmann Topic Author Posted 5 years ago · 46th in this Competition arrow_drop_up 1 more_vert Thanks @claytonmiller ! It's okay to use what's on my profile Appreciation (2) corochann Posted 5 years ago · 1758th in this Competition arrow_drop_up 1 more_vert Thanks for sharing YaGana Sheriff-Hussaini Posted 6 years ago · 72nd in this Competition arrow_drop_up 1 more_vert @wittmannf , thanks for sharing. Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events How much energy will a building consume? Assessing the value of energy efficiency improvements can be challenging as there's no way to truly know how much energy a building would have used without the improvements. The best we can do is to build counterfactual models. Once a building is overhauled the new (lower) energy consumption is compared against modeled values for the original building to calculate the savings from the retrofit. More accurate models could support better market incentives and enable lower cost financing. This competition challenges you to build these counterfactual models across four energy types based on historic usage rates and observed weather. The dataset includes three years of hourly meter readings from over one thousand buildings at several different sites around the world. Weather data from a meteorological station as close as possible to the site. The submission files use row numbers for ID codes in order to save space on the file uploads. test.csv has no feature data; it exists so you can get your predictions into the correct order. A valid sample submission. 6 files 2.61 GB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 2.61 GB building_metadata.csv sample_submission.csv test.csv train.csv weather_test.csv weather_train.csv 6 files 34 columns  Too many requests",
    "data_description": "ASHRAE - Great Energy Predictor III | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. ASHRAE · Featured Prediction Competition · 6 years ago Late Submission more_horiz ASHRAE - Great Energy Predictor III How much energy will a building consume? ASHRAE - Great Energy Predictor III Overview Data Code Models Discussion Leaderboard Rules Overview Start Oct 15, 2019 Close Dec 20, 2019 Merger & Entry Description link keyboard_arrow_up Q: How much does it cost to cool a skyscraper in the summer? A: A lot! And not just in dollars, but in environmental impact. Thankfully, significant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, are the improvements working? That’s where you come in. Under pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model. Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don’t work with different building types. In this competition, you’ll develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe. With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies. About the Host Founded in 1894, ASHRAE serves to advance the arts and sciences of heating, ventilation, air conditioning refrigeration and their allied fields. ASHRAE members represent building system design and industrial process professionals around the world. With over 54,000 members serving in 132 countries, ASHRAE supports research, standards writing, publishing and continuing education - shaping tomorrow’s built environment today. Banner photo by Federico Beccari on Unsplash Evaluation link keyboard_arrow_up Evaluation Metric The evaluation metric for this competition is Root Mean Squared Logarithmic Error. The RMSLE is calculated as ϵ = √ 1 n n ∑ i = 1 ( log ( p i + 1 ) − log ( a i + 1 ) ) 2 Where: \\\\(\\epsilon\\\\) is the RMSLE value (score) \\\\(n\\\\) is the total number of observations in the (public/private) data set, \\\\(p_i\\\\) is your prediction of target, and \\\\(a_i\\\\) is the actual target for \\\\(i\\\\). \\\\(\\log(x)\\\\) is the natural logarithm of \\\\(x\\\\) Note that not all rows will necessarily be scored. Notebook Submissions You can make submissions directly from Kaggle Notebooks. By adding your teammates as collaborators on a notebook, you can share and edit code privately with them. Submission File For each id in the test set, you must predict the target variable. The file should contain a header and have the following format: id, meter_reading 0, 0 1, 0 2, 0 etc. content_copy Prizes link keyboard_arrow_up 1st place - $10,000 2nd place - $7,000 3rd place - $5,000 4th place - $2,000 5th place - $1,000 Because this competition is being hosted in coordination with the ASHRAE Organization and its Winter or Annual meetings in 2020, winners will be invited and strongly encouraged to attend the conference, contingent on review of solution and fulfillment of winners' obligations. Note that in addition to the standard Kaggle Winners' Obligations (open-source licensing requirements, solution packaging/delivery, presentation to host), the host team also asks that you create a short video (under 5 minutes) summarizing your solution. Timeline link keyboard_arrow_up December 12, 2019 - Team Merger deadline. This is the last day participants may join or merge teams. December 12, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete. December 19, 2019 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prior Competitions link keyboard_arrow_up History of Great Building Energy Predictor Shootout I and II Competitions ASHRAE has previously hosted two data competitions, called the \"Great Building Energy Predictor Shootout I\" (1993) and \"Great Building Energy Predictor Shootout II\" (1994). If you are interested in seeing how the field has changed over the years, we have rehosted a copy of the first competition here with a leaderboard . It's entirely for fun and does not award points, medals, etc. For both of these competitions, participants were asked to develop empirical models for predicting building energy data from data sets and compare how those models could be used to forecast energy usage (Shootout I) and calculate energy conservation retrofit savings (Shootout II). The 1994 competition asked participants to retrieve the competition training data set via an FTP  server.  Teams were required to submit their empirical models along with predictions via floppy disks. The submitted package included predictions of energy savings and a sufficient explanation of how the specific method (calculation method, data removal, etc.) was applied. Submissions using black-box, or proprietary methods, were disqualified.  A combination accuracy metric of CV-RMSE (Coefficient of Root Mean Square Error) and MBE (Mean Bias Error) was used to evaluate prediction accuracy. While the 1994 competition awarded no monetary prizes, over 150 teams competed.  Six winning teams were formally recognized,  all participants were asked to write an ASHRAE paper to document their efforts and to present at ASHRAE conferences.  As a part of these efforts, over a dozen peer-reviewed papers were published and several software vendors incorporated the algorithms described in these papers. Haberl, J. (1994, July 1). Instructions - \"The Great Building Energy Predictor Shootout II: Measuring Retrofit Energy Savings\". Acknowledgments link keyboard_arrow_up In addition to the Data Driven Modeling (DDM) Subcommittee of ASHRAE Technical Committee 4.7: Energy Calculations, the following organizations have generously contributed to data collection, travel and resources to host this competition: Singapore Berkeley Building Efficiency and Sustainability in the Tropics BUDS Lab Engineering Experiment Station Texas A&M University Citation link keyboard_arrow_up Addison Howard, Chris Balbach, Clayton Miller, Jeff Haberl, Krishnan Gowri, and Sohier Dane. ASHRAE - Great Energy Predictor III. https://kaggle.com/competitions/ashrae-energy-prediction, 2019. Kaggle. Cite Competition Host ASHRAE Prizes & Awards $25,000 Awards Points & Medals Participation 19,059 Entrants 4,342 Participants 3,614 Teams 39,402 Submissions Tags Tabular Energy Root Mean Squared Logarithmic Error Table of Contents collapse_all Description Evaluation Prizes Timeline Prior Competitions Acknowledgments Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "champs-scalar-coupling",
    "discussion_links": [
      "/competitions/champs-scalar-coupling/discussion/106575",
      "/competitions/champs-scalar-coupling/discussion/106468",
      "/competitions/champs-scalar-coupling/discussion/106572",
      "/competitions/champs-scalar-coupling/discussion/106534",
      "/competitions/champs-scalar-coupling/discussion/106864",
      "/competitions/champs-scalar-coupling/discussion/106407",
      "/competitions/champs-scalar-coupling/discussion/106421",
      "/competitions/champs-scalar-coupling/discussion/106347",
      "/competitions/champs-scalar-coupling/discussion/106649",
      "/competitions/champs-scalar-coupling/discussion/106271",
      "/competitions/champs-scalar-coupling/discussion/106275",
      "/competitions/champs-scalar-coupling/discussion/106377",
      "/competitions/champs-scalar-coupling/discussion/106298",
      "/competitions/champs-scalar-coupling/discussion/106288",
      "/competitions/champs-scalar-coupling/discussion/106905",
      "/competitions/champs-scalar-coupling/discussion/106379",
      "/competitions/champs-scalar-coupling/discussion/106424",
      "/competitions/champs-scalar-coupling/discussion/106404",
      "/competitions/champs-scalar-coupling/discussion/107907",
      "/competitions/champs-scalar-coupling/discussion/106718",
      "/competitions/champs-scalar-coupling/discussion/106384",
      "/competitions/champs-scalar-coupling/discussion/106263",
      "/competitions/champs-scalar-coupling/discussion/106280"
    ],
    "discussion_texts": [
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules zkolter · 1st in this Competition  · Posted 6 years ago arrow_drop_up 147 more_vert #1 Solution - hybrid Hi everyone, Here's a brief writeup of the method we used for the #1 entry.  I was hoping to post this a bit sooner, and apologies for the delay. Update: 9/13/19 We have posted the code for our method, all of which is available under MIT license at: https://github.com/boschresearch/BCAI_kaggle_CHAMPS The main code for the model is available in the src/ directory, though for those interested, the models/ directory contains slight variants on this code that were used in the ensemble (mainly earlier versions of the same architecture) so that you can recreate the predictions exactly. Introduction First, a little bit of background on the team.  This project was done at Bosch Research, specifically as collaboration between two groups, one at Bosch Corporate Research, and one at the Bosch Center for AI (BCAI).  Our team consisted of both some ML experts and domain experts.  To introduce our team: Jonathan Mailoa and Mordechai Kornbluth are both Research Engineers working out of the Boston lab of Bosch Research.  They are domain experts on DFT and ML approach to molecular simulation, and have worked a great deal on molecular modeling, including lately some work with GNNs. Myself (Zico Kolter, I'm a faculty member working in machine learning at CMU, but work in industry one day a week at BCAI in Pittsburgh), Devin Willmott (Research Scienst at BCAI), and Shaojie Bai (my student at CMU, but doing this while while interning at BCAI) were all coming to the competition from the ML side.  I had actually done a bit of (pre-deep-learning, so ancient history) work in ML for molecular modeling, though we didn't end up using many of those methods. Overall architecture Our overall approach is what I would call a kind of \"soft\" graph transformer.  We wrote the model all from scratch for this work, instead of building upon any existing code.  The model processes an entire molecule at once, simultaneously making a prediction for each of the scalar couplings in the molecule (we hadn't considered the per-atom approach that Quantum Uncertainty used, and frankly it sounds like that may be a pretty competitive approach, given that they did nearly as well with much less physical information). Unlike a traditional graph model, though, we're really processing the data as more of a \"meta-graph\".  In constrast to most graph methods for molecules, where atoms are nodes and bonds are edges, in our graph each atom, bond (both chemical bonds, and non-chemical bonds, i.e., just pairs of atoms are included in the model), and even triplets or quads, if desired, all becomes nodes for the graph transformer.  This means that each molecule has on the order of ~500 nodes (depending on whether we include all the bonds or not, or whether we include triplets or quads, which only would be included for chemcial bonds).  At each layer of the network, we maintain an embedding of for each node in the graph, of dimension d ~= 600-750 in most of our models. Following the standard transformer architectures, at  each layer of the network, we use self-attention layer that mixes the embeddings between the nodes.  The \"standard\" scaled self-attention layer from the transformer paper would be something like (forgive the latex-esq notation formatted as code … I'm entirely unprepared to describe model architectures without being able to write some form of equation): Z' = W_1 Z softmax(Z^T W_2^T W_3 Z) where W_1, W_2, and W_3 are weights of the layer.  However, following the general practice of graph transformer architectures, we instead use a term Z' = W_1 Z softmax(Z^T W_2^T W_3 Z - gamma*D) where D is a distance matrix defined by the graph.  For a \"hard\" graph transformer, this will work like the mask in normal self-attention layers, and be infinite for nodes that are not connected, and zero for nodes that are connected (and the gamma term would be fixed to one, say).  In our \"soft\" version of the graph transformer, however, D was just the squared distance matrix between nodes in the graph, and gamma was a learnable parameters: as gamma went to zero, this would become a standard transformer with no notion of distance between objects, whereas as gamma went to infinity, it would become a hard graph transformer.  To be even more precise, in the final architecture we used a multi-head version of this self-attention layers, as is also common in transformer models. As a final note, for this to work, we needed to define a distance measure between all the nodes in the graph.  For e.g., atom-to-atom distances, we just used the actual distance between atoms, for atom-to-bond distances, we would use the the minimum distance from the atom to the two atoms in the bond, with similar extensions for triplets, quads, etc. After the self-attention layer, we used the normal fully-connected and layer-norm layers standard to transformer architectures, and used models of depth ranging from 14-16 (depending on available memory).  After the final embeddings, we had separate heads that would predict the final scalar coupling for the nodes that corresponded to fairs for which we needed the coupling value, using a simple two layer MLP for each type (or actually, for several sub-types of the bonds, which we'll mention below). Input features and embeddings The input representation (i.e., the first-layer embeddings for all nodes in the network), As our input representation, for each type of node in the network, we would include a kind of  hierarchical embedding, where were had different levels of specificity for the different atoms, bonds, etc. As an example, for each bond (again, really meaning just a pair of atoms … I'm referring to pairs generally as bonds even if they are not chemical bonds in the molecule), we described it in terms of the two atoms belonging to the bond, but also in in terms of the number of bonds that each atom would have.  Thus, each bond could be described by multiple given types at subtypes: first by just the type of atoms in the bond, then by the type and total number of bonds that each atom had, and then by a few additional properties such as the bond order, etc. This lead to substantially more coupling \"types\" than just the 8 that were used in the competition, and we actually had separate final layers for 33 different types of bonds, rather than the 8 in the competition (for instance, the 1JCH had very different properties depending on the number of bonds the C atom had), which definitely improved our predictions slightly. In addition to the \"discrete\" embedding, each node type would have associated with it one or two scalar constants that we would embed with a Fourier encoding, much like positional encoding in a standard sequential Tranformer model.  For atoms, this consisted of the partial charge of the atom, as given by the OpenBabel library (*correction: original post said this was from RDKit, but RDKit was used for bond orders and conenctions, whereas OpenBabel was used for charges), just using some simple rules based upon graph structure; for bonds it was the distance between the two atoms; for triplets the angle between the center atom and the other others; and for quads the dihedral angle between the two planes formed by the center bond and the two other bonds (quads didn't end up helping too much for this particular task, though, so were left out most of our final models). Ensembling In this end, we trained 13 models that we used for the final ensemble, which basically just corresponded to different iterations and versions of the same basic structure (at times we also included a few models based upon a more standard graph neural network approach from the PyTorch Geometric library, though they weren't included in the final ensemble).  We timed about 4 final models to complete on the final day of competition, including the model which eventually got the best performance, which is why we managed to sneak into the top spot on the very last day.  As I had mentioned in my last post, there really wasn't anything that much happening during the last ~4 days where we moved up the rankings, nor were we \"holding anything back\": our models simply kept improving each day, and we'd submit our best version of the ensemble, which kept bumping us up day by day. Our best single model got about -3.08 on the public leaderboard, which makes me actually quite surprised, given that Quantum Uncertainty's best model was substantially better.  But I think the fact that we predicted entire molecules at once actually may have increased the variance of predictions across all molecules, but therefore also seemingly made it work much better with the ensembling several different models.  By taking a straight median across predictions from the best models, for instance, we could get to the ~-3.22 range, and with a slightly more involved blending scheme (using the median of all 13 models to determine which 9 models seemed best, then taking the mean of a few different medians of the different model predictions), we were able to achieve our score of -3.245 on the private leaderboard. Other random notes We used small amounts of dropout at each layer, as in standard transformer models, though found that it was best to use a very small amount of dropout. At the very end of the competition, we did find that for our model a kind of cutout procedure (where we would randomly drop out two atoms from the network, plus all bonds, triplets, etc, that contained this atom), worked as a very effective regularizer. We didn't use QM7/QM9 or in fact any of the extra data that was included in the competition besides the structures and train/test files (so just atoms and bonds). We used RDKit/xyz2mol and a few other packages to parse the atomic structure to e.g., the bond and neighbor configuration feautres.  Jonathan had a post about this earlier listing the packages we used. I'm not even going to attempt to list all the things we tried that didn't work, but there was lots :-).  Quad / dihedral angle information, for instance, actually seemed to hurt generalization performance, as did including simple Coulomb forces at the bond level. As you'd expect, we had a fair amount of compute resources for the work.  Most of the models were trained on 4x RTX2080 Ti systems (we had 5 of these available through the month we were working on the project), with a handful also trained on six single V100s we got access to in the last week. Final thoughts I want to thank the CHAMPS team for putting on an amazing competition.  As many others have pointed out, the stability between the private/public leaderboards demonstrates an understanding of how to run a machine learning contest that sadly seems to be missing from many of the other contests I looked over on Kaggle previously. I also again want to thank the Quantum Uncertainty team, who as I mentioned before, were our goalposts the entire competition.  After reading their solution I'm coming away more convinced about Transformers as the architecture that's going to be dominant across many different domains, not just sequence models (despite the fact that I will never, ever, forgive the original paper for the monstrosity that is the \"query\", \"key\", and \"value\" terminology for self-attention layers ;-) ).  I also think their per-atom transformer is an awesome idea, and something I wish we had thought of … I think most likely it took us using a lot of domain knowledge and engineering to make back up the difference that their per-atom approach got.  And while it's wild to me that a non-rotationally invariant model would do so well (since we only used distance as a feature at the bond level, our model is rotationally invariant), it's impossible to argue with results.  Their model is excellent, and I think it actually goes to show there is substantial room for improvement still in the performance we can get on this task. Thanks again, Zico, Devin, Shaojie, Jonathan, and Mordechai Please sign in to reply to this topic. comment 38 Comments 2 appreciation  comments Hotness Andrés Miguel Torrubia Sáez Posted 6 years ago · 2nd in this Competition arrow_drop_up 9 more_vert Great write up, and impressive that you built a transformer with priors extracted from domain knowledge. despite the fact that I will never, ever, forgive the original paper for the monstrosity that is the \"query\", \"key\", and \"value\" terminology for self-attention layers ;-) Exactly my thought! This is more a naming retrofit than anything else. Re: rotational invariance, it was needed for us and helped with our first arch (pointnet-inspired), but didn't add much in the Atomic Transformer, I believe because the modules are already more or less canonically oriented. We added rotations during training and didn't up much. Im surprised that cutout (I called it knockout b/c I didn't know this tech already has a name ;-) ) worked for you. Maybe we didn't train it right or missed hyperparams b/c this is one of the things I was sure it would work. Finally, very cool (and simple, yet effective) ensembling strategy. In retrospect, this is one of our weakest spots. Congratulations! Shuhao Cao Posted 6 years ago · 39th in this Competition arrow_drop_up 4 more_vert Hi, Zico, first congrats on getting to the top. Just a heads-up that even though Kaggle doesn't have inline LaTeX support, equation environment $$ $$ is good: $$ \\left(-{\\frac {\\hbar ^{2}}{2m}}\\nabla ^{2}+v(\\mathbf {r} )\\right)\\phi {i}(\\mathbf {r} )=\\varepsilon {i}\\phi _{i}(\\mathbf {r} ) $$ Please enlighten us with more formulas! Caiqing Jian Posted 6 years ago arrow_drop_up 1 more_vert Congratulations! Thanks for sharing.  It's a pity that I miss this competition, Right now I am working on this problem, and I wonder if it is possible to get the test-set scalar coupling constant, so that I can test my model in a faster speed.  Thanks a lot, congrats once again. Jonathan Mailoa Posted 6 years ago · 1st in this Competition arrow_drop_up 1 more_vert Do you have the option to do 'late submission' to check your answers against the test set answers? I think no one aside from the organizer has it. rilesdg3 Posted 6 years ago · 2584th in this Competition arrow_drop_up 0 more_vert If I understand your question then I am going to say YES. If you do a late submission you will get the results for both the public and private test sets example  of late submission Private Score,         Public Score 1.49953,                     1.50156 or are you asking if you can download and have it for yourself? Mordechai Posted 6 years ago · 1st in this Competition arrow_drop_up 1 more_vert @kelvinhe @manojprabhaakr @zhenlanwang and everyone else asking for details and source code -- the post was updated today with the github link. Manoj Prabhakar Posted 6 years ago · 281st in this Competition arrow_drop_up 1 more_vert Thanks for sharing the code. Congrats once again. Zhenlan Posted 6 years ago · 15th in this Competition arrow_drop_up 0 more_vert Thanks for sharing the code :D AkiraSosa Posted 6 years ago · 10th in this Competition arrow_drop_up 0 more_vert @mordechai1 Thanks for sharing. Have you ever tried the simplest feature such as only atom type and distance? Then, how much score is estimated for it? Manoj Prabhakar Posted 6 years ago · 281st in this Competition arrow_drop_up 1 more_vert Congrats on this amazing work. Could you please share the code for us to have a sneak peak on the exact solution?. Youhan Lee Posted 6 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Wow, very impressive solutions :) We were really surprised about your performance. So fast step up! Congratulations! MaxR2D Posted 6 years ago · 1363rd in this Competition arrow_drop_up 0 more_vert Hello, first of all congratulations! I am interested in the time your whole model would take to preprocess + predict for one molecule (let's say with 10 atoms) the scalar coupling constant between all atom pairs in the molecule. In other words, it would be very interesting to know if your model can be useful for physicians (to replace their old and very heavy computational way to measure these constants). Because as you said, with such amount of compute resources to build this model, I think the question is natural. Jonathan Mailoa Posted 6 years ago · 1st in this Competition arrow_drop_up 1 more_vert Thanks! To pre-process the 140k molecules or so, it took ~1 hour or less. To do prediction on the test set (around 60k molecules), it took 10 minutes. So to process 1 molecule from scratch (geometry -> scalar coupling constants), it should take you around 0.03 - 0.04 s. Abdul Wadood Posted 13 hours ago arrow_drop_up 0 more_vert great knowledge, Thank you or sharing this. Muhammad Ehsan Posted a year ago arrow_drop_up 0 more_vert @zkolter and team, your hybrid approach to predicting molecular properties is impressive! The detailed explanation of your \"soft\" graph transformer model, including its innovative use of distance-based embeddings and multi-head self-attention, highlights the complexity and depth of your solution. The thorough breakdown of your methods and the transparent sharing of your code under the MIT license is incredibly valuable for the community. Your approach of processing entire molecules as a \"meta-graph\" rather than individual nodes is particularly intriguing, and your success in the competition speaks to the effectiveness of this strategy. It's also great to see your acknowledgment of the Quantum Uncertainty team's work and your insights on future improvements. Thanks for sharing this comprehensive write-up and for contributing to the advancement of molecular modeling techniques. fulowa Posted 6 years ago · 1664th in this Competition arrow_drop_up 0 more_vert Congrats! Did you train on AWS? Jonathan Mailoa Posted 6 years ago · 1st in this Competition arrow_drop_up 0 more_vert No, all the training were done on local workstation / HPC cluster GPUs Magichuang Posted 6 years ago · 16th in this Competition arrow_drop_up 0 more_vert Congratulations! The idea is very impressive. Deep Chatterjee Posted 6 years ago · 1529th in this Competition arrow_drop_up 0 more_vert Congratulations @zkolter and Team. Thank you for the detailed writeup. Vijay Ram Posted 6 years ago arrow_drop_up 0 more_vert Congrats Team, and thanks for sharing your solution approach. Kelvin Posted 6 years ago · 640th in this Competition arrow_drop_up 0 more_vert Congratulations. Could you please share the detailed features you use and which kind of models for training ? d_eremeev Posted 6 years ago · 171st in this Competition arrow_drop_up 0 more_vert Great to see Kaggle connecting ML enthusiasts  and domain experts 💥 💕 Jack nick Posted 6 years ago · 356th in this Competition arrow_drop_up 0 more_vert Amazing work and Congratulations to you and your team ! Zhenlan Posted 6 years ago · 15th in this Competition arrow_drop_up 0 more_vert Congrats for the amazing solution! Thank you for sharing. @zkolter Could you share the source code with us? Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 0 more_vert Thanks so much for the share. I understand there is a phenomenal amount of work and expertise behind a first position in this kind of competition - this is very humbling and inspiring at the same time. Congrats to you and your teammates @zkolter @jerrybai1995 @jpmailoa @mordechai1 @devster81 dan1234 Posted 6 years ago · 191st in this Competition arrow_drop_up 0 more_vert Great job! CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 0 more_vert Thanks, and congrats for the impressive LB progression. Stephen Foster Posted 6 years ago · 118th in this Competition arrow_drop_up 0 more_vert Very impressive. Congratulations! fnands Posted 6 years ago · 20th in this Competition arrow_drop_up 0 more_vert Very cool approach, and not really what I was expecting! Congratulations! Alexey Pustynnikov Posted 6 years ago · 171st in this Competition arrow_drop_up 0 more_vert Great Job! Congratulations! Tony Teng Posted 6 years ago · 416th in this Competition arrow_drop_up 0 more_vert Congratulations Joel Hanson Posted 6 years ago · 595th in this Competition arrow_drop_up 0 more_vert Congrats to your team and thank you for sharing your solution @zkolter . Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Andrés Miguel Torrubia Sáez · 2nd in this Competition  · Posted 6 years ago arrow_drop_up 176 more_vert #2 solution 🤖 Quantum Uncertainty 🤖 We want to thank Kaggle and CHAMPs for organizing such an awesome competition: No leakage. Same distribution in train, private and public test (very stable CV vs. LB, public LB vs. private LB). This is very relevant in other competitions given so many participants when the 3 distributions are different some winners (not all, e.g. CPMP approach to manually align test ~ train in the Microsoft malware comp ) are just lucky they fit private test distribution by chance; not the case here. Inspiring and useful science problem hopefully used for good purposes as described in the context (new drugs, etc.). Neither my teammate @pavelgonchar nor I had any previous domain expertise and we made the decision early on that we would tackle this problem using a very pure deep learning way: letting the model build the features for us, not the other way around (b/c obviously we were at a disadvantage if we tried to become quantum experts in 1 month… hence our team name 🤖Quantum Uncertainty🤖 we didn't know if our yet-to-be-developed approach was going to work). Our solution had two major parts: 1) the input representation and 2) deep learning architecture. Input representation This is in our opinion the key part: we take a molecule and a source atom and move it so the source atom is @ (0,0,0). For each molecule we create N molecule siblings (N being as many source atoms are defined for that molecule), and each molecule sibling is translated so its source is at (0,0,0). The x (input) are three arrays of dimension 29 (maximum number of atoms): 1) x,y,z position of each atom, 2) atom type index (C=0, H=1, etc…) 3) j-coupling type index (1JHC=0,'2JHH=1,etc.) Padding is done by placing -1 in atom type index and j-coupling type for molecules which have less than 29 atoms. The y (ground truth) is just an array of dimension 29 containing j-couplings located at target atom indices. Note that there is no graph information nor any other manually engineered features. Data augmentation We did two types of data augmentation: Rotations: which worked and were useful in our first attempt model: pointnet-based, but proved worthless in the final models (atomic transformer). J-coupling symmetry: as described in this discussion First attempt: Pointnet-inspired architecture (got up to -2.28200 LB) Our input representation is basically a point cloud: an unordered set of elements with absolute positions x,y,z and two attributes atom type and j-coupling type . We modified the Pointnet architecture to regress j-couplings. Training was a bit unstable and we tried many variations of the architecture, swapping FC layers by linear (fixed) projections (Hadamard), adding coulomb matrix as input, etc. While this worked OK and got us to -2.28200 LB (ensembling a few models) we felt that this architecture was limited by the extreme pooling/bottleneck operation so we decided to explore other architectures: meet the Atomic Transformer. Final architecture: meet the Atomic Transformer You may know that the recent NLP revolution is mostly due to the transformer architecture described in the Attention is all you need paper . The vanilla transformer architecture uses a very clever technique to add positional encodings that are needed for position-dependent input, such as language. Our input representation is a set, which means we can (and should) remove positional encoding. Prior to this competition we had no experience with transformers either but there's a section in Lex Fridman MIT podcast interviewing Orion Vinyals where he mentions the inherent position invariance of a barebone transformer encoder layer. This immediately triggered the idea of using transformer layers (encoders) stacked taking as an input x,y,z (normalized but otherwise as-is), and atom type and j-coupling type embeddings; just concatenated… nothing fancy. The dimension of the embeddings was such that the total dimension of the input vectors was d_model (as normally reference in transformer literature). We started with 256 and got immediately great results surpassing our pointnet-inspired architecture so we followed this path. We trained a total of 14 models, with varying dimensions from 512 to 2048 and layers from 6 to 24. Each model parameter size ranged from ~12M to ~100M (biggest model). We trained some models from scratch, others we fine-tuned. We also fine-tuned a few models on the troublesome j-couplings: reaching -2.12 CV on 1JHC on and -2.19 CV on 1JHN. Our best score is an ensemble of 14 models achieving private LB of -3.22349, and our best single model achieved private LB of -3.16234, again just with x,y,z , atom type and j-coupling type inputs (no QM9, etc.). What didn't work Many things! We tried: Multi-task learning using contributions and other organization provided values. Dropout: We tried multiple attempts to add dropout at various stages (embeddings, encoder layers, pre-decoder, etc.). None of them worked. Knock-out: We added a variation in which as input we deleted 10% of the input atoms, the idea being that the model would build an internal representation of the missing atoms. Surprisingly this worked in that the model still converged nicely but failed to reduce train ~ val gap. Rotations and TTA in Atomic Transformer: it didn't reduce train ~ val gap and didn't produce meaningful TTA gains. Deep decoder: Our decoder is just a projection of ~ the model dimension to 1 (scalar coupling). We tried adding more expressive power to the decoder but this didn't help. Fp16 training. This worked for models of dimensions 256 but as training evolved gave NaN s despite numerous attempts to fix it. Source code We will make source code available once we do clean up. It's a single jupyter notebook using FastAI. Be patient. Computational resources We had more ideas than computational resources, even if our computational resources were not tiny: 3 x 2080 Ti + 128 Gb RAM + 16c32t processor 2 x 1080 Ti + 64 Gb RAM + 8c16t processor Rented 8+ 2080 Ti + 64 Gb RAM + 16c32t processor (multiple machines rented as needed) Final thoughts This was our most fun and hardest competition so far: Challenging problem Most teams in top 5 had domain experts (although we went domainless as part of our strategy) Hungry computational resources. Even if we lost #1 position just a few hours before competition end we feel very excited we were able to achieve such useful results for the organizers . In retrospect we believe a single model of the Atomic Transformer may achieve ever better results with further training. Best - Pavel & Andres p.s. No graph NNs. We though graphs as manually engineered features that the model can infer by itself. Please sign in to reply to this topic. comment 49 Comments 4 appreciation  comments Hotness whyvr Posted 6 years ago arrow_drop_up 3 more_vert Hi Andres, very inspired by your approach. Is the notebook available for us to study, could you point me to it? Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 3 more_vert Very impressive how having machine learning expertise and no domain knowledge can get you a podium, it truely amazes me. Congrats @antorsae CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 3 more_vert Thanks, very impressive solution.  In a way my team mates approached the problem in a way similar to yours, by representing every atom position relative to the two atoms for which scaling is computed.  But you way is a bit different as you keep the coordinates, while they worked with distances and angles.  That using coordinates works that well is a bit of  a surprise to me.  If I were the organizers then i would have applied random rotations to the data…  This said, again, congrats on the approach.  I'll need to reread this to full grasp it. I forgot the most important: congrats on the end result and the incredible lead during the competition. CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 2 more_vert And thanks for citing my solution in Malware! ;) Andrés Miguel Torrubia Sáez Topic Author Posted 6 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Yeah! I liked the notion of alining distributions. In the IEEE fraud one I am trying to do the same automatically with deep learning instead of manual alignment. Not proven yet. CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 0 more_vert I tried to code it, and it led to a second dataset, but it performed worse than the fully manual one  I guess my pretrained NN (in my brain) worked better ;) Jonathan Mailoa Posted 6 years ago · 1st in this Competition arrow_drop_up 4 more_vert I just want to say that your team's work is very impressive, and I certainly look forward to reading the implementation in more detail! For one, we also use transformer architecture, but it was a graph transformer. I think Zico will make a post about it sometime soon. Andrés Miguel Torrubia Sáez Topic Author Posted 6 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Thanks! You made us sweat a lot in the final hours. It was fun tough and we learned a few lessons in those final hours. Jonathan Mailoa Posted 6 years ago · 1st in this Competition arrow_drop_up 2 more_vert On the day before the last day, we were very excited to finally try surpassing you guys! But we were 0.001 short… and couldn't do anything about it until the next day. =) Since your transformer graph does not satisfy physics requirement of rotation/translation invariance by construction, can you comment on how much data augmentation you need to do to get good result, as well as the extra training resource / compute needed correspondingly? Your single model achieving -3.16 is really quite impressive considering that your architecture does not satisfy physics rules by construction! Sean McConnell Posted 6 years ago · 779th in this Competition arrow_drop_up 1 more_vert Congrats! corochann Posted 6 years ago · 2115th in this Competition arrow_drop_up 1 more_vert Congratulations! Impressed that not used graph NNs but used transformer based architectures to deal with set, together with the position translation for target atom. Dondon2019 Posted 6 years ago · 194th in this Competition arrow_drop_up 1 more_vert Hi, Thanks a lot for sharing your solution and code. I was reading your code and I have a question: In your code atom-transfomer-good-norm-pytorch, the result under the line learner.summary() shows two emdedding layers and two dropout layers at the end. Where do they come from? I can't find them in the forward method of your AtomTorchTransformer. I apologize if the question is too detailed and thanks in advance. Andrés Miguel Torrubia Sáez Topic Author Posted 6 years ago · 2nd in this Competition arrow_drop_up 0 more_vert Check this code in the AtomTransformer constructor: n_atom_embedding = d_model //2 n_type_embedding = d_model - n_atom_embedding - 3 #- 1 - 1 - 1 self .type_embedding = nn .Embedding ( len (types)+ 1 ,n_type_embedding)\n        self .atom_embedding = nn .Embedding ( len (atoms)+ 1 ,n_atom_embedding)\n        self .drop_type , self .drop_atom = nn .Dropout (embed_p), nn .Dropout (embed_p) content_copy 3 more replies arrow_drop_down Deep Chatterjee Posted 6 years ago · 1529th in this Competition arrow_drop_up 1 more_vert Congratulations @antorsae @pavelgonchar .Thank you for the approach and waiting for the notebook. The one thing that is inspirational for me is that though you don't have any domain expertise, you understand and solve the problem using DeepLearning techniques. Sems Kurtoglu Posted 6 years ago · 2532nd in this Competition arrow_drop_up 1 more_vert Congratulations, good job Even Posted 6 years ago · 33rd in this Competition arrow_drop_up 1 more_vert Really looking forward to your solution, especially as a fellow fastai community member.  Your team really inspired us to keep striving in the final weeks of the competition.  Sub -3 didn't feel achievable until we saw you pull it off. Pankaj Joshi Posted 6 years ago · 413th in this Competition arrow_drop_up 1 more_vert Congratulations, you guys are inspirations. Murat Sarıbaş Posted 6 years ago arrow_drop_up 1 more_vert Congrats!! Xiao-Chuan Posted 6 years ago · 812th in this Competition arrow_drop_up 1 more_vert amazing work Joel Hanson Posted 6 years ago · 595th in this Competition arrow_drop_up 1 more_vert Congrats @antorsae and thanks for sharing. FGPC Posted 6 years ago · 235th in this Competition arrow_drop_up 1 more_vert Congratulations! Amazing solutions! :-) Zhuang Jia Posted 6 years ago · 52nd in this Competition arrow_drop_up 1 more_vert Congratulations! I'm very excited to learn from your solutions. Alexandre Sauvé Posted 6 years ago · 82nd in this Competition arrow_drop_up 1 more_vert I'm just impressed by the way of using xyz coordinates. Need to read more bout Trnasformer Encoders. Thank you @antorsae for sharing the solution and congratulation for your creativity! Goran Rakocevic Posted 6 years ago · 4th in this Competition arrow_drop_up 1 more_vert Thanks for sharing! When you say \"The y (ground truth) is just an array of dimension 29 containing j-couplings located at target atom indices.\" Which atom index do you place the j-coupling values, as there are two atoms participating in a coupling? Andrés Miguel Torrubia Sáez Topic Author Posted 6 years ago · 2nd in this Competition arrow_drop_up 0 more_vert Say there are N source/reference atoms ( atom_index_0 in the csv file) for a given molecule. We build N sibling molecules, and for each the GT is the array of 29 atoms (max) where we place valid j-couplings at target ( atom_index_1 in the csv) indices. Goran Rakocevic Posted 6 years ago · 4th in this Competition arrow_drop_up 0 more_vert Oh, so each coupling gets its own datapoint? We have one datapoint per molecule, that confused me. Andrés Miguel Torrubia Sáez Topic Author Posted 6 years ago · 2nd in this Competition arrow_drop_up 0 more_vert Yes. We started building a Molecule Transformer which had all couplings in one shot (diff input representation) but stopped it bc we focussed on training the Atomic Transformer instead. Zhenlan Posted 6 years ago · 15th in this Competition arrow_drop_up 1 more_vert I am not sure that I understand your Input representation. Could you give a simply example, say for H2O? Thanks Zhenlan Posted 6 years ago · 15th in this Competition arrow_drop_up 1 more_vert Congrats!! Thank you for sharing. I look forward to seeing your code :D XiaokangWang Posted 6 years ago · 121st in this Competition arrow_drop_up 2 more_vert Congratulations! You are not using graph NN. Amazing ideas! I think it would be an excellent paper if you wrote it up your ideas. CoreyJamesLevinson Posted 6 years ago · 39th in this Competition arrow_drop_up 2 more_vert looking forward to the code! Thanks for sharing Patrick Chan Posted 6 years ago · 461st in this Competition arrow_drop_up 2 more_vert Very impressive and the right way to go with AI discovery.  Like the domainless approach!  Congratulations! guiferviz Posted 6 years ago · 401st in this Competition arrow_drop_up 2 more_vert Congratulations! Looking forward to exploring your code, I'm very interested in solutions without domain knowledge. Btw, I just realized that we are form the same city lol Robin N Posted 6 years ago · 6th in this Competition arrow_drop_up 2 more_vert Really impressed to see that you were able to push a pure Tranformer Encoder to such great performances. I tried using a straightforward Transformer model as well on a set of engineered features (no raw coordinates), but wasn't able to get results anywhere near as good as yours. Like you said, I think the key is your input representation creating the N sibling molecules. Congrats on second place and the beautiful approach. Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Youhan Lee · 3rd in this Competition  · Posted 6 years ago arrow_drop_up 199 more_vert 3rd solution - BERT in chemistry - End to End  is all you need Our story As you know, we need to find the appropriate representation for data. That’s why we’re struggling to do feature engineering. As CPMP mentioned, two member of our team have domain knowledge on this competition. Sunghwan Choi has Ph.D at quantum chemistry and works on the field of quantum chemistry and chemical application of machine learning. I’m going on the Ph.D course in chemical engineering and have some experience for quantum calculation and dealing molecules. More important thing is that I’m kaggler :). We thought that our problem can be solved by conventional graph models whose edge features are distance-derived properties. Those models are quite conventional in machine learning applications on chemical system. Hence, we tried to figure out appropriate hyper parameters and edge features for models As many kagglers did, we also tested message passing neural networks by adopting many useful kernels. We thank to many kegglers; especially heng cher keng. :) by modification of their solution, we got a silver place 1 month ago. But, the gap between leading group and us was getting large. We had to find out breakthrough. We brainstormed a lot. At that time, limerobot, who is an expert of natural language processing suggested to use the raw xyz coordinates, I and Sunghwan didn’t agree with that, because if we use xyz coordinates instead of distance, translational and rotational invariances are not satisfied. The model which do not preserve those invariances seems to be ridiculous But, limerobot did on his way. And he showed that his model won my GNN. His model was based on BERT model. You can see the big success in toxic competition on his profile. Maybe, insights from toxic competition save us :). Anyway, he don’t have any domain knowledge. He just wanted to make a model to learn the complex representation using xyz coordinates. (end-to-end) He just input the xyz coordinates of atom1, atom2, coupling type, distance and difference of each xyz coordinates (he thought the model can learn the distance formula based on that…amazing) Because we had only one month, we decided to do all things based on the transformer. After that, we did a number of experiment for hyper parameter tuning. Because BERT is very large model, the performances differed according to the number of hidden layers, type of embedding and learning schedule. Before 1 week to end of competition, we found our own scheduling and parameters and tiny modification of readout layer ( I will explain model later ) We made multiples of models. Based on the ensemble of them, we got 3rd place :). Thanks for reading our stories. We want to share the specific magics below. Please keep going :) Overall architecture Here is our overall architecture for our model. As you can see, our input sequence is transformed into output sequence by BERT-encoder. •    The number of encoder layer: 8 •    The number of heads for attention 8 •    Dropout ratio is 0.1 which is conventional choice for BERT model. •    For each type, different readout networks but having same architecture are used Input features and embedding layer for them Our float sequences composed of multiple embedding results, are the magic to get the achievement. Multi-head attention layer itself preserve permutational invariance therefore, order of couplings do not change the results but invariance when atomic_index_0 and atomic_index1 are changed, is not preserved since we use feature vector as concatenation of  the embedding results of atomic charge, xyz coordinate(position), atomic number of two atoms. •    Size of embedding for atomic charge: 32 •    Size of embedding for position: 256 •    Size of embedding for atomic number: 64 •    Size of embedding for distance: 64 •    Size of embedding for type: 64 Total feature size for single feature vector is (32+256+64)*2+64+64=832 Augmentation In order to impose pseudo-invariance on our model, we use rotational and translational noise when augmenting data. •    Translational noise: For each axis, Gaussian noise( mean: 0, std: 2) was added •    Rotational noise: Rotational transformation whose axis is translational noise vector and angle is from Gaussian noise( mean:0, std: 3.14/2 ) Regression layers for predicting scalar coupling constants •    As you know that, spin-coupling (sc) value can be decomposed into four different terms (fc, sd, pso, dso) •    After optimizing architecture and various losses, we found that auxiliary target using contributions gave a high boost. •    During training, model minimize loss1 + loss2 with AdamW algorithm. •    There are 8 regression layers to cover 8 different coupling types Specific learning rate •    We were always using linear learning rate decay. •    We think that there might be improvement with various learning schedule such as cycle lr. But, we didn’t have time because BERT is very large…(about 75M parameters. It took 1~2 days to get a model using 2~4 V100 machine) Pseudo labeling •    To get the more results, we needed some magics. With having an insight that there are less probability to be overfitted (Sunghwan choi’s insight) and experiment result from limerobot, we adopted pseudo-labeling. •    After predicting for test set, we used the pseudo-labeled test dataset for training. •    The model showed more than -3.4 CV. So, we trained model more with only train data to minigate overfitting. (finally we got ~-3.11 LB single models) •    Overall learning process is illustrated below. Final submission •    We made 14 models with various seed and hidden layers (most have 8 layers, other have 6 layers) •    After weighted average according to cv score of types, we got -3.16. •    After multiple procedure of pseudo labeling, we had 8 models. •    With simple average, we got -3.19 (our final score :) 2 hours before the end of competition. What we’ve learned •    End-to-End works!!!. Amazing BERT. •    Learning schedule is very important for modelling of molecular property. Please sign in to reply to this topic. comment 132 Comments 3 appreciation  comments Hotness Ailurophile Posted 6 years ago · 490th in this Competition arrow_drop_up 6 more_vert Congratulations….. Amazing ideas… Inspiring. Thanks for sharing…. @youhanlee Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 2 more_vert Thanks! I've shocked for the performance of BERT :) Dan Ofer Posted 6 years ago · 293rd in this Competition arrow_drop_up 5 more_vert How did you do embeddings on continous variables such as distance? Binning? How did you get 32 size embedding on charge, that's an ordinalvariable with less than a dozen possible values? Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 3 more_vert We just embedded the continous value into 32 or 64 size not using binning. The 32 size was arbitrary. We wanted to optimize the size, but not enough time. We can expect that there are optimal size of embeddings for various inputs. Dan Ofer Posted 6 years ago · 293rd in this Competition arrow_drop_up 2 more_vert So you turned 1 column into 32 columns/variables in the net? I'm probably missing something about your setup - I don't see how that would have improved things vs just inputting the number as a variable. Really interesting overall! KhanhVD Posted 6 years ago · 884th in this Competition arrow_drop_up 6 more_vert Congrats @youhanlee and your team for Amazing Solution!! Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 2 more_vert Thanks for kind words! AkiraSosa Posted 5 years ago · 10th in this Competition arrow_drop_up 1 more_vert I have a question about \"input sequence\". You show that the max coupling of the input sequence is 135. I suppose that this number is from the total number of coupling constants in molecule dsgdb9nsd_042139. So, is your \"input sequence\" a set of coupling constants in a molecule? However, if the answer is true, your input does not contain some information, such as oxygen in molecule, because scalar coupling type is only for H, C and N. CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 4 more_vert Thanks for sharing, and congrats on the well deserved end result.  Again, using raw coordinates works surprisingly well.  I wonder if there isn't some information leak in the way molecules are built here,.  Would you and 2nd team get the same result if random rotations were applied to input molecules? Anyway, your solution is very original and works amazingly well.  I'll reread to grasp all details ;) Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 5 more_vert Thanks for kind words. We were also surprized about the performance of using raw coodinates. I don't think any leak in making the structure of molecules. The molecules might be optimized using QM calculations. Anyway, we used the rotation matrics which were not too much to rotate molecules. So, Rotated molecules showed very similar coupling constants compared to that of original structures. Mikhail Karchevskiy Posted 6 years ago · 68th in this Competition arrow_drop_up 5 more_vert Great solution! One question, did you use tensorflow or pytorch? Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 4 more_vert Thanks! Hi, we used pytorch_transformer ( https://github.com/huggingface/pytorch-transformers) . Mikhail Karchevskiy Posted 6 years ago · 68th in this Competition arrow_drop_up 2 more_vert thanks! Anubrata Bhowmick Posted 6 years ago arrow_drop_up 3 more_vert This is awesome! Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thanks for kind words ! ZexyZeky Posted 6 years ago arrow_drop_up 3 more_vert Woww.. amazing solution Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 2 more_vert Thanks for kind words! Saikat Biswas Posted 6 years ago arrow_drop_up 3 more_vert Simply Wow…!!! So, you just described the journey on this competition and along with it the whole thought process. Big Congratulations on your win and thanks a lot for sharing this..!!! Awesome read…!!! Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thanks for kind words! Hope this post is informative to you! Psi Posted 6 years ago arrow_drop_up 3 more_vert I love this! Congratz! Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 1 more_vert I've always liked your solutions :) Thanks for kind words! Rahul Pareek Posted 6 years ago · 2730th in this Competition arrow_drop_up 3 more_vert Great Solution. Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thanks for kind words ! Rashidul H Posted 6 years ago arrow_drop_up 3 more_vert Congrats and thanks a lot for sharing. Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thanks for kind words ! Saurabh Kumar Posted 6 years ago arrow_drop_up 3 more_vert Thanks for sharing. Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 2 more_vert Thanks for reading! Saad Aldin Posted 6 years ago arrow_drop_up 3 more_vert Good job! Thanks for sharing.. Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 2 more_vert Hi, Saad. Hope this solution is easy to read for you :) Sebastian Korsak Posted 6 years ago arrow_drop_up 3 more_vert Bravo! Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Bravo!! Thanks :) Rob Mulla Posted 6 years ago · 28th in this Competition arrow_drop_up 3 more_vert Great work. Happy to see your team do so well. It's well deserved. Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 3 more_vert Thanks for kind words ! You did also great job :) I've read your kernels which showed what's going on the competitions. Hayato Posted 6 years ago arrow_drop_up 3 more_vert Thank you for sharing unique method, congratulations! Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 3 more_vert Thanks! Hope this solution inspires you ! Neuron Engineer Posted 6 years ago arrow_drop_up 3 more_vert Absolutely amazing Youhan & your team!! @youhanlee !! Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 3 more_vert Hi, Thanks for kinds words! Wgyeong Posted 6 years ago arrow_drop_up 3 more_vert Amazing job!  I have little for the domain knowledge, but you have been a great collaboration. At last, you got it. Again, Congraturations on yours! Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 3 more_vert I'm really lucky. Our team work was wonderful :) Thanks for kind words! Zhenlan Posted 6 years ago · 15th in this Competition arrow_drop_up 3 more_vert Congrats @youhanlee and thank you for sharing. One question. how is position defined in \"embedding for position\"? Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 1 more_vert Thanks for kind words! We just embeded the xyz-coordinate (3 dimension) into 256 dimension! tamuhey Posted 5 years ago arrow_drop_up 0 more_vert So xyz * A, where A is a matrix and its size is (3, 256)? Murat Sarıbaş Posted 6 years ago arrow_drop_up 3 more_vert Congratulations! Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 3 more_vert Thanks!! Very happy now :) Zhenlan Posted 6 years ago · 15th in this Competition arrow_drop_up 3 more_vert Congrats for the amazing solution! Thank you for sharing. @youhanlee Could you share the source code with us? Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 2 more_vert Thanks for kind words! As illustrated in description, we will follow the policy of the CHAMPS. Massoud Hosseinali Posted 6 years ago · 136th in this Competition arrow_drop_up 3 more_vert Rarely have I seen a solution as complex as your team's. It's impressive. Thanks for sharing and congratulations. Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 2 more_vert We did because we're team :) Thanks! A_b Posted 6 years ago · 671st in this Competition arrow_drop_up 3 more_vert Fantastic teamwork for a very innovative approach, congrats! Hope to see you again in future competitions :-) Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 4 more_vert Thanks for kind words! I will keep going on until GM. We will meet always :) Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 3 more_vert Thanks for sharing - so interesting. Congrats to you and your teammates @youhanlee @sunghwanchoi @songwonho @limerobot @yslee04 . Youhan Lee Topic Author Posted 6 years ago · 3rd in this Competition arrow_drop_up 2 more_vert Thanks for kind words :) My teamates are amazing ! Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Milos Popovic · 4th in this Competition  · Posted 6 years ago arrow_drop_up 39 more_vert #4 Solution [Hyperspatial Engineers] Hi everyone, First I'd like to thank the organizers for the well organized competition, but I'd also like to thank other competitors for making things fun, your great scores pushed us to do much more and get much better scores than we have originally though we could achieve. Here we'd like to share our solution which is also based on a Graph Transformer network, though with a few twists. Input data We have used OpenBabel to infer bonds from atom coordinates, and have used some custom code to fix what we identified as mistakes made by OpenBabel. We construct a graph where each node is an atom, each bond becomes an edge, then we add artificial edges between all nodes at distance 2 and 3 in the graph (2J and 3J edges respectively). All edges are directional, so we have one edge for each direction to make the graph fully bidirectional. Attributes : Nodes have atom type embedding, electronegativity, first ionization energy, electron affinity, Mulliken charge taken from the QM9 data (originally predicted using the same model architecture trained on the provided Mulliken charges) All edges have distance and edge type embedding (single bond, double bond, triple bond, 2J, 3J) 2J edges have bond angle on the atom they are skipping over. 3J edges have dihedral angle between the atoms they are connecting No explicit XYZ data is used, as we wanted to make the model rotation and translation invariant. All input data was normalized to zero mean unit variance. Model Core of the architecture is the graph attention network with multiple heads, with a few twists: Attention heads do not attend to all data from previous layer, but only the output of the same head from previous layer All edge embeddings are first updated from triplets (src, edge, dst), the attention then updates atom embeddings by aggregating over edge embeddings, not neighboring node embeddings We use gated residual connections between attention blocks similar to: https://arxiv.org/abs/1805.10988 We output scalar coupling constants directly on the edges, this makes it two predictions, one for each edge direction and each is treated independently in the loss function. These two predictions are averaged to get a final prediction making this a kind of micro-ensemble. Loss function is a mean of MAEs per coupling type. We have tried mean log MAE but it was giving us worse results. We have used both scaled targets to zero mean unit variance and zero mean targets with no variance scaling, as different types benefited from different setups. Quickest way to outline the network would be using code: emb = 48 heads = 24 bias = False def AttnBlock ( in_emb, out_emb ): return nn.Sequential(\n        EdgeLinear(in_emb, out_emb),\n        NodeLinear(in_emb, out_emb),\n        GraphLambda( lambda x: x.view(x.shape[ 0 ], heads, - 1 )),\n        TripletCat(out= 'triplet' ),\n        MagicAttn(emb, 3 * emb, heads, attn_key= 'triplet' ),\n        TripletMultiLinear(emb, emb, emb, heads, bias=bias),\n        GraphLambda(torch.nn.LayerNorm(heads * emb))\n    )\n\nnet = nn.Sequential(\n    Embed(emb, emb),\n    AttnBlock(emb, emb * heads), GraphLambda(nn.PReLU()),\n    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n    GatedResidual(AttnBlock(emb * heads, emb * heads), emb * heads, emb * heads), GraphLambda(nn.PReLU()),\n    EdgeLinear(emb * heads, 512 , bias= True ), GraphLambda(nn.PReLU(), node_key= None ),\n    EdgeLinear( 512 , 8 , bias= True )\n) content_copy Optimizer We have used LAMB optimizer ( https://arxiv.org/abs/1904.00962) , again with a small twist, we have noticed that the weight decay is included in the step norm on which the trust ratio is calculated, this didn't make sense to us as weight decay should be independent of the update based on the batch gradient, so we moved the application of weight decay after the application of LAMB update, and this gave us better results. We call this LAMBW Training regime We have split the data into 90/10 train/eval, two times with two different. We have used one cycle learning rate for 30 epochs with high weight decay, then dropped weight decay and continued training until eval saturation (~70 more epochs). Then we fine tune 100 epochs for each type to get further improvement (except for *JHN types, as they didn't improve with fine tuning). We have used Stochastic Weight Averaging ( https://arxiv.org/abs/1803.05407 ) of last 25 epoch to get to a better minimum. We then average the predictions of the two training runs for the two splits we had. What didn't work Many, many things: RAdam combining RAdam and LAMBW many different forms of attention mechanisms mean log MAE loss, mean MSE loss GradNorm for multitask learning for different coupling types https://arxiv.org/abs/1711.02257 Multi-Task Learning as Multi-Objective Optimization https://arxiv.org/abs/1810.04650 And many many other things The full package with transformed data, additional data (and all code, which is also included in champs_code.tgz) is 7GB, so we are providing a link https://zenodo.org/record/3406154#.XXpX_ygzabg instead of uploading directly to Kaggle forum. Model Summary.pdf champs_code.tgz Please sign in to reply to this topic. comment 7 Comments 1 appreciation  comment Hotness Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 1 more_vert Thanks so much for sharing @milosh - very interesting. Congrats to you and your teammates @grakocevic @starvingmarvin @boysha . CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 1 more_vert Thanks for sharing, and congrats on the result! Another question is why you use 1 kernel size of Conv1D? I relied on bonds output by openbabel.  How much did you gain from this fix?  How did you identify mistakes in the first place?  Do you have background in chemistry that you used for that? Goran Rakocevic Posted 6 years ago · 4th in this Competition arrow_drop_up 0 more_vert We mostly looked for stuff like N with 2 bonds and no formal charge. Some of these popped up when looking at molecule graphs. No proper chemistry background, but we do work on applying DL to molecules (not in QM context, though). Kha Vo Posted 6 years ago · 10th in this Competition arrow_drop_up 1 more_vert Congrats on your great performance during this competition. I like your team name much. Zhenlan Posted 6 years ago · 15th in this Competition arrow_drop_up 0 more_vert Congratulations and thanks for sharing!! This comment has been deleted. Appreciation (1) AMRL Posted 6 years ago arrow_drop_up 0 more_vert thanks for sharing Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Guillaume Huard · 5th in this Competition  · Posted 6 years ago arrow_drop_up 39 more_vert 5th place solution : DL guys Thanks First of all, a big Thank You to the organizers for this outstanding competition. This competition is very challenging and interesting in many points of view. Secondly, congratulations to all teams which completed this competition, whether you are in the medals or not. We learned a lot from your solutions and your discussions. We didn’t have any expertise in neither chemistry nor in graph neural net before this competition, this experience has been very enlightening for the 3 of us. Also big kudos to the top 4 teams, your usage of Transformers are quite eye opening (honestly we did think about it but was not audacious/confident enough to test it out). Maybe this competition will open a new paradigm of Deep Learning for molecular properties - Transformer is all you need :D A last thank you to my teemmates and coworkers Lam Dang and Thanh Tu Nguyen :) It was very fun competing with you. Solution Without further ado, here is a highlight of our solution: On macro level our best submission is a 2 layer stacking: The base level consists of different variant of the general Graph Neural Net with edge, node and global representation with some variations (cf. Architectures below) It was implemented with pytorch and pytorch_geometric. The 2nd level is some metamodel trained on our validation set of 5000 molecules : 1 linear stacking model and 1 LGBM (cf. Stacking section below) The final submission is a blend of 2 meta model Architecture: The final architecture is based on the paper https://arxiv.org/abs/1812.05055 . We tried different variations to improve this architecture, here is a summary of what worked and what didn’t work: Normalization: We found that LayerNorm worked better that BatchNorm for this data and helps improve convergence Softplus vs ReLU: Softplus did provide a ~ 0.1 boost of logMAE for our models vs a ReLU baseline Edge to node message gating: We found that adding some gating mechanism to the edge representation before the scatter_mean (see torch_geomrtric) for node update helps Edge to edge convolution: Guillaume implemented something that seem to work very well. He noticed after a feature importance test that the most important one was by far was the angle between an edge and the edge with the closest atom to the first edge. To integrate this angle feature for more than the closest edge, we updated each edge with a convolution of the edge in question and its neighboring edges in the graph (more specifically the neighboring edges that chemically connects two atoms), and putting in this convolution the angle of the edge vectors. This architecture tweek made our architecture 5 times slower but gave us a 0.15 improvement compared to the best model without it. 1 prediction tail per type: All types share a GNN “body”, but we found that having different MLP for each type helps. In some variants, before feeding into output MLP layers, we pool all the edges and nodes in the chemical bond path from atom_0 to atom_1. It seems to have helped in the beginning of the competition but our best model did not use it. For our architectures, we found that having a representation of the link between atom_0 and atom_1 is important. Also including the global representation as inputs of the top layers is important Stacking: Our single best model is the one with edge to edge convolution which gives us -2.9. But we have various models around -2.7 which are the variants of it. By stacking all (20 models) we got -3.13 on LB. Another thing we found out at the last day helps improve our score from -3.13 to - 3.15 is adding checkpoints of our models to stacking pipeline. So finally, we have 50 predictions to do stacking. Our final result is a blend of LGBM and HuberRegressor. LGBM: 20 GroupKFold on all bond types together HuberRegressor: 20 GroupKFold on every bond type separately. Computation: We have : 1 GTX 1080ti x 2 months + 1 RTX 2080ti x 1 month 1 RTX 2080ti x 3 month 1 V100 x 2 month (rented) + 4 V100 x 2 weeks (rented) Please sign in to reply to this topic. comment 8 Comments 1 appreciation  comment Hotness Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 1 more_vert Congratulations to you and your teammates @grjhuard @lamdang @thanhtu . P Vijay Simha Reddy Posted 5 years ago arrow_drop_up 0 more_vert super Alin Cijov Posted 5 years ago arrow_drop_up 0 more_vert Congratulations! Zhen Tian Posted 5 years ago · 680th in this Competition arrow_drop_up 0 more_vert I want to appreciate your code, can you share with me, graet thanks!  my email: believemetoo@sina.com yuquan.li Posted 5 years ago arrow_drop_up 0 more_vert I really want to see your code. This is very helpful to me. Can you give it, open source, or my email yvquan.li@gmail.com EdgarPE Posted 6 years ago · 202nd in this Competition arrow_drop_up 0 more_vert Thanks for sharing your solution, and of course, congrats! Can I ask how about how many features did U use in LGBM and if you have time to describe some bird eye view of the features used there? Or you used the same features for LGBM and GNN? This comment has been deleted. Appreciation (1) Vinay Pratap Singh Posted 5 years ago arrow_drop_up 0 more_vert congratulations. thanks Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Robin N · 6th in this Competition  · Posted 6 years ago arrow_drop_up 108 more_vert 6th place solution First off, I would like to thank Kaggle and the hosts for a well organised competition and giving us such an interesting scientific problem to work on. I learned a great deal about training and developing neural nets (and a little bit about chemistry ;)). I hope that the final solutions are useful to the organizers as well. That being said, I’m excited to share my final model. In short: I used a custom GNN architecture that combines the message passing elements from the MPNN model ( https://arxiv.org/pdf/1704.01212.pdf ) with the heavy use of multi-head attention layers as in the Transformer Encoder ( https://arxiv.org/pdf/1706.03762.pdf ). On its own an 8-fold submission of this model scored -3.039 on private LB (single folds scored between -2.83 and -2.88 on their validation sets). Journey to the final model: Quite quickly upon entering this competition I found several discussion topics on Graph CNNs, which led me to the MPNN paper. I started implementing this model from scratch in Pytorch and given initial promising results and later also finding Cheng’s MPNN starter kit, I was convinced I was on the right track. As many others have noted, the MPNN model doesn’t necessarily work well as a plug and go solution for this problem. But with some customisation, I was able to get an 8-fold MPNN model to produce a -2.873 score on private LB. The most important things I changed or added to the MPNN architecture to get there: • Add separate message passing function for scalar-coupling atom pairs. • Incorporate additional edge, atom and molecule level features. Most significantly, angle based features for scalar coupling edges (i.e. dihedrals for 3J, cosine angles for 2J and nearest neighbour cosine angles for all types). • For the Message Passing Function applied to bond connections, I added an attention mechanism over incoming messages based on cosine angles. • Replace set2set with a Gaussian attention layer based on Euclidean distance. • Introduced scalar coupling edge state that is updated alongside atom states. • The head of the model used dense skip connection to the final node states of the update process and raw features. • The write head also included a residual block that was specialised per scalar coupling type. • Before the final prediction I first predicted each of the four scalar coupling contributions separately. These four terms plus a learned residual are finally added to predict the scalar coupling constant. The four contribution predictions where added to the loss function. • Increased hidden state dimension (up to 300). As the long list suggests, the model grew increasingly complex and the additions started to feel increasingly hacky. Also, I wasn’t happy with how the model scaled with hidden state dimension. Anything above 300 no longer fit on the Kaggle kernel GPU memory. This mostly stems from the edge network whose final layer has a weight parameter whose size grows at a cubic rate (i.e. 300^3 = 27M params). I wasn’t able to make any compromise on the flexibility of this edge network without drastically reducing performance. Final Model: With this in mind I started thinking of a different architecture where message passing layers and attention layers (which seemed to be the most promising elements from my MPNN) could be neatly stacked. The general architecture of the Transformer Encoder provided the basic building blocks for this: stacked encoder blocks and sublayers connected through residual connections with layer norm. In this case however the encoder blocks are build up of two message passing layers, followed by three different types of multi-head attention layers with a final point-wise feed-forward network. The message passing layers (one for bond connections and one for virtual scalar coupling edges) are largely as described above for the MPNN. Note that unlike the attention layers, the message passing layers' parameters are tied across encoder blocks. To allow for much larger node dimension I changed the full matrix multiplication for message passing into a convolution with a fixed kernel size of 128. In the MPNN model this significantly decreased performance but for the Transformer model the gain from increased hidden state size easily offset the loss due to less flexible message passing. The three multi-head attention layers are: Euclidean distance based Gaussian attention. Similar to the Gaussian attention module in my MPNN. Graph distance based attention. Computes attention through an embedding of the graph distance matrix. Scaled dot product self-attention. Exactly the same as in the Transformer Encoder. Although the final layers in the block resemble the encoder blocks of the Transformer model, there are several additional layers designed specifically to capture the structure and relationships among atoms in a molecule. The final write head is the same as described for the MPNN. Training: Training was facilitated by the fastai library. The final model was trained for 100 epochs on all folds using the one_cycle learning rate policy, a max learning rate of 5e-4 and weight decay of 1e-2. I also experimented with snapshot ensembling over an additional 40 epochs using a warm restart learning rate schedule, but the gains from this technique are marginal (~ -0.01). Up to the final submission I had done nearly all training and experimenting on Kaggle kernels. But, for the final submission I used a model with a hidden node dimension of 650, 10 encoder blocks and 10 attention heads. This model was pretty big (at least by my standards) and implied I had to do the final training on the GCP platform, using distributed training on 2 V100 GPUs. Total training time on this setup was roughly 1.5 days per fold. My top submission also included the 8-fold submission of the MPNN model. This gave me about -0.05 of a boost. I want to specially thank @hengck23 for sharing the MPNN starter kit, which really helped me along with how to setup training and load in data for these GNN models, and the people who submitted public kernels. Nearly all my features either came from or where heavily inspired by these kernels. I’ll try to get a kernel published in the next few days that trains a smaller version of the transformer model. Thanks for reading and I’ll happily answer any questions Update: github repo: https://github.com/robinniesert/kaggle-champs kaggle kernel of smaller model w 15 epochs of training: https://www.kaggle.com/robin111/molecule-transformer-w-message-passing Please sign in to reply to this topic. comment 29 Comments 2 appreciation  comments Hotness Dieter Posted 6 years ago · 8th in this Competition arrow_drop_up 5 more_vert Really impressive. Good job. I hope this will not be your last challenge and I am eager to see more of you in the future. CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 3 more_vert Thanks for sharing, and congrats for the result.  We were hoping to pass you but couldn't! I'll need to reread your write up more than one to fully grasp it. vithng Posted 6 years ago arrow_drop_up 3 more_vert Thanks for sharing. This is impressive. Could you elaborate more about these features that you used: scalar coupling edge state : do you mean type of bond? Graph distance: (how many step go from A-> B)? scalar coup pair feature: could you tell me precisely what they are? Thanks. Robin N Topic Author Posted 6 years ago · 6th in this Competition arrow_drop_up 3 more_vert Thanks! Similarly as for the hidden states for atoms I introduced a hidden state for the edges between atoms for which we had to predict the scalar coupling constant.  In the MPNN model the hidden states for atoms are iteratively updated by means of an update function, and I used a similar mechanism to iteratively update these hidden scalar coupling edge states. I think you're understanding of graph distance is correct. It represents the number of atoms on the shortest path from atom_A -> atom_B. I computed it using the RDkit GetDistanceMatrix function (documentation here http://rdkit.org/docs/source/rdkit.Chem.rdmolops.html . These are all the scalar coupling edge features I used: 'sc_type', 'eucl_dist', 'eucl_dist_min_atom_radius',  'dist_electroneg_adj',  'normed_dist',  'diangle',  'cos_angle',   'cos_angle_nn0', 'cos_angle_nn1'. The 'sc_type' is one hot encoded. The distance features 'eucl_dist_min_atom_radius',  'dist_electroneg_adj' were taken from Giba's kernel. Pilipeyko Grigory Posted 6 years ago · 488th in this Competition arrow_drop_up 1 more_vert @robin111 Thanks for sharing FGPC Posted 6 years ago · 235th in this Competition arrow_drop_up 1 more_vert Congratulations! Amazing solutions! :-) Andrés Miguel Torrubia Sáez Posted 6 years ago · 2nd in this Competition arrow_drop_up 1 more_vert Awesome solution and extra merit for doing this SOLO. Congratulations. pankaj_j Posted 6 years ago · 2204th in this Competition arrow_drop_up 1 more_vert Hi @robin111 , superb solution. Lots of learning from here. Many congratulations. Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 1 more_vert Are you planning to open source your code on GitHub? That'd be great. Thanks for this post, really interesting. Robin N Topic Author Posted 6 years ago · 6th in this Competition arrow_drop_up 2 more_vert Thanks Michel. Yes, I plan on cleaning up my code a bit and making the repository public over the weekend. This comment has been deleted. Andrew Lukyanenko Posted 6 years ago · 8th in this Competition arrow_drop_up 1 more_vert Using attention is a cool idea! We didn't do this. Congratulations and thanks for sharing! Ahmet Erdem Posted 6 years ago · 7th in this Competition arrow_drop_up 1 more_vert Very impressive architecture, congrats! I see that you have started Kaggle recently. Was it your first time working on deep learning? If so, it is even more impressive:) Robin N Topic Author Posted 6 years ago · 6th in this Competition arrow_drop_up 1 more_vert Thanks Ahmet! I had some prior experience with smaller neural nets (mostly tabular models), but nothing with models of this sort of size. XY Posted 6 years ago · 86th in this Competition arrow_drop_up 2 more_vert I’m reading your awesome kernel. Could you teach me where angle_in and angle_out come from ? I’m going to learn from you  ;) Robin N Topic Author Posted 6 years ago · 6th in this Competition arrow_drop_up 2 more_vert Hi XY, glad to see the kernel is of use to you. angle_in_df and angle_out_df together contain all the angles between the chemical bonds in the molecule and the index of the bond the angle corresponds to. Take for example a molecule with the following bonds: (a0, a1), (a0, a2), (a0, a3), (a1, a4) then angle_in_df would contain the following angles and angle indices: (angle_201, 0), (angle_201, 1), (angle_301, 2), (angle_014, 3) and angle_out_df would only have (angle_014, 0) . So angle out refers to the fact that it contains angles centred around the second atom in a bond pair. Check the github repo for the code used to create these inputs. XY Posted 6 years ago · 86th in this Competition arrow_drop_up 1 more_vert Thank you for detailed explanations. Sorry, I must have checked your repo beforehand. I’m very happy with your excellent works. Manoj Prabhakar Posted 6 years ago · 281st in this Competition arrow_drop_up 2 more_vert Awesome write-up Robin. Congrats on finishing 6th in this competition. XiaokangWang Posted 6 years ago · 121st in this Competition arrow_drop_up 2 more_vert Congratulations for your excellent work. Thanks for sharing your smart ideas in such a detailed level. Nanashi Posted 6 years ago · 42nd in this Competition arrow_drop_up 2 more_vert 2 months waiting for this kind of things… No words. Thank you for sharing your knowledge @robin111 Bill Holst Posted 6 years ago · 82nd in this Competition arrow_drop_up 2 more_vert Congratulations on your finish and excellent explanation. Your approach is very unique and well planned. The introduction of features at various levels is a nice approach. Mariam Akter Posted 2 years ago arrow_drop_up 0 more_vert nice solution gl huang Posted 4 years ago arrow_drop_up 0 more_vert Dear sir, where can I find Cheng’s MPNN starter kit? Thank you! David Patton Posted 6 years ago · 1634th in this Competition arrow_drop_up 0 more_vert Awesome. Thanks for sharing. Any estimate of how much money did you spent on this? Zhenlan Posted 6 years ago · 15th in this Competition arrow_drop_up 0 more_vert Congrats for such an achievement. One question, for multi-head attention layers, did you use the edge representation from message passing, meaning each edge (coupling edge states) attend to each edge in the same molecule? Robin N Topic Author Posted 6 years ago · 6th in this Competition arrow_drop_up 0 more_vert Thank you Zhenlan. I only used a hidden coupling edge state in the MPNN model. So I have only looked at applying multi-head attention layers on atom/node states. This comment has been deleted. Appreciation (2) Rashidul H Posted 6 years ago arrow_drop_up 1 more_vert Congrats and thanks a lot for sharing. Simon Moon Posted 3 years ago arrow_drop_up 0 more_vert Thank you for sharing. Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules CPMP · 7th in this Competition  · Posted 6 years ago arrow_drop_up 94 more_vert Solution 7th (CPMP view) First of all, I'd like to thank my team mates, without whom I would not have landed where we landed, and even would not have entered the competition.  Let me also way that I'm very happy to see Ahmet become competition GM.  And Outrunner is only one gold away from it.  I'm sure he'll become GM in his next competition.  I was very lucky to team with them.  It proves 1 + 1 + 1 > 3. I also want to thank @inversion , Kaggle staff in general, and CHAMPS for setting up this very challenging, unflawed contest.  It is refreshing to not have any leak.  Using test data wasn't helpful really either.  And the way @inversion managed the two crisis (QM9 use, and unplanned kernel share) was very effective. I was very reluctant to enter this competition, for two reasons.  First, I was convinced that people with domain knowledge would have an unfair advantage.  I was right as the top of LB shows a high proportion of teams with chemists on board.  Second, it was clear from early discussions that graph NN would be key, and I had absolutely no experience with it. This second reason is a bit weak given most of us here are in same situation! Then Ahmet asked me if I would join, because we almost teamed in a previous competition and said we would team ASAP.  It motivated me to complete an initial effort using lightgbm.  I developed a core set of features, then added features specific to each type.  The features are based on the graph defined by bonds computed using open babel (thanks @borisd for the starter code).  Angles for 2 bonds path, and dihedral angle for 3 bonds paths were key.  I then added features based on the number of 1, 2, 3 bond paths starting on every atom on the path, using aggregates like number by bond type, angle averages, distance averages, etc.  Depending on the type, number of features was between 500 and 2000.  I trained one lgb per type.  With this I got to 2.0 on LB with one fold the day I  entered.  Once trained on all folds, this model got 2.1 on LB. There are few things I did to improve the model. First, I use a very large number of leaves, 2**10 -1.  I am not sure why this is good, but it was.  Other parameter tuning wasn't very important. Second, I duplicated train and test rows with types 2JHH and 3JHH by swapping the two atoms, before feature engineering, because scalar coupling is symmetric.  Final prediction for one row in the original train or test was the average of the two rows predictions after expansion.  This was mentioned early in the forum I think. Third, for 3Jxx bond types, there is often more than one 3 bonds path between the two atoms.  Here again I duplicated, using one row for each 3 bonds path between the two atoms, then averaging their predictions back for final prediction.  I did not see this discussed anywhere. Row duplication improves lgb score by about 0.002, not much, but still useful. I then teamed with Ahmet, and given he had a NN with much better performance than my lgb model I gave up on my base model.  I'll let Ahmet describe his NN (edit: description is available here ).  Let me just say that with some improvements, like using logcosh as loss, and expanding its size, we got to -2.64 on LB using single 4 fold cv run.  I was very happy given this moved us way above where lgb could move us to. Lgb was useful still, this time as a post processing step.  As I did in a previous competition (Web Traffic Forecasting), I used lgb to predict the residuals of the NN, i.e.  predict the difference between the NN prediction and the target.  Tuning lgb with huber loss and a decaying learning rate (yes, this common deep learning practice can also be used for lgb) yields a -0.15 boost on LB.  This plus bagging several runs eventually led to a score better than -2.8. To make sure we get a gold medal we knew we would need another strong model.  I started developing a graph NN from scratch in keras, and was getting CV o -1.9 and LB -2.08 nwith just 4 features (atom and bond type, mulliken charge, and dihedral angle for 3 bond types).  I stopped working on it when we teamed with Outrunner.  Indeed, Outrunner skills in NN clearly outperfom mine, by far.   He had a set of 13 NN models that yield almost -2.9 LB once bagged.  He further improved them to yield -2.92 or so soon after merging.  I will let him describe these models if he wants to. The first thing we did after teaming was to run my lgb postprocessing on his validation prediction.  He was using a single train/val split with only 3% or so of validation data.  Yet, even with this limited data lgb could extract about 0.025 more.  Blending the result with what we had added another 0.03 or so, which moved us better than -2.95. The last progress we made was based on the NN approach used.  A lot of people have used Graph NN where one molecule is a sample, and all coupling for that molecule can be predicted together.  Outrunner and Ahmet use another way, with one sample per train and test row.  They have features that capture the relative position of every atom compared to the two for which the coupling is computed.  For instance distances to these two atoms, and an angle, and more.  But distance to the two atoms and an angle doesn't uniquely identify atom positions.  There is a circular symmetry.  A way to remove this symmetry is to consider other atoms.  Ahmet was doing it but not Outrunner.  I suggested to use middle atoms.  For 2JHx, a0 and a1 are both bond to a middle atom am.  For 3JHx, a0 is bond to am0 which is bond to am1 which is bond to a1.  Outrunner added  distance to am, or am0/am1 depending on the type.  This boosted performance to where we finally ended.  Unfortunately, given we teamed late, he could only retrain 3 of his models with the new feature.  Bagging these leads to -2.99 LB.  Running lgb on top of it and blending with Ahmet-CPMP  blend yields -3.032.  I guess we could have landed higher with more models retrained, but we lacked time and/or GPUs. Bottom line is I am happy this very challenging competition ended.  I probably should have entered earlier, my first sub is from July 22, only 5 weeks ago.  But teaming with two talented people more than made for the lack of time. Edt, you'll find Ahmet NN description here .  Outrunner told me his NN is similar to Ahmet's hence he won't provide a separate description. Please sign in to reply to this topic. comment 44 Comments 1 appreciation  comment Hotness Ahmet Erdem Posted 6 years ago · 7th in this Competition arrow_drop_up 22 more_vert It was a great experience to team up with top Kagglers like @cpmpml and @outrunner . As CPMP explained, I had a custom NN architecture. It wouldn't be competitive enough for gold without CPMP's boosting with LGB. And we wouldn't make it to 7th place without outrunner's NN expertise. I have started working on this competition on Kaggle Kernels and could reach -1.4 via this model: https://www.kaggle.com/divrikwicky/lightweight-version-of-2-65-custom-nn Later, I have added more interaction features, made network larger and run it longer. I have only one model for all the types. Therefore, I use sample weights to mimic the competition metric. This seems to work better than training single models for each type (there are exception types). My NN uses Conv1D extract features for each atom, and there is another Conv1D layer after which uses the feedback from the molecule. I am also happy to be a GM. Hopefully, other people who deserve become GM too, not the ones who try to cut corners:) I have learned a lot from the Kaggle community and my (ex-)teammates. And I hope I will continue learning. Every time I learn something very novel on Kaggle, I am like: CPMP Topic Author Posted 6 years ago · 7th in this Competition arrow_drop_up 1 more_vert Thanks Ahmet.  It was great teaming with you and Outrunner. 9 more replies arrow_drop_down +1 KhanhVD Posted 6 years ago · 884th in this Competition arrow_drop_up 3 more_vert Congrats @cpmpml and your team ! Great Solution and thank you for sharing!! Giba Posted 6 years ago · 28th in this Competition arrow_drop_up 3 more_vert Congrats @divrikwicky , @cpmpml and @outrunner !  Thanks for sharing. Kha Vo Posted 6 years ago · 10th in this Competition arrow_drop_up 3 more_vert Congrats on your team performance. We have some common ideas regarding symmetry and \"middle atom\". What I learned from you in this competition is this: \"Lgb was useful still, this time as a post processing step. As I did in a previous competition (Web Traffic Forecasting), I used lgb to predict the residuals of the NN, i.e. predict the difference between the NN prediction and the target. Tuning lgb with huber loss and a decaying learning rate (yes, this common deep learning practice can also be used for lgb) yields a -0.15 boost on LB. This plus bagging several runs eventually led to a score better than -2.8.\" olivier Posted 6 years ago · 20th in this Competition arrow_drop_up 3 more_vert Thanks for sharing @cpmpml , this is a very interesting solution. I'll keep in mind residual training for future comps. So Creative of you ! Vinayak Tyagi Posted 6 years ago arrow_drop_up 1 more_vert Thanks sir very informative and Congrats Optimo Posted 6 years ago · 55th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for sharing, I'm curious on how and why you apply learning rate decay with LGBM? Could you please share a bit more about this @cpmpml ? CPMP Topic Author Posted 6 years ago · 7th in this Competition arrow_drop_up 2 more_vert Thanks. how and why you apply learning rate decay with LGBM? How: using learning_rates parameter of lgb.train() Why: because of MAE.  lr decay helps converge. 7 more replies arrow_drop_down Jiwei Liu Posted 6 years ago · 33rd in this Competition arrow_drop_up 1 more_vert Big Congrats! I loved that you share the teaming experience. I have a question: How much does dihedral angle for 3 bonds gain? Thanks! CPMP Topic Author Posted 6 years ago · 7th in this Competition arrow_drop_up 1 more_vert How much does dihedral angle for 3 bonds gain? I haven't tested in the final models, but I think that when I added it is was at least 0.05 Vopani Posted 6 years ago · 20th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for sharing your solution @cpmpml ! FGPC Posted 6 years ago · 235th in this Competition arrow_drop_up 1 more_vert Congratulations! Amazing solutions! :-) senkin13 Posted 6 years ago · 15th in this Competition arrow_drop_up 1 more_vert Really creative solution,I want to try next time. CPMP Topic Author Posted 6 years ago · 7th in this Competition arrow_drop_up 1 more_vert Thanks, and congrat for your GM title, well deserved! Sukru Yavuz Posted 6 years ago · 362nd in this Competition arrow_drop_up 1 more_vert I was watching your team through whole competition and was looking forward to see your approach. Thanks for sharing. Proletheus Posted 6 years ago arrow_drop_up 1 more_vert With this I got to 2.0 on LB with one fold the day I entered. what the hell, so thats  what a grandmaster can do… you motivate me to keep going:) Shang Posted 6 years ago · 42nd in this Competition arrow_drop_up 1 more_vert Thanks for sharing! @cpmpml Can I ask a question? You used lgb to predict the residuals of the NN. What is the features for lgb? Is that you used the predictions of NN to be the features for lgb? CPMP Topic Author Posted 6 years ago · 7th in this Competition arrow_drop_up 2 more_vert I used the same features ad lgb standalone plus the NN prediction.  I think this works because the features used for lgb are very different from what the NN learned.  Think of it as stacking (using the NN prediction as feature), but also using original features. Shang Posted 6 years ago · 42nd in this Competition arrow_drop_up 0 more_vert Thanks a lot! Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 1 more_vert Thanks @divrikwicky @cpmpml and congrats! Would you be willing to share your computing resources for this competition + time to train your models? CPMP Topic Author Posted 6 years ago · 7th in this Competition arrow_drop_up 0 more_vert I have 2 1080 Ti that were mostly used to train Ahmet's model, and partly to develop mine.  Ahmet only has 2 K40.  And Outrunner has 2 1080 Ti as well. Ailurophile Posted 6 years ago · 490th in this Competition arrow_drop_up 1 more_vert Very impressive work - Congrats! Excellent write up! Thanks for sharing @cpmpml Tania J Posted 6 years ago · 103rd in this Competition arrow_drop_up 1 more_vert Very impressive work - Congrats! Manoj Prabhakar Posted 6 years ago · 281st in this Competition arrow_drop_up 2 more_vert Congrats to your team. Just one word exemplary work. Marco Gorelli Posted 6 years ago · 933rd in this Competition arrow_drop_up 2 more_vert Thanks for this write-up! You're an excellent writer, your #1 spot in the discussions leader-board is well-deserved. Bill Holst Posted 6 years ago · 82nd in this Competition arrow_drop_up 2 more_vert Excellent write up! Your approach where you used LGB to post-process the NN output is a very useful technique. Appreciation (1) Rashidul H Posted 6 years ago arrow_drop_up 1 more_vert Congrats and thanks a lot for sharing. Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Dieter · 8th in this Competition  · Posted 6 years ago arrow_drop_up 93 more_vert 8th place solution - a densely connected GNN Thanks to the organizers for such an interesting competition. It was a lot of fun exploring the current state of the art of graph neural networks and trying to improve it. I also want to thank @artgor , @tunguz , @borisdee and @psilogram for being amazing team mates. In short, our solution is a custom Graph Neural Network with SchNet interaction at its core, which alone scores ~ -2.95 on leaderboard. The model is a single model for all bonding types. Since my team already had good engineered features for their lgb (with lb score of -2.0) when I joined, I could fully concentrate on what I love: finding a strong deep learning based solution. The journey was as following I started with refactoring Hengs starter kit for MPNN which was at around (-1.4 LB) for speed optimization to have a better speed of iterating ideas. After 2 days I got it down from 15 min to 90 sec  per epoch on my GTX1080Ti , without losing any accuracy. That enabled to quickly explore different ideas to improve MPNN which I implemented then. However the leaderboard score was still worse than our lgb. So I continued to read a lot of papers related to GNN: papers referenced in forum, and newer papers referencing that papers papers and git repos found by stalking top LB positions papers shared by @artgor who posted new ones in our slack before I have read the previous I implemented most of them in hacky way to see if they fit for this competition. Besides others I implemented the papers listed below. Some had usable git repositories for others I need to implement more or less from scratch because they used a different input format or were coded in tensorflow/keras whereas I had setup everything in pytorch. All of them are supposed to predict properties on molecular level, so I adjusted where necessary to work on atom pair level: MPNN GAT Crystal GNN SchNet MegNet 3DGNN For me SchNet not only had the best code basis but also got decent results and was relatively fast, so I started building on that one. In general the model consists of 3 parts: encoder for nodes and edges consecutive blocks of interaction between encoded nodes and edges regression head As mentioned by other solutions SchNet lacks several things, but the core interaction between atoms and edges is good and fast. So I spent some weeks improving that baseline step by step by adding new stuff I read in other papers  or I found useful during computer vision competitions. From every 10 ideas I implemented a maximum of one worked, and it felt like digging for treasures. At the end not much was left from the original SchNet apart from the core interaction function between nodes and edges. The most architectural enhancements were the following using additionally edge features like bondtype, angles additionally updating edges (as in MegNet) densenet similar connections between interaction blocks squeeze and excitation block at the end of an Interaction encoding and using  also molecule level features as in Hengs starter using edge level hidden units for final regression using molecule level hidden units for final regression using a deep architecture (12 interaction blocks) some details on training preprocessing: save graph to disk (40 min) bs: 64 optimizer: Adam loss: weighted lmae (weights by inverse frequency of coupling type counts) lr_schedule: Cyclic Cosine annealing with initial lr 0.0005 and cycles of 200 epochs which enables to  continue training easily by just adding a new cycle. validation: groupkfold (10fold) per molecule epochs: 1200 (and still underfitting) our final submission: We basically just bagged versions of the above model trained with different hyperparameters which took some resources. So we split training by @borisdee @tunguz and myself. Some version of MegNet scoring around -2.5 LB made it also in the final blend but with very minor contribution. @psilogram then used a simple mae optimizer to blend the different models on bonding type level. -> private leaderboard score of -3.001 things that did not work additional task of autoencoding edge or node features metric learning and thousand of others. Happy to answer any questions and thanks for reading. Please sign in to reply to this topic. comment 32 Comments 2 appreciation  comments Hotness Kha Vo Posted 6 years ago · 10th in this Competition arrow_drop_up 12 more_vert May I ask a very impolite question? Are you the person in your avatar photo ? Dieter Topic Author Posted 6 years ago · 8th in this Competition arrow_drop_up 20 more_vert Bojan Tunguz Posted 6 years ago · 8th in this Competition arrow_drop_up 11 more_vert I used a GAN to figure out what he really looks like: Boris D. Posted 6 years ago · 8th in this Competition arrow_drop_up 9 more_vert Btw, my hardware is a DGX with 8 P100 on a single node, as well as 3 additional nodes with 4 V100 on each node. CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 1 more_vert I wanted to ask about it, thanks. 3 more replies arrow_drop_down Dieter Topic Author Posted 6 years ago · 8th in this Competition arrow_drop_up 10 more_vert working on some illustrations: Dieter Topic Author Posted 6 years ago · 8th in this Competition arrow_drop_up 9 more_vert Interaction Block: 3 more replies arrow_drop_down hengck23 Posted 6 years ago · 255th in this Competition arrow_drop_up 10 more_vert \"After 2 days I got it down from 15 min to 90 sec per epoch on my GTX1080Ti , without losing any accuracy. \" That is very impressive !!! XiaokangWang Posted 6 years ago · 121st in this Competition arrow_drop_up 1 more_vert Heng is back! Your code inspired a lot of people. Could you elaborate more what you did here? KhanhVD Posted 6 years ago · 884th in this Competition arrow_drop_up 3 more_vert Amazing Solution and Congrats to you and your team ! Tania J Posted 6 years ago · 103rd in this Competition arrow_drop_up 3 more_vert Congrats! I look forward to learning from your code. Not just the architectures but also how you optimized Heng's code to run so fast! Aditya Soni Posted 6 years ago arrow_drop_up 1 more_vert Who else is in love with the Arch? Thanks Dieter!!! Looking forward to see the snippets :) Joel Hanson Posted 6 years ago · 595th in this Competition arrow_drop_up 1 more_vert Amazing work @christofhenkel and congrats. Ailurophile Posted 6 years ago · 490th in this Competition arrow_drop_up 1 more_vert Congrats! I look forward to learning from your code…. Thanks…!! @christofhenkel Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 2 more_vert Thank you @christofhenkel - very informative. Congrats to you and your team @psilogram @tunguz @artgor and @borisdee . AkiraSosa Posted 6 years ago · 10th in this Competition arrow_drop_up 2 more_vert Congrats! So, you did not use QM9. Right? If it's true, really impressive. Ashesh. Posted 6 years ago · 62nd in this Competition arrow_drop_up 2 more_vert I'm pretty impressed and inspired by the amount of homework which has gone into this work. Thanks for sharing your methodology.  Like many others here, I'm pretty interested in what you did which reduced the epoch time by that great a factor. olivier Posted 6 years ago · 20th in this Competition arrow_drop_up 2 more_vert Amazing work and Congratulations to you and your team ! InnerVoice Posted 6 years ago · 91st in this Competition arrow_drop_up 2 more_vert Congrats for the result. 1200 EPOCHS; how much is your training time. I had implemented SchNEt with edges but due to lack of hardware resources and novice in deep learning I am able to train only ~150 epochs. Looking forward for your source code to learn more things. CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 2 more_vert Well done, congrats on the result!  Do you plan to share your code? Dieter Topic Author Posted 6 years ago · 8th in this Competition arrow_drop_up 8 more_vert Good question. I don't see why not. CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 3 more_vert Thanks, I'll learn a lot given I tried to build a GNN from scratch, and I only scratched the surface! KALE Posted 6 years ago arrow_drop_up 0 more_vert Hi Dieter, if you share the code, will you update the post? Aditya Soni Posted 6 years ago arrow_drop_up 0 more_vert Did you plan to release the code snippets for the arch? Thnx. Appreciation (2) Rashidul H Posted 6 years ago arrow_drop_up 1 more_vert Congrats and thanks a lot for sharing. Stanislav Blinov Posted 6 years ago · 99th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for sharing! Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Jaechang Lim · 9th in this Competition  · Posted 6 years ago arrow_drop_up 38 more_vert #9 Solution: model with only atom type and distance Hi, all Kagglers, I am glad to share a brief explanation of my method and why I design the model as follows. I really appreciate organizers for holding such a nice competition. I learned a lot and really enjoyed the competition. All questions and opinions are always welcome! Strategy I have a few domain knowledge in quantum chemistry but not specific in this competition. The only domain expertise I used is that \"all quantum mechanical properties are determined by atom type and distances between atoms\". This was my fundamental strategy so I used only atom type and distance as input features. However, other participants showed that other features can improve the performance of their models. More surprisingly, some models which don't preserve permutation, rotational, and translational invariance outperformed my models. It really shocked me! I really appreciate their efforts to find such an innovative idea. It gave me inspiration in my research topic. I guess that the reason why such things don't make any problem in this competition is that there are some hidden patterns in atom ordering and 3D geometry of molecules of QM9 set. I wonder how the performance of the models change if we randomly change atom ordering, randomly rotate and translate the geometry of molecules in a large amount. In addition, I still personally believe that such conditions must be satisfied to develop robust and reliable models to predict molecular properties. Model I represented a molecule with a fully connected graph. Initial node feature includes only atom type and initial edge feature includes an only distance between atoms. I started with MPNN architecture. In contrast to original MPNN, my model predict scalar coupling between two atoms from their node state obtained after the propagation step. I used 8 different fully connected layers for each type of scalar coupling. I found that not sharing interaction layers improved the performance. I used 4 interaction block with 8 towers and 256 dimensions. This model achieved about -1.7--1.8 in public LD. The followings are my major modification to my models and corresponding LB score. Replace onehot  encoding of distance to RBF proposed in Schnet paper : LB:-2.1---2.2 Edge update network: -2.4 Edge update network with GRU update function: -2.6 Hyperparameter tunning : -2.8 Ensemble with 10 models with different initial weight parameters: -2.98 Finally, I congrats all the prize winners and thank all participants. I appreciate organizers' efforts again. Please sign in to reply to this topic. comment 8 Comments Hotness fnands Posted 6 years ago · 20th in this Competition arrow_drop_up 1 more_vert Congrats with the gold @jaechang ! And thanks for the hints earlier! CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 1 more_vert Thanks for sharing, and congrats on the solo performance.  We were afraid of you jumping ahead of us at the last minute! More surprisingly, some models which don't preserve permutation, rotational, and translational invariance outperformed my models. It really shocked me! I really appreciate their efforts to find such an innovative idea. It gave me inspiration in my research topic. I guess that the reason why such things don't make any problem in this competition is that there are some hidden patterns in atom ordering and 3D geometry of molecules of QM9 set. I wonder how the performance of the models change if we randomly change atom ordering, randomly rotate and translate the geometry of molecules in a large amount. I have expressed the exact same after seeing team 2 and 3 solutions.  Not to downcast what they did, they did right given their model outperform ours ! Jaechang Lim Topic Author Posted 6 years ago · 9th in this Competition arrow_drop_up 4 more_vert Yes. We should appreciate them as a winner of this competition. Their results broadened my view and help to break my stereotype on developing deep learning models for molecules Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 0 more_vert It gave me inspiration in my research topic. I guess that the reason why such things don 't make any problem in this competition is that there are some hidden patterns in atom ordering and 3D geometry of molecules of QM9 set. I wonder how the performance of the models change if we randomly change atom ordering, randomly rotate and translate the geometry of molecules in a large amount. content_copy It enlightened me to see that one can generate new insights for his research based on ml models that are generating features that are new and unknown for a particular domain of expertise. So cool @jaechang . Pritesh Shrivastava Posted 6 years ago · 617th in this Competition arrow_drop_up 0 more_vert Great performance as a solo user !! Andrés Miguel Torrubia Sáez Posted 6 years ago · 2nd in this Competition arrow_drop_up 0 more_vert Re rotational invariance we needed it when doing Pointnet but did not add much with the Atom Transformer. I may train a model from scratch with rotations again to double check, but as @jaechang said we also suspected there's a relative canonical orientation in the dataset that minimizes need for rotational alignment. Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 0 more_vert Congrats and thanks for the share @jaechang - especially on getting there in solo. This comment has been deleted. Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Kha Vo · 10th in this Competition  · Posted 6 years ago arrow_drop_up 74 more_vert 10th place solution Thanks the organizers for such a great competition and great dataset. I just remembered the very recent nightmarish LANL competition, where its organizers were totally inactive and leaked dataset, then this Molecule competition is indeed one of the best ever. I love it very much. And special thanks to all of my teammates, who each one actually did a great deal of work from the beginning. Thanks so much. In summary, main methods are: SchNet1 (1st train) with input features: node-features (x, y, z, atom_type, mulliken_charges), edge-features (bond binary (for pairs of atoms with distance 1),  angle (for pairs of atoms with distance 2), and dihedral (for pairs of atoms with distance 3). We predict 4 scc (scalar_coupling_constant) contributions and scc itself. This model gave us around -2.7 LB. SchNet2 (2nd train): the model is the same as SchNet1, except we feed 4 out-of-fold scc contributions and scc itself as edge features where applicable, EXCEPT the edge of the learning instance itself (so in other words, we prevent leak from this, and this is not a stacking method). This model gave us -2.85 LB. Scheduling: Cyclic LR. Train 3 to 4 different seeds of 4-fold for each type. Some small types (xJHN) we also had an extra training of 8-fold. Quantile regression and median averaging: since MAE cares more about predicting median of the variables, we tried to use quantile regression (with q=0.5), instead of linear regression, for 2 sub-tasks: 1) blending periodical valley model checkpoints of CyclicLR, where we removed the first 50% earlier checkpoints and only kept the last 50% checkpoints. 2) blending oof predictions of seeds of SchNet1 and SchNet2. This gave us around 0.01-0.02 boost on LB. For averaging test folds predictions, we also use median instead of mean, and found that it worked perfectly on LB. Now, I would like to describe the process of methods from start to end, like a journey. First we began with tree-based models, and without much chemistry knowledge. Zidmie @zidmie in our team wrote a custom code to extract bond information from only 3D coords, which was really accurate and impressive (and later proved unnecessary since we have some chemistry toolboxes to do that). We even know aromatic indicator of each atom by his method! His early work laid a good foundation to get our team to top 15, at the time we were strong enough to add in @joshxsarah and Kyle @petersk20 , who had great mindsets and diverse ideas. It was proved that feature importance permutation is a spectacularly good technique to eliminate useless features. Josh and Kyle with their strong features, together with importance permutation once got us to 6th place. But not late after that we gradually slipped on the LB as we saw tree-based models can't compete GCN in this competition. We then developed a custom GCN code based on Heng's shared code in the forum, which got us -2.5 to -2.6 LB, but can never reach -2.7. Then we merged with Akira @akirasosa , who has extremely great skills in NN! In the remaining of the competition onwards when having 5 people, we exploited all possible local GPUs (6) as well as 6 accounts of Google Cloud Platform (each with $300 free credits), to boost the score until the end. My teammates, especially Akira, will add in necessary information regarding SchNet if I missed something. You can view his separate post about SchNet architecture here: https://www.kaggle.com/c/champs-scalar-coupling/discussion/106293#latest-610869 Our kernels are here: [GCN from Heng's code] https://www.kaggle.com/joshxsarah/custom-gcn-10th-place-solution [SchNet light Kaggle version] https://www.kaggle.com/petersk20/schnet-10th-place-solution Thanks for reading. Please sign in to reply to this topic. comment 44 Comments 2 appreciation  comments Hotness RNA Posted 6 years ago · 39th in this Competition arrow_drop_up 3 more_vert Congratulations to you and your team Kha Vo! Would you mind elaborating on your feature importance permutation work? I've gone in this direction before and am interested to see your methodology, it sounds really interesting. Joshua Colmer Posted 6 years ago · 10th in this Competition arrow_drop_up 1 more_vert @bigironsphere Thanks for your congratulations and great job getting silver! We used the approach found in this kernel - https://www.kaggle.com/speedwagon/permutation-importance A method like RFE or sequential feature selection would take far too long so permutation importance was an efficient method for feature selection (only requires the model to be trained once). I would have liked to experiment with permutation importance on our final SchNet models but it would have been too time-consuming. Back when 99% of people were using LGBM, it was vital to find the best features for specific types and permutation importance proved very helpful with this. Kha Vo Topic Author Posted 6 years ago · 10th in this Competition arrow_drop_up 1 more_vert Hi RNA, If you used permutation importance too, but cannot reach -2.1, I guess we might have just some better features. We crafted some binary features indicating groups of atoms, such as -CH3, -COOH, -OH… so we know that if those groups are connected to any atom of the molecule. RNA Posted 6 years ago · 39th in this Competition arrow_drop_up 1 more_vert Thank you for your replies and congratulations again on your performance! CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 3 more_vert Interesting share. Kha Vo Topic Author Posted 6 years ago · 10th in this Competition arrow_drop_up 0 more_vert 3 more replies arrow_drop_down Yongli Zhang Posted 6 years ago arrow_drop_up 1 more_vert good idea! fnands Posted 6 years ago · 20th in this Competition arrow_drop_up 1 more_vert Congrats on the Gold! FGPC Posted 6 years ago · 235th in this Competition arrow_drop_up 1 more_vert Congratulations! Amazing solutions! :-) Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 1 more_vert Thanks @khahuras and congrats! Would you be willing to share your computing resources for this competition? Kha Vo Topic Author Posted 6 years ago · 10th in this Competition arrow_drop_up 1 more_vert Akira in our team has 5 GPU 1080Ti (if I remember it right…), besides, we use 6 accounts worth of $1800 on GCP, and select P100. Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 0 more_vert Very good to know! Thanks a lot for the valuable info @khahuras . Did you mean that each account was worth 1800$ or all six combined wwas worth that amount? Kha Vo Topic Author Posted 6 years ago · 10th in this Competition arrow_drop_up 0 more_vert Each new credit card will have $300 first free from Google, so 6 accounts = $ 1800. However, computation on GCP is extremely expensive! We only managed to train 2 seeds (on 8 models of 8 types) with those $1800 credits. Alexandre Sauvé Posted 6 years ago · 82nd in this Competition arrow_drop_up 1 more_vert Hi @khahuras and congratulation for your achievement! :-) About your tree based solution, could you detail how many features did you used and which ratio was pruned with the importance permutation technique? We have used it with ~150 features but the outcome was not significant and I'd like to understand why? Kha Vo Topic Author Posted 6 years ago · 10th in this Competition arrow_drop_up 2 more_vert Thanks Sauve. Based on each type, we have 80-150 features. Best LGBM is -2.1. I will share more later. 3 more replies arrow_drop_down alijs Posted 6 years ago · 312th in this Competition arrow_drop_up 1 more_vert Great result, congratulations! Kha Vo Topic Author Posted 6 years ago · 10th in this Competition arrow_drop_up 2 more_vert Thanks Agnis. I want to compete in IEEE but I’m completely worn out. I wish you get a good result, and a little bit lucky in that competition (will it shuffle? I think so…) Yusuke Hamada Posted 6 years ago · 1232nd in this Competition arrow_drop_up 1 more_vert Congratulations! Your solution is very impressive! Ailurophile Posted 6 years ago · 490th in this Competition arrow_drop_up 1 more_vert Congratulations! Amazing work! Looking forward to your team's kernel… @khahuras Kaggoo Posted 6 years ago · 64th in this Competition arrow_drop_up 1 more_vert You guys are legendary. Congrats. Zhuang Jia Posted 6 years ago · 52nd in this Competition arrow_drop_up 1 more_vert Congratulations! Looking forward to see your team kernel! Ashesh. Posted 6 years ago · 62nd in this Competition arrow_drop_up 1 more_vert It is great to see the simplicity of features performing so well. How long did you train the SchNet? How many epochs ? Kha Vo Topic Author Posted 6 years ago · 10th in this Competition arrow_drop_up 0 more_vert 1 day for 1/4 fold of JHC on P100. Around 500-800 epochs as cyclicLR requires a lot of epochs. Nanashi Posted 6 years ago · 42nd in this Competition arrow_drop_up 1 more_vert Amazing work! highly skilled… I couldn't do that with SchNet. did you use chainer implementation? Kha Vo Topic Author Posted 6 years ago · 10th in this Competition arrow_drop_up 0 more_vert Thanks Nanashi @jesucristo . No, we just used conventional PyTorch implementation. Shuhao Cao Posted 6 years ago · 39th in this Competition arrow_drop_up 1 more_vert Heng's starter has 70 or so ACSF features on nodes, did you guys keep it? I found adding it makes the network extremely hard to train. fafafa fafafafa Posted 6 years ago · 94th in this Competition arrow_drop_up 1 more_vert We had a heck of a time trying to get Heng's network to give us anything more than -1.4.  We tried different ACSF settings, SOAP, adding some of the features we used in LightGBM, all sorts of different tweaks to layer sizes, learning rates, etc.  I'm really looking forward to hear how people were able to achieve <-2.0 with it.  Major props to Heng for releasing that code, BTW.  That was awesome of him. 3 more replies arrow_drop_down Rashidul H Posted 6 years ago arrow_drop_up 2 more_vert Congrats! Julya Isakova Posted 6 years ago arrow_drop_up 2 more_vert 👍 Andrew Lukyanenko Posted 6 years ago · 8th in this Competition arrow_drop_up 2 more_vert Congratulations and thank you for sharing! Quantile regression and median averaging approach is quite interesting! Kha Vo Topic Author Posted 6 years ago · 10th in this Competition arrow_drop_up 2 more_vert Thanks Andrew and I love all your kernels in all competitions. You are very passionate and a great asset for Kaggle community. Andrew Lukyanenko Posted 6 years ago · 8th in this Competition arrow_drop_up 1 more_vert Thank you for your kind words! This comment has been deleted. Appreciation (2) XiaokangWang Posted 6 years ago · 121st in this Competition arrow_drop_up 1 more_vert Thanks for sharing your kernel. Stanislav Blinov Posted 6 years ago · 99th in this Competition arrow_drop_up 1 more_vert Congratulations and thanks for sharing! Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Erik Thiede · 12th in this Competition  · Posted 6 years ago arrow_drop_up 52 more_vert 12 Place Solution We basically used this. https://arxiv.org/abs/1906.04015 We modified the architecture slightly so that the outputs would be on the connections of atoms, but that was pretty much it.  For simplicity, we trained one net each on j_1, j_2, and j_3 couplings, and then split it into one net for each coupling type (e.g. 1JHC). The only slightly fancy thing we did was that we pretrained by initially using the Mulliken charges as features, which we removed further on in the training. That's it, no particularly clever tricks :-). P.S.  If anyone has any questions about the paper, Brandon and I will be happy to address them to the best of our ability. Please sign in to reply to this topic. comment 18 Comments 1 appreciation  comment Hotness toshi_k Posted 6 years ago · 19th in this Competition arrow_drop_up 2 more_vert Your submission is uncorrelated with others relatively and contributes mega-merge ! https://www.kaggle.com/c/champs-scalar-coupling/discussion/107661 I wonder why you submission was different from others. It maybe pretraining. Erik Thiede Topic Author Posted 6 years ago · 12th in this Competition arrow_drop_up 1 more_vert I suspect it's because our model works on very different principles.  Most other models were based on graph neural networks or SchNet.  These are perform rotationally invariant calculations.  Ours, however, is rotationally equivariant .  We also do many of the calculations in rotational Fourier space. CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 0 more_vert I am trying to understand the paper, it is opening a whole new world.  How did you implement this?  Pytorch, Tensorflow, something else?  Do you plan to share your code at some point? CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 0 more_vert I found these two repos that seem relevant: https://github.com/HyTruongSon/GraphFlow/ https://github.com/horacepan/CCN Erik Thiede Topic Author Posted 6 years ago · 12th in this Competition arrow_drop_up 1 more_vert Our implementation is built on PyTorch. We're still working on making it presentable, but hopefully an alpha version should be up soon. Son's and Horace's codes are built for graph neural networks. There is a lot of ideological overlap with cormorant, but (unless I'm mistaken) they don't work explicitly with rotational equivariance. The method we are using does, however. https://github.com/zlin7/CGNet Until we get cormorant up, this code might be informative. In the long-run, we'd really like to a write a custom-library for performing rotational equivariant computations. The problem is that, in general, there is a lot of sparse linear algebra on objects that can look like lists of matrices of different shapes. This means that, to really do the calculations fast on a GPU, one needs custom codes. Hopefully we can this this up soon as well. CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 0 more_vert Thanks for the answer.  I'll dig into that too. btk1 Posted 6 years ago · 880th in this Competition arrow_drop_up 0 more_vert Thank you for sharing and congratulations! vithng Posted 6 years ago arrow_drop_up 0 more_vert Thanks for sharing, if I'm not mistaken, the Supplement sections has not yet added to the paper? Do you have any version of ELI5 of your paper for someone who doesn't have background in this topic? Erik Thiede Topic Author Posted 6 years ago · 12th in this Competition arrow_drop_up 1 more_vert Sorry for the belated answer.   The Supplement is coming :-). Here is a belated ELI5.  The basic approach is that we are building an architecture to generalize the idea of standard convolutional layers in Convolutional Neural Networks (CNNs) to molecules.  The key property that convolutional layers are designed to preserve is translational covariance : if you translate the input to the network, the output of the network translates as well.  Ultimately, this allows you to learn commonalities across an image:  if you learn what an object (e.g. a mug) looks likes in one part of the image, you can detect it in a different position in a different image.  It’s also worth discussing how CNN’s work: the covariance comes from a linear transformation (the convolution, hence the name), which is followed by a standard nonlinearity. We want to transfer this to molecules.  Let’s consider the linear convolution first.  But there’s a complication: now the symmetry that we care about isn’t just translation , but rotation in 3 dimensions .  If we rotate a molecule, we want our signal to rotate with it.  Rotations are harder than translations.  There are several complications, including the fact that it is not clear how to “discretize” rotations to perform a convolution without breaking symmetry in some way. Our approach is to use ideas from Fourier analysis. It turns out that it is possible to generalize Fourier analysis not just functions on the line, but in the space of rotations .  This is nice, because convolutions in Real space look like multiplications in Fourier space.  So if we have a Fourier transformed signal, performing convolutions is easy [1].  So we Fourier transform our data, at which point our linear convolutions are just linear algebra. So this helps us define the linear section: what about the nonlinear bit?  It’s actually somewhat nontrivial to build a nonlinear function that doesn’t break covariance, or require transformation back into Real space (which can lead to numerical artifacts).  However, one thing we can do is multiply signals together.  For regular Fourier analysis, this is like constructing beats out of our Fourier modes [2, 3].  We can then efficiently decompose the beating signals into it’s Fourier components using known relations (these are the famous Clebsch-Gordan coefficients), allowing us to keep everything in Fourier space. The only thing that is left to do is to define the signals.  This part is a bit technical, but basically in each layer, we grab signals from neighboring atoms based on their relative orientation of the atom and it’s signal (because our signals are rotationally covariant, they have a notion of orientation) [4]. We then perform the Fourier-space convolution and a few (tensor) multiplications, rinse and repeat! [1] See https://arxiv.org/pdf/1802.03690.pdf for further reading [2] See https://en.wikipedia.org/wiki/Beat_(acoustics ) [3] Specifically we actually take tensor products of signals.  This is more general, but just thinking about multiplications gets the right intuition across. [4] The coefficients for this message passing looks a lot like an edge network, and is what we actually read the signals off of for this Kaggle challenge. CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 0 more_vert Thanks for sharing, and congrats on the result! Andrew Lukyanenko Posted 6 years ago · 8th in this Competition arrow_drop_up 0 more_vert This is impressive! Using a correct architecture seemed to be the key. Zhuang Jia Posted 6 years ago · 52nd in this Competition arrow_drop_up 0 more_vert Congratulations! We're very interested in your solution, hope to see it soon! Bibek Posted 6 years ago arrow_drop_up 0 more_vert Elegant solution!! Congrats Alexey Kotlik Posted 6 years ago arrow_drop_up 0 more_vert Wow, brand new idea… You had a leg up in this :) Congratulations! Erik Thiede Topic Author Posted 6 years ago · 12th in this Competition arrow_drop_up 0 more_vert Thank you! We're very excited to see how far this architecture can go on point-cloud data. Nanashi Posted 6 years ago · 42nd in this Competition arrow_drop_up 0 more_vert wow, 1st time I see it, are you planning to share the implementation ? btw, spectacular result in such a short time :) Erik Thiede Topic Author Posted 6 years ago · 12th in this Competition arrow_drop_up 7 more_vert Absolutely!  We're hoping to release a software package soon (in the next few weeks) for public use. Appreciation (1) Stanislav Blinov Posted 6 years ago · 99th in this Competition arrow_drop_up 0 more_vert Congratulations and thanks for sharing! Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules KF · 13th in this Competition  · Posted 6 years ago arrow_drop_up 29 more_vert 13th place solution First of all, I would like to thank the competition hosts. Here I will try to summarize some of the main points of our solution. 1. Model architecture We chose SchNet (CFConv) as a base neural network architecture. Details are shown below. Message passing part SchNet with Edge Update Might be similar to \"SchNet EU\" in 10th place Atom pair features which we used are explained in 2. Features Prepare Edge Update Networks independently per graph distances (distance=1, 2, 3, and 4 or more) Readout part Sum-up target atom embeddings updated by CFConv (message passing part) Concatenate Atom pair feature (same as used in message passing part) Prediction part: MLP 2 Layer MLPs for each coupling type (1JHC, 1JHN etc.) 2. Features Atom features Only use one-hot expression of chemical elements (C, H, etc.) Atom pair features Gaussian expansion of 3D distance Gaussian expansion of dihedral angle features (cosθ, (cosθ)^2) Bond information (e.g. single-bond, double-bond, etc.) 3. Training 1st-stage Training the model for all coupling types simultaneously. 2nd-stage Fine-tuning the model trained on the 1st-stage to each coupling types individually. 3rd-stage Perform the same training as the 1st stage, using data including examples pseudo-labeled on 2nd-stage. 4th-stage Fine-tuning the model trained on the 3rd-stage to each coupling types individually. Please sign in to reply to this topic. comment 16 Comments Hotness senkin13 Posted 6 years ago · 15th in this Competition arrow_drop_up 1 more_vert Congrats! We want to catch up your team but failed. KF Topic Author Posted 6 years ago · 13th in this Competition arrow_drop_up 1 more_vert Thanks and congrats on the result and on becoming GM! I was afraid of your submissions that overtakes me every day in the last week. But it helped to improve our score, I think. Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 1 more_vert Congrats @kfujikawa , what software are you using to make the image of your model architecture? Very interesting read, thanks so much. KF Topic Author Posted 6 years ago · 13th in this Competition arrow_drop_up 1 more_vert Thank you! I used chainer and chainer-chemistry, and mainly customized the following parts: https://github.com/pfnet-research/chainer-chemistry/blob/master/chainer_chemistry/links/update/schnet_update.py CPMP Posted 6 years ago · 7th in this Competition arrow_drop_up 1 more_vert Congrats on the result!  We tried pseudo labeling and it did not help much.  Interesting to see that it helped you. KF Topic Author Posted 6 years ago · 13th in this Competition arrow_drop_up 0 more_vert Thanks, and congrats you too! I would show you group log MAE curve for the valid set at the 1st-stage and 3rd-stage training. Orange line (with pseudo label) is better than blue line (without pseudo label) in spite of the same NN architecture. I think one of the reasons why pseudo label worked is fine-tuning (2nd-stage) (I forgot to mention this procedure… Sorry. I revised the above document.) We got 8 models of different parameters with the same architecture in 2nd-stage, which could label with accuracy that could not be achieved by 1st-stage training. It might be a kind of distillation, I think. kenmatsu4 Posted 6 years ago · 43rd in this Competition arrow_drop_up 1 more_vert Congrats!!! KF Topic Author Posted 6 years ago · 13th in this Competition arrow_drop_up 0 more_vert Thanks!!! Ailurophile Posted 6 years ago · 490th in this Competition arrow_drop_up 1 more_vert Congrats.. Very Informative…!! Thanks for Sharing…!! @kfujikawa toshi_k Posted 6 years ago · 19th in this Competition arrow_drop_up 2 more_vert Goldおめでとうございます．終盤の追い上げは見事でした． 学習方法がとても工夫されていて，自分には無かった視点なので勉強になりました． KF Topic Author Posted 6 years ago · 13th in this Competition arrow_drop_up 0 more_vert ありがとうございます！ @toshik さんもsoloでこれだけの結果が出せるのは流石です。またtriplet featureもユニークで大変勉強になりました。 AkiraSosa Posted 6 years ago · 10th in this Competition arrow_drop_up 2 more_vert おめでとうございます！スライド参考にさせていただきました。お互い金がとれたようでなによりですね。 KF Topic Author Posted 6 years ago · 13th in this Competition arrow_drop_up 0 more_vert ありがとうございます、 @akirasosa さんもおめでとうございます！スライドがこんな所で活用されると思いませんでしたが、少しでも役立てて頂けたのなら幸いです。 Nanashi Posted 6 years ago · 42nd in this Competition arrow_drop_up 2 more_vert Wow, as I tought… SchNet really outperforms :) are you going to share the code?  How long was the training and what hardware did you use? Thanks. Michel ML Posted 6 years ago · 385th in this Competition arrow_drop_up 0 more_vert I'd be very interested in having the answers to this too. KF Topic Author Posted 6 years ago · 13th in this Competition arrow_drop_up 0 more_vert Thanks for your interest! I may publish my code, but need some time because of its complexity… I used Geforce 2080Ti x2, and p3.2xlarge or p3.8xlarge instances at the last week of the competition. 1st stage (300epoch) and 2nd stage (150epoch) training take about 1 day per each. The effect of 1st stage training on the overall performance was not significant, so I frozen the 1st stage model at an early timing. 3rd stage (130epoch) training takes about 10h ~ 1 day for each coupling type. (1JHN has less data, so computational time is short: 10h) As mentioned above, our model was not computationally efficient, so we could only complete 2 / 10 fold at the 4th stage. Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you measure the magnetic interactions between a pair of atoms? In this competition, you will be predicting the scalar_coupling_constant between atom pairs in molecules, given the two atom types (e.g., C and H), the coupling type (e.g., 2JHC ), and any features you are able to create from the molecule structure ( xyz ) files. For this competition, you will not be predicting all the atom pairs in each molecule rather, you will only need to predict the pairs that are explicitly listed in the train and test files. For example, some molecules contain Fluorine (F), but you will not be predicting the scalar coupling constant for any pair that includes F. The training and test splits are by molecule , so that no molecule in the training data is found in the test data. NOTE: additional data is provided for the molecules in Train only! 130798 files 1.22 GB xyz, csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 1.22 GB structures dipole_moments.csv magnetic_shielding_tensors.csv mulliken_charges.csv potential_energy.csv sample_submission.csv scalar_coupling_contributions.csv structures.csv test.csv train.csv 131k files 47 columns  Too many requests",
    "data_description": "Predicting Molecular Properties | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CHAMPS (CHemistry And Mathematics in Phase Space) · Featured Prediction Competition · 6 years ago Late Submission more_horiz Predicting Molecular Properties Can you measure the magnetic interactions between a pair of atoms? Predicting Molecular Properties Overview Data Code Models Discussion Leaderboard Rules Overview Start May 29, 2019 Close Aug 29, 2019 Merger & Entry Description link keyboard_arrow_up Think you can use your data science smarts to make big predictions at a molecular level? This challenge aims to predict interactions between atoms. Imaging technologies like MRI enable us to see and understand the molecular composition of tissues. Nuclear Magnetic Resonance (NMR) is a closely related technology which uses the same principles to understand the structure and dynamics of proteins and molecules. Researchers around the world conduct NMR experiments to further understanding of the structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, and materials science. This competition is hosted by members of the CHemistry and Mathematics in Phase Space (CHAMPS) at the University of Bristol, Cardiff University, Imperial College and the University of Leeds. Winning teams will have an opportunity to partner with this multi-university research program on an academic publication Your Challenge In this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant). Once the competition finishes, CHAMPS would like to invite the top teams to present their work, discuss the details of their models, and work with them to write a joint research publication which discusses an open-source implementation of the solution. About Scalar Coupling Using NMR to gain insight into a molecule’s structure and dynamics depends on the ability to accurately predict so-called “scalar couplings”. These are effectively the magnetic interactions between a pair of atoms. The strength of this magnetic interaction depends on intervening electrons and chemical bonds that make up a molecule’s three-dimensional structure. Using state-of-the-art methods from quantum mechanics, it is possible to accurately calculate scalar coupling constants given only a 3D molecular structure as input. However, these quantum mechanics calculations are extremely expensive (days or weeks per molecule), and therefore have limited applicability in day-to-day workflows. A fast and reliable method to predict these interactions will allow medicinal chemists to gain structural insights faster and cheaper, enabling scientists to understand how the 3D chemical structure of a molecule affects its properties and behavior. Ultimately, such tools will enable researchers to make progress in a range of important problems, like designing molecules to carry out specific cellular tasks, or designing better drug molecules to fight disease. Join the CHAMPS Scalar Coupling challenge to apply predictive analytics to chemistry and chemical biology. Evaluation link keyboard_arrow_up Submissions are evaluated on the Log of the Mean Absolute Error, calculated for each scalar coupling type, and then averaged across types, so that a 1% decrease in MAE for one type provides the same improvement in score as a 1% decrease for another type. s c o r e = 1 T T ∑ t = 1 log ( 1 n t n t ∑ i = 1 | y i − ^ y i | ) Where: T is the number of scalar coupling types n t is the number of observations of type t y i is the actual scalar coupling constant for the observation ^ y i is the predicted scalar coupling constant for the observation For this metric, the MAE for any group has a floor of 1e-9 , so that the minimum (best) possible score for perfect predictions is approximately -20.7232. Submission File For each id in the test set, you must predict the scalar_coupling_constant variable. The file should contain a header and have the following format: id ,scalar_coupling_constant 4659076 , 0 . 0 4659077 , 0 . 0 4659078 , 0 . 0 etc . content_copy Timeline link keyboard_arrow_up August 21, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete. August 21, 2019 - Pre-trained model and external data disclosure deadline. Participants must disclose any external data or pre-trained models used in the official forum thread in adherence with competition rules . August 21, 2019 - Team merger deadline. This is the last day participants may join or merge teams. August 28, 2019 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up The following prizes will be awarded to the winners of the competition: 1st Place - $12,500 2nd Place - $7,500 3rd Place - $5,000 4th Place - $3,000 5th Place - $2,000 Citation link keyboard_arrow_up Addison Howard, inversion, and Lars Bratholm. Predicting Molecular Properties. https://kaggle.com/competitions/champs-scalar-coupling, 2019. Kaggle. Cite Competition Host CHAMPS (CHemistry And Mathematics in Phase Space) Prizes & Awards $30,000 Awards Points & Medals Participation 15,849 Entrants 3,296 Participants 2,737 Teams 47,719 Submissions Tags Chemistry Tabular Regression Custom Metric Table of Contents collapse_all Description Evaluation Timeline Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "instant-gratification",
    "discussion_links": [
      "/competitions/instant-gratification/discussion/96549",
      "/competitions/instant-gratification/discussion/96504",
      "/competitions/instant-gratification/discussion/96510",
      "/competitions/instant-gratification/discussion/96496",
      "/competitions/instant-gratification/discussion/96506",
      "/competitions/instant-gratification/discussion/96588",
      "/competitions/instant-gratification/discussion/96556",
      "/competitions/instant-gratification/discussion/96531",
      "/competitions/instant-gratification/discussion/96533",
      "/competitions/instant-gratification/discussion/96495",
      "/competitions/instant-gratification/discussion/96616",
      "/competitions/instant-gratification/discussion/96525",
      "/competitions/instant-gratification/discussion/96532",
      "/competitions/instant-gratification/discussion/96711",
      "/competitions/instant-gratification/discussion/96602"
    ],
    "discussion_texts": [
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events A synchronous Kernels-only competition This is an anonymized, binary classification dataset found on a USB stick that washed ashore in a bottle. There was no data dictionary with the dataset, but this poem was handwritten on an accompanying scrap of paper: Silly column names abound, but the test set is a mystery. Careful how you pick and slice, or be left behind by history. In a synchronous Kernels-only competition, the files you can observe and download will be different than the private test set and sample submission. The files may have different id s, may be a different size, and may vary in other ways, depending on the problem. You should structure your code so that it predicts on the public test.csv in the format specified by the public sample_submission.csv , but does not hard code aspects like the id or number of rows. When Kaggle runs your Kernel privately, it substitutes the private test set and sample submission in place of the public ones.  3 files 972.6 MB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 972.6 MB sample_submission.csv test.csv train.csv 3 files 517 columns  Too many requests",
    "data_description": "Instant Gratification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Kaggle · Featured Code Competition · 6 years ago Late Submission more_horiz Instant Gratification A synchronous Kernels-only competition Instant Gratification Overview Data Code Models Discussion Leaderboard Rules Overview Start May 17, 2019 Close Jun 21, 2019 Description link keyboard_arrow_up Welcome to Instant (well, almost ) Gratification! In 2015, Kaggle introduced Kernels as a resource to competition participants. It was a controversial decision to add a code-sharing tool to a competitive coding space. We thought it was important to make Kaggle more than a place where competitions are solved behind closed digital doors. Since then, Kernels has grown from its infancy--essentially a blinking cursor in a docker container--into its teenage years. We now have more compute, longer runtimes, better datasets, GPUs, and an improved interface. We have iterated and tested several Kernels-only (KO) competition formats with a true holdout test set, in particular deploying them when we would have otherwise substituted a two-stage competition . However, the experience of submitting to a Kernels-only competition has typically been asynchronous and imperfect; participants wait many days after a competition has concluded for their selected Kernels to be rerun on the holdout test dataset, the leaderboard updated, and the winners announced. This flow causes heartbreak to participants whose Kernels fail on the unseen test set, leaving them with no way to correct tiny errors that spoil months of hard work. Say Hello to Synchronous KO We're now pleased to announce general support for a synchronous Kernels-only format. When you submit from a Kernel, Kaggle will run the code against both the public test set and private test set in real time. This small-but-substantial tweak improves the experience for participants, the host, and Kaggle: With a truly withheld test set, we are practicing proper, rigorous machine learning. We will be able to offer more varieties of competitions and intend to run many fewer confusing two-stage competitions. You will be able to see if your code runs successfully on the withheld test set and have the leeway to intervene if it fails. We will run all submissions against the private data, not just selected ones. Participants will get the complete and familiar public/private scores available in a traditional competition. The final leaderboard can be released at the end of the competition, without the delay of rerunning Kernels. This competition is a low-stakes, trial-run introduction to our new synchronous KO implementation. We want to test that the process goes smoothly and gather feedback on your experiences. While it may feel like a normal KO competition, there are complicated new mechanics in play, such as the selection logic of Kernels that are still running when the deadline passes. Since the competition also presents an authentic machine learning problem, it will also award Kaggle medals and points. Have fun, good luck, and welcome to the world of synchronous Kernels competitions! Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each id in the test set, you must predict a probability for the target variable. The file should contain a header and have the following format: id ,target ba88c155ba898fc8b5099893036ef205 , 0 . 5 7cbab5cea99169139e7e6d8ff74ebb77 , 0 . 5 7baaf361537fbd8a1aaa2c97a6d4ccc7 , 0 . 5 etc . content_copy Timeline link keyboard_arrow_up The competition will conclude June 20th, 2019 at 11:59 PM UTC. Prizes link keyboard_arrow_up The following prizes will be awarded to the winners of the competition: 1st Place - $2,500 2nd Place - $1,500 3rd Place  - $1,000 Kernels Requirements link keyboard_arrow_up This is a Kernels-only competition Submissions to this competition must be made through Kernels. Your kernel will re-run automatically against an unseen test set. In order for the \"Submit to Competition\" button to be active after a commit, the following conditions must be met: CPU or GPU Kernels <= 9 hours run-time Internet must be turned off Custom Docker images are not permitted In synchronous KO competitions, note that a submission that results in an error--either within the kernel or within the process of scoring--will count against your daily submission limit and will not return the specific error message. This is necessary to prevent probing the private test set. Please see the Kernels-only FAQ for more information on how to submit. Citation link keyboard_arrow_up Will Cukierski. Instant Gratification. https://kaggle.com/competitions/instant-gratification, 2019. Kaggle. Cite Competition Host Kaggle Prizes & Awards $5,000 Awards Points & Medals Participation 5,368 Entrants 2,021 Participants 1,818 Teams 35,776 Submissions Tags Tabular Binary Classification Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Timeline Prizes Kernels Requirements Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "santander-customer-transaction-prediction",
    "discussion_links": [
      "/competitions/santander-customer-transaction-prediction/discussion/89003",
      "/competitions/santander-customer-transaction-prediction/discussion/88939",
      "/competitions/santander-customer-transaction-prediction/discussion/88902",
      "/competitions/santander-customer-transaction-prediction/discussion/88970",
      "/competitions/santander-customer-transaction-prediction/discussion/88897",
      "/competitions/santander-customer-transaction-prediction/discussion/89023",
      "/competitions/santander-customer-transaction-prediction/discussion/88886",
      "/competitions/santander-customer-transaction-prediction/discussion/89302",
      "/competitions/santander-customer-transaction-prediction/discussion/88997",
      "/competitions/santander-customer-transaction-prediction/discussion/88906",
      "/competitions/santander-customer-transaction-prediction/discussion/89009",
      "/competitions/santander-customer-transaction-prediction/discussion/88927",
      "/competitions/santander-customer-transaction-prediction/discussion/88888",
      "/competitions/santander-customer-transaction-prediction/discussion/88946",
      "/competitions/santander-customer-transaction-prediction/discussion/88936",
      "/competitions/santander-customer-transaction-prediction/discussion/89117",
      "/competitions/santander-customer-transaction-prediction/discussion/89112",
      "/competitions/santander-customer-transaction-prediction/discussion/89070",
      "/competitions/santander-customer-transaction-prediction/discussion/88901",
      "/competitions/santander-customer-transaction-prediction/discussion/88962",
      "/competitions/santander-customer-transaction-prediction/discussion/88943",
      "/competitions/santander-customer-transaction-prediction/discussion/88915",
      "/competitions/santander-customer-transaction-prediction/discussion/88974",
      "/competitions/santander-customer-transaction-prediction/discussion/89034",
      "/competitions/santander-customer-transaction-prediction/discussion/88956",
      "/competitions/santander-customer-transaction-prediction/discussion/88967"
    ],
    "discussion_texts": [
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you identify who will make a transaction? You are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column. The task is to predict the value of target column in the test set. 3 files 606.35 MB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 606.35 MB sample_submission.csv test.csv train.csv 3 files 405 columns  Too many requests",
    "data_description": "Santander Customer Transaction Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 6 years ago Late Submission more_horiz Santander Customer Transaction Prediction Can you identify who will make a transaction? Santander Customer Transaction Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Feb 14, 2019 Close Apr 11, 2019 Merger & Entry Description link keyboard_arrow_up At Santander our mission is to help people and businesses prosper.  We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals. Our data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure  we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan? In this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem. Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each Id in the test set, you must make a binary prediction of the target variable. The file should contain a header and have the following format: ID_code, target test_0, 0 test_1, 1 test_2, 0 etc. content_copy Prizes link keyboard_arrow_up 1st Place -   $ 25,000 2nd Place - $ 17,000 3rd Place -  $ 10,000 4th Place -  $ 8,000 5th Place -  $ 5,000 Timeline link keyboard_arrow_up April 3, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete. April 3, 2019 - Team Merger deadline. This is the last day participants may join or merge teams. April 10, 2019 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Mercedes Piedra, Sohier Dane, and Soraya_Jimenez. Santander Customer Transaction Prediction. https://kaggle.com/competitions/santander-customer-transaction-prediction, 2019. Kaggle. Cite Competition Host Banco Santander Prizes & Awards $65,000 Awards Points & Medals Participation 25,493 Entrants 9,787 Participants 8,751 Teams 104,129 Submissions Tags Banking Tabular Binary Classification Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "vsb-power-line-fault-detection",
    "discussion_links": [
      "/competitions/vsb-power-line-fault-detection/discussion/87038",
      "/competitions/vsb-power-line-fault-detection/discussion/86616",
      "/competitions/vsb-power-line-fault-detection/discussion/85170",
      "/competitions/vsb-power-line-fault-detection/discussion/85258",
      "/competitions/vsb-power-line-fault-detection/discussion/85301",
      "/competitions/vsb-power-line-fault-detection/discussion/85186",
      "/competitions/vsb-power-line-fault-detection/discussion/85163"
    ],
    "discussion_texts": [
      "VSB Power Line Fault Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Enet Centre, VSB - T.U. of Ostrava · Featured Prediction Competition · 6 years ago Late Submission more_horiz VSB Power Line Fault Detection Can you detect faults in above-ground electrical lines? VSB Power Line Fault Detection Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "VSB Power Line Fault Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Enet Centre, VSB - T.U. of Ostrava · Featured Prediction Competition · 6 years ago Late Submission more_horiz VSB Power Line Fault Detection Can you detect faults in above-ground electrical lines? VSB Power Line Fault Detection Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "VSB Power Line Fault Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Enet Centre, VSB - T.U. of Ostrava · Featured Prediction Competition · 6 years ago Late Submission more_horiz VSB Power Line Fault Detection Can you detect faults in above-ground electrical lines? VSB Power Line Fault Detection Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "VSB Power Line Fault Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Enet Centre, VSB - T.U. of Ostrava · Featured Prediction Competition · 6 years ago Late Submission more_horiz VSB Power Line Fault Detection Can you detect faults in above-ground electrical lines? VSB Power Line Fault Detection Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "VSB Power Line Fault Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Enet Centre, VSB - T.U. of Ostrava · Featured Prediction Competition · 6 years ago Late Submission more_horiz VSB Power Line Fault Detection Can you detect faults in above-ground electrical lines? VSB Power Line Fault Detection Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "VSB Power Line Fault Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Enet Centre, VSB - T.U. of Ostrava · Featured Prediction Competition · 6 years ago Late Submission more_horiz VSB Power Line Fault Detection Can you detect faults in above-ground electrical lines? VSB Power Line Fault Detection Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "VSB Power Line Fault Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Enet Centre, VSB - T.U. of Ostrava · Featured Prediction Competition · 6 years ago Late Submission more_horiz VSB Power Line Fault Detection Can you detect faults in above-ground electrical lines? VSB Power Line Fault Detection Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you detect faults in above-ground electrical lines? Faults in electric transmission lines can lead to a destructive phenomenon called partial discharge . If left alone, partial discharges can damage equipment to the point that it stops functioning entirely. Your challenge is to detect partial discharges so that repairs can be made before any lasting harm occurs. Each signal contains 800,000 measurements of a power line's voltage, taken over 20 milliseconds. As the underlying electric grid operates at 50 Hz, this means each signal covers a single complete grid cycle. The grid itself operates on a 3-phase power scheme , and all three phases are measured simultaneously. metadata_[train/test].csv [train/test].parquet - The signal data. Each column contains one signal; 800,000 int8 measurements as exported with pyarrow.parquet version 0.11. Please note that this is different than our usual data orientation of one row per observation; the switch makes it possible loading a subset of the signals efficiently. If you haven't worked with Apache Parquet before, please refer to either the Python data loading starter kernel . sample_submission.csv : a valid sample submission. 5 files 12.62 GB csv, parquet Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 12.62 GB metadata_test.csv metadata_train.csv sample_submission.csv test.parquet train.parquet 5 files 9 columns  Too many requests",
    "data_description": "VSB Power Line Fault Detection | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Enet Centre, VSB - T.U. of Ostrava · Featured Prediction Competition · 6 years ago Late Submission more_horiz VSB Power Line Fault Detection Can you detect faults in above-ground electrical lines? VSB Power Line Fault Detection Overview Data Code Models Discussion Leaderboard Rules Overview Start Dec 21, 2018 Close Mar 22, 2019 Merger & Entry Description link keyboard_arrow_up Medium voltage overhead power lines run for hundreds of miles to supply power to cities. These great distances make it expensive to manually inspect the lines for damage that doesn't immediately lead to a power outage, such as a tree branch hitting the line or a flaw in the insulator. These modes of damage lead to a phenomenon known as partial discharge — an electrical discharge which does not bridge the electrodes between an insulation system completely. Partial discharges slowly damage the power line, so left unrepaired they will eventually lead to a power outage or start a fire. Your challenge is to detect partial discharge patterns in signals acquired from these power lines with a new meter designed at the ENET Centre at VŠB . Effective classifiers using this data will make it possible to continuously monitor power lines for faults. ENET Centre researches and develops renewable energy resources with the goal of reducing or eliminating harmful environmental impacts. Their efforts focus on developing technology solutions around transportation and processing of energy raw materials. By developing a solution to detect partial discharge you’ll help reduce  maintenance costs, and prevent power outages. Evaluation link keyboard_arrow_up Submissions are evaluated on the Matthews correlation coefficient (MCC) between the predicted and the observed response. The MCC is given by: M C C = ( T P ∗ T N ) − ( F P ∗ F N ) √ ( T P + F P ) ( T P + F N ) ( T N + F P ) ( T N + F N ) , where TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives. Submission File For each signal in the test set, you must predict a binary prediction for the target variable. The file should contain a header and have the following format: signal_id,target 0,0 1,1 2,0 etc. Prizes link keyboard_arrow_up Cash Prizes Participants with the best score on the private leaderboard are eligible to receive: 1st Place - $ 12,000 2nd Place - $ 8,000 3rd Place - $ 5,000 The winning participants will have the option to contribute a description of their solution to a review paper the hosts will write to to celebrate the progress in this field and also this competition. The host may also invite other contributions at their discretion, so we encourage you to post a description of the novel components of your solution once the competition is over. Timeline link keyboard_arrow_up March 14, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete. March 14, 2019 - Team Merger deadline. This is the last day participants may join or merge teams. March 21, 2019 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Addison Howard, Sohier Dane, and Tomas Vantuch. VSB Power Line Fault Detection. https://kaggle.com/competitions/vsb-power-line-fault-detection, 2018. Kaggle. Cite Competition Host Enet Centre, VSB - T.U. of Ostrava Prizes & Awards $25,000 Awards Points & Medals Participation 7,695 Entrants 1,589 Participants 1,445 Teams 19,481 Submissions Tags Tabular Binary Classification Signal Processing Matthews correlation coefficient Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "elo-merchant-category-recommendation",
    "discussion_links": [
      "/competitions/elo-merchant-category-recommendation/discussion/82036",
      "/competitions/elo-merchant-category-recommendation/discussion/82314",
      "/competitions/elo-merchant-category-recommendation/discussion/82055",
      "/competitions/elo-merchant-category-recommendation/discussion/82093",
      "/competitions/elo-merchant-category-recommendation/discussion/82127",
      "/competitions/elo-merchant-category-recommendation/discussion/82375",
      "/competitions/elo-merchant-category-recommendation/discussion/82166",
      "/competitions/elo-merchant-category-recommendation/discussion/82107",
      "/competitions/elo-merchant-category-recommendation/discussion/82178",
      "/competitions/elo-merchant-category-recommendation/discussion/82235",
      "/competitions/elo-merchant-category-recommendation/discussion/82057",
      "/competitions/elo-merchant-category-recommendation/discussion/82040",
      "/competitions/elo-merchant-category-recommendation/discussion/82075",
      "/competitions/elo-merchant-category-recommendation/discussion/82084"
    ],
    "discussion_texts": [
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Help understand customer loyalty Note: All data is simulated and fictitious, and is not real customer data You will need, at a minimum, the train.csv and test.csv files.  These contain the card_id s that we'll be using for training and prediction. The historical_transactions.csv and new_merchant_transactions.csv files contain information about each card's transactions. historical_transactions.csv contains up to 3 months' worth of transactions for every card at any of the provided merchant_id s. new_merchant_transactions.csv contains the transactions at new merchants ( merchant_id s that this particular card_id has not yet visited) over a period of two months. merchants.csv contains aggregate information for each merchant_id represented in the data set. The data is formatted as follows: train.csv and test.csv contain card_id s and information about the card itself - the first month the card was active, etc. train.csv also contains the target . historical_transactions.csv and new_merchant_transactions.csv are designed to be joined with train.csv , test.csv , and merchants.csv .  They contain information about transactions for each card, as described above. merchants can be joined with the transaction sets to provide additional merchant-level information. You are predicting a loyalty score for each card_id represented in test.csv and sample_submission.csv . Data field descriptions are provided in Data Dictionary.xlsx . 8 files 3.1 GB csv, xlsx Subject to Competition Rules Loading... 3.1 GB Data Dictionary.xlsx Data_Dictionary.xlsx historical_transactions.csv merchants.csv new_merchant_transactions.csv sample_submission.csv test.csv train.csv 8 files 79 columns  Too many requests Could not load files: To see the data you need to agree to the competition rules",
    "data_description": "Elo Merchant Category Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Elo · Featured Prediction Competition · 6 years ago Late Submission more_horiz Elo Merchant Category Recommendation Help understand customer loyalty Elo Merchant Category Recommendation Overview Data Code Models Discussion Leaderboard Rules Overview Start Nov 27, 2018 Close Feb 27, 2019 Merger & Entry Description link keyboard_arrow_up Imagine being hungry in an unfamiliar part of town and getting restaurant recommendations served up, based on your personal preferences, at just the right moment. The recommendation comes with an attached discount from your credit card provider for a local place around the corner! Right now, Elo , one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. But do these promotions work for either the consumer or the merchant? Do customers enjoy their experience? Do merchants see repeat business? Personalization is key. Elo has built machine learning models to understand the most important aspects and preferences in their customers’ lifecycle, from food to shopping. But so far none of them is specifically tailored for an individual or profile. This is where you come in. In this competition, Kagglers will develop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. Your input will improve customers’ lives and help Elo reduce unwanted campaigns, to create the right experience for customers. Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = √ 1 n n ∑ i = 1 ( y i − ˆ y i ) 2 , where ˆ y is the predicted loyalty score for each card_id , and y is the actual loyalty score assigned to a card_id . Submission File card_id , target C_ID_9e86007114 , 0 C_ID_1c9f77086c , 0 . 5 C_ID_07b20e9908 , 0 C_ID_63d6bac69a , 0 C_ID_bbc26a86eb , 0 C_ID_f749aad790 , 0 C_ID_7b5c15ff41 ,- 0 . 25 C_ID_ec6b0f2d30 , 0 C_ID_0a11e759c5 , 0 content_copy Timeline link keyboard_arrow_up February 19, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete. February 19, 2019 - Team Merger deadline. This is the last day participants may join or merge teams. February 19, 2019 - External Data Disclosure deadline. All external data used in the competition must be disclosed in the forums by this date. February 26, 2019 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up 1st Place - $20,000 2nd Place - $15,000 3rd Place - $ 5,000 4th Place - $ 5,000 5th Place - $ 5,000 Citation link keyboard_arrow_up Addison Howard, Breno Esteves, and Phil Culliton. Elo Merchant Category Recommendation. https://kaggle.com/competitions/elo-merchant-category-recommendation, 2018. Kaggle. Cite Competition Host Elo Prizes & Awards $50,000 Awards Points & Medals Participation 18,014 Entrants 4,712 Participants 4,110 Teams 81,772 Submissions Tags Tabular Regression Banking Root Mean Squared Error Table of Contents collapse_all Description Evaluation Timeline Prizes Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "PLAsTiCC-2018",
    "discussion_links": [
      "/competitions/PLAsTiCC-2018/discussion/75033",
      "/competitions/PLAsTiCC-2018/discussion/75059",
      "/competitions/PLAsTiCC-2018/discussion/75131",
      "/competitions/PLAsTiCC-2018/discussion/75011",
      "/competitions/PLAsTiCC-2018/discussion/75040",
      "/competitions/PLAsTiCC-2018/discussion/75061",
      "/competitions/PLAsTiCC-2018/discussion/75012",
      "/competitions/PLAsTiCC-2018/discussion/75316",
      "/competitions/PLAsTiCC-2018/discussion/75174",
      "/competitions/PLAsTiCC-2018/discussion/75237",
      "/competitions/PLAsTiCC-2018/discussion/75134",
      "/competitions/PLAsTiCC-2018/discussion/75054",
      "/competitions/PLAsTiCC-2018/discussion/75167",
      "/competitions/PLAsTiCC-2018/discussion/75262",
      "/competitions/PLAsTiCC-2018/discussion/75140",
      "/competitions/PLAsTiCC-2018/discussion/75213"
    ],
    "discussion_texts": [
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you help make sense of the Universe? A few caveats about the light-curve data are as follows: 18 files 40.12 GB csv, pdf Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 40.12 GB data_note.pdf sample_submission.csv test_set.csv test_set_batch1.csv test_set_batch10.csv test_set_batch11.csv test_set_batch2.csv test_set_batch3.csv test_set_batch4.csv test_set_batch5.csv test_set_batch6.csv test_set_batch7.csv test_set_batch8.csv test_set_batch9.csv test_set_metadata.csv test_set_sample.csv training_set.csv training_set_metadata.csv 18 files 123 columns  Too many requests",
    "data_description": "PLAsTiCC Astronomical Classification | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. LSST Project · Featured Prediction Competition · 7 years ago Late Submission more_horiz PLAsTiCC Astronomical Classification Can you help make sense of the Universe? PLAsTiCC Astronomical Classification Overview Data Code Models Discussion Leaderboard Rules Overview Start Sep 28, 2018 Close Dec 18, 2018 Merger & Entry Description link keyboard_arrow_up Help some of the world's leading astronomers grasp the deepest properties of the universe. The human eye has been the arbiter for the classification of astronomical sources in the night sky for hundreds of years. But a new facility -- the Large Synoptic Survey Telescope (LSST) -- is about to revolutionize the field, discovering 10 to 100 times more astronomical sources that vary in the night sky than we've ever known. Some of these sources will be completely unprecedented! The Photometric LSST Astronomical Time-Series Classification Challenge (PLAsTiCC) asks Kagglers to help prepare to classify the data from this new survey. Competitors will classify astronomical sources that vary with time into different classes, scaling from a small training set to a very large test set of the type the LSST will discover. More background information is available here . Acknowledgements PLAsTiCC is funded through LSST Corporation Grant Award # 2017-03 and administered by the University of Toronto.  Financial support for LSST comes from the National Science Foundation (NSF) through Cooperative Agreement No. 1258333, the Department of Energy (DOE) Office of Science under Contract No. DE-AC02-76SF00515, and private funding raised by the LSST Corporation. The NSF-funded LSST Project Office for construction was established as an operating center under management of the Association of Universities for Research in Astronomy (AURA).  The DOE-funded effort to build the LSST camera is managed by the SLAC National Accelerator Laboratory (SLAC). The National Science Foundation (NSF) is an independent federal agency created by Congress in 1950 to promote the progress of science. NSF supports basic research and people to create knowledge that transforms the future. Photo Credit: M. Park/Inigo Films/LSST/AURA/NSF [1]: https://arxiv.org/abs/1810.00001 Evaluation link keyboard_arrow_up Submissions are evaluated using a weighted multi-class logarithmic loss. The overall effect is such that each class is roughly equally important for the final score. Each object has been labeled with one type. For each object, you must submit a set of predicted probabilities (one for every category). The formula is then $$\\text{Log Loss} = - \\left( \\frac{\\sum^{M}_{i=1} w_{i} \\cdot \\sum_{j=1}^{N_{i}} \\frac{y_{ij}}{N_{i}} \\cdot \\ln  p_{ij} }{\\sum^{M}_{i=1} w_{i}} \\right)$$ where N is the number of objects in the class set, M is the number of classes,  \\\\(ln\\\\) is the natural logarithm, \\\\(y_{ij}\\\\) is 1 if observation (i) belongs to class (j) and 0 otherwise, \\\\(p_{ij}\\\\) is the predicted probability that observation \\\\(i\\\\) belongs to class \\\\(j\\\\). The submitted probabilities for a given object are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\\\(max(min(p,1-10^{-15}),10^{-15})\\\\). Submission File For each object ID in the test set, you must predict a probability for each of the different possible classes. The file should contain a header and have the following format: object_id ,class_6,class_15,class_16,class_42,class_52,class_53,class_62,class_64,class_65,class_67,class_88,class_90,class_92,class_95,class_99 13 , 0 , 0 . 1 , 0 , 0 . 1 , 0 , 0 . 3 , 0 , 0 , 0 , 0 , 0 , 0 . 5 , 0 , 0 , 0 14 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 17 , 0 . 75 , 0 . 23 , 0 , 0 , 0 . 01 , 0 , 0 , 0 , 0 , 0 . 01 , 0 , 0 , 0 , 0 , 0 etc . content_copy Prizes link keyboard_arrow_up Cash Prizes Participants with the best score on the private leaderboard are eligible to receive: 1st Place - $ 12,000 2nd Place - $ 8,000 3rd Place - $ 5,000 Additional Opportunities Thank you for joining our efforts! If you felt that you made innovative contributions in this competition we want to invite you to submit your work to present at an upcoming Large Synoptic Survey Telescope (LSST) workshop. Any participant, not necessarily with the highest score, is welcome to submit model (code) and documentation to a panel with international experts in astronomical transients and machine learning. The panel will select the contribution submissions with the most original approach with respect to traditional astronomical strategy. We will provide a limited budget for travel to the workshop to share your experience with our community. An entry form for those of you interested in this phase will be posted in a discussion thread before the end of the competition. Selected entrants can attend one of the upcoming LSST workshops: Invitations (at least one) to LSST collaboration meeting Feb 2019 in San Francisco,  USA. Invitations (at least one) to LSST collaboration meeting in May 2019 in Sydney, Australia Invitations(at least one) to LSST collaboration meeting in July 2019 in Paris, France Timeline link keyboard_arrow_up December 10, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete. December 10, 2018 - Team Merger deadline. This is the last day participants may join or merge teams. December 17, 2018 - Final submission deadline. January 15, 2019 - LSST Workshop entry deadline. February 15, 2019 - LSST Workshop announcement. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. PLAsTiCC's Team link keyboard_arrow_up We are happy to acknowledge here the people who contributed to the development of PLAsTiCC. PLAsTiCC team: Tarek Allam Jr.* (University College of London, UK) Anita Bahmanyar (University of Toronto, Canada) Rahul Biswas (Stockolm University, Sweden) Mi Dai (Rutgers, The State University of New Jersey, USA) Lluís Galbany (University  of  Pittsburgh, USA) Renée Hložek (Univeristy of Toronto, Canada) Emille E. O. Ishida (CNRS/Universite Clermont Auvergne, France) Saurabh W. Jha (Rutgers, The State University of New Jersey, USA) David O. Jones (University of California Santa Cruz, USA) Michelle Lochner* (African Institute for Mathematical Sciences, South Africa) Ashish A. Mahabal (California Institute of Technology, USA) Alex I. Malz* (New York University, USA) Kaisey S. Mandel (Cambridge University, UK) Juan Rafael Martínez-Galarza (Harvard-Smithsonian Center for Astrophysics, USA) Jason D. McEwen (University College of London, UK) Daniel Muthukrishna (Kavli Institute for Cosmology, UK) Gautham Narayan (Space Telescope Science Institute, USA) Hiranya Peiris* (Stockolm University, Sweden) Christina M. Peters (University of Toronto, Canada) Kara Ponder (University of California Berkeley, USA) Christian N. Setzer* (Stockolm University, Sweden) *These researchers are part of the PLAsTiCC team focusing on metric selection and competition design,  they did not have a role in producing,  seeing,  nor validating the data,  and as such are allowed to compete in the challenge fully. Modelers: In addition, the  PLAsTiCC data set generation relied on numerous members of the astronomical community who provided models of astronomical transients and variables. While we cannot thank them by name at this point, we acknowledge here their anonymous contributions - which will be disclosed once the competition is over. Citation link keyboard_arrow_up Emille, Gautham Narayan, Ranpal Gill, Renee Hlozek, RichardKessler, Sohier Dane, and Timo Bozsolik. PLAsTiCC Astronomical Classification. https://kaggle.com/competitions/PLAsTiCC-2018, 2018. Kaggle. Cite Competition Host LSST Project Prizes & Awards $25,000 Awards Points & Medals Participation 6,941 Entrants 1,320 Participants 1,089 Teams 22,851 Submissions Tags Tabular Astronomy Weighted Multiclass Loss Table of Contents collapse_all Description Evaluation Prizes Timeline PLAsTiCC's Team Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "ga-customer-revenue-prediction",
    "discussion_links": [
      "/competitions/ga-customer-revenue-prediction/discussion/82614",
      "/competitions/ga-customer-revenue-prediction/discussion/81542",
      "/competitions/ga-customer-revenue-prediction/discussion/82263",
      "/competitions/ga-customer-revenue-prediction/discussion/81639",
      "/competitions/ga-customer-revenue-prediction/discussion/82746"
    ],
    "discussion_texts": [
      "Google Analytics Customer Revenue Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. RStudio · Featured Prediction Competition · 6 years ago Late Submission more_horiz Google Analytics Customer Revenue Prediction Predict how much GStore customers will spend Google Analytics Customer Revenue Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Google Analytics Customer Revenue Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. RStudio · Featured Prediction Competition · 6 years ago Late Submission more_horiz Google Analytics Customer Revenue Prediction Predict how much GStore customers will spend Google Analytics Customer Revenue Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Google Analytics Customer Revenue Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. RStudio · Featured Prediction Competition · 6 years ago Late Submission more_horiz Google Analytics Customer Revenue Prediction Predict how much GStore customers will spend Google Analytics Customer Revenue Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Google Analytics Customer Revenue Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. RStudio · Featured Prediction Competition · 6 years ago Late Submission more_horiz Google Analytics Customer Revenue Prediction Predict how much GStore customers will spend Google Analytics Customer Revenue Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Google Analytics Customer Revenue Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. RStudio · Featured Prediction Competition · 6 years ago Late Submission more_horiz Google Analytics Customer Revenue Prediction Predict how much GStore customers will spend Google Analytics Customer Revenue Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict how much GStore customers will spend We have now updated the data to work with the new forward-looking problem formulation.  Note that in this competition you will be predicting the target for ALL users in the posted test set: test_v2.csv , for their transactions in the future time period of December 1st 2018 through January 31st 2019. You will need to download train_v2.csv and test_v2.csv .  These contain the data necessary to make predictions for each fullVisitorId listed in sample_submission_v2.csv . Unfortunately, due to time constraints, the BigQuery version of this data will not be made available immediately. Both train_v2.csv and test_v2.csv contain the columns listed under Data Fields.  Each row in the dataset is one visit to the store.  Because we are predicting the log of the total revenue per user , be aware that not all rows in test_v2.csv will correspond to a row in the submission, but all unique fullVisitorId s will correspond to a row in the submission. There are multiple columns which contain JSON blobs of varying depth.  In one of those JSON columns, totals , the sub-column transactionRevenue contains the revenue information we are trying to predict.  This sub-column exists only for the training data. We are predicting the natural log of the sum of all transactions per user .  Once the data is updated, as noted above, this will be for all users in  test_v2.csv for December 1st, 2018 to January 31st, 2019 .  For every user in the test set, the target is: y u s e r = n ∑ i = 1 t r a n s a c t i o n u s e r i t a r g e t u s e r = ln ( y u s e r + 1 ) Note that the dataset does NOT contain data for December 1st 2018 to January 31st 2019.  You must identify the unique fullVisitorId s in the provided test_v2.csv and make predictions for them for those unseen months. sample_submission_v2.csv is composed of all fullVisitorIds for the 5/1/18 to 10/15/18 time period. The public leaderboard submission for this file is based on a separate timeframe than the private leaderboard. The Public LB is being calculated for those visitors during the same timeframe of 5/1/18 to 10/15/18. This is all publicly-available for the target prediction, but intended to provide some means of signal for those who wish to use it in that way. The Private LB is being calculated on the future-looking timeframe of 12/1/18 to 1/31/19 - for those same set of users. Therefore, your submission that is intended for the public LB timeframe will be different from the private LB timeframe, which will be rescored/recalculated on the future timeframe. Knowing this, competitors should be making explicit final submission selections for those submissions which represent what they expect those future-looking predictions to be. These final submission selections for which this competition permits 2, per the rules, can be made under \"My Submissions\" by the final submission deadline. External data is permitted for this competition, per this forum post . This includes the Google Merchandise Store Demo Account . Although the Demo Account contains the predicted variable, final standings will not benefit from access to this external data, because it requires future-looking predictions. 6 files 35.9 GB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 35.9 GB sample_submission.csv sample_submission_v2.csv test.csv test_v2.csv train.csv train_v2.csv 6 files 54 columns  Too many requests",
    "data_description": "Google Analytics Customer Revenue Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. RStudio · Featured Prediction Competition · 6 years ago Late Submission more_horiz Google Analytics Customer Revenue Prediction Predict how much GStore customers will spend Google Analytics Customer Revenue Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Sep 13, 2018 Close Feb 21, 2019 Merger & Entry Description link keyboard_arrow_up The 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies. RStudio, the developer of free and open tools for R and enterprise-ready products for teams to scale and share work, has partnered with Google Cloud and Kaggle to demonstrate the business impact that thorough data analysis can have. In this competition, you’re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data. Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = √ 1 n n ∑ i = 1 ( y i − ˆ y i ) 2 , where y hat is the natural log of the predicted revenue for a customer and y is the natural log of the actual summed revenue value plus one. Submission File For each fullVisitorId in the test set, you must predict the natural log of their total revenue in PredictedLogRevenue . The submission file should contain a header and have the following format: fullVisitorId ,PredictedLogRevenue 0000000259678714014 , 0 0000049363351866189 , 0 0000053049821714864 , 0 etc . content_copy Prizes link keyboard_arrow_up Leaderboard Prizes : Based on final private leaderboard ranking 1st Place - $12,000 2nd Place - $8,000 3rd Place - $5,000 Special R Usage Prizes : Based on top ranking selected solution using R. At the end of the competition, participants who have created their models in R can volunteer to be eligible for the Special R Usage Prize. The top 3 performers by leaderboard rank whose models are verified by the Competition Sponsor to use R will be awarded these prizes. 1st Place - $10,000 2nd Place - $7,000 3rd Place - $3,000 Note that winners may be eligible to win both prizes. Timeline link keyboard_arrow_up November 23, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete. November 23, 2018 - Team Merger deadline. This is the last day participants may join or merge teams. November 30, 2018 - Final submission deadline. December 1, 2018 to January 31, 2019 - Transaction time period to be used as future-looking test set. By February 15, 2019 - Submissions scored on test set stated above. Private Leaderboard revealed. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. NOTE: The competition's deadline will show as February 15, 2019, even though submissions will not be accepted past November 30, 2018. This allows the test evaluation period's actual transactions to take place, in order to rescore all submissions on the actual test period from December 1, 2018 to January 31, 2019. Citation link keyboard_arrow_up ChristopherCrosbie, Mark McDonald, Mikhail Chrestkha, Phil Culliton, Roger Oberg, and sina chavoshi. Google Analytics Customer Revenue Prediction. https://kaggle.com/competitions/ga-customer-revenue-prediction, 2018. Kaggle. Cite Competition Host RStudio Prizes & Awards $45,000 Awards Points & Medals Participation 25,167 Entrants 1,369 Participants 3,611 Teams 4,171 Submissions Tags Regression Tabular Root Mean Squared Error Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "home-credit-default-risk",
    "discussion_links": [
      "/competitions/home-credit-default-risk/discussion/64821",
      "/competitions/home-credit-default-risk/discussion/64722",
      "/competitions/home-credit-default-risk/discussion/64596",
      "/competitions/home-credit-default-risk/discussion/64487",
      "/competitions/home-credit-default-risk/discussion/64625",
      "/competitions/home-credit-default-risk/discussion/64580",
      "/competitions/home-credit-default-risk/discussion/64474",
      "/competitions/home-credit-default-risk/discussion/64536",
      "/competitions/home-credit-default-risk/discussion/64598",
      "/competitions/home-credit-default-risk/discussion/64504",
      "/competitions/home-credit-default-risk/discussion/64593",
      "/competitions/home-credit-default-risk/discussion/64502",
      "/competitions/home-credit-default-risk/discussion/64505",
      "/competitions/home-credit-default-risk/discussion/64503",
      "/competitions/home-credit-default-risk/discussion/64592",
      "/competitions/home-credit-default-risk/discussion/64548",
      "/competitions/home-credit-default-risk/discussion/64693",
      "/competitions/home-credit-default-risk/discussion/66010",
      "/competitions/home-credit-default-risk/discussion/64609",
      "/competitions/home-credit-default-risk/discussion/64600"
    ],
    "discussion_texts": [
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you predict how capable each applicant is of repaying a loan? application_{train|test}.csv bureau.csv bureau_balance.csv POS_CASH_balance.csv credit_card_balance.csv previous_application.csv installments_payments.csv HomeCredit_columns_description.csv  10 files 2.68 GB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 2.68 GB HomeCredit_columns_description.csv POS_CASH_balance.csv application_test.csv application_train.csv bureau.csv bureau_balance.csv credit_card_balance.csv installments_payments.csv previous_application.csv sample_submission.csv 10 files 346 columns  Too many requests",
    "data_description": "Home Credit Default Risk | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Home Credit Group · Featured Prediction Competition · 7 years ago Late Submission more_horiz Home Credit Default Risk Can you predict how capable each applicant is of repaying a loan? Home Credit Default Risk Overview Data Code Models Discussion Leaderboard Rules Overview Start May 18, 2018 Close Aug 30, 2018 Merger & Entry Description link keyboard_arrow_up Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders. Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities. While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful. Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each SK_ID_CURR in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format: SK_ID_CURR ,TARGET 100001 , 0 . 1 100005 , 0 . 9 100013 , 0 . 2 etc . content_copy Prizes link keyboard_arrow_up 1st Place - $ 35,000 2nd Place - $ 25,000 3rd Place - $ 10,000 Timeline link keyboard_arrow_up August 22, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete. August 22, 2018 - Team Merger deadline. This is the last day participants may join or merge teams. August 29, 2018 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Anna Montoya, inversion, KirillOdintsov, and Martin Kotek. Home Credit Default Risk. https://kaggle.com/competitions/home-credit-default-risk, 2018. Kaggle. Cite Competition Host Home Credit Group Prizes & Awards $70,000 Awards Points & Medals Participation 31,511 Entrants 8,373 Participants 7,176 Teams 131,888 Submissions Tags Tabular Banking Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "trackml-particle-identification",
    "discussion_links": [
      "/competitions/trackml-particle-identification/discussion/63249",
      "/competitions/trackml-particle-identification/discussion/63256",
      "/competitions/trackml-particle-identification/discussion/63330",
      "/competitions/trackml-particle-identification/discussion/67943",
      "/competitions/trackml-particle-identification/discussion/63313",
      "/competitions/trackml-particle-identification/discussion/63250",
      "/competitions/trackml-particle-identification/discussion/63303",
      "/competitions/trackml-particle-identification/discussion/63244",
      "/competitions/trackml-particle-identification/discussion/63254",
      "/competitions/trackml-particle-identification/discussion/63302",
      "/competitions/trackml-particle-identification/discussion/63400"
    ],
    "discussion_texts": [
      "TrackML Particle Tracking Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CERN · Featured Prediction Competition · 7 years ago Late Submission more_horiz TrackML Particle Tracking Challenge High Energy Physics particle tracking in CERN detectors TrackML Particle Tracking Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TrackML Particle Tracking Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CERN · Featured Prediction Competition · 7 years ago Late Submission more_horiz TrackML Particle Tracking Challenge High Energy Physics particle tracking in CERN detectors TrackML Particle Tracking Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TrackML Particle Tracking Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CERN · Featured Prediction Competition · 7 years ago Late Submission more_horiz TrackML Particle Tracking Challenge High Energy Physics particle tracking in CERN detectors TrackML Particle Tracking Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TrackML Particle Tracking Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CERN · Featured Prediction Competition · 7 years ago Late Submission more_horiz TrackML Particle Tracking Challenge High Energy Physics particle tracking in CERN detectors TrackML Particle Tracking Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TrackML Particle Tracking Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CERN · Featured Prediction Competition · 7 years ago Late Submission more_horiz TrackML Particle Tracking Challenge High Energy Physics particle tracking in CERN detectors TrackML Particle Tracking Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TrackML Particle Tracking Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CERN · Featured Prediction Competition · 7 years ago Late Submission more_horiz TrackML Particle Tracking Challenge High Energy Physics particle tracking in CERN detectors TrackML Particle Tracking Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TrackML Particle Tracking Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CERN · Featured Prediction Competition · 7 years ago Late Submission more_horiz TrackML Particle Tracking Challenge High Energy Physics particle tracking in CERN detectors TrackML Particle Tracking Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TrackML Particle Tracking Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CERN · Featured Prediction Competition · 7 years ago Late Submission more_horiz TrackML Particle Tracking Challenge High Energy Physics particle tracking in CERN detectors TrackML Particle Tracking Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TrackML Particle Tracking Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CERN · Featured Prediction Competition · 7 years ago Late Submission more_horiz TrackML Particle Tracking Challenge High Energy Physics particle tracking in CERN detectors TrackML Particle Tracking Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TrackML Particle Tracking Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CERN · Featured Prediction Competition · 7 years ago Late Submission more_horiz TrackML Particle Tracking Challenge High Energy Physics particle tracking in CERN detectors TrackML Particle Tracking Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TrackML Particle Tracking Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CERN · Featured Prediction Competition · 7 years ago Late Submission more_horiz TrackML Particle Tracking Challenge High Energy Physics particle tracking in CERN detectors TrackML Particle Tracking Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events High Energy Physics particle tracking in CERN detectors A python library is available to simplify the data handling. The following files are available for the participants: A dataset comprises multiple independent events, where each event contains simulated measurements (essentially 3D points) of particles generated in a collision between proton bunches at the Large Hadron Collider at CERN . The goal of the tracking machine learning challenge is to group the recorded measurements or hits for each event into tracks, sets of hits that belong to the same initial particle. A solution must uniquely associate each hit to one track. The training dataset contains the recorded hits, their ground truth counterpart and their association to particles, and the initial parameters of those particles. The test dataset contains only the recorded hits. Once unzipped, the dataset is provided as a set of plain .csv files. Each event has four associated files that contain hits, hit cells, particles, and the ground truth association between them. The common prefix, e.g. event000000010 , is always event followed by 9 digits. Submissions must be provided as a single .csv file for the whole dataset with a name starting with submission , e.g. The hits file contains the following values for each hit/entry:  The volume/layer/module id could in principle be deduced from x, y, z. They are given here to simplify detector-specific data handling. The truth file contains the mapping between hits and generating particles and the true particle state at each measured hit. Each entry maps one hit to one particle. The particles files contains the following values for each particle/entry: All entries contain the generated information or ground truth. The cells file contains the constituent active detector cells that comprise each hit. The cells can be used to refine the hit to track association. A cell is the smallest granularity inside each detector module, much like a pixel on a screen, except that depending on the volume_id a cell can be a square or a long rectangle. It is identified by two channel identifiers that are unique within each detector module and encode the position, much like column/row numbers of a matrix. A cell can provide signal information that the detector module has recorded in addition to the position. Depending on the detector type only one of the channel identifiers is valid, e.g. for the strip detectors, and the value might have different resolution. The submission file must associate each hit in each event to one and only one reconstructed particle track. The reconstructed tracks must be uniquely identified only within each event.  Participants are advised to compress the submission file (with zip, bzip2, gzip) before submission to the Kaggle site . The detector is built from silicon slabs (or modules, rectangular or trapezoïdal), arranged in cylinders and disks, which measure the position (or hits) of the particles that cross them. The detector modules are organized into detector groups or volumes identified by a volume id. Inside a volume they are further grouped into layers identified by a layer id. Each layer can contain an arbitrary number of detector modules, the smallest geometrically distinct detector object, each identified by a module_id. Within each group, detector modules are of the same type have e.g. the same granularity. All simulated detector modules are so-called semiconductor sensors that are build from thin silicon sensor chips. Each module can be represented by a two-dimensional, planar, bounded sensitive surface. These sensitive surfaces are subdivided into regular grids that define the detectors cells, the smallest granularity within the detector.  Each module has a different position and orientation described in the detectors file. A local, right-handed coordinate system is defined on each sensitive surface such that the first two coordinates u and v are on the sensitive surface and the third coordinate w is normal to the surface. The orientation and position are defined by the following transformation that transform a position described in local coordinates u,v,w into the equivalent position x,y,z in global coordinates using a rotation matrix and an translation vector (cx,cy,cz) . There are two different module shapes in the detector, rectangular and trapezoidal. The pixel detector ( with volume_id = 7,8,9 ) is fully built from rectangular modules, and so are the cylindrical barrels in volume_id=13,17 . The remaining layers are made out  disks that need trapezoidal shapes to cover the full disk.  [mit_license]: http://www.opensource.org/licenses/MIT 10 files 81.39 GB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 81.39 GB blacklist_training.zip detectors.zip sample_submission.csv.zip test.zip train_1.zip train_2.zip train_3.zip train_4.zip train_5.zip train_sample.zip 10 files  Too many requests",
    "data_description": "TrackML Particle Tracking Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. CERN · Featured Prediction Competition · 7 years ago Late Submission more_horiz TrackML Particle Tracking Challenge High Energy Physics particle tracking in CERN detectors TrackML Particle Tracking Challenge Overview Data Code Models Discussion Leaderboard Rules Overview Start Apr 30, 2018 Close Aug 14, 2018 Merger & Entry Description link keyboard_arrow_up To explore what our universe is made of, scientists at CERN are colliding protons, essentially recreating mini big bangs, and meticulously observing these collisions with intricate silicon detectors. While orchestrating the collisions and observations is already a massive scientific accomplishment, analyzing the enormous amounts of data produced from the experiments is becoming an overwhelming challenge. Event rates have already reached hundreds of millions of collisions per second, meaning physicists must sift through tens of petabytes of data per year. And, as the resolution of detectors improve, ever better software is needed for real-time pre-processing and filtering of the most promising events, producing even more data. To help address this problem, a team of Machine Learning experts and physics scientists working at CERN (the world largest high energy physics laboratory),  has partnered with Kaggle and prestigious sponsors to answer the question: can machine learning assist high energy physics in discovering and characterizing new particles? Specifically, in this competition, you’re challenged to build an algorithm that quickly reconstructs particle tracks from 3D points left in the silicon detectors. This challenge consists of two phases: The Accuracy phase has run on Kaggle from May to 13th August 2018 (Winners to be announced by end September). Here we’ll be focusing on the highest score, irrespective of the evaluation time. This phase is an official IEEE WCCI competition (Rio de Janeiro, Jul 2018). The Throughput phase will run on Codalab starting in September 2018. Participants will submit their software which is evaluated by the platform. Incentive is on the throughput (or speed) of the evaluation while reaching a good score. This phase is an official NIPS competition (Montreal, Dec 2018). All the necessary information for the Accuracy phase is available here on Kaggle site. The overall TrackML challenge web site is there . Evaluation link keyboard_arrow_up Custom metric The evaluation metric for this competition is a custom metric. In one line : it is the intersection between the reconstructed tracks and the ground truth particles, normalized to one for each event, and averaged on the events of the test set . First, each hit is assigned a weight: the few first (starting from the center of the detector) and last hits have a larger weight hits from the more straight tracks (more rare, but more interesting) have a larger weight random hits or hits from very short tracks have weight zero the sum of the weights of all the hits of one event is 1 by construction the hit weights are available in the truth file. They are not revealed for the test dataset Then, the score is constructed as follows: tracks are uniquely matched to particles by the double majority rule: for a given track, the matching particle is the one to which the absolute majority (strictly more that 50%) of the track points belong. the track should have the absolute majority of the points of the matching particle. If any of these constraints is not met, the score for this track is zero the score of a surviving track is the sum of the weights of the points of the intersection between the track and the matching particle. the score of an event is the sum of the score of all its tracks. the final score is the average on the events of the public and private leaderboard test respectively. A perfect algorithm will have a score of 1, while a random one will have a score 0. An example implementation can be found in the trackml python library . Submission Format The submission file should contain three columns: event_id, hit_id, track_id, and should have exactly one line for every hit of every event. event_id is the event number hit_id is the hit number, within that event track_id is the user defined numerical identifier (non negative integer) of the track (the track being the group or cluster of hits). The file should contain a header and have the following format: event_id ,hit_id,track_id 0 , 0 , 21 0 , 1 , 49 0 , 3 , 32 0 , 4 , 0 0 , 5 , 21 etc ... content_copy Timeline link keyboard_arrow_up August 6, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete. August 6, 2018 - Team Merger deadline. This is the last day participants may join or merge teams. August 13, 2018 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Prizes link keyboard_arrow_up Cash Prizes Participants with the best score on the private leaderboard are eligible to receive: 1st Place - $ 12,000 2nd Place - $ 8,000 3rd Place - $ 5,000 Additional Awards and Opportunities The second set of prizes will be attributed by  a jury (with international experts in particle physics tracking algorithms and machine learning) which will select the submission with the most promising balance between the score, evaluation speed and originality with respect to the traditional particle physics combinatorial approach. We will provide an entry form for those of you interested in this phase. To be eligible for this HEP meets ML Award, the team must submit the model (code) by the deadline set by organizers after the end of the competition. The team will decide how the amount of the Award will be divided internally, it being understood that the Award will not cover the travel expenses of team members who belong to the ATLAS nor CMS collaboration or are based at CERN. Prizes which will be distributed under this category : One NVIDIA Tesla V100 GPU Invitations (at least one) to NIPS dec 2018 in Montreal Invitations (at least one) to a grand finale workshop at CERN (Geneva) in spring 2019. Strong performers in this competition may be invited to contribute to a NIPS workshop associated with this competition (pending acceptance from NIPS) with limited travel and conference support. About The Sponsors link keyboard_arrow_up Competition brought to you by: CERN : the European Organization for Nuclear Research, is the world largest high energy physics laboratory. Kaggle is the world's largest data science and machine learning community. While best known for hosting predictive modeling and analytics competitions, Kaggle also hosts thousands of public datasets on its public datasets platform and maintains a free, cloud computational environment called Kaggle Kernels. Platinum sponsors NVIDIA GPU's are powering the world's fastest supercomputers GPU computing is the most pervasive,  accessible, energy-efficient path forward for HPC data centers. GPU's are ushering in the era of Convergence, where modeling and simulation are combined with AI to spur a wave of innovation and insight unmatched since computers were first applied to science problems. UNIGE : The University of Geneva and its faculty of science is heavily invested in fundamental research and machine learning applications.  Its department of particle physics (DPNC) is a strong and long-standing member of the ATLAS collaboration of CERN's LHC. Gold sponsors ChaLearn : is a non-for-profit organization dedicated to educate the public with the organization of scientific competitions, particularly in machine learning. The DATAIA Institute : aims to gather and structure on a scientific site, multidisciplinary expertises of great scope and high visibility to better address the major challenges of data science, artificial intelligence and their applications through decompartmentalization between mathematics, computer science and legal, economic and social sciences. Silver sponsors CERN openlab : is a unique public-private partnership that accelerates the development of cutting-edge ICT solutions for the worldwide LHC community and wider scientific research. Through CERN openlab, CERN collaborates with leading ICT companies and research institutes. Paris-Saclay CDS Using data science to advance domain sciences. INRIA is French research institute of computer science. ERC mPP : This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement no 772369). mPP is an ERC Consolidator Grant coordinated by CERN aiming to promote applications based on modern machine learning for particle physics experiments. ERC RECPT : This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement no 724777). RECEPT is an ERC consolidator grant concerned with studies of lepton universality and real-time reconstruction and analysis of particle trajectories. Common Ground :  the only professional online platform tailored to academics with STEM qualifications looking for a rewarding career in the private sector.\nWe combine job opportunities in Machine Learning, Data Science and Software Engineering with unique company, education and support services so academics can discover the companies they really want to work for and better prepare themselves for the transition into industry. University Paris Sud :  is a place dedicated to high-level research, member of the League of European Research Universities. Itis particularly famous for its very high level in basic research, especially in Mathematics and Physics, while hosting numerous  research programs in Computer Science, Chemistry and Biology. INQNET (INtelligent Quantum NEtworks and Technologies) is a research program of the Alliance of Quantum Technologies.  It aims to accelerate progress in areas of fundamental QIS&T, including quantum AI. Fermilab is America's particle physics and accelerator laboratory. We bring the world together to solve the mysteries of matter, energy, space and time. pytorch is an open source deep learning platform built to be flexible and modular for research, with the stability and support needed for production deployment. It enables fast, flexible experimentation through a tape-based autograd system designed for immediate and python-like execution. Citation link keyboard_arrow_up Anaderi, Andreas Salzburger, CecileGermain, David Rousseau, Guillaume Charpiat, Heather Gray, inversion, Isabelle, JR, Laurent Basara, Maggie, Mikhail Hushchyn, Moritz Kiehn, Paolo Calafiura, Sorme, and Steve Farrell. TrackML Particle Tracking Challenge. https://kaggle.com/competitions/trackml-particle-identification, 2018. Kaggle. Cite Competition Host CERN Prizes & Awards $25,000 Awards Points & Medals Participation 7,174 Entrants 739 Participants 651 Teams 5,776 Submissions Tags Physics Tabular Custom Metric Table of Contents collapse_all Description Evaluation Timeline Prizes About The Sponsors Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "avito-demand-prediction",
    "discussion_links": [
      "/competitions/avito-demand-prediction/discussion/59880",
      "/competitions/avito-demand-prediction/discussion/59871",
      "/competitions/avito-demand-prediction/discussion/59885",
      "/competitions/avito-demand-prediction/discussion/59881",
      "/competitions/avito-demand-prediction/discussion/59914",
      "/competitions/avito-demand-prediction/discussion/60026",
      "/competitions/avito-demand-prediction/discussion/60154",
      "/competitions/avito-demand-prediction/discussion/59886",
      "/competitions/avito-demand-prediction/discussion/60059",
      "/competitions/avito-demand-prediction/discussion/59922",
      "/competitions/avito-demand-prediction/discussion/59936",
      "/competitions/avito-demand-prediction/discussion/60102",
      "/competitions/avito-demand-prediction/discussion/59902",
      "/competitions/avito-demand-prediction/discussion/60006",
      "/competitions/avito-demand-prediction/discussion/59959",
      "/competitions/avito-demand-prediction/discussion/59872",
      "/competitions/avito-demand-prediction/discussion/59899",
      "/competitions/avito-demand-prediction/discussion/60005"
    ],
    "discussion_texts": [
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict demand for an online classified ad To make it easier to download the training images, we have added several smaller zip archives that hold the same images as train_jpg.zip. If you have already downloaded train_jpg.zip you can ignore these completely. 14 files 146.76 GB zip, csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 146.76 GB periods_test.csv periods_train.csv sample_submission.csv test.csv test_active.csv test_jpg.zip train.csv train_active.csv train_jpg.zip train_jpg_0.zip train_jpg_1.zip train_jpg_2.zip train_jpg_3.zip train_jpg_4.zip 14 files 75 columns  Too many requests",
    "data_description": "Avito Demand Prediction Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 7 years ago Late Submission more_horiz Avito Demand Prediction Challenge Predict demand for an online classified ad Avito Demand Prediction Challenge Overview Data Code Models Discussion Leaderboard Rules Overview Start Apr 24, 2018 Close Jun 28, 2018 Merger & Entry Description link keyboard_arrow_up When selling used goods online, a combination of tiny, nuanced details in a product description can make a big difference in drumming up interest. Details like: And, even with an optimized product listing, demand for a product may simply not exist–frustrating sellers who may have over-invested in marketing. Avito , Russia’s largest classified advertisements website, is deeply familiar with this problem. Sellers on their platform sometimes feel frustrated with both too little demand (indicating something is wrong with the product or the product listing) or too much demand (indicating a hot item with a good description was underpriced). In their fourth Kaggle competition, Avito is challenging you to predict demand for an online advertisement based on its full description (title, description, images, etc.), its context (geographically where it was posted, similar ads already posted) and historical demand for similar ads in similar contexts. With this information, Avito can inform sellers on how to best optimize their listing and provide some indication of how much interest they should realistically expect to receive. Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is defined as: RMSE = √ 1 n n ∑ i = 1 ( y i − ˆ y i ) 2 , where y hat is the predicted value and y is the original value. Submission File For each item_id in the test set, you must predict a probability for the deal_probability . Your predictions must be in the range [0, 1]. The submission file should contain a header and have the following format: item_id ,deal_probability 2 , 0 5 , 0 6 , 0 etc . content_copy Prizes link keyboard_arrow_up 1st Place - $12,000 2nd Place - $8,000 3rd Place - $5,000 Timeline link keyboard_arrow_up June 20, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete. June 20, 2018 - Team Merger deadline. This is the last day participants may join or merge teams. June 27, 2018 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Ivan Guz, Julia Elliott, Myagkikh Konstantin, Sohier Dane, Vladislav Kassym, and Wendy Kan. Avito Demand Prediction Challenge. https://kaggle.com/competitions/avito-demand-prediction, 2018. Kaggle. Cite Competition Host Avito Prizes & Awards $25,000 Awards Points & Medals Participation 8,293 Entrants 2,313 Participants 1,868 Teams 43,509 Submissions Tags Text Tabular Image Root Mean Squared Error Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "favorita-grocery-sales-forecasting",
    "discussion_links": [
      "/competitions/favorita-grocery-sales-forecasting/discussion/47582",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47568",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47560",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47529",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47556",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47575",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47564",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47667",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47542",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47534",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47603",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47554",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47535",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47537",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47567",
      "/competitions/favorita-grocery-sales-forecasting/discussion/47609"
    ],
    "discussion_texts": [
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you accurately predict sales for a large grocery chain? In this competition, you will be predicting the unit sales for thousands of items sold at different Favorita stores located in Ecuador. The training data includes dates, store and item information, whether that item was being promoted, as well as the unit sales. Additional files include supplementary information that may be useful in building your models. 8 files 479.88 MB 7z Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 479.88 MB holidays_events.csv.7z items.csv.7z oil.csv.7z sample_submission.csv.7z stores.csv.7z test.csv.7z train.csv.7z transactions.csv.7z 8 files  Too many requests",
    "data_description": "Corporación Favorita Grocery Sales Forecasting | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Corporación Favorita · Featured Prediction Competition · 7 years ago Late Submission more_horiz Corporación Favorita Grocery Sales Forecasting Can you accurately predict sales for a large grocery chain? Corporación Favorita Grocery Sales Forecasting Overview Data Code Models Discussion Leaderboard Rules Overview Start Oct 19, 2017 Close Jan 16, 2018 Merger & Entry Description link keyboard_arrow_up Brick-and-mortar grocery stores are always in a delicate dance with purchasing and sales forecasting. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leaving money on the table and customers fuming. The problem becomes more complex as retailers add new locations with unique needs, new products, ever transitioning seasonal tastes, and unpredictable product marketing. Corporación Favorita , a large Ecuadorian-based grocery retailer, knows this all too well. They operate hundreds of supermarkets, with over 200,000 different products on their shelves. Corporación Favorita has challenged the Kaggle community to build a model that more accurately forecasts product sales. They currently rely on subjective forecasting methods with very little data to back them up and very little automation to execute plans. They’re excited to see how machine learning could better ensure they please customers by having just enough of the right products at the right time. Evaluation link keyboard_arrow_up Submissions are evaluated on the Normalized Weighted Root Mean Squared Logarithmic Error (NWRMSLE), calculated as follows: $$ NWRMSLE = \\sqrt{ \\frac{\\sum_{i=1}^n w_i \\left( \\ln(\\hat{y} i + 1) - \\ln(y_i +1)  \\right)^2  }{\\sum {i=1}^n w_i}} $$ where for row i, ˆ y i is the predicted unit_sales of an item and y i is the actual unit_sales; n is the total number of rows in the test set. The weights, w i , can be found in the items.csv file (see the Data page ). Perishable items are given a weight of 1.25 where all other items are given a weight of 1.00 . This metric is suitable when predicting values across a large range of orders of magnitudes. It avoids penalizing large differences in prediction when both the predicted and the true number are large: predicting 5 when the true value is 50 is penalized more than predicting 500 when the true value is 545. Submission File For each id in the test set, you must predict the unit_sales . Because the metric uses ln(y+1) , submissions are validated to ensure there are no negative predictions. The file should contain a header and have the following format: id ,unit_sales 125497040 , 2 . 5 125497041 , 0 . 0 125497042 , 27 . 9 etc . content_copy Prizes link keyboard_arrow_up Leaderboard Prizes 1st place - $15,000 2nd place - $10,000 3rd place - $5,000 Please note that prize eligibility requires that winners complete & send signed License and Non-Disclosure Terms, per local Ecuadorian law. Special Ecuadorian Local Prize The top 3 ranked individually competing* Ecuadorian residents will be considered by Corporación Favorita for interviews. If hired, the candidates will be eligible for a $5,000 welcome bonus upon their one year anniversary of employment. * Local residents who who compete in teams are not eligible for this special prize. Timeline link keyboard_arrow_up January 8, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete. January 8, 2018 - Team Merger deadline. This is the last day participants may join or merge teams. January 15, 2018 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Corporación Favorita, inversion, Julia Elliott, and Mark McDonald. Corporación Favorita Grocery Sales Forecasting. https://kaggle.com/competitions/favorita-grocery-sales-forecasting, 2017. Kaggle. Cite Competition Host Corporación Favorita Prizes & Awards $30,000 Awards Points & Medals Participation 9,832 Entrants 1,868 Participants 1,671 Teams 31,241 Submissions Tags Regression Food Tabular Custom Metric Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "porto-seguro-safe-driver-prediction",
    "discussion_links": [
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44629",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44558",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44608",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44601",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44700",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44642",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44655",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44579",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44659",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44759",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44821",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44614",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44545",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44711",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44621",
      "/competitions/porto-seguro-safe-driver-prediction/discussion/44784"
    ],
    "discussion_texts": [
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict if a driver will file an insurance claim next year. In this competition, you will predict the probability that an auto insurance policy holder files a claim. In the train and test data, features that belong to similar groupings are tagged as such in the feature names (e.g., ind , reg , car , calc ). In addition, feature names include the postfix bin to indicate binary features and cat to indicate categorical features. Features without these designations are either continuous or ordinal. Values of -1 indicate that the feature was missing from the observation. The target columns signifies whether or not a claim was filed for that policy holder. 3 files 300.58 MB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 300.58 MB sample_submission.csv test.csv train.csv 3 files 119 columns  Too many requests",
    "data_description": "Porto Seguro’s Safe Driver Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Porto Seguro · Featured Prediction Competition · 8 years ago Late Submission more_horiz Porto Seguro’s Safe Driver Prediction Predict if a driver will file an insurance claim next year. Porto Seguro’s Safe Driver Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Sep 29, 2017 Close Nov 30, 2017 Merger & Entry Description link keyboard_arrow_up Nothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years. Porto Seguro , one of Brazil’s largest auto and homeowner insurance companies, completely agrees. Inaccuracies in car insurance company’s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones. In this competition, you’re challenged to build a model that predicts the probability that a driver will initiate an auto insurance claim in the next year. While Porto Seguro has used machine learning for the past 20 years, they’re looking to Kaggle’s machine learning community to explore new, more powerful methods. A more accurate prediction will allow them to further tailor their prices, and hopefully make auto insurance coverage more accessible to more drivers. Evaluation link keyboard_arrow_up Scoring Metric Submissions are evaluated using the Normalized Gini Coefficient. During scoring, observations are sorted from the largest to the smallest predictions. Predictions are only used for ordering observations; therefore, the relative magnitude of the predictions are not used during scoring. The scoring algorithm then compares the cumulative proportion of positive class observations to a theoretical uniform proportion. The Gini Coefficient ranges from approximately 0 for random guessing, to approximately 0.5 for a perfect score. The theoretical maximum for the discrete calculation is (1 - frac_pos) / 2 . The Normalized Gini Coefficient adjusts the score by the theoretical maximum so that the maximum score is 1. The code to calculate Normalized Gini Coefficient in a number of different languages can be found in this forum thread . Submission File For each id in the test set, you must predict a probability of an insurance claim in the target column. The file should contain a header and have the following format: id,target 0,0.1 1,0.9 2,1.0 etc. Prizes link keyboard_arrow_up See the Rules Section labeled \"Prizes\" for the terms on receiving prize money. 1st place - $12,000 2nd place - $8,000 3rd place - $5,000 Timeline link keyboard_arrow_up November 22, 2017 - Entry deadline. You must accept the competition rules before this date in order to compete. November 22, 2017 - Team Merger deadline. This is the last day participants may join or merge teams. November 29, 2017 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Addison Howard, Adriano Moala, and Walter Reade. Porto Seguro’s Safe Driver Prediction. https://kaggle.com/competitions/porto-seguro-safe-driver-prediction, 2017. Kaggle. Cite Competition Host Porto Seguro Prizes & Awards $25,000 Awards Points & Medals Participation 16,556 Entrants 5,784 Participants 5,156 Teams 93,568 Submissions Tags Tabular Binary Classification Normalized Gini Index Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "mercedes-benz-greener-manufacturing",
    "discussion_links": [
      "/competitions/mercedes-benz-greener-manufacturing/discussion/37700",
      "/competitions/mercedes-benz-greener-manufacturing/discussion/36390",
      "/competitions/mercedes-benz-greener-manufacturing/discussion/36128",
      "/competitions/mercedes-benz-greener-manufacturing/discussion/36242",
      "/competitions/mercedes-benz-greener-manufacturing/discussion/36461"
    ],
    "discussion_texts": [
      "Mercedes-Benz Greener Manufacturing | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Daimler · Featured Prediction Competition · 8 years ago Late Submission more_horiz Mercedes-Benz Greener Manufacturing Can you cut the time a Mercedes-Benz spends on the test bench? Mercedes-Benz Greener Manufacturing Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Mercedes-Benz Greener Manufacturing | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Daimler · Featured Prediction Competition · 8 years ago Late Submission more_horiz Mercedes-Benz Greener Manufacturing Can you cut the time a Mercedes-Benz spends on the test bench? Mercedes-Benz Greener Manufacturing Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Mercedes-Benz Greener Manufacturing | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Daimler · Featured Prediction Competition · 8 years ago Late Submission more_horiz Mercedes-Benz Greener Manufacturing Can you cut the time a Mercedes-Benz spends on the test bench? Mercedes-Benz Greener Manufacturing Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Mercedes-Benz Greener Manufacturing | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Daimler · Featured Prediction Competition · 8 years ago Late Submission more_horiz Mercedes-Benz Greener Manufacturing Can you cut the time a Mercedes-Benz spends on the test bench? Mercedes-Benz Greener Manufacturing Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Mercedes-Benz Greener Manufacturing | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Daimler · Featured Prediction Competition · 8 years ago Late Submission more_horiz Mercedes-Benz Greener Manufacturing Can you cut the time a Mercedes-Benz spends on the test bench? Mercedes-Benz Greener Manufacturing Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you cut the time a Mercedes-Benz spends on the test bench?  This dataset contains an anonymized set of variables, each representing a custom feature in a Mercedes car. For example, a variable could be 4WD, added air suspension, or a head-up display. The ground truth is labeled ‘y’ and represents the time (in seconds) that the car took to pass testing for each variable. Variables with letters are categorical. Variables with 0/1 are binary values. 3 files 351.45 kB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 351.45 kB sample_submission.csv.zip test.csv.zip train.csv.zip 3 files  Too many requests",
    "data_description": "Mercedes-Benz Greener Manufacturing | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Daimler · Featured Prediction Competition · 8 years ago Late Submission more_horiz Mercedes-Benz Greener Manufacturing Can you cut the time a Mercedes-Benz spends on the test bench? Mercedes-Benz Greener Manufacturing Overview Data Code Models Discussion Leaderboard Rules Overview Start May 30, 2017 Close Jul 11, 2017 Merger & Entry Description link keyboard_arrow_up Since the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include, for example, the passenger safety cell with crumple zone, the airbag and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium car makers. Daimler’s Mercedes-Benz cars are leaders in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams. . To ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler’s engineers have developed a robust testing system. But, optimizing the speed of their testing system for so many possible feature combinations is complex and time-consuming without a powerful algorithmic approach. As one of the world’s biggest manufacturers of premium cars, safety and efficiency are paramount on Daimler’s production lines. In this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing. Winning algorithms will contribute to speedier testing, resulting in lower carbon dioxide emissions without reducing Daimler’s standards. Evaluation link keyboard_arrow_up Submissions are evaluated on the R^2 value, also called the coefficient of determination . Submission File For each 'ID' in the test set, you must predict the 'y' variable. The file should contain a header and have the following format: ID,y 1 , 100 2,100.33 3,105.81 ... content_copy Prizes link keyboard_arrow_up 1st place - $12,000 2nd place - $8,000 3rd place - $5,000 Timeline link keyboard_arrow_up July 3, 2017 - Entry deadline. You must accept the competition rules before this date in order to compete. July 3, 2017 - Team Merger deadline. This is the last day participants may join or merge teams. July 10, 2017 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Alexander Novy, CH1Mercedes, Christian Drescher, Christian Pfaundler, KOESIM, and Will Cukierski. Mercedes-Benz Greener Manufacturing. https://kaggle.com/competitions/mercedes-benz-greener-manufacturing, 2017. Kaggle. Cite Competition Host Daimler Prizes & Awards $25,000 Awards Points & Medals Participation 8,609 Entrants 4,032 Participants 3,823 Teams 75,157 Submissions Tags Automobiles and Vehicles Regression Tabular R^2 score (coefficient of determination) Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "sberbank-russian-housing-market",
    "discussion_links": [
      "/competitions/sberbank-russian-housing-market/discussion/35684",
      "/competitions/sberbank-russian-housing-market/discussion/35912",
      "/competitions/sberbank-russian-housing-market/discussion/36256",
      "/competitions/sberbank-russian-housing-market/discussion/35700",
      "/competitions/sberbank-russian-housing-market/discussion/35570",
      "/competitions/sberbank-russian-housing-market/discussion/36198"
    ],
    "discussion_texts": [
      "Sberbank Russian Housing Market | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Sberbank · Featured Prediction Competition · 8 years ago Late Submission more_horiz Sberbank Russian Housing Market Can you predict realty price fluctuations in Russia’s volatile economy? Sberbank Russian Housing Market Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Sberbank Russian Housing Market | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Sberbank · Featured Prediction Competition · 8 years ago Late Submission more_horiz Sberbank Russian Housing Market Can you predict realty price fluctuations in Russia’s volatile economy? Sberbank Russian Housing Market Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Sberbank Russian Housing Market | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Sberbank · Featured Prediction Competition · 8 years ago Late Submission more_horiz Sberbank Russian Housing Market Can you predict realty price fluctuations in Russia’s volatile economy? Sberbank Russian Housing Market Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Sberbank Russian Housing Market | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Sberbank · Featured Prediction Competition · 8 years ago Late Submission more_horiz Sberbank Russian Housing Market Can you predict realty price fluctuations in Russia’s volatile economy? Sberbank Russian Housing Market Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Sberbank Russian Housing Market | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Sberbank · Featured Prediction Competition · 8 years ago Late Submission more_horiz Sberbank Russian Housing Market Can you predict realty price fluctuations in Russia’s volatile economy? Sberbank Russian Housing Market Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Sberbank Russian Housing Market | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Sberbank · Featured Prediction Competition · 8 years ago Late Submission more_horiz Sberbank Russian Housing Market Can you predict realty price fluctuations in Russia’s volatile economy? Sberbank Russian Housing Market Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you predict realty price fluctuations in Russia’s volatile economy? The aim of this competition is to predict the sale price of each property. The target variable is called price_doc in train.csv. The training data is from August 2011 to June 2015, and the test set is from July 2015 to May 2016. The dataset also includes information about overall conditions in Russia's economy and finance sector, so you can focus on generating accurate price forecasts for individual properties, without needing to second-guess what the business cycle will do. Update: please see the pinned discussion thread for some optional extra data, resolving an issue with some GIS features. 5 files 22.71 MB zip, txt Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 22.71 MB data_dictionary.txt macro.csv.zip sample_submission.csv.zip test.csv.zip train.csv.zip 5 files  Too many requests",
    "data_description": "Sberbank Russian Housing Market | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Sberbank · Featured Prediction Competition · 8 years ago Late Submission more_horiz Sberbank Russian Housing Market Can you predict realty price fluctuations in Russia’s volatile economy? Sberbank Russian Housing Market Overview Data Code Models Discussion Leaderboard Rules Overview Start Apr 27, 2017 Close Jun 30, 2017 Merger & Entry Description link keyboard_arrow_up Housing costs demand a significant investment from both consumers and developers. And when it comes to planning a budget—whether personal or corporate—the last thing anyone needs is uncertainty about one of their biggets expenses. Sberbank , Russia’s oldest and largest bank, helps their customers by making predictions about realty prices so renters, developers, and lenders are more confident when they sign a lease or purchase a building. Although the housing market is relatively stable in Russia, the country’s volatile economy makes forecasting prices as a function of apartment characteristics a unique challenge. Complex interactions between housing features such as number of bedrooms and location are enough to make pricing predictions complicated. Adding an unstable economy to the mix means Sberbank and their customers need more than simple regression models in their arsenal. In this competition, Sberbank is challenging Kagglers to develop algorithms which use a broad spectrum of features to predict realty prices. Competitors will rely on a rich dataset that includes housing data and macroeconomic patterns. An accurate forecasting model will allow Sberbank to provide more certainty to their customers in an uncertain economy. Evaluation link keyboard_arrow_up Submissions are evaluated on the RMSLE between their predicted prices and the actual data. The target variable, called price_doc in the training set, is the sale price of each property. Submission File For each id in the test set, you must predict the price that the property sold for. The file should contain a header and have the following format: id,price_doc 30474,7118500.44 30475,7118500.44 30476,7118500.44 etc. Prizes link keyboard_arrow_up 1st place - $12,000 2nd place - $8,000 3rd place - $5,000 Timeline link keyboard_arrow_up June 22, 2017 - Entry deadline. You must accept the competition rules before this date in order to compete. June 22, 2017 - Team Merger deadline. This is the last day participants may join or merge teams. June 29, 2017 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Alex Matveev, Anastasia_Sidorova_50806198, and DataCanary. Sberbank Russian Housing Market. https://kaggle.com/competitions/sberbank-russian-housing-market, 2017. Kaggle. Cite Competition Host Sberbank Prizes & Awards $25,000 Awards Points & Medals Participation 10,513 Entrants 3,658 Participants 3,264 Teams 67,594 Submissions Tags Banking Housing Regression Tabular Root Mean Squared Logarithmic Error Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "quora-question-pairs",
    "discussion_links": [
      "/competitions/quora-question-pairs/discussion/34355",
      "/competitions/quora-question-pairs/discussion/34310",
      "/competitions/quora-question-pairs/discussion/34288",
      "/competitions/quora-question-pairs/discussion/34349",
      "/competitions/quora-question-pairs/discussion/34359",
      "/competitions/quora-question-pairs/discussion/34294",
      "/competitions/quora-question-pairs/discussion/34697",
      "/competitions/quora-question-pairs/discussion/34371",
      "/competitions/quora-question-pairs/discussion/34342",
      "/competitions/quora-question-pairs/discussion/34549",
      "/competitions/quora-question-pairs/discussion/34534"
    ],
    "discussion_texts": [
      "Quora Question Pairs | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Quora · Featured Prediction Competition · 8 years ago Late Submission more_horiz Quora Question Pairs Can you identify question pairs that have the same intent? Quora Question Pairs Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Quora Question Pairs | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Quora · Featured Prediction Competition · 8 years ago Late Submission more_horiz Quora Question Pairs Can you identify question pairs that have the same intent? Quora Question Pairs Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Quora Question Pairs | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Quora · Featured Prediction Competition · 8 years ago Late Submission more_horiz Quora Question Pairs Can you identify question pairs that have the same intent? Quora Question Pairs Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Quora Question Pairs | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Quora · Featured Prediction Competition · 8 years ago Late Submission more_horiz Quora Question Pairs Can you identify question pairs that have the same intent? Quora Question Pairs Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Quora Question Pairs | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Quora · Featured Prediction Competition · 8 years ago Late Submission more_horiz Quora Question Pairs Can you identify question pairs that have the same intent? Quora Question Pairs Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Quora Question Pairs | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Quora · Featured Prediction Competition · 8 years ago Late Submission more_horiz Quora Question Pairs Can you identify question pairs that have the same intent? Quora Question Pairs Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Quora Question Pairs | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Quora · Featured Prediction Competition · 8 years ago Late Submission more_horiz Quora Question Pairs Can you identify question pairs that have the same intent? Quora Question Pairs Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Quora Question Pairs | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Quora · Featured Prediction Competition · 8 years ago Late Submission more_horiz Quora Question Pairs Can you identify question pairs that have the same intent? Quora Question Pairs Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Quora Question Pairs | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Quora · Featured Prediction Competition · 8 years ago Late Submission more_horiz Quora Question Pairs Can you identify question pairs that have the same intent? Quora Question Pairs Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Quora Question Pairs | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Quora · Featured Prediction Competition · 8 years ago Late Submission more_horiz Quora Question Pairs Can you identify question pairs that have the same intent? Quora Question Pairs Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Quora Question Pairs | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Quora · Featured Prediction Competition · 8 years ago Late Submission more_horiz Quora Question Pairs Can you identify question pairs that have the same intent? Quora Question Pairs Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you identify question pairs that have the same intent? The goal of this competition is to predict which of the provided pairs of questions contain two questions with the same meaning. The ground truth is the set of labels that have been supplied by human experts. The ground truth labels are inherently subjective, as the true meaning of sentences can never be known with certainty. Human labeling is also a 'noisy' process, and reasonable people will disagree. As a result, the ground truth labels on this dataset should be taken to be 'informed' but not 100% accurate, and may include incorrect labeling. We believe the labels, on the whole, to represent a reasonable consensus, but this may often not be true on a case by case basis for individual items in the dataset. Please note: as an anti-cheating measure, Kaggle has supplemented the test set with computer-generated question pairs. Those rows do not come from Quora, and are not counted in the scoring. All of the questions in the training set are genuine examples from Quora. 4 files 523.24 MB zip, csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 523.24 MB sample_submission.csv.zip test.csv test.csv.zip train.csv.zip 4 files 3 columns  Too many requests",
    "data_description": "Quora Question Pairs | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Quora · Featured Prediction Competition · 8 years ago Late Submission more_horiz Quora Question Pairs Can you identify question pairs that have the same intent? Quora Question Pairs Overview Data Code Models Discussion Leaderboard Rules Overview Start Mar 16, 2017 Close Jun 7, 2017 Merger & Entry Description link keyboard_arrow_up Where else but Quora can a physicist help a chef with a math problem and get cooking tips in return? Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world. Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term. Currently, Quora uses a Random Forest model to identify duplicate questions. In this competition, Kagglers are challenged to tackle this natural language processing problem by applying advanced techniques to classify whether question pairs are duplicates or not. Doing so will make it easier to find high quality answers to questions resulting in an improved experience for Quora writers, seekers, and readers. Evaluation link keyboard_arrow_up Submissions are evaluated on the log loss between the predicted values and the ground truth. Submission File For each ID in the test set, you must predict the probability that the questions are duplicates (a number between 0 and 1). The file should contain a header and have the following format: test_id,is_duplicate 0,0.5 1,0.4 2,0.9 etc. Prizes link keyboard_arrow_up 1st place - $12,500 2nd place - $8,500 3rd place - $4,000 Timeline link keyboard_arrow_up May 30, 2017 - Pretrained model posting deadline. May 30, 2017 - Entry deadline. You must accept the competition rules before this date in order to compete. May 30, 2017 - Team Merger deadline. This is the last day participants may join or merge teams. June 6, 2017 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, and tomtung. Quora Question Pairs. https://kaggle.com/competitions/quora-question-pairs, 2017. Kaggle. Cite Competition Host Quora Prizes & Awards $25,000 Awards Points & Medals Participation 13,937 Entrants 3,850 Participants 3,295 Teams 53,687 Submissions Tags Text Tabular Linguistics Internet Log Loss Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "santander-product-recommendation",
    "discussion_links": [
      "/competitions/santander-product-recommendation/discussion/26835",
      "/competitions/santander-product-recommendation/discussion/26824",
      "/competitions/santander-product-recommendation/discussion/26899",
      "/competitions/santander-product-recommendation/discussion/26845",
      "/competitions/santander-product-recommendation/discussion/26841",
      "/competitions/santander-product-recommendation/discussion/26802",
      "/competitions/santander-product-recommendation/discussion/26838",
      "/competitions/santander-product-recommendation/discussion/26809",
      "/competitions/santander-product-recommendation/discussion/26823",
      "/competitions/santander-product-recommendation/discussion/26816",
      "/competitions/santander-product-recommendation/discussion/26808",
      "/competitions/santander-product-recommendation/discussion/26812"
    ],
    "discussion_texts": [
      "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you pair products with people? In this competition, you are provided with 1.5 years of customers behavior data from Santander bank to predict what new products customers will purchase. The data starts at 2015-01-28 and has monthly records of products a customer has, such as \"credit card\", \"savings account\", etc. You will predict what additional products a customer will get in the last month, 2016-06-28, in addition to what they already have at 2016-05-28. These products are the columns named: ind_(xyz)_ult1, which are the columns #25 - #48 in the training data. You will predict what a customer will buy in addition to what they already had at 2016-05-28 . The test and train sets are split by time, and public and private leaderboard sets are split randomly. Please note: This sample does not include any real Santander Spain customers, and thus it is not representative of Spain's customer base. 3 files 240.06 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 240.06 MB sample_submission.csv.zip test_ver2.csv.zip train_ver2.csv.zip 3 files  Too many requests",
    "data_description": "Santander Product Recommendation | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Product Recommendation Can you pair products with people? Santander Product Recommendation Overview Data Code Models Discussion Leaderboard Rules Overview Start Oct 26, 2016 Close Dec 22, 2016 Merger & Entry Description link keyboard_arrow_up Ready to make a downpayment on your first house? Or looking to leverage the equity in the home you have? To support needs for a range of financial decisions, Santander Bank offers a lending hand to their customers through personalized product recommendations. Under their current system, a small number of Santander’s customers receive many recommendations while many others rarely see any resulting in an uneven customer experience. In their second competition, Santander is challenging Kagglers to predict which products their existing customers will use in the next month based on their past behavior and that of similar customers. With a more effective recommendation system in place, Santander can better meet the individual needs of all customers and ensure their satisfaction no matter where they are in life. Disclaimer: This data set does not include any real Santander Spain's customer, and thus it is not representative of Spain's customer base. Evaluation link keyboard_arrow_up Submissions are evaluated according to the Mean Average Precision @ 7 (MAP@7): M A P @ 7 = 1 | U | | U | ∑ u = 1 1 m i n ( m , 7 ) m i n ( n , 7 ) ∑ k = 1 P ( k ) where |U| is the number of rows (users in two time points), P(k) is the precision at cutoff k, n is the number of predicted products, and m is the number of added products for the given user at that time point. If m = 0, the precision is defined to be 0. Submission File For every user at each time point, you must predict a space-delimited list of the products they added. The file should contain a header and have the following format: ncodpers,added_products 15889,ind_tjcr_fin_ult1 15890,ind_tjcr_fin_ult1 ind_recibo_ult1 15892,ind_nomina_ult1 15893, etc. Prizes link keyboard_arrow_up 1st place - $30,000 2nd place - $20,000 3rd place - $10,000 Timeline link keyboard_arrow_up December 14, 2016 - Entry deadline. You must accept the competition rules before this date in order to compete. December 14, 2016 - Team Merger deadline. This is the last day participants may join or merge teams. December 21 , 2016 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Meg Risdal, Mercedes Piedra, and Wendy Kan. Santander Product Recommendation. https://kaggle.com/competitions/santander-product-recommendation, 2016. Kaggle. Cite Competition Host Banco Santander Prizes & Awards $60,000 Awards Points & Medals Participation 2,997 Entrants 2,051 Participants 1,779 Teams 28,733 Submissions Tags Banking Multiclass Classification Tabular MAP@{K} Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "outbrain-click-prediction",
    "discussion_links": [
      "/competitions/outbrain-click-prediction/discussion/27977",
      "/competitions/outbrain-click-prediction/discussion/27923",
      "/competitions/outbrain-click-prediction/discussion/27926",
      "/competitions/outbrain-click-prediction/discussion/29001",
      "/competitions/outbrain-click-prediction/discussion/27897"
    ],
    "discussion_texts": [
      "Outbrain Click Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Outbrain · Featured Prediction Competition · 8 years ago Late Submission more_horiz Outbrain Click Prediction Can you predict which recommended content each user will click? Outbrain Click Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Outbrain Click Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Outbrain · Featured Prediction Competition · 8 years ago Late Submission more_horiz Outbrain Click Prediction Can you predict which recommended content each user will click? Outbrain Click Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Outbrain Click Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Outbrain · Featured Prediction Competition · 8 years ago Late Submission more_horiz Outbrain Click Prediction Can you predict which recommended content each user will click? Outbrain Click Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Outbrain Click Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Outbrain · Featured Prediction Competition · 8 years ago Late Submission more_horiz Outbrain Click Prediction Can you predict which recommended content each user will click? Outbrain Click Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Outbrain Click Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Outbrain · Featured Prediction Competition · 8 years ago Late Submission more_horiz Outbrain Click Prediction Can you predict which recommended content each user will click? Outbrain Click Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you predict which recommended content each user will click? Data Use Update : The Competition Sponsor has updated the permitted use of this competition's dataset. You may access and use the Competition Data for any purpose, whether commercial or non-commercial, including for participating in the Competition and on Kaggle.com forums, and for academic research and education.  The dataset for this challenge contains a sample of users’ page views and clicks, as observed on multiple publisher sites in the United States between 14-June-2016 and 28-June-2016. Each viewed page or clicked recommendation is further accompanied by some semantic attributes of those documents. For full details, see data specifications below. The dataset contains numerous sets of content recommendations served to a specific user in a specific context. Each context (i.e. a set of recommendations) is given a display_id. In each such set, the user has clicked on at least one recommendation. The identities of the clicked recommendations in the test set are not revealed. Your task is to rank the recommendations in each group by decreasing predicted likelihood of being clicked. As a warning, this is a very large relational dataset. While most of the tables are small enough to fit in memory, the page views log (page_views.csv) is over 2 billion rows and 100GB uncompressed. We have also uploaded a sample version of this file with the first 10,000,000 rows. The MD5 checksum of page_views.csv.zip is 3742c116bab4030e0a7ea1c0be623bd9. Each user in the dataset is represented by a unique id (uuid). A person can view a document (document_id), which is simply a web page with content (e.g.  a news article). On each document, a set of ads (ad_id) are displayed. Each ad belongs to a campaign (campaign_id) run by an advertiser (advertiser_id). You are also provided metadata about the document, such as which entities are mentioned, a taxonomy of categories, the topics mentioned, and the publisher. page_views.csv is a the log of users visiting documents. To save disk space, the timestamps in the entire dataset are relative to the first time in the dataset. If you wish to recover the actual epoch time of the visit, add 1465876799998 to the timestamp. clicks_train.csv is the training set, showing which of a set of ads was clicked. clicks_test.csv is the same as clicks_train.csv, except it does not have the clicked ad. This is the file you should use to predict. Each display_id has only one clicked ad. Note that test set contains display_ids from the entire dataset timeframe. Additionally, the public/private sampling for the competition is uniformly random, not based on time. These sampling choices were intentional, in spite of the possibility that participants can look ahead in time. sample_submission.csv shows the correct submission format. events.csv provides information on the display_id context. It covers both the train and test set. promoted_content.csv provides details on the ads. documents_meta.csv provides details on the documents. documents_topics.csv , documents_entities.csv , and documents_categories.csv all provide information about the content in a document, as well as Outbrain's confidence in each respective relationship. For example, an entity_id can represent a person, organization, or location. The rows in documents_entities.csv give the confidence that the given entity was referred to in the document.  Outbrain is releasing 2 Billion page views and 16,900,000 clicks of 700 Million unique users, across 560 sites. The data is anonymized. Please remember that participants are prohibited from de-anonymizing or reverse engineering data or combining the data with other publicly available information. Outbrain does not collect or hold PII (personally identifiable information), and the user identifiers we are releasing here are obscured. To protect its publisher partners, Outbrain is not releasing URLs of viewed or clicked stories, but rather anonymized document and site identifiers. The task at hand is click prediction, and by downloading the dataset, participants agree to use the data for that task alone, and will not attempt to reverse engineer the mapping from document, site, and user identifiers to URLs, site names or actual users. 11 files 38.96 GB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 38.96 GB clicks_test.csv.zip clicks_train.csv.zip documents_categories.csv.zip documents_entities.csv.zip documents_meta.csv.zip documents_topics.csv.zip events.csv.zip page_views.csv.zip page_views_sample.csv.zip promoted_content.csv.zip sample_submission.csv.zip 11 files  Too many requests",
    "data_description": "Outbrain Click Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Outbrain · Featured Prediction Competition · 8 years ago Late Submission more_horiz Outbrain Click Prediction Can you predict which recommended content each user will click? Outbrain Click Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Oct 5, 2016 Close Jan 19, 2017 Merger & Entry Description link keyboard_arrow_up The internet is a stimulating treasure trove of possibility. Every day we stumble on news stories relevant to our communities or experience the serendipity of finding an article covering our next travel destination. Outbrain , the web’s leading content discovery platform, delivers these moments while we surf our favorite sites. Currently, Outbrain pairs relevant content with curious readers in about 250 billion personalized recommendations every month across many thousands of sites. In this competition, Kagglers are challenged to predict which pieces of content its global base of users are likely to click on. Improving Outbrain’s recommendation algorithm will mean more users uncover stories that satisfy their individual tastes. Evaluation link keyboard_arrow_up Submissions are evaluated according to the Mean Average Precision @12 (MAP@12): M A P @ 12 = 1 | U | | U | ∑ u = 1 m i n ( 12 , n ) ∑ k = 1 P ( k ) where |U| is the number of display_ids, P(k) is the precision at cutoff k, n is the number of predicted ad_ids. Submission File For each display_id in the test set, you must predict a space-delimited list of ad_ids, ordered by decreasing likelihood of being clicked. The candidate ad_ids for each display_id are provided in clicks_test.csv . Note that each display_id can have a different number of associated ads. The file should contain a header and have the following format: display_id,ad_id 16874594,66758 150083 162754 170392 172888 180797 16874595,8846 30609 143982 16874596,11430 57197 132820 153260 173005 288385 289122 289915 etc. Prizes link keyboard_arrow_up 1st place - $12,000 2nd place - $8,000 3rd place - $5,000 Timeline link keyboard_arrow_up January 11, 2017 - Entry deadline. You must accept the competition rules before this date in order to compete. January 11, 2017 - Team Merger deadline. This is the last day participants may join or merge teams. January 18, 2017 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up mjkistler, Ran Locar, Ronny Lempel, RoySassonOB, Rwagner, and Will Cukierski. Outbrain Click Prediction. https://kaggle.com/competitions/outbrain-click-prediction, 2016. Kaggle. Cite Competition Host Outbrain Prizes & Awards $25,000 Awards Points & Medals Participation 2,192 Entrants 1,301 Participants 978 Teams 6,642 Submissions Tags Tabular Internet MAP@{K} Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "bosch-production-line-performance",
    "discussion_links": [
      "/competitions/bosch-production-line-performance/discussion/25434",
      "/competitions/bosch-production-line-performance/discussion/25359",
      "/competitions/bosch-production-line-performance/discussion/25370",
      "/competitions/bosch-production-line-performance/discussion/25382"
    ],
    "discussion_texts": [
      "Bosch Production Line Performance | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Bosch · Featured Prediction Competition · 9 years ago Late Submission more_horiz Bosch Production Line Performance Reduce manufacturing failures Bosch Production Line Performance Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Bosch Production Line Performance | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Bosch · Featured Prediction Competition · 9 years ago Late Submission more_horiz Bosch Production Line Performance Reduce manufacturing failures Bosch Production Line Performance Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Bosch Production Line Performance | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Bosch · Featured Prediction Competition · 9 years ago Late Submission more_horiz Bosch Production Line Performance Reduce manufacturing failures Bosch Production Line Performance Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Bosch Production Line Performance | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Bosch · Featured Prediction Competition · 9 years ago Late Submission more_horiz Bosch Production Line Performance Reduce manufacturing failures Bosch Production Line Performance Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Reduce manufacturing failures The data for this competition represents measurements of parts as they move through Bosch's production lines. Each part has a unique Id. The goal is to predict which parts will fail quality control (represented by a 'Response' = 1). The dataset contains an extremely large number of anonymized features. Features are named according to a convention that tells you the production line, the station on the line, and a feature number. E.g. L3_S36_F3939 is a feature measured on line 3, station 36, and is feature number 3939. On account of the large size of the dataset, we have separated the files by the type of feature they contain: numerical, categorical, and finally, a file with date features. The date features provide a timestamp for when each measurement was taken. Each date column ends in a number that corresponds to the previous feature number. E.g. the value of L0_S0_D1 is the time at which L0_S0_F0 was taken. In addition to being one of the largest datasets (in terms of number of features) ever hosted on Kaggle, the ground truth for this competition is highly imbalanced. Together, these two attributes are expected to make this a challenging problem. 7 files 729.35 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 729.35 MB sample_submission.csv.zip test_categorical.csv.zip test_date.csv.zip test_numeric.csv.zip train_categorical.csv.zip train_date.csv.zip train_numeric.csv.zip 7 files  Too many requests",
    "data_description": "Bosch Production Line Performance | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Bosch · Featured Prediction Competition · 9 years ago Late Submission more_horiz Bosch Production Line Performance Reduce manufacturing failures Bosch Production Line Performance Overview Data Code Models Discussion Leaderboard Rules Overview Start Aug 17, 2016 Close Nov 12, 2016 Merger & Entry Description link keyboard_arrow_up A good chocolate soufflé is decadent, delicious, and delicate. But, it's a challenge to prepare. When you pull a disappointingly deflated dessert out of the oven, you instinctively retrace your steps to identify at what point you went wrong. Bosch , one of the world's leading manufacturing companies, has an imperative to ensure that the recipes for the production of its advanced mechanical components are of the highest quality and safety standards. Part of doing so is closely monitoring its parts as they progress through the manufacturing processes. Because Bosch records data at every step along its assembly lines, they have the ability to apply advanced analytics to improve these manufacturing processes. However, the intricacies of the data and complexities of the production line pose problems for current methods. In this competition, Bosch is challenging Kagglers to predict internal failures using thousands of measurements and tests made for each component along the assembly line. This would enable Bosch to bring quality products at lower costs to the end user. Evaluation link keyboard_arrow_up Submissions are evaluated on the Matthews correlation coefficient (MCC) between the predicted and the observed response. The MCC is given by: M C C = ( T P ∗ T N ) − ( F P ∗ F N ) √ ( T P + F P ) ( T P + F N ) ( T N + F P ) ( T N + F N ) , where TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives. Submission File For each Id in the test set, you must predict a binary prediction for the Response variable. The file should contain a header and have the following format: Id,Response 1,0 2,1 3,0 etc. Prizes link keyboard_arrow_up 1st place - $15,000 2nd place - $10,000 3rd place - $5,000 Timeline link keyboard_arrow_up Timeline for IEEE Big Data Symposium on Data Analytics for Advanced Manufacturing September 30, 2016 - Due date for full symposium papers submission Timeline for Bosch Kaggle Competition November 4, 2016 - Entry deadline. You must accept the competition rules before this date in order to compete. November 4, 2016 - Team Merger deadline. This is the last day competitors may join or merge teams. November 11, 2016 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Ieee Bigdata 2016 link keyboard_arrow_up Symposium on Data Analytics for Advanced Manufacturing Dec.5-8, 2016 Washington D.C., USA This symposium intends to provide a platform for researchers and industry practitioners from manufacturing, information science, and data science disciplines to share their data mining and big-data-analytics-related research results, and practical design or development experiences in the manufacturing industry. Bosch invites participants to apply for one of three $2,000 travel stipends to attend the conference and present research from their work in this competition. Anyone interested in being considered for a stipend should submit a paper (up to 10 page IEEE 2-column format through the online submission portal ) no later than September 30th, 2016 . In addition, they should also fill out this application form . Stipends will be awarded on the basis of application merit, unrelated to performance in the Bosch challenge. Citation link keyboard_arrow_up Meg Risdal, Prasanth, RumiGhosh, soundar, Stefanie W., and Will Cukierski. Bosch Production Line Performance. https://kaggle.com/competitions/bosch-production-line-performance, 2016. Kaggle. Cite Competition Host Bosch Prizes & Awards $30,000 Awards Points & Medals Participation 2,651 Entrants 1,599 Participants 1,370 Teams 26,126 Submissions Tags Manufacturing Tabular Binary Classification Matthews correlation coefficient Table of Contents collapse_all Description Evaluation Prizes Timeline Ieee Bigdata 2016 Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "predicting-red-hat-business-value",
    "discussion_links": [
      "/competitions/predicting-red-hat-business-value/discussion/23786",
      "/competitions/predicting-red-hat-business-value/discussion/23824",
      "/competitions/predicting-red-hat-business-value/discussion/23803",
      "/competitions/predicting-red-hat-business-value/discussion/23780",
      "/competitions/predicting-red-hat-business-value/discussion/23773"
    ],
    "discussion_texts": [
      "Predicting Red Hat Business Value | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Predicting Red Hat Business Value Classify customer potential Predicting Red Hat Business Value Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Red Hat Business Value | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Predicting Red Hat Business Value Classify customer potential Predicting Red Hat Business Value Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Red Hat Business Value | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Predicting Red Hat Business Value Classify customer potential Predicting Red Hat Business Value Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Red Hat Business Value | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Predicting Red Hat Business Value Classify customer potential Predicting Red Hat Business Value Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Predicting Red Hat Business Value | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Predicting Red Hat Business Value Classify customer potential Predicting Red Hat Business Value Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Classify customer potential This competition uses two separate data files that may be joined together to create a single, unified data table: a people file and an activity file. The people file contains all of the unique people (and the corresponding characteristics) that have performed activities over time. Each row in the people file represents a unique person. Each person has a unique people_id. The activity file contains all of the unique activities (and the corresponding activity characteristics) that each person has performed over time. Each row in the activity file represents a unique activity performed by a person on a certain date. Each activity has a unique activity_id. The challenge of this competition is to predict the potential business value of a person who has performed a specific activity. The business value outcome is defined by a yes/no field attached to each unique activity in the activity file. The outcome field indicates whether or not each person has completed the outcome within a fixed window of time after each unique activity was performed. The activity file contains several different categories of activities. Type 1 activities are different from type 2-7 activities because there are more known characteristics associated with type 1 activities (nine in total) than type 2-7 activities (which have only one associated characteristic). To develop a predictive model with this data, you will likely need to join the files together into a single data set. The two files can be joined together using person_id as the common key. All variables are categorical, with the exception of 'char_38' in the people file, which is a continuous numerical variable. 4 files 26.74 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 26.74 MB act_test.csv.zip act_train.csv.zip people.csv.zip sample_submission.csv.zip 4 files  Too many requests",
    "data_description": "Predicting Red Hat Business Value | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Predicting Red Hat Business Value Classify customer potential Predicting Red Hat Business Value Overview Data Code Models Discussion Leaderboard Rules Overview Start Aug 1, 2016 Close Sep 20, 2016 Merger & Entry Description link keyboard_arrow_up Like most companies, Red Hat is able to gather a great deal of information over time about the behavior of individuals who interact with them. They’re in search of better methods of using this behavioral data to predict which individuals they should approach—and even when and how to approach them. In this competition, Kagglers are challenged to create a classification algorithm that accurately identifies which customers have the most potential business value for Red Hat based on their characteristics and activities. With an improved prediction model in place, Red Hat will be able to more efficiently prioritize resources to generate more business and better serve their customers. Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted and the observed outcome. Submission File For each activity_id in the test set, you must predict a probability for the 'outcome' variable, represented by a number between 0 and 1. The file should contain a header and have the following format: activity_id,outcome act1_1,0 act1_100006,0 act1_100050,0 etc. Prizes link keyboard_arrow_up 1st place - $25,000 2nd place - $15,000 3rd place - $10,000 Timeline link keyboard_arrow_up September 12, 2016 - First Submission deadline. A team must make at least one submission by this date in order to compete. September 12, 2016 - Team Merger deadline. This is the last day competitors may join or merge teams. September 19, 2016 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up AJ2642, Leslie Schenkel, Meg Risdal, and Will Cukierski. Predicting Red Hat Business Value. https://kaggle.com/competitions/predicting-red-hat-business-value, 2016. Kaggle. Cite Prizes & Awards $50,000 Awards Points & Medals Participation 3,016 Entrants 2,398 Participants 2,260 Teams 33,554 Submissions Tags Tabular Business Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "talkingdata-mobile-user-demographics",
    "discussion_links": [
      "/competitions/talkingdata-mobile-user-demographics/discussion/23465",
      "/competitions/talkingdata-mobile-user-demographics/discussion/23445",
      "/competitions/talkingdata-mobile-user-demographics/discussion/23424",
      "/competitions/talkingdata-mobile-user-demographics/discussion/23463"
    ],
    "discussion_texts": [
      "TalkingData Mobile User Demographics | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. TalkingData · Featured Prediction Competition · 9 years ago Late Submission more_horiz TalkingData Mobile User Demographics Get to know millions of mobile device users TalkingData Mobile User Demographics Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TalkingData Mobile User Demographics | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. TalkingData · Featured Prediction Competition · 9 years ago Late Submission more_horiz TalkingData Mobile User Demographics Get to know millions of mobile device users TalkingData Mobile User Demographics Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TalkingData Mobile User Demographics | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. TalkingData · Featured Prediction Competition · 9 years ago Late Submission more_horiz TalkingData Mobile User Demographics Get to know millions of mobile device users TalkingData Mobile User Demographics Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "TalkingData Mobile User Demographics | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. TalkingData · Featured Prediction Competition · 9 years ago Late Submission more_horiz TalkingData Mobile User Demographics Get to know millions of mobile device users TalkingData Mobile User Demographics Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Get to know millions of mobile device users In this competition, you are going to predict the demographics of a user (gender and age) based on their app download and usage behaviors. The Data is collected from TalkingData SDK integrated within mobile apps TalkingData serves under the service term between TalkingData and mobile app developers. Full recognition and consent from individual user of those apps have been obtained, and appropriate anonymization have been performed to protect privacy. Due to confidentiality, we won't provide details on how the gender and age data was obtained. Please treat them as accurate ground truth for prediction. The data schema can be represented in the following chart: 三星 samsung 天语 Ktouch 海信 hisense 联想 lenovo 欧比 obi 爱派尔 ipair 努比亚 nubia 优米 youmi 朵唯 dowe 黑米 heymi 锤子 hammer 酷比魔方 koobee 美图 meitu 尼比鲁 nibilu 一加 oneplus 优购 yougo 诺基亚 nokia 糖葫芦 candy 中国移动 ccmc 语信 yuxin 基伍 kiwu 青橙 greeno 华硕 asus 夏新 panosonic 维图 weitu 艾优尼 aiyouni 摩托罗拉 moto 乡米 xiangmi 米奇 micky 大可乐 bigcola 沃普丰 wpf 神舟 hasse 摩乐 mole 飞秒 fs 米歌 mige 富可视 fks 德赛 desci 梦米 mengmi 乐视 lshi 小杨树 smallt 纽曼 newman 邦华 banghua E派 epai 易派 epai 普耐尔 pner 欧新 ouxin 西米 ximi 海尔 haier 波导 bodao 糯米 nuomi 唯米 weimi 酷珀 kupo 谷歌 google 昂达 ada 聆韵 lingyun 8 files 296.96 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 296.96 MB app_events.csv.zip app_labels.csv.zip events.csv.zip gender_age_test.csv.zip gender_age_train.csv.zip label_categories.csv.zip phone_brand_device_model.csv.zip sample_submission.csv.zip 8 files  Too many requests",
    "data_description": "TalkingData Mobile User Demographics | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. TalkingData · Featured Prediction Competition · 9 years ago Late Submission more_horiz TalkingData Mobile User Demographics Get to know millions of mobile device users TalkingData Mobile User Demographics Overview Data Code Models Discussion Leaderboard Rules Overview Start Jul 11, 2016 Close Sep 6, 2016 Merger & Entry Description link keyboard_arrow_up Nothing is more comforting than being greeted by your favorite drink just as you walk through the door of the corner café. While a thoughtful barista knows you take a macchiato every Wednesday morning at 8:15, it’s much more difficult in a digital space for your preferred brands to personalize your experience. TalkingData , China’s largest third-party mobile data platform, understands that everyday choices and behaviors paint a picture of who we are and what we value. Currently, TalkingData is seeking to leverage behavioral data from more than 70% of the 500 million mobile devices active daily in China to help its clients better understand and interact with their audiences. In this competition, Kagglers are challenged to build a model predicting users’ demographic characteristics based on their app usage, geolocation, and mobile device properties. Doing so will help millions of developers and brand advertisers around the world pursue data-driven marketing efforts which are relevant to their users and catered to their preferences. Acknowledgements Evaluation link keyboard_arrow_up Submissions are evaluated using the multi-class logarithmic loss . Each device has been labeled with one true class. For each device, you must submit a set of predicted probabilities (one for each class). The formula is then, l o g l o s s = − 1 N N ∑ i = 1 M ∑ j = 1 y i j log ( p i j ) , where N is the number of devices in the test set, M is the number of class labels,  \\\\(log\\\\) is the natural logarithm, \\\\(y_{ij}\\\\) is 1 if device \\\\(i\\\\) belongs to class \\\\(j\\\\) and 0 otherwise, and \\\\(p_{ij}\\\\) is the predicted probability that observation \\\\(i\\\\) belongs to class \\\\(j\\\\). The submitted probabilities for a given device are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum), but they need to be in the range of [0, 1]. In order to avoid the extremes of the log function, predicted probabilities are replaced with \\\\(max(min(p,1-10^{-15}),10^{-15})\\\\). Submission File You must submit a csv file with the device id, and a probability for each class. The 12 classes to predict are: 'F23-', 'F24-26','F27-28','F29-32', 'F33-42', 'F43+', 'M22-', 'M23-26', 'M27-28', 'M29-31', 'M32-38', 'M39+' The order of the rows does not matter. The file must have a header and should look like the following: device_id,F23-,F24-26,F27-28,F29-32,F33-42,F43+,M22-,M23-26,M27-28,M29-31,M32-38,M39+ 1234,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833 5678,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833,0.0833 ... Prizes link keyboard_arrow_up 1st place - $12,500 2nd place - $7,500 3rd place - $5,000 Timeline link keyboard_arrow_up August 29, 2016 - First Submission deadline. A team must make at least one submission by this date in order to compete. August 29, 2016 - Team Merger deadline. This is the last day competitors may join or merge teams. September 5, 2016 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Graphlab Create License link keyboard_arrow_up Turi (formerly Dato) is giving free GraphLab Create licenses to participants in the competition, available here . After registration, a 1-month trial period is given; for the trial to last until the duration of the competition, please contact Guy Rapaport, guy+kaggle@turi.com, from the same email address used to register for the free license. Citation link keyboard_arrow_up DannyBickson, Freedom, Guy Rapaport, HanZhu, Ibrahim, RossWang, Wendy Kan, Yangyang, and Yao Lu. TalkingData Mobile User Demographics. https://kaggle.com/competitions/talkingdata-mobile-user-demographics, 2016. Kaggle. Cite Competition Host TalkingData Prizes & Awards $25,000 Awards Points & Medals Participation 2,623 Entrants 1,952 Participants 1,680 Teams 24,460 Submissions Tags Multiclass Classification Demographics Mobile and Wireless Tabular Multiclass Loss Table of Contents collapse_all Description Evaluation Prizes Timeline Graphlab Create License Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "grupo-bimbo-inventory-demand",
    "discussion_links": [
      "/competitions/grupo-bimbo-inventory-demand/discussion/23863",
      "/competitions/grupo-bimbo-inventory-demand/discussion/23633",
      "/competitions/grupo-bimbo-inventory-demand/discussion/23232",
      "/competitions/grupo-bimbo-inventory-demand/discussion/23208",
      "/competitions/grupo-bimbo-inventory-demand/discussion/23202"
    ],
    "discussion_texts": [
      "Grupo Bimbo Inventory Demand | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Grupo Bimbo Inventory Demand Maximize sales and minimize returns of bakery goods Grupo Bimbo Inventory Demand Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Grupo Bimbo Inventory Demand | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Grupo Bimbo Inventory Demand Maximize sales and minimize returns of bakery goods Grupo Bimbo Inventory Demand Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Grupo Bimbo Inventory Demand | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Grupo Bimbo Inventory Demand Maximize sales and minimize returns of bakery goods Grupo Bimbo Inventory Demand Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Grupo Bimbo Inventory Demand | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Grupo Bimbo Inventory Demand Maximize sales and minimize returns of bakery goods Grupo Bimbo Inventory Demand Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Grupo Bimbo Inventory Demand | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Grupo Bimbo Inventory Demand Maximize sales and minimize returns of bakery goods Grupo Bimbo Inventory Demand Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Maximize sales and minimize returns of bakery goods In this competition, you will forecast the demand of a product for a given week, at a particular store. The dataset you are given consists of 9 weeks of sales transactions in Mexico. Every week, there are delivery trucks that deliver products to the vendors. Each transaction consists of sales and returns. Returns are the products that are unsold and expired. The demand for a product in a certain week is defined as the sales this week subtracted by the return next week. The train and test dataset are split based on time, as well as the public and private leaderboard dataset split. Things to note: 6 files 502.44 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 502.44 MB cliente_tabla.csv.zip producto_tabla.csv.zip sample_submission.csv.zip test.csv.zip town_state.csv.zip train.csv.zip 6 files  Too many requests",
    "data_description": "Grupo Bimbo Inventory Demand | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Grupo Bimbo Inventory Demand Maximize sales and minimize returns of bakery goods Grupo Bimbo Inventory Demand Overview Data Code Models Discussion Leaderboard Rules Overview Start Jun 8, 2016 Close Aug 31, 2016 Merger & Entry Description link keyboard_arrow_up Planning a celebration is a balancing act of preparing just enough food to go around without being stuck eating the same leftovers for the next week. The key is anticipating how many guests will come. Grupo Bimbo must weigh similar considerations as it strives to meet daily consumer demand for fresh bakery products on the shelves of over 1 million stores along its 45,000 routes across Mexico. Currently, daily inventory calculations are performed by direct delivery sales employees who must single-handedly predict the forces of supply, demand, and hunger based on their personal experiences with each store. With some breads carrying a one week shelf life, the acceptable margin for error is small. In this competition, Grupo Bimbo invites Kagglers to develop a model to accurately forecast inventory demand based on historical sales data. Doing so will make sure consumers of its over 100 bakery products aren’t staring at empty shelves, while also reducing the amount spent on refunds to store owners with surplus product unfit for sale. Evaluation link keyboard_arrow_up The evaluation metric for this competition is Root Mean Squared Logarithmic Error . The RMSLE is calculated as ϵ = √ 1 n n ∑ i = 1 ( log ( p i + 1 ) − log ( a i + 1 ) ) 2 Where: \\\\(\\epsilon\\\\) is the RMSLE value (score) \\\\(n\\\\) is the total number of observations in the (public/private) data set, \\\\(p_i\\\\) is your prediction of demand, and \\\\(a_i\\\\) is the actual demand for \\\\(i\\\\). \\\\(\\log(x)\\\\) is the natural logarithm of \\\\(x\\\\) Submission File For every row in the dataset , submission files should contain two columns: id and Demanda_uni_equi.  The id corresponds to the column of that id in the test.csv. The file should contain a header and have the following format: id,Demanda_uni_equil 0,1 1,0 2,500 3,100 etc. Prizes link keyboard_arrow_up 1st place - $12,000 2nd place - $8,000 3rd place - $5,000 Timeline link keyboard_arrow_up August 23, 2016 - First Submission deadline. A team must make at least one submission by this date in order to compete. August 23, 2016 - Team Merger deadline. This is the last day competitors may join or merge teams. August 30, 2016 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Anna Montoya, Grupo Bimbo, Meghan O'Connell, and Wendy Kan. Grupo Bimbo Inventory Demand. https://kaggle.com/competitions/grupo-bimbo-inventory-demand, 2016. Kaggle. Cite Prizes & Awards $25,000 Awards Points & Medals Participation 3,058 Entrants 2,263 Participants 1,963 Teams 21,739 Submissions Tags Tabular Food Root Mean Squared Logarithmic Error Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "expedia-hotel-recommendations",
    "discussion_links": [
      "/competitions/expedia-hotel-recommendations/discussion/21607",
      "/competitions/expedia-hotel-recommendations/discussion/21615"
    ],
    "discussion_texts": [
      "Expedia Hotel Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Expedia Hotel Recommendations Which hotel type will an Expedia customer book? Expedia Hotel Recommendations Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Expedia Hotel Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Expedia Hotel Recommendations Which hotel type will an Expedia customer book? Expedia Hotel Recommendations Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Which hotel type will an Expedia customer book? Expedia has provided you logs of customer behavior. These include what customers searched for, how they interacted with search results (click/book), whether or not the search result was a travel package. The data in this competition is a random selection from Expedia and is not representative of the overall statistics. Expedia is interested in predicting which hotel group a user is going to book. Expedia has in-house algorithms to form hotel clusters , where similar hotels for a search (based on historical price, customer star ratings, geographical locations relative to city center, etc) are grouped together. These hotel clusters serve as good identifiers to which types of hotels people are going to book, while avoiding outliers such as new hotels that don't have historical data. Your goal of this competition is to predict the booking outcome (hotel cluster) for a user event, based on their search and other attributes associated with that user event. The train and test datasets are split based on time: training data from 2013 and 2014, while test data are from 2015. The public/private leaderboard data are split base on time as well. Training data includes all the users in the logs, including both click events and booking events. Test data only includes booking events. destinations.csv data consists of features extracted from hotel reviews text. Note that some srch_destination_id's in the train/test files don't exist in the destinations.csv file. This is because some hotels are new and don't have enough features in the latent space. Your algorithm should be able to handle this missing information. train/test.csv destinations.csv 4 files 4.52 GB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 4.52 GB destinations.csv sample_submission.csv test.csv train.csv 4 files 198 columns  Too many requests",
    "data_description": "Expedia Hotel Recommendations | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Expedia Hotel Recommendations Which hotel type will an Expedia customer book? Expedia Hotel Recommendations Overview Data Code Models Discussion Leaderboard Rules Overview Start Apr 15, 2016 Close Jun 11, 2016 Merger & Entry Description link keyboard_arrow_up Planning your dream vacation, or even a weekend escape, can be an overwhelming affair. With hundreds, even thousands, of hotels to choose from at every destination, it's difficult to know which will suit your personal preferences. Should you go with an old standby with those pillow mints you like, or risk a new hotel with a trendy pool bar? Expedia wants to take the proverbial rabbit hole out of hotel search by providing personalized hotel recommendations to their users. This is no small task for a site with hundreds of millions of visitors every month! Currently, Expedia uses search parameters to adjust their hotel recommendations, but there aren't enough customer specific data to personalize them for each user. In this competition, Expedia is challenging Kagglers to contextualize customer data and predict the likelihood a user will stay at 100 different hotel groups. The data in this competition is a random selection from Expedia and is not representative of the overall statistics. Evaluation link keyboard_arrow_up Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5): M A P @ 5 = 1 | U | | U | ∑ u = 1 m i n ( 5 , n ) ∑ k = 1 P ( k ) where |U| is the number of user events, P(k) is the precision at cutoff k, n is the number of predicted hotel clusters. Submission File For every user event, you must predict a space-delimited list of the hotel clusters they booked. You may submit up to 5 predictions for each user event. The file should contain a header and have the following format: id,hotel_cluster 0,99 3 1 75 20 1,2 50 30 23 9 etc... Prizes link keyboard_arrow_up 1st place - $12,500 2nd place - $7,500 3rd place -  $5,000 Timeline link keyboard_arrow_up June 3, 2016 - First Submission deadline. A team must make at least one submission by this date in order to compete. June 3, 2016 - Team Merger deadline. This is the last day competitors may join or merge teams. June 10, 2016 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Adam and Wendy Kan. Expedia Hotel Recommendations. https://kaggle.com/competitions/expedia-hotel-recommendations, 2016. Kaggle. Cite Prizes & Awards $25,000 Awards Points & Medals Participation 2,943 Entrants 2,176 Participants 1,971 Teams 22,632 Submissions Tags Tabular Hotels and Accommodations Recommender Systems MAP@{K} Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "santander-customer-satisfaction",
    "discussion_links": [
      "/competitions/santander-customer-satisfaction/discussion/20978",
      "/competitions/santander-customer-satisfaction/discussion/20786",
      "/competitions/santander-customer-satisfaction/discussion/22089",
      "/competitions/santander-customer-satisfaction/discussion/20858"
    ],
    "discussion_texts": [
      "Santander Customer Satisfaction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Customer Satisfaction Which customers are happy customers? Santander Customer Satisfaction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Satisfaction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Customer Satisfaction Which customers are happy customers? Santander Customer Satisfaction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Satisfaction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Customer Satisfaction Which customers are happy customers? Santander Customer Satisfaction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santander Customer Satisfaction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Customer Satisfaction Which customers are happy customers? Santander Customer Satisfaction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Which customers are happy customers? You are provided with an anonymized dataset containing a large number of numeric variables. The \"TARGET\" column is the variable to predict. It equals one for unsatisfied customers and 0 for satisfied customers. The task is to predict the probability that each customer in the test set is an unsatisfied customer. 3 files 119.04 MB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 119.04 MB sample_submission.csv test.csv train.csv 3 files 743 columns  Too many requests",
    "data_description": "Santander Customer Satisfaction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Banco Santander · Featured Prediction Competition · 9 years ago Late Submission more_horiz Santander Customer Satisfaction Which customers are happy customers? Santander Customer Satisfaction Overview Data Code Models Discussion Leaderboard Rules Overview Start Mar 2, 2016 Close May 3, 2016 Merger & Entry Description link keyboard_arrow_up From frontline support teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving. Santander Bank is asking Kagglers to help them identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer's happiness before it's too late. In this competition, you'll work with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience. Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each ID in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format: ID,TARGET 2,0 5,0 6,0 etc. Prizes link keyboard_arrow_up 1st place - $30,000 2nd place - $20,000 3rd place - $10,000 Timeline link keyboard_arrow_up April 25, 2016 - First Submission deadline. A team must make at least one submission by this date in order to compete. April 25, 2016 - Team Merger deadline. This is the last day competitors may join or merge teams. May 2, 2016 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Soraya_Jimenez and Will Cukierski. Santander Customer Satisfaction. https://kaggle.com/competitions/santander-customer-satisfaction, 2016. Kaggle. Cite Competition Host Banco Santander Prizes & Awards $60,000 Awards Points & Medals Participation 6,584 Entrants 5,688 Participants 5,115 Teams 93,333 Submissions Tags Tabular Binary Classification Banking Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "march-machine-learning-mania-2016",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict the 2016 NCAA Basketball Tournament If you are unfamiliar with the format and intricacies of the NCAA tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided team-level historical data to jump-start the modeling process, but there is also player-level and game-level data that may be useful. We extend our gratitude to Kenneth Massey for providing much of the historical data. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 4 NCAA tournaments (2012-2015). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2016 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the \"essential\" data files. Optional files may be added to the data while the competition is running. You can assume that we will provide the essential files for the current season. You should not assume that we will provide optional files for the current season. Teams This file identifies the different college teams present in the dataset. Each team has a 4 digit id number. Seasons This file identifies the different seasons included in the historical data, along with certain season-level properties. RegularSeasonCompactResults This file identifies the game-by-game results for 31 seasons of historical data, from 1985 to 2015. Each year, it includes all games played from daynum 0 through 132 (which by definition is \"Selection Sunday,\" the day that tournament pairings are announced). Each row in the file represents a single game played. RegularSeasonDetailedResults This file is a more detailed set of game results, covering seasons 2003-2015. This includes team-level total statistics for each game (total field goals attempted, offensive rebounds, etc.) The column names should be self-explanatory to basketball fans (as above, \"w\" or \"l\" refers to the winning or losing team): TourneyCompactResults This file identifies the game-by-game NCAA tournament results for all seasons of historical data. The data is formatted exactly like the regular_season_compact_results.csv data. Note that these games also include the play-in games (which always occurred on day 134/135) for those years that had play-in games. TourneyDetailedResults This file contains the more detailed results for tournament games from 2003 onward. TourneySeeds This file identifies the seeds for all teams in each NCAA tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on the bracket structure. TourneySlots This file identifies the mechanism by which teams are paired against each other, depending upon their seeds. Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. If there were N teams in the tournament during a particular year, there were N-1 teams eliminated (leaving one champion) and therefore N-1 games played, as well as N-1 slots in the tournament bracket, and thus there will be N-1 records in this file for that season. 1 files 7.24 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 7.24 MB march-machine-learning-mania-2016-v2.zip 1 file  Too many requests",
    "data_description": "March Machine Learning Mania 2016 | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz March Machine Learning Mania 2016 Predict the 2016 NCAA Basketball Tournament March Machine Learning Mania 2016 Overview Data Code Models Discussion Leaderboard Rules Overview Start Feb 11, 2016 Close Apr 5, 2016 Description link keyboard_arrow_up Update : although the tournament is over, we're continuing our analysis under the predictions dataset page . Back for its third year, March Machine Learning Mania challenges data scientists to predict winners and losers of the men's 2016 NCAA basketball tournament. You're provided data covering three decades of historical NCAA games and freely encouraged to use other sources of data to gain a winning edge. In stage one of this two-stage competition, participants will build and test their models against the previous four tournaments. In the second stage, participants will predict the outcome of the 2016 tournament. You don’t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2016 results. Acknowledgments SAP is the presenting sponsor of March Machine Learning Mania 2016. Please see About the Sponsor to read more. Evaluation link keyboard_arrow_up Submissions are scored on the log loss, also called the predictive binomial deviance: LogLoss = − 1 n n ∑ i = 1 [ y i log ( ˆ y i ) + ( 1 − y i ) log ( 1 − ˆ y i ) ] , where n is the number of games played \\\\( \\hat{y}_i \\\\) is the predicted probability of team 1 beating team 2 \\\\( y_i \\\\) is 1 if team 1 wins, 0 if team 2 wins \\\\( log() \\\\) is the natural (base e) logarithm A smaller log loss is better. Games which are not played are ignored in the scoring. Play-in games are also ignored (only the games among the final 64 teams are scored). The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value. Submission File The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2016 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2  = 2278 matchups. Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, \"2013_1104_1129\" indicates team 1104 played team 1129 in the year 2013. You must predict the probability that the team with the lower id beats the team with the higher id. The resulting submission format looks like the following, where \"pred\" represents the predicted probability that the first team will win: id,pred 2012_1104_1124,0.5 2012_1104_1125,0.5 2012_1104_1140,0.5 ... Prizes link keyboard_arrow_up 1st Place - $10,000 2nd Place - $6,000 3rd Place - $4,000 4th Place - $3,000 5th Place - $2,000 Stage 1 will not count towards Kaggle rankings/points. Stage 2 is weighted to award 50% of the usual competition points (on account of the element of luck involved). It does not count towards earning the Kaggle Masters tier. Timeline link keyboard_arrow_up Stage 1 - Model Building Mar 12, 2016 - prior to this deadline competitors build and test models on historical data. The leaderboard shows the model performance on historical tournament outcomes. Stage 2 - 2016 Championship Sunday, Mar 13 - Selection Sunday (68 teams announced) Monday, Mar 14 - Kaggle begins to accept 2016 predictions. Release of up-to-date 2015-2016 season data. Wednesday, Mar 16 - Final deadline to submit 2016 predictions (11:59PM UTC). Mar 17 - Apr 4 - Watch your predictions come true! All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Faqs link keyboard_arrow_up Is one tournament enough to decide the best basketball algorithm? It's better to be lucky than good! Besides, when was the last time your office held a cross validation pool? Can I update my predictions after the tournament starts? No changes are permitted once the tournament begins. How is the leaderboard going to work when the event being scored hasn't yet happened? You won't submit after the tournament starts. We'll update the solution file as the games occur, which will cause the ranks on the leaderboard to change. Why do we have to predict every match up? Why 68 teams and not 64? This was done for timing purposes. Predicting every possible matchup for the 68 teams announced on Selection Sunday gives participants the most time to get their current year predictions ready in time. There is a small \"play-in\" round, sometimes called the first round, where the 68 are narrowed to 64. While you are asked to predict these games (and you may be predicting them after they occur), we will not be scoring them. Why don't predictions in later rounds count for more? While it is possible to weight the later games, we chose to keep scoring simple and count all games equally. Any weights we pick would be mostly arbitrary (how many first-round games is a championship game worth?). Also, weighting any game increases the role that luck plays in determining a winner. We've also structured the competition so that people can still be in the running even if there are early-round upsets. Can the results of an algorithm like this be used for sports betting? Maybe, but that's not a goal of this competition. Kaggle claims no rights to the intellectual property developed by competitors. Have fun and learn. If you want to use the results to gamble, that's between you and your bookie and your local laws. About The Sponsor link keyboard_arrow_up At SAP, we believe in supporting great feats of human accomplishment. That’s why we’re proud to sponsor a variety of world-renowned sports, cultural, and entertainment organizations – from professional sports such as tennis, basketball, and soccer to entertainment venues and artistic programs like Cirque du Soleil. As the market leader in enterprise application software, SAP is at the center of today’s business and technology revolution. Our innovations enable approximately 300,000 customers worldwide to work together more efficiently and use business insight more effectively. More information about SAP (NYSE: SAP) is available at http://www.sap.com Explore analytics solutions from SAP In this hyper-connected world, with data volumes constantly increasing, analytics solutions from SAP can help you to simplify, innovate, and accelerate. Make your life easier by analyzing, predicting, and running your business in real time. Discover and execute on innovations that create value for your organization. And achieve rapid insight into action across your organization, closing the gap between transactions, data preparation, analysis, and action – all with analytics. > Discover Now Predictive Analytics and SAP SAP Predictive Analytics software brings predictive insight to business users, analysts, and data scientists in your company and includes the following capabilities: Automated preparation of data Automated modeling for users at all levels Scoring, model management, and recommendation functionality Advanced data visualization Connectivity to Big Data and third-party data sources Integration with R > Try SAP Predictive Analytics Now > Product Documentation ; See the Product in Action ; Watch the tutorials Data Visualization and SAP SAP Lumira allows you to connect to and visualize nearly any type of business data, whether it resides in various enterprise databases, the cloud, or spreadsheets. You can combine, enrich, and visualize data for immediate insights. Start making sense of all your data – go beyond static spreadsheets and basic presentations – without the complexity of traditional reporting. With SAP Lumira, you can: Acquire and enrich spreadsheet and enterprise data, connecting to SAP BusinessObjects Universe, SAP HANA, freehand SQL, Hortonworks Sandbox, MapR Sandbox, and many other sources. Maximize data knowledge and get the most up-to-date information without any coding or support from IT Expand your business intelligence by merging multiple datasets into meaningful visualizations Discover hidden insights by exploring any amount and any type of data in real time, without sacrificing performance or security Unify your view of the business by bringing together data from enterprise sources across all lines of business as well as personal sources on your desktop > Watch the SAP Lumira video overview > Download SAP Lumira 30-Day Trial Now Citation link keyboard_arrow_up Anna Montoya, Jeff Sonas, Mark Glickman, SAP Analytics, and Will Cukierski. March Machine Learning Mania 2016. https://kaggle.com/competitions/march-machine-learning-mania-2016, 2016. Kaggle. Cite Prizes & Awards $25,000 Awards Points Does not award Medals Participation 1,320 Entrants 653 Participants 596 Teams 1,045 Submissions Tags Basketball Tabular Sports Log Loss Table of Contents collapse_all Description Evaluation Prizes Timeline Faqs About The Sponsor Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "bnp-paribas-cardif-claims-management",
    "discussion_links": [
      "/competitions/bnp-paribas-cardif-claims-management/discussion/20247",
      "/competitions/bnp-paribas-cardif-claims-management/discussion/20252",
      "/competitions/bnp-paribas-cardif-claims-management/discussion/20258"
    ],
    "discussion_texts": [
      "BNP Paribas Cardif Claims Management | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz BNP Paribas Cardif Claims Management Can you accelerate BNP Paribas Cardif's claims management process? BNP Paribas Cardif Claims Management Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "BNP Paribas Cardif Claims Management | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz BNP Paribas Cardif Claims Management Can you accelerate BNP Paribas Cardif's claims management process? BNP Paribas Cardif Claims Management Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "BNP Paribas Cardif Claims Management | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz BNP Paribas Cardif Claims Management Can you accelerate BNP Paribas Cardif's claims management process? BNP Paribas Cardif Claims Management Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you accelerate BNP Paribas Cardif's claims management process? You are provided with an anonymized dataset containing both categorical and numeric variables available when the claims were received by BNP Paribas Cardif. All string type variables are categorical. There are no ordinal variables. The \"target\" column in the train set is the variable to predict. It is equal to 1 for claims suitable for an accelerated approval. The task is to predict a probability (\"PredictedProb\") for each claim in the test set. 3 files 103.75 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 103.75 MB sample_submission.csv.zip test.csv.zip train.csv.zip 3 files  Too many requests",
    "data_description": "BNP Paribas Cardif Claims Management | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz BNP Paribas Cardif Claims Management Can you accelerate BNP Paribas Cardif's claims management process? BNP Paribas Cardif Claims Management Overview Data Code Models Discussion Leaderboard Rules Overview Start Feb 3, 2016 Close Apr 19, 2016 Merger & Entry Description link keyboard_arrow_up As a global specialist in personal insurance, BNP Paribas Cardif serves 90 million clients in 36 countries across Europe, Asia and Latin America. In a world shaped by the emergence of new uses and lifestyles, everything is going faster and faster. When facing unexpected events, customers expect their insurer to support them as soon as possible. However, claims management may require different levels of check before a claim can be approved and a payment can be made. With the new practices and behaviors generated by the digital economy, this process needs adaptation thanks to data science to meet the new needs and expectations of customers. In this challenge, BNP Paribas Cardif is providing an anonymized database with two categories of claims: claims for which approval could be accelerated leading to faster payments claims for which additional information is required before approval Kagglers are challenged to predict the category of a claim based on features available early in the process, helping BNP Paribas Cardif accelerate its claims process and therefore provide a better service to its customers. Evaluation link keyboard_arrow_up The evaluation metric for this competition is Log Loss l o g l o s s = − 1 N N ∑ i = 1 ( y i log ( p i ) + ( 1 − y i ) log ( 1 − p i ) ) where N is the number of observations, \\\\(log\\\\) is the natural logarithm, \\\\(y_{i}\\\\) is the binary target, and \\\\(p_{i}\\\\) is the predicted probability that \\\\(y_i\\\\) equals 1. Note: the actual submitted predicted probabilities are replaced with \\\\(max(min(p,1-10^{-15}),10^{-15})\\\\). Submission File For every observation in the test dataset , submission files should contain two columns: ID and PredictedProb. The file should contain a header and have the following format: ID,PredictedProb 0,0.5 1,0.5 2,0.5 7,0.5 etc. Prizes link keyboard_arrow_up 1st place - $15,000 2nd place - $10,000 3rd place - $5,000 Timeline link keyboard_arrow_up April 11, 2015 - First Submission deadline. A team must make at least one submission by this date in order to compete. April 11, 2015 - Team Merger deadline. This is the last day competitors may join or merge teams. April 18, 2015 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. About Bnp Paribas Cardif link keyboard_arrow_up BNP Paribas Cardif creates innovative savings and insurance solutions designed for performance in a world shaped by the emergence of new uses and lifestyles. A subsidiary of BNP Paribas, BNP Paribas Cardif has become a recognized global specialist in personal insurance, serving 90 million clients in 36 countries with strong positions in three regions – Europe, Asia and Latin America. BNP Paribas Cardif has a unique business model anchored in partnerships. The company co-creates solutions with distributors in a variety of sectors (including banks, credit companies, the insurance units of automobile manufacturers, telecoms operators, insurance brokers, retailers, etc), who then market the products to their customers. These strong relationships with its partners allow the company to focus on the different steps of the client experience, in order to determine the right moment to propose a value-added offer. To achieve that, the company analyses the markets and identifies new opportunities by associating its client knowledge to its insurance expertise. The global footprint of BNP Paribas Cardif is backed by integrated information systems and platforms to facilitate either local or global management of partnerships. What's more, this organization allows efficient adaptation of successful products in different countries or different market segments. Consistent and high service quality, continual innovation and optimized revenue performance; those are the promises of BNP Paribas Cardif. Citation link keyboard_arrow_up Anna Montoya, detoldim, Dumora, Lam Dang, Sebastien Conort, and Will Cukierski. BNP Paribas Cardif Claims Management. https://kaggle.com/competitions/bnp-paribas-cardif-claims-management, 2016. Kaggle. Cite Prizes & Awards $30,000 Awards Points & Medals Participation 3,703 Entrants 3,283 Participants 2,920 Teams 54,337 Submissions Tags Banking Tabular Binary Classification Log Loss Table of Contents collapse_all Description Evaluation Prizes Timeline About Bnp Paribas Cardif Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "home-depot-product-search-relevance",
    "discussion_links": [
      "/competitions/home-depot-product-search-relevance/discussion/20428"
    ],
    "discussion_texts": [
      "Home Depot Product Search Relevance | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Home Depot Product Search Relevance Predict the relevance of search results on homedepot.com Home Depot Product Search Relevance Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict the relevance of search results on homedepot.com This data set contains a number of products and real customer search terms from Home Depot's website. The challenge is to predict a relevance score for the provided combinations of search terms and products. To create the ground truth labels, Home Depot has crowdsourced the search/product pairs to multiple human raters. The relevance is a number between 1 (not relevant) to 3 (highly relevant). For example, a search for \"AA battery\" would be considered highly relevant to a pack of size AA batteries (relevance = 3), mildly relevant to a cordless drill battery (relevance = 2), and not relevant to a snow shovel (relevance = 1). Each pair was evaluated by at least three human raters. The provided relevance scores are the average value of the ratings. There are three additional things to know about the ratings: Your task is to predict the relevance for each pair listed in the test set. Note that the test set contains both seen and unseen search terms. 6 files 72.93 MB zip, docx Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 72.93 MB attributes.csv.zip product_descriptions.csv.zip relevance_instructions.docx sample_submission.csv.zip test.csv.zip train.csv.zip 6 files  Too many requests",
    "data_description": "Home Depot Product Search Relevance | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Home Depot Product Search Relevance Predict the relevance of search results on homedepot.com Home Depot Product Search Relevance Overview Data Code Models Discussion Leaderboard Rules Overview Start Jan 18, 2016 Close Apr 26, 2016 Merger & Entry Description link keyboard_arrow_up Shoppers rely on Home Depot’s product authority to find and buy the latest products and to get timely solutions to their home improvement needs. From installing a new ceiling fan to remodeling an entire kitchen, with the click of a mouse or tap of the screen, customers expect the correct results to their queries – quickly. Speed, accuracy and delivering a frictionless customer experience are essential. In this competition, Home Depot is asking Kagglers to help them improve their customers' shopping experience by developing a model that can accurately predict the relevance of search results. Search relevancy is an implicit measure Home Depot uses to gauge how quickly they can get customers to the right products. Currently, human raters evaluate the impact of potential changes to their search algorithms, which is a slow and subjective process. By removing or minimizing human input in search relevance evaluation, Home Depot hopes to increase the number of iterations their team can perform on the current search algorithms. Evaluation link keyboard_arrow_up Submissions are evaluated on the root mean squared error (RMSE) . Submission File For each id in the test set, you must predict a relevance. This is a real number in [1,3]. The file should contain a header and have the following format: id,relevance 1,1 4,2 5,3 etc. Prizes link keyboard_arrow_up 1st place - $20,000 2nd place - $12,000 3rd place - $8,000 Timeline link keyboard_arrow_up April 18, 2016 - First Submission deadline. A team must make at least one submission by this date in order to compete. April 18, 2016 - Team Merger deadline. This is the last day competitors may join or merge teams. April 25, 2016 - Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Anna Montoya, RG, and Will Cukierski. Home Depot Product Search Relevance. https://kaggle.com/competitions/home-depot-product-search-relevance, 2016. Kaggle. Cite Prizes & Awards $40,000 Awards Points & Medals Participation 3,112 Entrants 2,551 Participants 2,123 Teams 35,492 Submissions Tags Tabular Root Mean Squared Error Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "santas-stolen-sleigh",
    "discussion_links": [
      "/competitions/santas-stolen-sleigh/discussion/18312",
      "/competitions/santas-stolen-sleigh/discussion/18313",
      "/competitions/santas-stolen-sleigh/discussion/18336",
      "/competitions/santas-stolen-sleigh/discussion/18334"
    ],
    "discussion_texts": [
      "Santa's Stolen Sleigh | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Santa's Stolen Sleigh ♫ Alarm bells ring, are you listening? Santa's sleigh has gone missing ♫ Santa's Stolen Sleigh Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santa's Stolen Sleigh | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Santa's Stolen Sleigh ♫ Alarm bells ring, are you listening? Santa's sleigh has gone missing ♫ Santa's Stolen Sleigh Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santa's Stolen Sleigh | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Santa's Stolen Sleigh ♫ Alarm bells ring, are you listening? Santa's sleigh has gone missing ♫ Santa's Stolen Sleigh Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Santa's Stolen Sleigh | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Santa's Stolen Sleigh ♫ Alarm bells ring, are you listening? Santa's sleigh has gone missing ♫ Santa's Stolen Sleigh Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events ♫ Alarm bells ring, are you listening? Santa's sleigh has gone missing ♫ See on user's exploration of the data on Kaggle Scripts For this competition, you are asked to optimize the total weighted distance traveled (weighted reindeer weariness). You are given a list of gifts with their destinations and their weights. You will plan sleigh trips to deliver all the gifts to their destinations while optimizing the routes.  4 files 5.57 MB csv, mos Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 5.57 MB SantasSledges_Main.mos SantasSledges_Subsolve.mos gifts.csv sample_submission.csv 4 files 6 columns  Too many requests",
    "data_description": "Santa's Stolen Sleigh | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Santa's Stolen Sleigh ♫ Alarm bells ring, are you listening? Santa's sleigh has gone missing ♫ Santa's Stolen Sleigh Overview Data Code Models Discussion Leaderboard Rules Overview Start Dec 1, 2015 Close Jan 9, 2016 Merger & Entry Description link keyboard_arrow_up Fork this script and get started on the problem The North Pole is in an uproar over news that Santa's magic sleigh has been stolen. Able to carry all the world's presents in one trip, it was considered crucial to successfully delivering holiday goodies across the globe in one night. Unwilling to cancel Christmas, Santa is determined to deliver toys to all the good girls and boys using his day-to-day, magic-less sleigh. With so little time to pull off this plan, Santa is once again counting on Kagglers to help. Given the sleigh's antiquated, weight-limited specifications, your challenge is to optimize the routes and loads Santa will take to and from the North Pole. And don't forget about Dasher, Dancer, Prancer, and Vixen; Santa is adamant that the best solutions will minimize the toll of this hectic night on his reindeer friends. Acknowledgements This competition is brought to you by FICO . Evaluation link keyboard_arrow_up Your goal is to minimize total weighted reindeer weariness: Weighted Reindeer Weariness = (distance traveled) * (weights carried for that segment) All sleighs start from north pole (Lat=90, Long=0), then head to each gift in the order that a user gives, and then head back to north pole Sleighs have a base weight = 10 Each sleigh has a weight limit = 1000 (excluding the sleigh base weight) Mathematically, weighted reindeer weariness is calculated by: W R W = m ∑ j = 1 n ∑ i = 1 [ ( n ∑ k = 1 w k j − i ∑ k = 1 w k j ) ⋅ D i s t ( L o c i , L o c i − 1 ) ] j , where m is the number of trips, n is the number of gifts for each trip \\\\(j\\\\), \\\\(w_{ij}\\\\) is the weight of the \\\\(i^{th}\\\\) gift at trip \\\\(j\\\\), \\\\(Dist()\\\\) is calculated with Haversine Distance between two locations, and \\\\(Loc_i\\\\) is the location of gift \\\\(i\\\\). \\\\(Loc_0\\\\) and \\\\(Loc_n\\\\) are North Pole, and \\\\(w_{nj}\\\\), a.k.a. the last leg of each trip, is always the base weight of the sleigh. For example, if you have 2 gifts A, B to deliver in the trip, then the WRW is calculated as: dist( North pole to A ) * ( weight A + weight B + base_weight ) + dist( A to B ) * ( weight B + base_weight ) + dist( B to North pole ) * ( base_weight ) Submission File Submission files should contain two columns: GiftId and TripId. GiftId should be ordered by the order of delivery, and different trips should have different TripIds. The file should contain a header and have the following format: GiftId,TripId 1,1 5,1 3,2 4,2 2,1 etc. Prizes link keyboard_arrow_up Xpress Prize - $5,000: Awarded to the highest placing team to submit a model built using FICO tools. Leaderboard Prize - $10,000: Awarded to the 1st place team on the leaderboard at the close of the competition. Rudolph Prize - $5,000: Awarded to the team holding 1st place on the leaderboard for the longest period of time between Monday, November 30rd midnight UTC and Wednesday, January 6, 2016 11:59 PM UTC. To qualifying for the FICO Xpress prize, we expect that FICO tools contribute significantly to obtaining the submitted solution. Participants are free to implement certain parts of the solution method in an environment of their choice, like Java, C++, R or MATLAB. Examples for significant contribution are a heuristic implemented in Xpress-Mosel or a solution approach which makes use of the optimization solvers provided with Xpress. The solvers do not necessarily need to be accessed using Xpress-Mosel. Timeline link keyboard_arrow_up January 1, 2016 - First Submission deadline. Your team must make its first submission by this deadline. January 1, 2016 - Team Merger deadline. This is the last day you may merge with another team January 8, 2016 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. About Fico Xpress link keyboard_arrow_up FICO Xpress Optimization Suite provides a modelling language and a development environment together with linear, mixed-integer, nonlinear and constraint programming solvers. As part of this competition we make Xpress available to all participants. We kindly ask you to register for the FICO community to get access to the software and additional tools and information: We have recently added a connection to R. Check out the Xpress Tutorial for details. After having solved the challenge at hand, you might want to turn your Mosel model into a full solution including a user interface. One can rapidly achieve this by utilizing FICO Optimization Modeler . Participants of the competition are provided with a copy of Xpress for the duration of the competition. Xpress Tutorial link keyboard_arrow_up The challenge can be formulated using Xpress as a Mixed Integer Programming (MIP) problem. Solving the MIP formulation to completion would yield an optimal solution. However, due to the size of the data, and complexity of the problem, such a formulation is not expected to be directly practical. We provide you an example formulation using a standard assignment approach as a Mosel model. This formulation works for a problem with 20-50 gifts (depending on your hardware). To extend from the basic Mosel model, a possible approach to overcome the size limitation is either to aggregate or to cluster the gifts. We provide you a clustering second Mosel model , which uses R to create clusters of sizes that are more appropriate for the MIP formulation. Once the clusters are created, this model executes the MIP formulation in parallel and collects up the results to a single submission file. More details on the Xpress R integration can be found here . About Fico link keyboard_arrow_up FICO is a leading analytics software company, helping businesses in 80+ countries make better decisions that drive higher levels of growth, profitability and customer satisfaction. The company’s ground-breaking use of Big Data and mathematical algorithms to predict consumer behavior has transformed entire industries. FICO provides analytics software and tools used across multiple industries to manage risk, fight fraud, build more profitable customer relationships, optimize operations and address regulatory challenges. Many FICO offerings reach industry-wide adoption — such as the FICO® Score, the standard measure of consumer credit risk in the United States. FICO solutions leverage open-source standards and cloud computing to maximize flexibility, speed deployment and reduce costs. The company also helps millions of people manage their personal credit health. FICO: Make every decision count™. Founded in 1956, FICO introduced analytic solutions such as credit scoring that have made credit more widely available, not just in the United States but around the world. FICO has also pioneered the development and application of game-changing technologies behind decision management. These include predictive analytics , business rules management and optimization . We use these technologies to help businesses of all sizes improve the precision, consistency and agility of their complex, high–volume decisions – and operationalize these decisions to turn them into timely actions. FICO’s decision management tools are available on the FICO Decision Management Platform and the FICO Analytic Cloud . Citation link keyboard_arrow_up Oliver Bastert, Wendy Kan, Xpress, and yaelol. Santa's Stolen Sleigh. https://kaggle.com/competitions/santas-stolen-sleigh, 2015. Kaggle. Cite Prizes & Awards $20,000 Awards Points & Medals Participation 1,372 Entrants 1,213 Participants 1,126 Teams 10,540 Submissions Tags Tabular Custom Metric Table of Contents collapse_all Description Evaluation Prizes Timeline About Fico Xpress Xpress Tutorial About Fico Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "prudential-life-insurance-assessment",
    "discussion_links": [
      "/competitions/prudential-life-insurance-assessment/discussion/19010",
      "/competitions/prudential-life-insurance-assessment/discussion/19003",
      "/competitions/prudential-life-insurance-assessment/discussion/19016",
      "/competitions/prudential-life-insurance-assessment/discussion/18996"
    ],
    "discussion_texts": [
      "Prudential Life Insurance Assessment | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Prudential Life Insurance Assessment Can you make buying life insurance easier? Prudential Life Insurance Assessment Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Prudential Life Insurance Assessment | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Prudential Life Insurance Assessment Can you make buying life insurance easier? Prudential Life Insurance Assessment Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Prudential Life Insurance Assessment | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Prudential Life Insurance Assessment Can you make buying life insurance easier? Prudential Life Insurance Assessment Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Prudential Life Insurance Assessment | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Prudential Life Insurance Assessment Can you make buying life insurance easier? Prudential Life Insurance Assessment Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Can you make buying life insurance easier? In this dataset, you are provided over a hundred variables describing attributes of life insurance applicants. The task is to predict the \"Response\" variable for each Id in the test set. \"Response\" is an ordinal measure of risk that has 8 levels. The following variables are all categorical (nominal): Product_Info_1, Product_Info_2, Product_Info_3, Product_Info_5, Product_Info_6, Product_Info_7, Employment_Info_2, Employment_Info_3, Employment_Info_5, InsuredInfo_1, InsuredInfo_2, InsuredInfo_3, InsuredInfo_4, InsuredInfo_5, InsuredInfo_6, InsuredInfo_7, Insurance_History_1, Insurance_History_2, Insurance_History_3, Insurance_History_4, Insurance_History_7, Insurance_History_8, Insurance_History_9, Family_Hist_1, Medical_History_2, Medical_History_3, Medical_History_4, Medical_History_5, Medical_History_6, Medical_History_7, Medical_History_8, Medical_History_9, Medical_History_11, Medical_History_12, Medical_History_13, Medical_History_14, Medical_History_16, Medical_History_17, Medical_History_18, Medical_History_19, Medical_History_20, Medical_History_21, Medical_History_22, Medical_History_23, Medical_History_25, Medical_History_26, Medical_History_27, Medical_History_28, Medical_History_29, Medical_History_30, Medical_History_31, Medical_History_33, Medical_History_34, Medical_History_35, Medical_History_36, Medical_History_37, Medical_History_38, Medical_History_39, Medical_History_40, Medical_History_41 The following variables are continuous: Product_Info_4, Ins_Age, Ht, Wt, BMI, Employment_Info_1, Employment_Info_4, Employment_Info_6, Insurance_History_5, Family_Hist_2, Family_Hist_3, Family_Hist_4, Family_Hist_5 The following variables are discrete: Medical_History_1, Medical_History_10, Medical_History_15, Medical_History_24, Medical_History_32 Medical_Keyword_1-48 are dummy variables. 3 files 3.4 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 3.4 MB sample_submission.csv.zip test.csv.zip train.csv.zip 3 files  Too many requests",
    "data_description": "Prudential Life Insurance Assessment | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Prudential Life Insurance Assessment Can you make buying life insurance easier? Prudential Life Insurance Assessment Overview Data Code Models Discussion Leaderboard Rules Overview Start Nov 23, 2015 Close Feb 16, 2016 Description link keyboard_arrow_up Picture this. You are a data scientist in a start-up culture with the potential to have a very large impact on the business. Oh, and you are backed up by a company with 140 years' business experience. Curious? Great! You are the kind of person we are looking for. Prudential , one of the largest issuers of life insurance in the USA, is hiring passionate data scientists to join a newly-formed Data Science group solving complex challenges and identifying opportunities. The results have been impressive so far but we want more. The Challenge In a one-click shopping world with on-demand everything, the life insurance application process is antiquated. Customers provide extensive information to identify risk classification and eligibility, including scheduling medical exams, a process that takes an average of 30 days. The result? People are turned off. That’s why only 40% of U.S. households own individual life insurance. Prudential wants to make it quicker and less labor intensive for new and existing customers to get a quote while maintaining privacy boundaries. By developing a predictive model that accurately classifies risk using a more automated approach, you can greatly impact public perception of the industry. The results will help Prudential better understand the predictive power of the data points in the existing assessment, enabling us to significantly streamline the process. Evaluation link keyboard_arrow_up Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement between the raters than expected by chance, this metric may go below 0. The response variable has 8 possible ratings.  Each application is characterized by a tuple (e a , e b ) , which corresponds to its scores by Rater A (actual risk) and Rater B (predicted risk).  The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix O is constructed, such that O i,j corresponds to the number of applications that received a rating i by A and a rating j by B . An N-by-N matrix of weights, w , is calculated based on the difference between raters' scores: w i , j = ( i − j ) 2 ( N − 1 ) 2 An N-by-N histogram matrix of expected ratings, E , is calculated, assuming that there is no correlation between rating scores.  This is calculated as the outer product between each rater's histogram vector of ratings, normalized such that E and O have the same sum. From these three matrices, the quadratic weighted kappa is calculated as: κ = 1 − ∑ i , j w i , j O i , j ∑ i , j w i , j E i , j . Submission File For each Id in the test set, you must predict the Response variable. The file should contain a header and have the following format: Id,Response 1,4 3,8 4,3 etc. Prizes link keyboard_arrow_up 1st place - $15,000 2nd place - $10,000 3rd place - $5,000 Timeline link keyboard_arrow_up February 10, 2016 - First Submission deadline. You must make its first submission by this deadline. February 15, 2016 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. About Prudential link keyboard_arrow_up Prudential helps people of all ages and backgrounds grow and protect their wealth through a variety of products and services, including life insurance, annuities, retirement-related services, mutual funds and investment management. And in Prudential’s Customer Office, we do the usual stuff – marketing, technology, data and cool stuff like intelligent experimentation to surprise our customers - things they would not expect from an insurance company. New things. But with an eye toward creating better economics and higher shareholder value. At times we argue with each other – using complex words like shareholder value vs. gini coefficient improvement. But you get the drift. We are seeking dynamic and resourceful team members who take initiative, thrive in a fast paced environment and are excited about being part of a cultural and business transformation. The usual corporate stuff! What we are really looking for – self starter thinkers, planners and doers. Multi-dimensional skill sets highly welcome. About Data Analytics: Our mission is to harness the power of advanced data analytics to solve high impact business problems that will drive simplicity and superior customer experiences across all customer touch points. Our work is critical to the development of internal customer knowledge, an element essential in building a customer-centric, solutions based environment. We focus on a combination of new data, technologies and advanced mathematics techniques to enhance: Customer retention and management, product design, profitability, risk selection and pricing, customer experience across channels, marketing and distribution targeting. Our team merges the entrepreneurial spirit of a start-up with the stability of one of the world’s most established financial services companies. We promote and nurture a culture of collaboration, experimentation and innovation We leverage the knowledge, experience and abilities of internal and external partners What you can do Be part of the customer-centric thinking in a new and growing organization You can… Help engage 30M+ customers who range from newborns to 90+ year olds Drive innovation to grow ~$1 trillion in assets we already manage Understand breakthroughs in medicine and genomics that will transform how long we live Geek out on defining how we should think about various risks (is geeking out a risk?) Engage with some of the largest institutions globally including central banks and sovereign funds What we offer Get the perks of a big company with the culture of a start up. Sure, we offer competitive pay, medical and dental benefits and we even have an employee pension plan (who does that anymore?). But we also offer tuition assistance, 401K, stock purchase plan and a host of discount programs. Need some balance in your life? No problem. We offer life and health coaching, fitness facilities, adult and child care benefits and alternative work arrangements and more! Join Us If you think you have what it takes to build a career with a company like Prudential , then let’s talk. We’re anxious to learn more about you and how your unique talents, perspective and abilities will help us transform our company. To learn more about Data Analytics career opportunities at Prudential, please connect with our Recruitment team via the competition forum . View examples of our current openings here: Greater New York Area, USA Letterkenny, Ireland Citation link keyboard_arrow_up Anna Montoya, BigJek14, Bull, denisedunleavy, egrad, FleetwoodHack, Imbayoh, PadraicS, Pru_Admin, tpitman, and Will Cukierski. Prudential Life Insurance Assessment. https://kaggle.com/competitions/prudential-life-insurance-assessment, 2015. Kaggle. Cite Prizes & Awards $30,000 Awards Points & Medals Participation 3,098 Entrants 2,610 Participants 2,610 Teams 45,348 Submissions Tags Tabular QuadraticWeightedKappa Table of Contents collapse_all Description Evaluation Prizes Timeline About Prudential Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "homesite-quote-conversion",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Which customers will purchase a quoted insurance plan? This dataset represents the activity of a large number of customers who are interested in buying policies from Homesite. Each QuoteNumber corresponds to a potential customer and the QuoteConversion_Flag indicates whether the customer purchased a policy. The provided features are anonymized and provide a rich representation of the prospective customer and policy. They include specific coverage information, sales information, personal information, property information, and geographic information. Your task is to predict QuoteConversion_Flag for each QuoteNumber in the test set. 3 files 65.13 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 65.13 MB sample_submission.csv.zip test.csv.zip train.csv.zip 3 files  Too many requests",
    "data_description": "Homesite Quote Conversion | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Homesite Quote Conversion Which customers will purchase a quoted insurance plan? Homesite Quote Conversion Overview Data Code Models Discussion Leaderboard Rules Overview Start Nov 9, 2015 Close Feb 9, 2016 Merger & Entry Description link keyboard_arrow_up Before asking someone on a date or skydiving, it's important to know your likelihood of success. The same goes for quoting home insurance prices to a potential customer. Homesite , a leading provider of homeowners insurance, does not currently have a dynamic conversion rate model that can give them confidence a quoted price will lead to a purchase. Using an anonymized database of information on customer and sales activity, including property and coverage information, Homesite is challenging you to predict which customers will purchase a given quote. Accurately predicting conversion would help Homesite better understand the impact of proposed pricing changes and maintain an ideal portfolio of customer segments. Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each QuoteNumber in the test set, you must predict a probability for QuoteConversion_Flag. The file should contain a header and have the following format: QuoteNumber,QuoteConversion_Flag 3,0 5,0.3 7,0 etc. Prizes link keyboard_arrow_up 1st place - $10,000 2nd place - $6,000 3rd place - $4,000 Timeline link keyboard_arrow_up February 1, 2016 - First Submission deadline. Your team must make its first submission by this deadline. February 1, 2016 - Team Merger deadline. This is the last day you may merge with another team February 8, 2016 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Darrel, Stephen D Stayton, and Will Cukierski. Homesite Quote Conversion. https://kaggle.com/competitions/homesite-quote-conversion, 2015. Kaggle. Cite Prizes & Awards $20,000 Awards Points & Medals Participation 2,185 Entrants 1,916 Participants 1,755 Teams 36,264 Submissions Tags Binary Classification Tabular Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "the-winton-stock-market-challenge",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Join a multi-disciplinary team of research scientists Updated 2015-12-21: Winton have added new data into the test set. If you downloaded the test set before 2015-12-21 please re-download the data set and submit predictions on this instead. In this competition the challenge is to predict the return of a stock, given the history of the past few days. We provide 5-day windows of time, days D-2, D-1, D, D+1, and D+2. You are given returns in days D-2, D-1, and part of day D, and you are asked to predict the returns in the rest of day D, and in days D+1 and D+2. During day D, there is intraday return data, which are the returns at different points in the day. We provide 180 minutes of data, from t=1 to t=180. In the training set you are given the full 180 minutes, in the test set just the first 120 minutes are provided. For each 5-day window, we also provide 25 features, Feature_1 to Feature_25. These may or may not be useful in your prediction. Each row in the dataset is an arbitrary stock at an arbitrary 5 day time window.  How these returns are calculated is defined by Winton, and will not to be revealed to you in this competition. The data set is designed to be representative of real data and so should bring about a number of challenges. 3 files 213.13 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 213.13 MB sample_submission_2.csv.zip test_2.csv.zip train.csv.zip 3 files  Too many requests",
    "data_description": "The Winton Stock Market Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz The Winton Stock Market Challenge Join a multi-disciplinary team of research scientists The Winton Stock Market Challenge Overview Data Code Models Discussion Leaderboard Rules Overview Start Oct 27, 2015 Close Jan 27, 2016 Merger & Entry Description link keyboard_arrow_up Do you laugh (and then get down to work) in the face of terabytes of noisy, non-stationary data? Winton Capital is looking for data scientists who excel at finding the hidden signal in the proverbial haystack, and who are excited by creating novel statistical modelling and data mining techniques. In this recruiting competition, Winton challenges you to take on the very difficult task of predicting the future (stock returns). Given historical stock performance and a host of masked features, can you predict intra and end of day returns without being deceived by all the noise? Research scientists at Winton have crafted this competition to be challenging and fun for the community while providing a taste of the types of problems they work on everyday. They're excited to connect with Kagglers who bring a unique background and creative approach to the competition. Winton is offering cash prizes to winning teams as a reward for their work, but the intent of the competition is not commercial. The intellectual property you create remains your own and will be evaluated in the context of suitability for employment. For more on the culture at Winton, check out the About Winton page or their careers page . Evaluation link keyboard_arrow_up Submissions are evaluated using the Weighted Mean Absolute Error . Each return you predicted is compared with the actual return. The formula is then W M A E = 1 n n ∑ i = 1 w i ⋅ | y i − ^ y i | , where \\\\(w_i\\\\) is the weight associated with the return, Weight_Intraday, Weight_Daily for intraday and daily returns, \\\\(i\\\\), \\\\(y_i\\\\) is the predicted return, \\\\(\\hat{y_i}\\\\) is the actual return, \\\\(n\\\\) is the number of predictions. The weights for the training set are given in the training data. The weights for the test set are unknown. Submission File The submission file should contain two columns: Id and Predicted. For each 5-day window , you need to predict 62 returns. For example, for the first time window, you will predict 1_1, 1_2, to 1_62. 1_1 to 1_60 are predicting Ret_121 through Ret_180, 1_61 the prediction for Ret_PlusOne, and 1_62 the prediction for Ret_PlusTwo. The file should contain a header and have the following format: Id,Predicted 1_1,0 1_2,0 1_3,0 1_4,0 ... 1_60,0 1_61,0 1_62,0 2_1,0 2_2,0 etc. Prizes link keyboard_arrow_up 1st place - $20,000 2nd place - $12,000 3rd place - $6,000 4th place - $4,000 5th place - $2,000 6th place - $1,000 Student Prize - $5,000 The Student Prize will be awarded to the highest ranked participant currently enrolled in an undergraduate or graduate program. In addition to the prizes above, we’ll invite any winners to meet members of our research groups in Oxford, London, New York, San Francisco, Hong Kong, Shanghai or Zurich dependent on their location. Timeline link keyboard_arrow_up January 19, 2016 - First Submission deadline. Your team must make its first submission by this deadline. January 26, 2016 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. About Winton link keyboard_arrow_up DATA DRIVEN INVESTMENT MANAGEMENT AT WINTON Winton is an investment management company powered by the revolutions in data analytics and computing. We work on large, noisy datasets to extract information that can help us to build successful automated trading systems for our clients. From our research centres in London, Oxford, New York, San Francisco, Hong Kong, Shanghai and Zurich we work to identify new data sources, integrate them into our data store and mine them for information. We’re looking for talented scientists and engineers to join us in building the future of the investment management industry. Data Engineering at Winton We’ve invested in building a data pipeline for a wide range of data types for our research programme. Our Data Engineers rapidly prototype solutions to incorporate new datasets, develop new analytics frameworks and educate our researchers about what’s available. To find out more about available Data Engineer roles in San Francisco and London, visit our website . Data Research at Winton Our research scientists bring the rigour and collaborative mind-set of academic research and apply it to the complex problems surrounding our automated trading portfolios. They use statistical techniques taken from across the applied sciences (plus many we’ve created ourselves) to identify information in data that can enhance our ability to trade in markets for our clients. To find out more about available Research Scientist roles in our research centres in Europe and the US, visit our website . (If you’re looking for more information or can’t see a role in your location/at your level, just drop a line to c.read@wintoncapital.com to discuss how you might contribute at Winton.) Life at Winton We look for enthusiasm and a desire to share your expertise. In return we offer stimulating working environments and the opportunity to be well rewarded from the company’s success. Our benefits include: Quarterly bonuses Substantial training budgets Access to the latest technology Fast feedback on things you build A great programme of social events Citation link keyboard_arrow_up AMPires, Christopher Read @ Winton, Fiveof12, Geoffrey Cross, JWJAnderson, Voldemort, and Wendy Kan. The Winton Stock Market Challenge. https://kaggle.com/competitions/the-winton-stock-market-challenge, 2015. Kaggle. Cite Prizes & Awards $50,000 Awards Points & Medals Participation 1,786 Entrants 829 Participants 829 Teams 10,717 Submissions Tags Finance Tabular Weighted Mean Absolute Error Table of Contents collapse_all Description Evaluation Prizes Timeline About Winton Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "the-allen-ai-science-challenge",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Is your model smarter than an 8th grader? The training data consists of 2,500 multiple choice questions from a typical US 8th grade science curriculum. Each question has four possible answers, of which exactly one is correct. Note that the questions in these datasets are private intellectual property, and by acknowledging the competition rules, you agree to not sharing or publishing any questions to parties other than yourself at any point in time, both during and after the competition. This is a two-stage competition. In the first stage, you are building models based on the training dataset, and testing your models by submitting predictions on the validation set. One week before the final deadline, you will submit your model to Kaggle. At this point, the second stage of the competition starts. Kaggle will release the final test dataset, on which you will run your predictive models. The final scores will be calculated based on this final test set. The validation set contains 8,132 questions of the same type without providing the correct answer. This set should only be used to submit automatically generated answers and should not be used for training purposes. To discourage inappropriate use, only a small proportion of these questions are real competition questions that will count for scoring. All the valid questions are used for public leaderboard, and none for private leaderboard. The final test set , to be released at stage 2 of the competition, will contain 21,298 questions of the same type (including the 8,132 from the validation set). Again, only a small proportion will be used in the scoring. All the validation set questions will be used for the public leaderboard, and all the new test set questions used for private leaderboard. EDIT: Train/Validation/Test datasets are removed as of 2/13/2016 at the end of the competition 1 files 52.61 kB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 52.61 kB submission_sample_phase2.csv.zip 1 file  Too many requests",
    "data_description": "The Allen AI Science Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Allen Institute for Artificial Intelligence · Featured Prediction Competition · 9 years ago Late Submission more_horiz The Allen AI Science Challenge Is your model smarter than an 8th grader? The Allen AI Science Challenge Overview Data Code Models Discussion Leaderboard Rules Overview Start Oct 7, 2015 Close Feb 14, 2016 Merger & Entry Description link keyboard_arrow_up The Allen Institute for Artificial Intelligence (AI2) is working to improve humanity through fundamental advances in artificial intelligence. One critical but challenging problem in AI is to demonstrate the ability to consistently understand and correctly answer general questions about the world. The Aristo project at AI2 is focused on building such a system. One way Aristo \"learns\" is by extracting facts from various sources and processing them into a structured knowledge base. When taking an exam, questions are parsed and processed along with any accompanying diagrams to determine a strategy for answering. Aristo then uses entailment, statistical analysis, and inference methods to select a final answer. While Aristo's abilities have improved significantly in the last two years, it still doesn't have perfect, reliable methods of gathering knowledge, understanding questions, or reasoning through answers. Using a dataset of multiple choice question and answers from a standardized 8th grade science exam, AI2 is challenging you to create a model that gets to the head of the class. Evaluation link keyboard_arrow_up Submissions are evaluated by categorization accuracy, i.e., the fraction of multiple choice questions answered correctly. Random guessing should produce an evaluation score of around 0.25. Submission file For every question in the dataset , submission files should contain two columns: id and correctAnswer, where the correctAnswer is one of A, B, C or D just like in the training set and the id is the question id from the dataset. The file should contain a header and have the following format: id,correctAnswer 102501,A 102502,B 102503,B 102504,D etc. Model submission and final test set During the last two weeks of the model training period, you will be able to upload your models to Kaggle. This model submission must contain all data, code, and parameter settings necessary to evaluate your models on new questions, and include a README file with instructions on how to do so. The purpose of this is to ensure a fair competition and that no manual answering of the test set questions has been done. You must also make at least one submission on the validation set with this model before the model submission deadline. If you would like, you may submit your model as an encrypted archive, and you will only be asked to provide the decryption key if you are one of the preliminary winners. The model submission is required to be eligible to win prize money. When the final test set is released, the same model should be used to submit answers to the test set, and the prize winning models will be verified manually as to fulfilling these requirements. To avoid random discrepancies, make sure to seed any random number generators used in the model. Prizes link keyboard_arrow_up In order to be eligible for prizes, models must demonstrate automated answer generation for questions and must be runnable and verifiable by a third party. Winning participants will be required to provide a complete written description of their analysis and methodology, and should be willing to engage with any interested media parties about their winning solutions, answering questions or providing requested statements about the competition, their model, or their thoughts on the challenge in general. 1st place - $50,000 2nd place - $20,000 3rd place - $10,000 Timeline link keyboard_arrow_up February 5, 2016 - First Submission deadline. Your team must make its first submission by this deadline. February 5, 2016 - Team Merger deadline. This is the last day you may merge with another team. February 5, 2016 - Model upload deadline. This is the last day you may upload your model to be eligible for a prize. February 6, 2016 - Start of competition Stage II. Release of final test dataset. February 13, 2016 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Anna Montoya, Carissa Schoenick, OrenEtzioni, Oyvind Tafjord, and Wendy Kan. The Allen AI Science Challenge. https://kaggle.com/competitions/the-allen-ai-science-challenge, 2015. Kaggle. Cite Competition Host Allen Institute for Artificial Intelligence Prizes & Awards $80,000 Awards Points & Medals Participation 1,595 Entrants 302 Participants 170 Teams 679 Submissions Tags Artificial Intelligence Text Tabular Multiclass Classification Categorization Accuracy Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "rossmann-store-sales",
    "discussion_links": [
      "/competitions/rossmann-store-sales/discussion/18024",
      "/competitions/rossmann-store-sales/discussion/17974"
    ],
    "discussion_texts": [
      "Rossmann Store Sales | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Rossmann Store Sales Forecast sales using store, promotion, and competitor data Rossmann Store Sales Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Rossmann Store Sales | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Rossmann Store Sales Forecast sales using store, promotion, and competitor data Rossmann Store Sales Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Forecast sales using store, promotion, and competitor data You are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set. Note that some stores in the dataset were temporarily closed for refurbishment. Most of the fields are self-explanatory. The following are descriptions for those that aren't. 4 files 39.85 MB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 39.85 MB sample_submission.csv store.csv test.csv train.csv 4 files 29 columns  Too many requests",
    "data_description": "Rossmann Store Sales | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Rossmann Store Sales Forecast sales using store, promotion, and competitor data Rossmann Store Sales Overview Data Code Models Discussion Leaderboard Rules Overview Start Sep 30, 2015 Close Dec 15, 2015 Merger & Entry Description link keyboard_arrow_up Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied. In their first Kaggle competition, Rossmann is challenging you to predict 6 weeks of daily sales for 1,115 stores located across Germany. Reliable sales forecasts enable store managers to create effective staff schedules that increase productivity and motivation. By helping Rossmann create a robust prediction model, you will help store managers stay focused on what’s most important to them: their customers and their teams! Evaluation link keyboard_arrow_up Submissions are evaluated on the Root Mean Square Percentage Error (RMSPE). The RMSPE is calculated as RMSPE = √ 1 n n ∑ i = 1 ( y i − ˆ y i y i ) 2 , where y_i denotes the sales of a single store on a single day and yhat_i denotes the corresponding prediction. Any day and store with 0 sales is ignored in scoring. Submission File The file should contain a header and have the following format: Id,Sales 1,0 2,0 3,0 etc. Prizes link keyboard_arrow_up 1st place - $15,000 2nd place - $10,000 3rd place - $5,000 In addition, a single $5,000 reward will go to the team whose methodology is implemented by Rossmann. This award may be given to a team at any position on the leaderboard. Rossmann is interested in hiring top Kagglers from this competition. If you're interested in a position with Rossmann, you may append \"JOB\" next to your team name for consideration. Timeline link keyboard_arrow_up December 7, 2015 - First Submission deadline. Your team must make its first submission by this deadline. December 7, 2015 - Team Merger deadline. This is the last day you may merge with another team. December 14, 2015 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up FlorianKnauer and Will Cukierski. Rossmann Store Sales. https://kaggle.com/competitions/rossmann-store-sales, 2015. Kaggle. Cite Prizes & Awards $35,000 Awards Points & Medals Participation 4,271 Entrants 3,735 Participants 3,298 Teams 70,114 Submissions Tags Tabular Time Series Analysis Root Mean Square Percentage Error Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "springleaf-marketing-response",
    "discussion_links": [
      "/competitions/springleaf-marketing-response/discussion/17089",
      "/competitions/springleaf-marketing-response/discussion/17083"
    ],
    "discussion_texts": [
      "Springleaf Marketing Response | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Springleaf Marketing Response Determine whether to send a direct mail piece to a customer Springleaf Marketing Response Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Springleaf Marketing Response | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Springleaf Marketing Response Determine whether to send a direct mail piece to a customer Springleaf Marketing Response Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Determine whether to send a direct mail piece to a customer See this example R Script that trains an XGBoost model and creates a submission You are provided a high-dimensional dataset of anonymized customer information. Each row corresponds to one customer. The response variable is binary and labeled \"target\". You must predict the target variable for every row in the test set. The features have been anonymized to protect privacy and are comprised of a mix of continuous and categorical features. You will encounter many \"placeholder\" values in the data, which represent cases such as missing values. We have intentionally preserved their encoding to match with internal systems at Springleaf. The meaning of the features, their values, and their types are provided \"as-is\" for this competition; handling a huge number of messy features is part of the challenge here. 3 files 314.54 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 314.54 MB sample_submission.csv.zip test.csv.zip train.csv.zip 3 files  Too many requests",
    "data_description": "Springleaf Marketing Response | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Springleaf Marketing Response Determine whether to send a direct mail piece to a customer Springleaf Marketing Response Overview Data Code Models Discussion Leaderboard Rules Overview Start Aug 14, 2015 Close Oct 20, 2015 Merger & Entry Description link keyboard_arrow_up Springleaf puts the humanity back into lending by offering their customers personal and auto loans that help them take control of their lives and their finances. Direct mail is one important way Springleaf's team can connect with customers whom may be in need of a loan. Direct offers provide huge value to customers who need them, and are a fundamental part of Springleaf's marketing strategy. In order to improve their targeted efforts, Springleaf must be sure they are focusing on the customers who are likely to respond and be good candidates for their services. Using a large set of anonymized features, Springleaf is asking you to predict which customers will respond to a direct mail offer. You are challenged to construct new meta-variables and employ feature-selection methods to approach this dauntingly wide dataset. Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. Submission File For each ID in the test set, you should predict a probability. The file should contain a header and have the following format: ID,target 1,0.35 3,0.001 6,0.9333 etc. Prizes link keyboard_arrow_up 1st place - $40,000 2nd place - $30,000 3rd place - $15,000 4th place - $10,000 5th place - $5,000 Timeline link keyboard_arrow_up October 12, 2015 - First Submission deadline. Your team must make its first submission by this deadline. October 12, 2015 - Team Merger deadline. This is the last day you may merge with another team October 19, 2015 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up gonza95c, Meghan O'Connell, Skyline, and Will Cukierski. Springleaf Marketing Response. https://kaggle.com/competitions/springleaf-marketing-response, 2015. Kaggle. Cite Prizes & Awards $100,000 Awards Points & Medals Participation 3,117 Entrants 2,482 Participants 2,221 Teams 39,297 Submissions Tags Binary Classification Tabular Marketing Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "dato-native",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict which web pages served by StumbleUpon are sponsored In this competition, your task is to predict whether the content in an HTML file is sponsored or not. 9 files 7.97 GB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 7.97 GB 0.zip 1.zip 2.zip 3.zip 4.zip 5.zip sampleSubmission.csv.zip sampleSubmission_v2.csv.zip train.csv.zip train_v2.csv.zip 9 files  Too many requests",
    "data_description": "Truly Native?  | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Truly Native? Predict which web pages served by StumbleUpon are sponsored Truly Native? Overview Data Code Models Discussion Leaderboard Rules Overview Start Aug 6, 2015 Close Oct 15, 2015 Merger & Entry Description link keyboard_arrow_up Online media companies rely more and more on paid advertising to keep their lights on and their content engines humming. \" Native advertising \" is a popular alternative to the unsightly banner ads and infuriating pop-ups of Internet Advertising 1.0. Native ads mimic the core content of the site they're advertising on, ideally avoiding any interruption of the user's experience. When native advertising is done right, users aren't desperately scanning an ad for a hidden \"x\". In fact, they don't even know they're viewing one. To pull this off, native ads need to be just as interesting, fun, and informative as the unpaid content on a site. Dato is sponsoring this competition with the noble goal of making native advertising live up to its name. With a dataset of over 300,000 raw HTML files containing text, links, and downloadable images, they also want to give Kagglers a challenge that encourages creativity. Given the HTML of websites served to users of StumbleUpon, your challenge is to identify the paid content disguised as just another internet gem you've stumbled upon. If media companies can better identify poorly designed native ads, they can keep them off your feed and out of your user experience. For details on using GraphLab Create for the competition, check out Dato's post . Acknowledgements The dataset for this competition was generously provided by StumbleUpon . Evaluation link keyboard_arrow_up Submissions are judged on area under the ROC curve . Submission File Each line of your submission should contain an Id and a prediction of the probability that this file is sponsored. Your submission file must have a header row. The file should have the following format: file,sponsored 1923087_raw_html.txt,0.1 2216383_raw_html.txt,0.7 ... etc Prizes link keyboard_arrow_up 1st place - $5,000 Dato Creators Award - $5,000 for the best performing submission using GraphLab Create models Dato swag for top 10 teams To be considered for the GraphLab Create Award, you must indicate in your team name \"GLC\". Your solution will be verified by Dato to claim the winner. For details on using GraphLab Create for the competition, check out Dato's post . Timeline link keyboard_arrow_up October 7, 2015 - First Submission deadline. Your team must make its first submission by this deadline. October 7, 2015 - Team Merger deadline. This is the last day you may merge with another team October 14, 2015 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up cloofa, Shawn Scully, and Wendy Kan. Truly Native? . https://kaggle.com/competitions/dato-native, 2015. Kaggle. Cite Prizes & Awards $10,000 Awards Points & Medals Participation 569 Entrants 339 Participants 273 Teams 3,220 Submissions Tags Binary Classification Marketing Tabular Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "flavours-of-physics",
    "discussion_links": [
      "/competitions/flavours-of-physics/discussion/17142",
      "/competitions/flavours-of-physics/discussion/17062",
      "/competitions/flavours-of-physics/discussion/17000",
      "/competitions/flavours-of-physics/discussion/17008",
      "/competitions/flavours-of-physics/discussion/17172"
    ],
    "discussion_texts": [
      "Flavours of Physics: Finding τ  →  μμμ | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Flavours of Physics: Finding τ  →  μμμ Identify a rare decay phenomenon Flavours of Physics: Finding τ  →  μμμ Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Flavours of Physics: Finding τ  →  μμμ | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Flavours of Physics: Finding τ  →  μμμ Identify a rare decay phenomenon Flavours of Physics: Finding τ  →  μμμ Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Flavours of Physics: Finding τ  →  μμμ | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Flavours of Physics: Finding τ  →  μμμ Identify a rare decay phenomenon Flavours of Physics: Finding τ  →  μμμ Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Flavours of Physics: Finding τ  →  μμμ | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Flavours of Physics: Finding τ  →  μμμ Identify a rare decay phenomenon Flavours of Physics: Finding τ  →  μμμ Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Flavours of Physics: Finding τ  →  μμμ | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Flavours of Physics: Finding τ  →  μμμ Identify a rare decay phenomenon Flavours of Physics: Finding τ  →  μμμ Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Identify a rare decay phenomenon In this competition, you are given a list of collision events and their properties. You will then predict whether a τ → 3μ decay happened in this collision. This τ → 3μ is currently assumed by scientists not to happen, and the goal of this competition is to discover τ → 3μ happening more frequently than scientists currently can understand. It is challenging to design a machine learning problem for something you have never observed before. Scientists at CERN developed the following designs to achieve the goal. This is a labelled dataset (the label ‘signal’ being ‘1’ for signal events, ‘0’ for background events) to train the classifier. Signal events have been simulated, while background events are real data. This real data is collected by the LHCb detectors observing collisions of accelerated particles with a specific mass range in which τ → 3μ can’t happen. We call these events “background” and label them 0. The test dataset has all the columns that training.csv has, except mass , production , min_ANNmuon , and signal . The test dataset consists of a few parts: real data for the control channel (ignored for scoring, used by agreement test) You need to submit predictions for ALL the test entries. You will need to treat them all the same and predict as if they are all the same channel's collision events. A submission is only scored after passing both the agreement test and the correlation test. This dataset contains simulated and real events from the Control channel Ds → φπ to evaluate your simulated-real data of submission agreement locally. It contains the same columns as test.csv and weight column. For more details see agreement test . This dataset contains only real background events recorded at LHCb to evaluate your submission correlation with mass locally. It contains the same columns as test.csv and mass column to check correlation with. For more details see correlation test . 5 files 421.79 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 421.79 MB check_agreement.csv.zip check_correlation.csv.zip sample_submission.csv.zip test.csv.zip training.csv.zip 5 files  Too many requests",
    "data_description": "Flavours of Physics: Finding τ  →  μμμ | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Flavours of Physics: Finding τ  →  μμμ Identify a rare decay phenomenon Flavours of Physics: Finding τ  →  μμμ Overview Data Code Models Discussion Leaderboard Rules Overview Start Jul 21, 2015 Close Oct 13, 2015 Merger & Entry Description link keyboard_arrow_up Like last year's Higgs Boson Machine Learning Challenge , this competition deals with the  physics at the Large Hadron Collider (LHC) . However, the subject of last year's challenge, the Higgs Boson, was already known to exist. The aim of this year's challenge is to find a phenomenon that is not already known to exist – charged lepton flavour violation – thereby helping to establish \" new physics \". Flavours of Physics 101 The laws of nature ensure that some physical quantities, such as energy or momentum, are conserved. From Noether’s theorem , we know that each conservation law is associated with a fundamental symmetry. For example, conservation of energy is due to the time-invariance (the outcome of an experiment would be the same today or tomorrow) of physical systems. The fact that physical systems behave the same, regardless of where they are located or how they are oriented, gives rise to the conservation of linear and angular momentum. Symmetries are also crucial to the structure of the Standard Model of particle physics, our present theory of interactions at microscopic scales. Some are built into the model, while others appear accidentally from it. In the Standard Model, lepton flavour, the number of electrons and electron-neutrinos, muons and muon-neutrinos, and tau and tau-neutrinos, is one such conserved quantity. (opens in a new tab)\"> Interestingly, in many proposed extensions to the Standard Model, this symmetry doesn’t exist, implying decays that do not conserve lepton flavour are possible. One decay searched for at the LHC is τ - → μ + μ - μ - (or τ → 3μ). Observation of this decay would be a clear indication of the violation of lepton flavour and a sign of long-sought new physics. Competition Design You will be working with real data from the LHCb experiment at the LHC, mixed with simulated datasets of the decay. The metric used in this challenge includes checks that physicists do in their analysis to make sure the results are unbiased. These checks have been built into the competition design to help ensure that the results will be useful for physicists in future studies. To get started, review the Data Page , and be sure to download the Starter Kit . The Starter Kit will help you to get used to the unique submission procedure for this competition. Competition Video Tutorial You've got lots of questions. Researchers at CERN & LCHb have the answers. - What is the goal of this competition? (1:56) - Why is finding τ → μμμ exciting? (2:18) - What are flavours? (4:10) - Why use machine learning to find τ → μμμ? (4:57) - How did you decide on the size of the dataset? (5:31) - Why is weighted AUC the evaluation metric? (6:09) - Why use Ds → φπ data for the Agreement Test? (7:53) - Why do we need a Correlation Check? (8:44) - How will the competition results impact what you do? (11:38) - How will the competition results be used at CERN? (12:17) Resources Flavour of Physics, Research Documentation Roel Aaij et al., Search for the lepton flavour violating decay τ → µµµ, 2015, JHEP, 1502:121, 2015 New approaches for boosting to uniformity Acknowledgements This competition is brought to you by: (opens in a new tab)\"> (opens in a new tab)\"> (opens in a new tab)\"> (opens in a new tab)\"> Co-sponsored by: Additional support from: (opens in a new tab)\"> (opens in a new tab)\"> (opens in a new tab)\"> (opens in a new tab)\"> Evaluation link keyboard_arrow_up The evaluation metric for this competition is Weighted Area Under the ROC Curve. The ROC curve is divided into sections based on the True Positive Rate (TPR). To calculate the total area, multiply the area with TPR in [0., 0.2] by weight 2.0, the area with TPR in [0.2, 0.4] by 1.5, the area with TPR [0.4, 0.6] with weight 1.0, and the area with TPR [0.6, 0.8] with weight 0.5. Anything above a TPR of 0.8 has weight 0. These weights were chosen to match the evaluation methodology used by CERN scientists. Note that the weighted AUC is calculated only for events (simulated signal events for tau->µµµ and real background events for tau->µµµ) with min_ANNmuon > 0.4 (see details in section 2.2 Physics background ). Before your predictions are scored with weighted AUC, they also must pass two addition checks: first an agreement test and then the correlation test . Please refer to their respective pages to learn about these tests and what is needed to pass them. Submission File For every event in the dataset , submission files should contain two columns: id and prediction . The prediction should be a floating point value between 0 and 1.0, indicating the probability that this event is τ → 3μ decay. The file should contain a header and have the following format: id,prediction 14711831,0.3 16316387,0.3 6771382,0.3 686045,0.3 8755882,0.3 10247299,0.3 etc. Prizes link keyboard_arrow_up Cash Prizes Participants with the best score on the private leaderboard who meet the eligibility criteria detailed in the competition rules will be awarded: 1st Place - $7,000 2nd Place - $5,000 3rd Place - $3,000 Additional Awards and Opportunities HEP Meets ML Award An award will be given to the team that, as judged by the LHCb collaboration members on the organizing committee, creates a model that is most useful for the LHCb experiment. The selection criteria include: T he simplicity/straightforwardness/originality of the approach T he computing requirements (CPU and memory demands) T he suitability for use in production T he robustness with respect to lack of training statistics The winning team will be awarded $2000 and invited to meet the LHCb collaboration physicists at dedicated workshop held by University of Zurich on Feb. 18-19, 2016. NIPS workshop Strong performers in this competition may be invited to contribute to a NIPS workshop associated with this competition (pending acceptance from NIPS) with limited travel and conference support. Eligibility The winners of the cash prizes must provide a paper & code for presentation at the NIPs workshop. However, they do not need to submit a speaker's proposal. To be eligible for the HEP Meets ML Award, the team must submit their model with code and a textual description (500-1000 words) at most one week after the end of the main competition. The committee will then select up to 10 submissions, based on the description and the place that the team took, and invite them to the NIPS workshop provided they prepare a talk & paper for the workshop. The committee will select the best paper/solution from these 10 papers and award them the HEP Meets ML Award prize at the workshop. The team will decide how the amount of the HEP Meets ML Award will be divided internally, it being understood that the Award will not cover the travel expenses of team members who belong to the LHCb collaboration or are based at CERN. A single team can win more than one prize. Scripts for Swag The authors of the 3 most up-voted scripts for this competition will get their choice of an official Kaggle sweatshirt, t-shirt, or mug! Your position on the leaderboard does not factor into winning Scripts for Swag. Timeline link keyboard_arrow_up Competition Monday, October 5, 2015 : First submission deadline. You must make your first submission by this deadline. New entrants are not allowed after this date. Monday, October 5, 2015 : Team Merger deadline. Team mergers are not allowed after this date. Monday, October 12, 2015 : Final submission deadline. All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Code Verification & Selection October 14, 2015 : The three top ranking teams on the private leaderboard must turn in their code to qualify for prizes ( see Eligibility instructions ). Post-challenge verification starts. October 26, 2015 : Teams wanting to submit for the HEP Meets ML Award must provide code for their models and a textual description (500-1000 words). November 2, 2015 : Winners announced, invitations to NIPS workshop go out. December 11, 2015 : NIPS workshop on ML & HEP. Announcement of HEP Meets ML Award. Agreement Test link keyboard_arrow_up Classifier should agree on real and simulated data Since the classifier is trained on a mix of simulated signal and real data background, it is possible to reach a high performance by picking features that are not perfectly modeled in the simulation. We require that the classifier not have a large discrepancy when applied to real and simulated data. To verify this, we use a control channel, Ds → φπ, that has a similar topology as the signal decay, τ → 3μ. Ds → φπ is a much more well-known, well-observed behavior, as it happens more frequently. We provide both data and simulation samples for this decay, to which the classifier is applied. A Kolmogorov–Smirnov (KS) test is used to evaluate the differences between the classifier distribution on each sample. Ds → φπ data is not included in the training set. This control channel data is used purely in the test set as \"ignored\" data, meaning that they don’t contribute to the final scoring. The control channel dataset is used purely in Kolmogorov–Smirnov test. We require the KS-value of the test to be smaller than 0.09. Only when a submission can pass the agreement test on these ignored data, we can then regard the predictions as valid, and score on the submission. Details about agreement test In particle physics experiments, it is well-known how to split the signal/background by their “weights”. The method is called sPlot . This weight is a measurement derived from the likelihood/probability that the event is a Ds → φπ and the weights range from -∞ to ∞. Higher weight means likely this event is signal, lower weights means it’s likely to be background. The Control channel, Ds → φπ, is split into two main parts, real (data1) and simulated (data2): data1 = real data with weight from sPlot function and signal = 0 data2 = simulated data (just signal) with weight = 1 and signal = 1 We use K-S test to tell that classifier outputs for data1 and data2 are from the same distribution. Please check out the github repo to learn more about how the agreement test is implemented. Correlation Test link keyboard_arrow_up Model should be uncorrelated to the τ mass Each particle has its own mass. In an ideal world, one would just the mass of a particle to tell which particle it is. However, in reality, mass is an estimation, and it isn't a feature that scientists trust when building a model. Correlations with mass can cause an artificial signal-like mass peak or lead to incorrect background estimations. The mass column is not included in the test dataset. However, we use the hidden mass information to perform a Cramer-von Mises (cvm) test. This test iteratively compares two distributions of a) predicted values from submission for entire dataset and b) predicted values within a certain mass region in rolling window fashion along the whole mass range. Then we get average value for those checks. Getting similar distributions for all mass sub-regions means that your classifier is not correlated with the mass. Your submission must give a cvm value less than 0.002 to pass the test. Below is an illustration of this correlation test. On the left shows no correlation between prediction and mass, while on the right there is correlation. Please check out the github repo to learn more about how the correlation test is implemented. Starter Kit link keyboard_arrow_up There are several starter kits prepared for making submissions using different computing languages (Python, C++/ROOT). All the starter kits are available in the following GitHub repository: https://github.com/yandexdataschool/flavours-of-physics-start Also you can find implementations of the competition Agreement and Correlation checks there. About The Sponsors link keyboard_arrow_up The European Organization for Nuclear Research is the world’s largest high energy physics laboratory. LHCb is an experiment set up to explore what happened after the Big Bang that allowed matter to survive and build the Universe we inhabit today. The Yandex School of Data Analysis (YSDA) is a free Master’s-level program in Computer Science and Data Analysis, which is offered by Yandex since 2007. The aim of the School is to train specialists in data analysis and information retrieval to be able to solve cutting edge industry problems as well as fundamental research challenges. YSDA is associated member of LHCb since December 2014. Yandex Data Factory are the Machine Learning and data analytics experts that use data science to improve business’ operations, revenues and profitability. By building upon the real-time personalisation and predictive analytics technology of parent company, Yandex, the fourth largest search engine in the world, Yandex Data Factory helps clients improve their business awareness through the exploitation of their own data. Yandex Data Factory’s proven data science and technology continually analyses, tests, refines and reapplies hundreds of hypotheses to the customers’ datasets to determine the best next course of action. It offers tailored, scalable, SaaS-driven Machine Learning services to a wide variety of data-reliant verticals, such as retail, financial services, travel and telecoms, who wish to use their data for purposes such as improving personalisation, segmentation, churn prevention or fraud detection. Yandex Data Factory was founded in 2014 by Yandex and is headquartered in Amsterdam, operating throughout Europe. Intel (NASDAQ: INTC) is a world leader in computing innovation. The company designs and builds the essential technologies that serve as the foundation for the world’s computing devices. As a leader in corporate responsibility and sustainability, Intel also manufactures the world’s first commercially available “conflict-free” microprocessors. Additional information about Intel is available at http://newsroom.intel.com and http://blogs.intel.com . The University of Zurich is one of the leading research universities in Europe and offers the widest range of degree programs in Switzerland. It was founded in 1833 and currently has seven faculties: Philosophy, Human Medicine, Economic Sciences, Law, Mathematics and Natural Sciences, Theology and Veterinary Medicine. Warwick is one of the UK's leading universities, with an acknowledged reputation for excellence in research and teaching, for innovation, and for links with business and industry. Institute of Nuclear Physics, Polish Academy of Sciences. Founded in 1955 Institute of Nuclear Physics has become leading Particle Physics research institution and ranked as class A+ by Polish Ministry of Higher Education. Consistently ranked as one of Russia’s top universities, the Higher School of Economics is a leader in Russian education and one of the preeminent economics and social sciences universities in eastern Europe and Eurasia. Citation link keyboard_arrow_up Albercik1905, Alex.Rogozhnikov, Anaderi, Eugene, serdyukovpv, Tatiana Likhomanenko, Wendy Kan, and Zalina. Flavours of Physics: Finding τ  →  μμμ. https://kaggle.com/competitions/flavours-of-physics, 2015. Kaggle. Cite Prizes & Awards $15,000 Awards Points & Medals Participation 905 Entrants 706 Participants 673 Teams 10,124 Submissions Tags Physics Binary Classification Tabular Custom Metric Table of Contents collapse_all Description Evaluation Prizes Timeline Agreement Test Correlation Test Starter Kit About The Sponsors Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "coupon-purchase-prediction",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict which coupons a customer will buy You are provided with a year of transactional data for 22,873 users on the site ponpare.jp. The training set spans the dates 2011-07-01 to 2012-06-23. The test set spans the week after the end of the training set, 2012-06-24 to 2012-06-30. The goal of the competition is to recommend a ranked list of coupons for each user in the dataset (found in user_list.csv ). Your predictions are scored against the actual coupon purchases, made during the test set week, of the 310 possible test set coupons. The dataset has relational format, with hashed ID columns for each entity.  10 files 98.94 MB zip, csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 98.94 MB coupon_area_test.csv.zip coupon_area_train.csv.zip coupon_detail_train.csv.zip coupon_list_test.csv.zip coupon_list_train.csv.zip coupon_visit_train.csv.zip documentation.zip prefecture_locations.csv sample_submission.csv.zip user_list.csv.zip 10 files 4 columns  Too many requests",
    "data_description": "Coupon Purchase Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Recruit Holdings · Featured Prediction Competition · 10 years ago Late Submission more_horiz Coupon Purchase Prediction Predict which coupons a customer will buy Coupon Purchase Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Jul 16, 2015 Close Oct 1, 2015 Merger & Entry Description link keyboard_arrow_up Recruit Ponpare is Japan's leading joint coupon site, offering huge discounts on everything from hot yoga, to gourmet sushi, to a summer concert bonanza. Ponpare's coupons open doors for customers they've only dreamed of stepping through. They can learn difficult to acquire skills, go on unheard of adventures, and dine like (and with) the stars. Investing in a new experience is not cheap. We fear wasting our time and money on a product or service that we may not enjoy or fully understand. Ponpare takes the high price out of this equation, making it easier for you to take the leap towards your first sky-dive or diamond engagement ring. Using past purchase and browsing behavior, this competition asks you to predict which coupons a customer will buy in a given period of time. The resulting models will be used to improve Ponpare's recommendation system, so they can make sure their customers don't miss out on their next favorite thing. Evaluation link keyboard_arrow_up Submissions are evaluated according to the Mean Average Precision @ 10 (MAP@10): M A P @ 10 = 1 | U | | U | ∑ u = 1 1 m i n ( m , 10 ) m i n ( n , 10 ) ∑ k = 1 P ( k ) where |U| is the number of users, P(k) is the precision at cutoff k, n is the number of predicted coupons, and m is the number of purchased coupons for the given user. If m = 0, the precision is defined to be 0. Submission File For every user, you must predict a space-delimited list of the coupons they purchased. The file should contain a header and have the following format (we have substituted the coupon hashes with dummy values to fit below, but in your prediction file you should use the real hash values): USER_ID_hash,PURCHASED_COUPONS 0004901ba699a49fd93a3c6bb1768b8f,hash4 0006d6ac7c6ef3fc0ab0dc40deb3c960,hash1 hash2 00078d03b4dda619293c1793c251f783, etc... Prizes link keyboard_arrow_up 1st place - $30,000 2nd place - $15,000 3rd place - $5,000 Scripts for Swag The authors of the 3 most up-voted scripts for this competition will get their choice of an official Kaggle sweatshirt, t-shirt, or mug! Your position on the leaderboard does not factor into winning Scripts for Swag. Timeline link keyboard_arrow_up September 23, 2015 - First Submission deadline. Your team must make its first submission by this deadline. September 23, 2015 - Team Merger deadline. This is the last day you may merge with another team September 30, 2015 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up haisland0909, Shingo KATO, suharay, and Will Cukierski. Coupon Purchase Prediction. https://kaggle.com/competitions/coupon-purchase-prediction, 2015. Kaggle. Cite Competition Host Recruit Holdings Prizes & Awards $50,000 Awards Points & Medals Participation 1,793 Entrants 1,188 Participants 1,072 Teams 18,390 Submissions Tags Multiclass Classification Marketing Tabular MAP@{K} Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "machinery-tube-pricing",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Model quoted prices for industrial tube assemblies Too many requests",
    "data_description": "Machinery Tube Pricing | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Machinery Tube Pricing Model quoted prices for industrial tube assemblies Machinery Tube Pricing Overview Data Code Models Discussion Leaderboard Rules Overview Start Jun 29, 2015 Close Sep 1, 2015 Merger & Entry Description link keyboard_arrow_up Construction machines rely on a complex set of tubes to keep the forklift lifting, the loader loading, and the bulldozer from dozing off. Tubes can vary across a number of dimensions, including base materials, number of bends, bend radius, bolt patterns, and end types. Tubes come from a variety manufacturers, each having their own unique pricing model. This competition provides detailed tube, component, and annual volume datasets, and challenges you to predict the price a supplier will quote for a given tube assembly. Evaluation link keyboard_arrow_up Submissions are evaluated one the Root Mean Squared Logarithmic Error (RMSLE). The RMSLE is calculated as √ 1 n n ∑ i = 1 ( log ( p i + 1 ) − log ( a i + 1 ) ) 2 Where: \\\\(n\\\\) is the number of price quotes in the test set \\\\(p_i\\\\) is your predicted price \\\\(a_i\\\\) is the actual price \\\\(\\log(x)\\\\) is the natural logarithm Submission File The file should contain a header and have the following format: id,cost 1,0 2,0 3,0 ... Prizes link keyboard_arrow_up 1st place - $15,000 2nd place - $10,000 3rd place - $5,000 Scripts for Swag The authors of the 3 most up-voted scripts for this competition will get their choice of an official Kaggle sweatshirt, t-shirt, or mug! Your position on the leaderboard does not factor into winning Scripts for Swag. Timeline link keyboard_arrow_up August 24, 2015 - First Submission deadline. Your team must make its first submission by this deadline. August 24, 2015 - Team Merger deadline. This is the last day you may merge with another team August 31, 2015 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up bryanfsu and Will Cukierski. Machinery Tube Pricing. https://kaggle.com/competitions/machinery-tube-pricing, 2015. Kaggle. Cite Prizes & Awards $30,000 Awards Points & Medals Participation 1,681 Entrants 1,448 Participants 1,320 Teams 26,239 Submissions Tags Regression Manufacturing Tabular Root Mean Squared Logarithmic Error Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "avito-context-ad-clicks",
    "discussion_links": [
      "/competitions/avito-context-ad-clicks/discussion/16084"
    ],
    "discussion_texts": [
      "Avito Context Ad Clicks | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 10 years ago Late Submission more_horiz Avito Context Ad Clicks Predict if context ads will earn a user's click Avito Context Ad Clicks Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict if context ads will earn a user's click This competition has 8 relational datasets. All these files were encoded in UTF-8 and stored into tab separated format (.tsv). There is also an sqlite database (database.sqlite) alternative with all data available. Relationships between the datasets are captured in the following schema:  These two files are the main datasets related to your predictive models. trainSearchStream is a random sample of previously selected users' searches on avito.ru during at least 16 consecutive days from April'25 until the target impression. Different types of ads on the site are shown in the picture below:  Regular ads are shifted down constantly as new ads come in. (Normally, a visitor's search results are sorted by the time an ad is submitted to Avito). Each line in the file describes one \"impression\" (an ad that is shown to a particular user based on a search). testSearchStream shares the same format, except the target variable field \" IsClick\" is omitted. These are samples of users' visits to non-contextual ad landing pages and the corresponding phone request (if one occurred). Each ad's landing page shows the hidden seller's phone number. To be able to contact the seller, the user needs to click the request phone button:  Consequently, a user's phone request event could be considered a proxy for a user's response to the advertisement. We believe that clicking the phone request indicates a high level of interest in the ad. Note that both Params from AdsInfo.tsv and SearchParams from SearchInfo.tsv shares same dictionary (keys and values). The Params are semi-structured to reflect the nature of the search and the product. For example, the Params for clothing might be gender, size, color, brand; while the Params for houses might be size, # of bedrooms, # of bathrooms, etc. The dataset contains a sample of users and their behavior. For each user, one target impression between time point A (May, 12) and time point B (May, 20) was selected randomly. The task of this competition is to provide the probability that a user will click on the selected target ad, given all the information generated by the user from the Start time point (April, 25) until the target impression.  Note that some users from the testSearchStream may not have any historical information in VisitStream, PhoneRequestsStream and trainSearchStream as some of them may be new registered or had no activity within Start - Test event time interval. 12 files 19.07 GB 7z Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 19.07 GB AdsInfo.tsv.7z Category.tsv.7z Location.tsv.7z PhoneRequestsStream.tsv.7z SearchInfo.tsv.7z UserInfo.tsv.7z VisitsStream.tsv.7z database.sqlite.7z sampleSubmission.csv.7z sampleSubmission_HistCTR.csv.7z testSearchStream.tsv.7z trainSearchStream.tsv.7z 12 files  Too many requests",
    "data_description": "Avito Context Ad Clicks | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Avito · Featured Prediction Competition · 10 years ago Late Submission more_horiz Avito Context Ad Clicks Predict if context ads will earn a user's click Avito Context Ad Clicks Overview Data Code Models Discussion Leaderboard Rules Overview Start Jun 2, 2015 Close Jul 29, 2015 Merger & Entry Description link keyboard_arrow_up In Russia, if you're looking to sell a tractor, a designer dress, a vintage lunchbox, or even a house, your first stop will likely be Avito.ru . As the largest general classified website in Russia, Avito connects buyers and sellers across the world's biggest country. Sellers are highly motivated to place ads on Avito, hoping to gain attention from the site's 70 million unique monthly visitors. There are three different types of ads available to sellers on Avito: regular, highlighted, and context. Context ads are seen as the best way to target users with goods and services. Currently, Avito uses general statistics on ad performance to drive the placement of context ads. Their existing model ignores individual user behavior, making it difficult to predict which ad will be the most relevant for (and earn the most clicks from) each potential buyer. In this competition, Avito is challenging you to improve on their model by predicting if individual users will click a given context ad. To create the most robust model and fun competition possible, Avito has provided eight comprehensive relational datasets for you to explore. This competition will help Avito more accurately predict click-through rates for their ads, creating a world where both buyers and sellers win. Evaluation link keyboard_arrow_up Submissions are scored on the log loss, also called the predictive binomial deviance: LogLoss = − 1 n n ∑ i = 1 [ y i log ( ˆ y i ) + ( 1 − y i ) log ( 1 − ˆ y i ) ] , where n is the number of rows in sampleSubmissions, each row represents one impression, which is a unique combination of an ad and a search \\\\( \\hat{y}_i \\\\) is the predicted probability of this ad being clicked \\\\( y_i \\\\) is 1 if the actual ad's IsClick = 1, 0 if IsClick = 0 \\\\( log() \\\\) is the natural (base e) logarithm A smaller log loss is better. The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value of \\\\(1.0 * 10 ^ {-15} \\\\). Submission File The file should contain a header and have the following format: TestId,IsClick 0,0 1,0 5,0 7,0 9,0 etc. Prizes link keyboard_arrow_up 1st place - $10,000 2nd place - $6,000 3rd place - $4,000 Scripts for Swag The authors of the 3 most up-voted scripts for this competition will get their choice of an official Kaggle sweatshirt, t-shirt, or mug! Your position on the leaderboard does not factor into winning Scripts for Swag. Timeline link keyboard_arrow_up July 21, 2015 - First Submission deadline. Your team must make its first submission by this deadline. July 21, 2015 - Team Merger deadline. This is the last day you may merge with another team July 28, 2015 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Ivan Guz, night_bat, and Wendy Kan. Avito Context Ad Clicks. https://kaggle.com/competitions/avito-context-ad-clicks, 2015. Kaggle. Cite Competition Host Avito Prizes & Awards $20,000 Awards Points & Medals Participation 680 Entrants 455 Participants 413 Teams 5,939 Submissions Tags Tabular Marketing Log Loss Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "icdm-2015-drawbridge-cross-device-connections",
    "discussion_links": [
      "/competitions/icdm-2015-drawbridge-cross-device-connections/discussion/16122",
      "/competitions/icdm-2015-drawbridge-cross-device-connections/discussion/16126"
    ],
    "discussion_texts": [
      "ICDM 2015: Drawbridge Cross-Device Connections | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz ICDM 2015: Drawbridge Cross-Device Connections Identify individual users across their digital devices ICDM 2015: Drawbridge Cross-Device Connections Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "ICDM 2015: Drawbridge Cross-Device Connections | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz ICDM 2015: Drawbridge Cross-Device Connections Identify individual users across their digital devices ICDM 2015: Drawbridge Cross-Device Connections Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Identify individual users across their digital devices Update 8/31/15 - this data set has been removed at the request of the host. Thank you for your participation! See this script for a quick exploration of the data This competition asks you to determine which cookies belong to an individual using a device. You are provided with relational information about users (represented by the id column drawbridge_handle), devices (device_id), cookies (cookie_id), as well as other information on IP addresses and behavior. For each device in the test set (dev_test_basic.csv), you must provide a list of cookie ids (from cookie_all_basic.csv) which you believe belong to the person using the given device_id. As you will see, the drawbridge_handle column is missing, denoted by the value -1, in the test set. Training set (semi-)supervised learning methods If you want to construct the training set and apply supervised learning, you can take the training data (dev_train_basic.csv), and find those cookies in the file cookie_all_basic.csv with the same drawbridge_handle. You could use device and cookie pairs with different drawbridge_handles as negative training data. Please note that some of the cookies have drawbridge_handle = -1, which means the drawbridge_handle for that cookie is unknown. The same set of cookies that will be used for both training and testing purposes. Data types There are four different types of data attributes: Index, Categorical, Boolean and Int . Index and Categorical are both enumerated type. Index has bigger set of elements (e.g. device_id or cookie_id), while Categorical has smaller set of elements (e.g. all the device types, or all the desktop browser version). Boolean applies to those attributes with only 2 possible values, and Int describe the count of the attribute in a continuous way. Meaning of the attributes For some attributes, the meanings are publicly available, as specified in the table schema. For some other attributes, the meanings are anonymous. Data table schema and meaning 1) Device basic information table (dev_train_basic.csv and dev_test_basic.csv) and cookie basic information table (cookie_all_basic.csv). Basic information tables provide high-level summary information regarding the device and cookie. For devices, the data is split into train and test parts. For cookies, there is one table that has the basic information for all the cookies. The schema of basic table is as below: Device basic Info tables (device_train_basic.csv and device_test_basic.csv) Feature Name Type Meaning 1 Drawbridge Handle Index Drawbridge identifier, uniquely identify a person behind device and cookie. Device and cookie belong to the same person will have the same handle. Different handles represent different users. 2 Device ID Index Index of each device. Uniquely identify each device 3 Device type Categorical Device type, iphone, android phone, ipad, android pad, etc. 4 Device OS version Categorical Device OS version. e.g. ios 8.0, 5 Device Country Info Categorical Which country this device belongs to 6 Anonymous_c0 Boolean Drawbridge anonymous feature to describe device. If the value is unknown, it will be -1 7 Anonymous_c1 Categorical Drawbridge anonymous feature to describe device. Categorical value. Will be -1 for unknown value. 8 Anonymous_c2 Categorical Drawbridge anonymous feature to describe device. Categorical value. Will be -1 for unknown value. 9 Anonymous_5 Int Drawbridge anonymous feature to describe device. Integer value. 10 Anonymous_6 Int Drawbridge anonymous feature to describe device. Integer value. 11 Anonymous_7 Int Drawbridge anonymous feature to describe device. Integer value. cookie basic info table (cookie_basic.csv) Feature Name Type Meaning 1 Drawbridge handle Index Drawbridge identifier, uniquely identify a person behind device and cookie. Device and cookie belong to the same person will have the same handle. Different handles represent different users. 2 cookie ID Index Index of each cookie. Uniquely identify each cookie 3 computer OS type Categorical cookie computer operation system type(e.g. window xp) 4 Browser version Categorical cookie browser version (e.g. Safari-6.0) 5 cookie country info Categorical Which country this cookie belongs to 6 Anonymous _c0 Boolean Drawbridge anonymous feature to describe device. Same meaning as Anonymous_c0 feature in device_basic table. 7 Anonymous_c1 Categorical Drawbridge anonymous feature to describe device. Categorical value. Will be -1 for unknown value. Same meaning as Anonymous_c1 feature in device_basic table. 8 Anonymous_c2 Categorical Drawbridge anonymous feature to describe device. Categorical value. Will be -1 for unknown value. Same meaning as Anonymous_c2 feature in device_basic table. 9 Anonymous_5 Int Drawbridge anonymous feature to describe device. Integer value. Same meaning as Anonymous_5 feature in device_basic table 10 Anonymous_6 Int Drawbridge anonymous feature to describe device. Integer value. Same meaning as Anonymous_6 feature in device_basic table 11 Anonymous_7 Int Drawbridge anonymous feature to describe device. Integer value. Same meaning as Anonymous_7 feature in device_basic table 2) IP table (id_all_ip.csv) describes the joint behavior of device or cookie on IP address. All the info of devices and cookies are merged into one single table, and we can use column 2, a boolean type, to differentiate if it's a device or cookie. One device or cookie may appear on multiple IPs, and we put all the IPs where we have seen a device/cookie into a bag. The data from column 3 to column 9 in the table makes up a tuple inside the bag, and that tuple describes the behavior of device/cookie on that particular IP. The table schema is below: device/cookie ip table (id_all_ip.csv) Feature name Type Meaning 1 Device/cookie ID Index ID of the device or cookie 2 Device or Cookie Boolean specify if it's a device or cookie. 0 for device and 1 for cookie. Only has two possible values 3 IP Index IP address (First field in the tuple) 4 Freq count Int How many times have we seen dev or cookie in column 1 appear on the IP in column 3 5 Anonymous Count 1 Int Anonymous number that describes the behavior of the specified device or cookie on the IP 6 Anonymous Count 2 Int Anonymous number that describes the behavior of the specified device or cookie on the IP 7 anonymous Count 3 Int Anonymous number that describes the behavior of the specified device or cookie on the IP 8 Anonymous Count 4 Int Anonymous number that describes the behavior of the specified device or cookie on the IP 9 Anonymous Count 5 Int Anonymous number that describes the behavior of the specified device or cookie on the IP (Last field in the tuple) 3) IP aggregation table (ipagg_all.csv). In general, we could see many different devices or cookies from a single IP. While the device/cookie IP table provides the information regarding the individual behavior of one device or one cookie on one IP, the IP aggregation table provides summary information that describe each IP across all the devices or cookies seen on that IP. IP aggregation table (provide aggregated behavior of each IP) (ipagg_all.csv) Feature name Type Meaning 1 IP Address Index Ip address 2 Is cell IP Boolean If IP is cellular IP or not. 1 for cellular and 0 for non cellular. 3 Total Freq Int Total number of observations seen on this IP (This number is the aggregated observation count on all the devices and cookies seen from this IP) 4 Anonymous count c0 Int Anonymous count that describes the behavior of the IP 5 Anonymous count c1 Int Anonymous count that describes the behavior of the IP 6 Anonymous count c2 Int Anonymous count that describes the behavior of the IP 4) Property observation and property category tables provide the information regarding website (for cookie) and mobile app (for device) that user has visited before. \"id_all_property.csv\" table lists the specific name of the website or mobile app, and property_category.csv table lists the categorical information of the website/mobile app. They schemas are listed below: Property observation table (id_all_property.csv) Feature name type meaning 1 Device/cookie ID Index ID of device or cookie 2 device or cookie indicator Boolean specify if it's a device or cookie. 0 for device and 1 for cookie. Only has two possible values 3 Property ID Index Website name for cookie, and mobile app name for the device 4 Property unique count Int How many times have we seen device or cookie on this property Property category table (property_category.csv) Feature name type meaning 1 Property ID Index Website name for cookie, and mobile app name for the device 2 Property category Categorical Category of the website or the mobile app How tojoin all the above tables? Device or cookie ID can be used as persistent keys to join tables. For IP observation and aggregation tables, we can also use IP to join the tables. For property information, we can use property ID to join the id_all_property.csv and property_category.csv tables. 1 files 0 B txt Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. empty.txt 1 file  Too many requests",
    "data_description": "ICDM 2015: Drawbridge Cross-Device Connections | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz ICDM 2015: Drawbridge Cross-Device Connections Identify individual users across their digital devices ICDM 2015: Drawbridge Cross-Device Connections Overview Data Code Models Discussion Leaderboard Rules Overview Start Jun 1, 2015 Close Aug 25, 2015 Merger & Entry Description link keyboard_arrow_up Imagine you're planning a summer holiday to Iceland: you read a travel blog on your smartphone on the subway to work, search for hotels on your laptop during lunch, browse Reykjavik restaurants on a tablet while half-watching TV after dinner, and then download a travel book to your e-reader to skim before bed. As consumers move across devices to complete online tasks, their identity becomes fragmented. Marketers, hoping to target them with meaningful messages, recommendations, and customized experiences, aren't always able to discern when activity on different devices is tied to one user vs. many users. Given usage data and a set of fabricated non-personally-identifiable IDs, this competition tasks you with making individual user connections across a variety of digital devices. Improving marketers' ability to identify individual users as they switch between devices means you'll see relevant messages wherever you go, making it easy for you to plan the best, most fjord-filled trip ever. Acknowledgements The competition dataset and prize pool have been generously provided by Drawbridge in sponsorship of the ICDM 2015 conference. Evaluation link keyboard_arrow_up Submissions will be evaluated based on their mean \\\\(F_{0.5}\\\\) score. The F score, commonly used in information retrieval, measures accuracy using the precision p and recall r. Precision is the ratio of true positives (tp) to all predicted positives (tp + fp). Recall is the ratio of true positives to all actual positives (tp + fn). The \\\\(F_{0.5}\\\\) score is given by ( 1 + β 2 ) p r β 2 p + r w h e r e p = t p t p + f p , r = t p t p + f n , β = 0.5. Note that the \\\\(F_{0.5}\\\\) score weights precision higher than recall. The mean \\\\(F_{0.5}\\\\) score is formed by averaging the individual \\\\(F_{0.5}\\\\) scores for each row in the test set. Submission File For each device listed in the test set, predict a space-delimited list of cookie_ids which you believe are associated with the device. The file should contain a header and have the following format: device_id,cookie_id id_1,id_10 id_100002,id_10 id_20 id_30 id_1000035,id_10 id_20 etc. Prizes link keyboard_arrow_up 1st place - $5000 2nd place - $3000 3rd place - $2000 Top teams will be invited to write a paper on their methodology for a workshop at ICDM 2015. Attendance at the conference is not mandatory to win a prize. More details of the workshop will follow. Scripts for Swag The authors of the 3 most up-voted scripts for this competition will get their choice of an official Kaggle sweatshirt, t-shirt, or mug! Your position on the leaderboard does not factor into winning Scripts for Swag. ICDM contest 2015 (Drawbridge Cross-device Connections Challenge) workshop This workshop is dedicated to the ICDM contest 2015 (Drawbridge Cross-device connection challenge). During the contest, participants are tasked with identifying a set of user connections across different devices without using common user handle information, for the purpose of proving that a technological, probabilistic approach to cross-device identity is a viable alternative to relying on deterministic user handle information. This workshop will give participants of the contest a chance to meet peers working in the field, share insights, thoughts and learnings from the contest and beyond. The workshop will feature presentations from the contest participants, invited keynote talk from renowned researcher in the field, and a panel discussion session as well. It will be held in conjunction with the IEEE ICDM 2015 conference in Atlantic City. The papers accepted to the workshop will appear in the workshop proceedings of ICDM 2015. Call for papers: **All participants whose final scores equal or above 0.75 are encouraged to submit papers** Please indicate team name in your paper submission. The topics include but are not limited to: Ideas of constructing the problem Feature engineering Modeling and other useful techniques used during the contest Insights/Learnings from the contest Important Dates (updated as of August 23rd, 2015): Initial submission due: August 30th, 2015 Author notification: September 4th, 2015 Camera ready paper: September 14th, 2015 Workshop date: November 14th, 2015 Organizers: Devin Guan (Drawbridge) Xiang Li (Drawbridge) Program Committee: Devin Guan (Drawbridge) Long-ji Lin (Rocket fuel) Shou-de Lin (National Taiwan University) Xiang Li (Drawbridge) Aditya Prakash (Virginia Tech) Chandan Reddy (Wayne State University) Jie Tang (Tsinghua University) Hanghang Tong (Arizona State University) Takashi Washio (Osaka University) Shipeng Yu (Linkedin) Submission: All submissions must be in english. The number of pages should not exceed 6 pages. All papers must be submitted to the email address: icdmcontest2015@drawbrid.ge, with subject: icdm workshop submission. The submissions will be peer-reviewed by the program committee, and judged based on the novelty, uniqueness, and final rankings of the participants in the contest. Authors of the accepted papers will be required to present at the workshop, either by attending the workshop in person, which we strongly encourage to, teleconferencing, or playing pre-recorded presentations. Program: Coming soon Keynote Speakers: Coming soon Timeline link keyboard_arrow_up August 17, 2015 - First Submission deadline. Your team must make its first submission by this deadline. August 17, 2015 - Team Merger deadline. This is the last day you may merge with another team August 24, 2015 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up 2121PokerGame, Anna Montoya, DevinGuan, drawbridge, myhu, Obuli, randoo, and Will Cukierski. ICDM 2015: Drawbridge Cross-Device Connections. https://kaggle.com/competitions/icdm-2015-drawbridge-cross-device-connections, 2015. Kaggle. Cite Prizes & Awards $10,000 Awards Points & Medals Participation 667 Entrants 405 Participants 338 Teams 2,353 Submissions Tags Multiclass Classification Tabular F-Score Beta (Micro) Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "crowdflower-search-relevance",
    "discussion_links": [
      "/competitions/crowdflower-search-relevance/discussion/15186",
      "/competitions/crowdflower-search-relevance/discussion/15299",
      "/competitions/crowdflower-search-relevance/discussion/15152",
      "/competitions/crowdflower-search-relevance/discussion/15181"
    ],
    "discussion_texts": [
      "Crowdflower Search Results Relevance | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Figure Eight · Featured Prediction Competition · 10 years ago Late Submission more_horiz Crowdflower Search Results Relevance Predict the relevance of search results from eCommerce sites Crowdflower Search Results Relevance Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Crowdflower Search Results Relevance | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Figure Eight · Featured Prediction Competition · 10 years ago Late Submission more_horiz Crowdflower Search Results Relevance Predict the relevance of search results from eCommerce sites Crowdflower Search Results Relevance Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Crowdflower Search Results Relevance | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Figure Eight · Featured Prediction Competition · 10 years ago Late Submission more_horiz Crowdflower Search Results Relevance Predict the relevance of search results from eCommerce sites Crowdflower Search Results Relevance Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Crowdflower Search Results Relevance | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Figure Eight · Featured Prediction Competition · 10 years ago Late Submission more_horiz Crowdflower Search Results Relevance Predict the relevance of search results from eCommerce sites Crowdflower Search Results Relevance Overview Data Code Models Discussion Leaderboard Rules"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict the relevance of search results from eCommerce sites See this script for a quick exploration of the data To evaluate search relevancy, CrowdFlower has had their crowd evaluate searches from a handful of eCommerce websites. A total of 261 search terms were generated, and CrowdFlower put together a list of products and their corresponding search terms. Each rater in the crowd was asked to give a product search term a score of 1, 2, 3, 4, with 4 indicating the item completely satisfies the search query, and 1 indicating the item doesn't match the search term.  The challenge in this competition is to predict the relevance score given the product description and product title. To ensure that your algorithm is robust enough to handle any noisy HTML snippets in the wild real world, the data provided in the product description field is raw and contains information that is irrelevant to the product. To discourage hand-labeling the data, CrowdFlower has also provided extra data that was not labeled by the crowd in the test set. This data is ignored when calculating your score. Ready to explore the data? Scripts is the most frictionless way to get familiar with the competition dataset! See the data at a glance here. No download needed to start publishing and forking code in R and Python. It's already pre-loaded with our favorite packages and ready for you to start competing! External data, such as dictionaries, thesaurus, language corpuses, are allowed. However, they must not be directly related to this specific dataset. The source of your external data must be posted to the forum to ensure fairness for all the participants in the community. 3 files 6.74 MB zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 6.74 MB sampleSubmission.csv.zip test.csv.zip train.csv.zip 3 files  Too many requests",
    "data_description": "Crowdflower Search Results Relevance | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Figure Eight · Featured Prediction Competition · 10 years ago Late Submission more_horiz Crowdflower Search Results Relevance Predict the relevance of search results from eCommerce sites Crowdflower Search Results Relevance Overview Data Code Models Discussion Leaderboard Rules Overview Start May 11, 2015 Close Jul 7, 2015 Merger & Entry Description link keyboard_arrow_up So many of our favorite daily activities are mediated by proprietary search algorithms. Whether you're trying to find a stream of that reality TV show on cat herding or shopping an eCommerce site for a new set of Japanese sushi knives, the relevance of search results is often responsible for your (un)happiness. Currently, small online businesses have no good way of evaluating the performance of their search algorithms, making it difficult for them to provide an exceptional customer experience. The goal of this competition is to create an open-source model that can be used to measure the relevance of search results. In doing so, you'll be helping enable small business owners to match the experience provided by more resource rich competitors. It will also provide more established businesses a model to test against. Given the queries and resulting product descriptions from leading eCommerce sites, this competition asks you to evaluate the accuracy of their search algorithms. Make a first submission with this Python benchmark on Kaggle scripts . The dataset for this competition was created using query-result pairings enriched on the CrowdFlower platform. They are sponsoring this competition as an investment in the open-source data science community. A dataset collected, cleaned, and labeled by CrowdFlower can make your supervised machine learning dreams come true. Evaluation link keyboard_arrow_up Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two ratings. This metric typically varies from 0 (random agreement between raters) to 1 (complete agreement between raters). In the event that there is less agreement between the raters than expected by chance, the metric may go below 0. The quadratic weighted kappa is calculated between the scores assigned by the human rater and the predicted scores. Results have 4 possible ratings, 1,2,3,4.  Each search record is characterized by a tuple (e a , e b ) , which corresponds to its scores by Rater A (human) and Rater B (predicted).  The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix O is constructed, such that O i,j corresponds to the number of search records that received a rating i by A and a rating j by B . An N-by-N matrix of weights, w , is calculated based on the difference between raters' scores: w i , j = ( i − j ) 2 ( N − 1 ) 2 An N-by-N histogram matrix of expected ratings, E , is calculated, assuming that there is no correlation between rating scores.  This is calculated as the outer product between each rater's histogram vector of ratings, normalized such that E and O have the same sum. From these three matrices, the quadratic weighted kappa is calculated as: κ = 1 − ∑ i , j w i , j O i , j ∑ i , j w i , j E i , j . Submission Format You must submit a csv file with the product id and a predicted search relevance for each search record. The order of the rows does not matter. The file must have a header and should look like the following: id,prediction 1,3 3,2 4,1 5,4 etc.. Prizes link keyboard_arrow_up 1st place - $10,000 2nd place - $6,000 3rd place - $4,000 Scripts for Swag The authors of the 3 most up-voted scripts for this competition will get their choice of an official Kaggle hoodie, t-shirt, or mug! Your position on the leaderboard does not factor into winning Scripts for Swag. Timeline link keyboard_arrow_up June 29, 2015 - First Submission deadline. Your team must make its first submission by this deadline. June 29, 2015 - Team Merger deadline. This is the last day you may merge with another team July 6, 2015 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up AaronZukoff, Anna Montoya, JustinTenuto, and Wendy Kan. Crowdflower Search Results Relevance. https://kaggle.com/competitions/crowdflower-search-relevance, 2015. Kaggle. Cite Competition Host Figure Eight Prizes & Awards $20,000 Awards Points & Medals Participation 1,617 Entrants 1,422 Participants 1,324 Teams 23,177 Submissions Tags Tabular Internet QuadraticWeightedKappa Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "predict-west-nile-virus",
    "discussion_links": [
      "/competitions/predict-west-nile-virus/discussion/14869",
      "/competitions/predict-west-nile-virus/discussion/14781"
    ],
    "discussion_texts": [
      "West Nile Virus Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz West Nile Virus Prediction Predict West Nile virus in mosquitos across the city of Chicago West Nile Virus Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "West Nile Virus Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz West Nile Virus Prediction Predict West Nile virus in mosquitos across the city of Chicago West Nile Virus Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict West Nile virus in mosquitos across the city of Chicago Ready to explore the data? Kaggle Scripts is the most frictionless way to get familiar with the competition dataset! No data download needed to start publishing and forking code in R and Python. It's already pre-loaded with our favorite packages and ready for you to start competing! In this competition, you will be analyzing weather data and GIS data and predicting whether or not West Nile virus is present, for a given time, location, and species. Every year from late-May to early-October, public health workers in Chicago setup mosquito traps scattered across the city. Every week from Monday through Wednesday, these traps collect mosquitos, and the mosquitos are tested for the presence of West Nile virus before the end of the week. The test results include the number of mosquitos, the mosquitos species, and whether or not West Nile virus is present in the cohort. Main dataset These test results are organized in such a way that when the number of mosquitos exceed 50, they are split into another record (another row in the dataset), such that the number of mosquitos are capped at 50. The location of the traps are described by the block number and street name. For your convenience, we have mapped these attributes into Longitude and Latitude in the dataset. Please note that these are derived locations. For example, Block=79, and Street= \"W FOSTER AVE\" gives us an approximate address of \"7900 W FOSTER AVE, Chicago, IL\", which translates to (41.974089,-87.824812) on the map . Some traps are \"satellite traps\". These are traps that are set up near (usually within 6 blocks) an established trap to enhance surveillance efforts. Satellite traps are postfixed with letters. For example, T220A is a satellite trap to T220. Please note that not all the locations are tested at all times. Also, records exist only when a particular species of mosquitos is found at a certain trap at a certain time. In the test set, we ask you for all combinations/permutations of possible predictions and are only scoring the observed ones. Spray Data The City of Chicago also does spraying to kill mosquitos. You are given the GIS data for their spray efforts in 2011 and 2013. Spraying can reduce the number of mosquitos in the area, and therefore might eliminate the appearance of West Nile virus.  Weather Data It is believed that hot and dry conditions are more favorable for West Nile virus than cold and wet. We provide you with the dataset from NOAA of the weather conditions of 2007 to 2014, during the months of the tests. Station 1: CHICAGO O'HARE INTERNATIONAL AIRPORT Lat: 41.995 Lon: -87.933 Elev: 662 ft. above sea level Station 2: CHICAGO MIDWAY INTL ARPT Lat: 41.786 Lon: -87.752 Elev: 612 ft. above sea level Map Data The map files mapdata_copyright_openstreetmap_contributors.rds and mapdata_copyright_openstreetmap_contributors.txt are from Open Streetmap and are primarily provided for use in visualizations (but you are allowed to use them in your models if you wish). Here's an example using mapdata_copyright_openstreetmap_contributors.rds , and here's one using mapdata_copyright_openstreetmap_contributors.txt . 9 files 15.84 MB zip, rds, pdf Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 15.84 MB mapdata_copyright_openstreetmap_contributors.rds mapdata_copyright_openstreetmap_contributors.txt.zip noaa_weather_qclcd_documentation.pdf sampleSubmission.csv.zip spray.csv.zip test.csv.zip train.csv.zip weather.csv.zip west_nile.zip 9 files  Too many requests",
    "data_description": "West Nile Virus Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz West Nile Virus Prediction Predict West Nile virus in mosquitos across the city of Chicago West Nile Virus Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Apr 22, 2015 Close Jun 18, 2015 Merger & Entry Description link keyboard_arrow_up West Nile virus is most commonly spread to humans through infected mosquitos. Around 20% of people who become infected with the virus develop symptoms ranging from a persistent fever, to serious neurological illnesses that can result in death. In 2002, the first human cases of West Nile virus were reported in Chicago. By 2004 the City of Chicago and the Chicago Department of Public Health (CDPH) had established a comprehensive surveillance and control program that is still in effect today. Every week from late spring through the fall, mosquitos in traps across the city are tested for the virus. The results of these tests influence when and where the city will spray airborne pesticides to control adult mosquito populations. Given weather, location, testing, and spraying data, this competition asks you to predict when and where different species of mosquitos will test positive for West Nile virus. A more accurate method of predicting outbreaks of West Nile virus in mosquitos will help the City of Chicago and CPHD more efficiently and effectively allocate resources towards preventing transmission of this potentially deadly virus. We've jump-started your analysis with some visualizations and starter code in R and Python on Kaggle Scripts . No data download or local environment setup needed! Acknowledgements This competition is sponsored by the Robert Wood Johnson Foundation . Data is provided by the Chicago Department of Public Health . Evaluation link keyboard_arrow_up Submissions are evaluated on area under the ROC curve between the predicted probability that West Nile Virus is present and the observed outcomes. Submission File For each record in the test set, you should predict a real-valued probability that WNV is present. The file should contain a header and have the following format: Id,WnvPresent 1,0 2,1 3,0.9 4,0.2 etc. Prizes link keyboard_arrow_up 1st place - $20,000 2nd place - $12,000 3rd place - $8,000 Scripts for Swag The authors of the 3 most up-voted Scripts for this competition will get their choice of an official Kaggle hoodie, t-shirt, or mug! Your position on the leaderboard does not factor into winning Scripts for Swag. Timeline link keyboard_arrow_up June 10, 2015 - First Submission deadline. Your team must make its first submission by this deadline. June 10, 2015 - Team Merger deadline. This is the last day you may merge with another team June 17, 2015 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Getting Started With Scripts link keyboard_arrow_up Kaggle Scripts are a great way to share models, visualizations, or analyses with other Kagglers. You can run scripts right on Kaggle without ever downloading the data, but for iterating on your script, you'll probably find it's easier to work locally (and then copy your script to Kaggle for sharing). Directory Structure It helps to set up the same directory structure that we have running on Kaggle. (Then you don't have to change any paths when copying the script to Kaggle.) This file sets up the directory structure (including all of the competition data): input : this contains all of the data files for the competition working : on Kaggle, scripts run with this as the working directory. We recommend you do the same thing locally to avoid mixing output files with source files. src : Source scripts. We've provided some examples to get you started. Python and R Environments We have Github repositories showing our R and Python environments are set up. We plan to make it very easy to work with the exact same environment locally, but at this point it may be easier to work with whatever environment you already have. (If you use Python or R packages locally that turn out to be missing in our online environment, we can probably add them for you.) Do make sure you're using Python 3, though. Conda is great for managing Python environments. RMarkdown If src/measurement_locations.Rmd is the RMarkdown file you want to render as HTML, you can say: Rscript render_rmarkdown.R src/measurement_locations.Rmd Then open working/output.html to view the results! Command Line Execution In your shell, you can navigate to the working directory, and run a script by saying: Rscript ../src/measurement_locations.R or python ../src/measurement_locations.py R We all love RStudio for interactive work. If you open a script in src in RStudio, your working directory will probably default to src . So we've included a line in the example that switches you to working at the top of the script. Python While we don't support iPython Notebooks in Scripts at this point, we know many people like to work in notebooks interactively. We've included an example notebook. The comments indicate the couple small changes required for transitioning to a script. Citation link keyboard_arrow_up Wendy Kan. West Nile Virus Prediction. https://kaggle.com/competitions/predict-west-nile-virus, 2015. Kaggle. Cite Prizes & Awards $40,000 Awards Points & Medals Participation 1,687 Entrants 1,445 Participants 1,304 Teams 29,882 Submissions Tags Binary Classification Tabular Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Prizes Timeline Getting Started With Scripts Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "restaurant-revenue-prediction",
    "discussion_links": [
      "/competitions/restaurant-revenue-prediction/discussion/14066"
    ],
    "discussion_texts": [
      "Restaurant Revenue Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Restaurant Revenue Prediction Predict annual restaurant sales based on objective measurements Restaurant Revenue Prediction Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict annual restaurant sales based on objective measurements TFI has provided a dataset with 137 restaurants in the training set, and a test set of 100000 restaurants. The data columns include the open date, location, city type, and three categories of obfuscated data: Demographic data, Real estate data, and Commercial data. The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. 3 files 4.16 MB zip, csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 4.16 MB sampleSubmission.csv test.csv.zip train.csv.zip 3 files 2 columns  Too many requests",
    "data_description": "Restaurant Revenue Prediction | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Restaurant Revenue Prediction Predict annual restaurant sales based on objective measurements Restaurant Revenue Prediction Overview Data Code Models Discussion Leaderboard Rules Overview Start Mar 23, 2015 Close May 5, 2015 Merger & Entry Description link keyboard_arrow_up With over 1,200 quick service restaurants across the globe, TFI is the company behind some of the world's most well-known brands: Burger King, Sbarro, Popeyes, Usta Donerci, and Arby’s. They employ over 20,000 people in Europe and Asia and make significant daily investments in developing new restaurant sites. Right now, deciding when and where to open new restaurants is largely a subjective process based on the personal judgement and experience of development teams. This subjective data is difficult to accurately extrapolate across geographies and cultures. New restaurant sites take large investments of time and capital to get up and running. When the wrong location for a restaurant brand is chosen, the site closes within 18 months and operating losses are incurred. Finding a mathematical model to increase the effectiveness of investments in new restaurant sites would allow TFI to invest more in other important business areas, like sustainability, innovation, and training for new employees. Using demographic, real estate, and commercial data, this competition challenges you to predict the annual restaurant sales of 100,000 regional locations. TFI would love to hire an expert Kaggler like you to head up their growing data science team in Istanbul or Shanghai. You'd be tackling problems like the one featured in this competition on a global scale. See the job description here >> Evaluation link keyboard_arrow_up Root Mean Squared Error (RMSE) Submissions are scored on the root mean squared error. RMSE is very common and is a suitable general-purpose error metric. Compared to the Mean Absolute Error, RMSE punishes large errors: RMSE = √ 1 n n ∑ i = 1 ( y i − ˆ y i ) 2 , where y hat is the predicted value and y is the original value. Submission File For every restaurant in the dataset , submission files should contain two columns: Id and Prediction. The file should contain a header and have the following format: Id,Prediction 0,1.0 1,1.0 2,1.0 etc. Prizes link keyboard_arrow_up 1st place - $15,000 2nd place - $10,000 3rd place - $5,000 TFI is interested in hiring top Kagglers from this competition. If you're interested in a position with TFI, put (TFI) next to your team name to be considered. You can review details about the job and apply directly to the role here . Timeline link keyboard_arrow_up April 27, 2015 - First Submission deadline. Your team must make its first submission by this deadline. April 27, 2015 - Team Merger deadline. This is the last day you may merge with another team May 4, 2015 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Ekrem Ozer, Meghan O'Connell, and Wendy Kan. Restaurant Revenue Prediction. https://kaggle.com/competitions/restaurant-revenue-prediction, 2015. Kaggle. Cite Prizes & Awards $30,000 Awards Points & Medals Participation 2,896 Entrants 2,459 Participants 2,257 Teams 32,745 Submissions Tags Regression Tabular Root Mean Squared Error Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "otto-group-product-classification-challenge",
    "discussion_links": [
      "/competitions/otto-group-product-classification-challenge/discussion/14335",
      "/competitions/otto-group-product-classification-challenge/discussion/14296",
      "/competitions/otto-group-product-classification-challenge/discussion/14295",
      "/competitions/otto-group-product-classification-challenge/discussion/14315"
    ],
    "discussion_texts": [
      "Otto Group Product Classification Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Otto Group Product Classification Challenge Classify products into the correct category Otto Group Product Classification Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Otto Group Product Classification Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Otto Group Product Classification Challenge Classify products into the correct category Otto Group Product Classification Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Otto Group Product Classification Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Otto Group Product Classification Challenge Classify products into the correct category Otto Group Product Classification Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "Otto Group Product Classification Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Otto Group Product Classification Challenge Classify products into the correct category Otto Group Product Classification Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Classify products into the correct category See, fork, and run a random forest benchmark model through Kaggle Scripts Each row corresponds to a single product. There are a total of 93 numerical features, which represent counts of different events. All features have been obfuscated and will not be defined any further. There are nine categories for all products. Each target category represents one of our most important product categories (like fashion, electronics, etc.). The products for the training and testing sets are selected randomly. 3 files 43.84 MB csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 43.84 MB sampleSubmission.csv test.csv train.csv 3 files 199 columns  Too many requests",
    "data_description": "Otto Group Product Classification Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Otto Group Product Classification Challenge Classify products into the correct category Otto Group Product Classification Challenge Overview Data Code Models Discussion Leaderboard Rules Overview Start Mar 17, 2015 Close May 19, 2015 Merger & Entry Description link keyboard_arrow_up Get started on this competition through Kaggle Scripts The Otto Group is one of the world’s biggest e-commerce companies, with subsidiaries in more than 20 countries, including Crate & Barrel (USA), Otto.de (Germany) and 3 Suisses (France). We are selling millions of products worldwide every day, with several thousand products being added to our product line. A consistent analysis of the performance of our products is crucial. However, due to our diverse global infrastructure, many identical products get classified differently. Therefore, the quality of our product analysis depends heavily on the ability to accurately cluster similar products. The better the classification, the more insights we can generate about our product range. For this competition, we have provided a dataset with 93 features for more than 200,000 products. The objective is to build a predictive model which is able to distinguish between our main product categories. The winning models will be open sourced. Evaluation link keyboard_arrow_up Submissions are evaluated using the multi-class logarithmic loss. Each product has been labeled with one true category. For each product, you must submit a set of predicted probabilities (one for every category). The formula is then, l o g l o s s = − 1 N N ∑ i = 1 M ∑ j = 1 y i j log ( p i j ) , where N is the number of products in the test set, M is the number of class labels, \\\\(log\\\\) is the natural logarithm, \\\\(y_{ij}\\\\) is 1 if observation \\\\(i\\\\) is in class \\\\(j\\\\) and 0 otherwise, and \\\\(p_{ij}\\\\) is the predicted probability that observation \\\\(i\\\\) belongs to class \\\\(j\\\\). The submitted probabilities for a given product are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\\\(max(min(p,1-10^{-15}),10^{-15})\\\\). Submission Format You must submit a csv file with the product id, all candidate class names, and a probability for each class. The order of the rows does not matter. The file must have a header and should look like the following: id,Class_1,Class_2,Class_3,Class_4,Class_5,Class_6,Class_7,Class_8,Class_9 1,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0 2,0.0,0.2,0.3,0.3,0.0,0.0,0.1,0.1,0.0 ... etc. Prizes link keyboard_arrow_up The total prize pool for this competition is $10,000, distributed as follows: 1st place - $5000 2nd place - $3000 3rd place - $2000 Timeline link keyboard_arrow_up May 11, 2015 - First Submission deadline. Your team must make its first submission by this deadline. May 11, 2015 - Team Merger deadline. This is the last day you may merge with another team May 18, 2015 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Benjamin Bossan, Josef Feigl, and Wendy Kan. Otto Group Product Classification Challenge. https://kaggle.com/competitions/otto-group-product-classification-challenge, 2015. Kaggle. Cite Prizes & Awards $10,000 Awards Points & Medals Participation 4,363 Entrants 3,841 Participants 3,507 Teams 43,398 Submissions Tags Tabular Internet Multiclass Loss Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "march-machine-learning-mania-2015",
    "discussion_links": [],
    "discussion_texts": [],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Predict the 2015 NCAA Basketball Tournament If you are unfamiliar with the format and intricacies of the NCAA tournament, we encourage reading the wikipedia page before diving into the data.  The data description and schema may seem daunting at first, but is not as complicated as it appears. As a reminder, you are encouraged to incorporate your own sources of data. We have provided team-level historical data to jump-start the modeling process, but there is also player-level and game-level data that may be useful. If you're looking for some powerful technology to help get the most from all your data, you could load your data into the Vertica Analytics Platform for data preparation and use Distributed R , a scaleable and high-performance platform for R with out-of-the-box parallel algorithms. After you build and evaluate predictive model in Distributed R you can even deploy model(s) back to Vertica Analytics Platform for in-database prediction scoring using simple SQL and can combine prediction results with other insights that you may have derived from the data. Note that Vertica Analytics Platform offers wide array of SQL analytic functions such as in-database sentiment analysis functions, geospatial functions that can help you derive new and interesting attributes. We extend our gratitude to Kenneth Massey for his work gathering and providing much of the historical data. Stage 1 - You should submit predicted probabilities for every possible matchup in the past 4 NCAA tournaments (2011-2014). Stage 2 - You should submit predicted probabilities for every possible matchup before the 2015 tournament begins. Refer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict. Below we describe the format and fields of the \"essential\" data files. Optional files may be added to the data while the competition is running. You can assume that we will provide the essential files for the current season. You should not assume that we will provide optional files for the current season. To avoid confusion, we will keep the current season data (for stage 2) separate from the historical data (stage 1). teams.csv This file identifies the different college teams present in the dataset. Each team has a 4 digit id number. seasons.csv This file identifies the different seasons included in the historical data, along with certain season-level properties. regular_season_compact_results.csv This file identifies the game-by-game results for 30 seasons of historical data, from 1985 to 2014. Each year, it includes all games played from daynum 0 through 132 (which by definition is \"Selection Sunday\", the day that tournament pairings are announced). Each row in the file represents a single game played. regular_season_detailed_results.csv This file is a more detailed set of game results, covering seasons 2003-2014. This includes team-level total statistics for each game (total field goals attempted, offensive rebounds, etc.) The column names should be self-explanatory to basketball fans (as above, \"w\" or \"l\" refers to the winning or losing team): tourney_compact_results.csv This file identifies the game-by-game NCAA tournament results for all seasons of historical data. The data is formatted exactly like the regular_season_compact_results.csv data. Note that these games also include the play-in games (which always occurred on day 134/135) for those years that had play-in games. tourney_detailed_results.csv This file contains the more detailed results for tournament games from 2003 onward. tourney_seeds.csv This file identifies the seeds for all teams in each NCAA tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on the bracket structure. tourney_slots.csv This file identifies the mechanism by which teams are paired against each other, depending upon their seeds. Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. If there were N teams in the tournament during a particular year, there were N-1 teams eliminated (leaving one champion) and therefore N-1 games played, as well as N-1 slots in the tournament bracket, and thus there will be N-1 records in this file for that season. 14 files 22.36 MB csv, zip Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 22.36 MB all_selected_predictions.zip regular_season_compact_results.csv regular_season_compact_results_2015.csv regular_season_detailed_results.csv regular_season_detailed_results_2015.csv sample_submission_2015.csv seasons.csv teams.csv tourney_compact_results.csv tourney_detailed_results.csv tourney_seeds.csv tourney_seeds_2015.csv tourney_slots.csv tourney_slots_2015.csv 14 files 150 columns  Too many requests",
    "data_description": "March Machine Learning Mania 2015 | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz March Machine Learning Mania 2015 Predict the 2015 NCAA Basketball Tournament March Machine Learning Mania 2015 Overview Data Code Models Discussion Leaderboard Rules Overview Start Feb 2, 2015 Close Apr 8, 2015 Merger & Entry Description link keyboard_arrow_up At Kaggle HQ and in offices across the country, March is a month when bracketology is in bloom. Back by popular demand, our second annual March Machine Learning Mania competition pits you against the millions of sports fans and office-pool bandwagoners who are hoping to win big by correctly predicting the outcome of the men's NCAA basketball tournament . While the odds of forecasting a perfect bracket are astronomical, these odds are improved by the growing amount of data collected throughout the season, including player statistics, tournament seeds, geographical factors and social media. How well can machine learning and statistical techniques improve the forecast? Presented by HP Software's industry leading Big Data group and the HP Haven Big Data platform , this competition will test how well predictions based on data stack up against a (jump) shot in the dark. This competition allows you to get creative with the datasets you use to create your model. We provide data covering three decades of historical games, but you're highly encouraged to pull in data from external sources. The 50+ REST APIs from HP IDOL OnDemand are a great way to get started augmenting the dataset. Developer accounts are free and includes free monthly quota! Begin by extracting trending topics and identifying entities from the IDOL OnDemand news dataset (accessed via the Query Text Index API ) or by analyzing public sentiment about players and teams using data from your social media feed. In stage one of this two-stage competition, participants will build and test their models against the previous four tournaments. In the second stage, participants will predict the outcome of the 2015 tournament. You don’t need to participate in the first stage to enter the second, but the first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2015 results, for which you’ll predict winning percentages for the likelihood of each possible matchup, not just a traditional bracket. HP is sponsoring $15,000 in cash prizes for the winners. Please visit the FAQs for more information. Acknowledgements March Machine Learning Mania 2015 is presented by HP. Please see About the sponsor to read more. Evaluation link keyboard_arrow_up Submissions are scored on the log loss, also called the predictive binomial deviance: LogLoss = − 1 n n ∑ i = 1 [ y i log ( ˆ y i ) + ( 1 − y i ) log ( 1 − ˆ y i ) ] , where n is the number of games played \\\\( \\hat{y}_i \\\\) is the predicted probability of team 1 beating team 2 \\\\( y_i \\\\) is 1 if team 1 wins, 0 if team 2 wins \\\\( log() \\\\) is the natural (base e) logarithm A smaller log loss is better. Games which are not played are ignored in the scoring. Play-in games are also ignored (only the games among the final 64 teams are scored). The use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value. Submission File The file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2015 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2  = 2278 matchups. Each game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, \"2013_1104_1129\" indicates team 1104 played team 1129 in the year 2013. You must predict the probability that the team with the lower id beats the team with the higher id. The resulting submission format looks like the following, where \"pred\" represents the predicted probability that the first team will win: id,pred 2011_1103_1106,0.5 2011_1103_1112,0.5 2011_1103_1114,0.5 ... ... Prizes link keyboard_arrow_up 1st Place - $10,000 2nd Place - $5,000 Stage 1 will not count towards Kaggle rankings/points. Stage 2 is weighted to award 50% of the usual competition points (on account of the element of luck involved). It does not count towards earning the Kaggle Masters tier. Timeline link keyboard_arrow_up Stage 1 - Model Building Feb 2 - Mar 14, 2015 - competitors build and test models on historical data. During this phase, the leaderboard shows the model performance on historical tournament outcomes. Stage 2 - 2015 Championship Sunday, Mar 15 - Selection Sunday (68 teams announced) Monday, Mar 16 - Kaggle begins to accept 2015 predictions. Release of up-to-date 2014-2015 season data. Wednesday, Mar 18 - Final deadline to submit 2015 predictions (11:59PM UTC). Mar 19 - Apr 6 - sit back, relax, and watch your predictions come true! All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary as the contest progresses. Faqs link keyboard_arrow_up Is one tournament enough to decide the best basketball algorithm? It's better to be lucky than good! Besides, when was the last time your office held a cross validation pool? Can I update my predictions after the tournament starts? No changes are permitted once the tournament begins. How is the leaderboard going to work when the event being scored hasn't yet happened? You won't submit after the tournament starts. We'll update the solution file as the games occur, which will cause the ranks on the leaderboard to change. Why do we have to predict every match up? Why 68 teams and not 64? This was done for timing purposes. Predicting every possible matchup for the 68 teams announced on Selection Sunday gives participants the most time to get their 2015 predictions ready in time. There is a small \"play-in\" round, sometimes called the first round, where the 68 are narrowed to 64. While you are asked to predict these games (and you may be predicting them after they occur), we will not be scoring them. Why don't predictions in later rounds count for more? While it is possible to weight the later games, we chose to keep scoring simple and count all games equally. Any weights we pick would be mostly arbitrary (how many first-round games is a championship game worth?). Also, weighting any game increases the role that luck plays in determining a winner. We've also structured the competition so that people can still be in the running even if there are early-round upsets. Can the results of an algorithm like this be used for sports betting? Maybe, but that's not a goal of this competition. Kaggle claims no rights to the intellectual property developed by competitors. Have fun and learn. If you want to use the results to gamble, that's between you and your bookie and your local laws. Organizers link keyboard_arrow_up Jeff Sonas Jeff Sonas is a statistical chess analyst who invented the Chessmetrics system for rating chess players. He is the founder and proprietor of the Chessmetrics.com website, which gives Sonas' calculations of the ratings of current players and historical ratings going back as far as January 1843. He has written dozens of articles since 1999 for ChessBase.com and other chess websites. He was a participant in the FIDE ratings committee meeting in Athens in June 2010. He graduated with honors with a B.S. in Mathematical and Computational Sciences from Stanford University in 1991. Mark Glickman, Ph.D. Mark Glickman is Research Professor of Health Policy and Management at the Boston University School of Public Health, and Senior Statistician at the Center for Healthcare Organization and Implementation Research, a Veterans Administration Center of Innovation. Dr. Glickman is known for having invented the Glicko and Glicko-2 rating systems, both of which have been adopted by many gaming organizations internationally. Dr. Glickman has served as a member of the United States Chess Federation Ratings Committee since 1985, and has been the Chair of the Committee continuously since 1992. He has also served as Chair and Program Chair for the American Statistical Association's Section on Statistics in Sports, and has earned its award for Sports Statistician of the Year in 2009. He has co-founded and co-organizes the New England Symposium on Statistics in Sports, a bi-annual conference on the research and practice of applying statistical methods in sports. He is also Editor in Chief of the Journal of Quantitative Analysis in Sports. About The Sponsor link keyboard_arrow_up HP creates new possibilities for technology to have a meaningful impact on people, businesses, governments and society. With the broadest technology portfolio spanning printing, personal systems, software, services and IT infrastructure, HP delivers solutions for customers’ most complex challenges in every region of the world. More information about HP (NYSE: HPQ) is available at http://www.hp.com HP Software - Big Data Group We are on a journey. Today, buyers for analytics platforms that enable the creation of new mobile and enterprise applications as well as provide actionable business insights are very different than those who seek solutions for Information Management & Governance. They access information, collaborate, innovate and require support in very different ways. We will ensure that all of these buyers/practitioners get the expertise and the focus that they need from our sales and services teams. At the same time, we are building for the future. Our architects, our engineers, our product managers are innovating to build, integrate and deliver a modular portfolio that leverages HP’s innovative Haven Big Data Platform. Haven Big Data Platform The HP Haven Big Data platform harnesses 100% of your data—structured and unstructured—to inform every decision and help you capitalize on opportunities and solve problems. Available on-premise or in the cloud, Haven offers Big Data analytics and next gen applications at unmatched speed and scale. Introducing Haven OnDemand HP takes its Big Data platform to the cloud with HP Haven OnDemand, providing a fast-track to data-driven insights. It’s an easy-to-consume suite of cloud services that you’ll have up and running in minutes. Developer-friendly APIs and advanced analytics enable you to create next-gen apps and services with ease. Vertica OnDemand No-Compromise Big Data Analytics in the Cloud. Purpose-built Big Data analytics – delivering extreme speed, massive scale, and openness and flexibility for the most demanding analytical workloads. Get up and running with Vertica in less than 30 minutes > Get started now HP IDOL OnDemand Structure and insight from chaos – includes industry-leading collection of 50+ REST APIs for cloud-, mobile-, and IOT-apps. Connect your data with web & file system connectors. Extract meaning from unstructured data e.g. Concepts, context, sentiment, entities (incl. people, places, addresses, companies, drugs, films, profanities, PII…). Index any data from multiple sources. Powerful search APIs with contextual and conceptual search across public and private datasets. Combine APIs to discover and unlock valuable insights from your data like never before! > Free API key for developers Predictive Analytics and R HP Vertica Distributed R is a High-Performance Scalable Platform for the R language. Distributed R provides R language extensions — distributed arrays, data frames, data partitioning and parallel looping constructs. These extensions enable R developers to parallelize statistical and machine learning algorithms for the scalability and performance needs of Big Data. > Watch the video overview > Download HP Vertica Distributed-R now Haven Marketplace Explore the Haven Marketplace – the online community where developers, data scientists, and technology partners come together to showcase their apps and help others to get the most out of the Haven platform. Not only will you find useful add-ons and extensions, but you will also have the opportunity to publish and distribute your own Haven apps when they’re ready for prime time! Tutorials link keyboard_arrow_up If you're planning to go BIG with your data-sets, you will probably need more analytics horsepower than regular R can provide. This is where using HP Vertica and Distributed-R from the HP Haven Predictive Analytics suite can give you the jump on the competition. To help accelerate your learning, HP's team has put together a tutorial covering Vertica + Distributed-R using the competition data-sets. You are encouraged to work through the tutorial at your own pace, and if you have questions, feel free to ask questions in the HP Big Data & Analytics forums. Go to the tutorial now . You may also want to enhance your data-sets with using APIs. Check out the HP IDOL OnDemand tutorials to get you familiar with the basics. Citation link keyboard_arrow_up Jeff Sonas, Mark Glickman, Meghan O'Connell, Sean Hughes, and Will Cukierski. March Machine Learning Mania 2015. https://kaggle.com/competitions/march-machine-learning-mania-2015, 2015. Kaggle. Cite Prizes & Awards $15,000 Awards Points Does not award Medals Participation 796 Entrants 404 Participants 340 Teams 611 Submissions Tags Basketball Tabular Sports Log Loss Table of Contents collapse_all Description Evaluation Prizes Timeline Faqs Organizers About The Sponsor Tutorials Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "axa-driver-telematics-analysis",
    "discussion_links": [
      "/competitions/axa-driver-telematics-analysis/discussion/12850"
    ],
    "discussion_texts": [
      "Driver Telematics Analysis | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Driver Telematics Analysis Use telematic data to identify a driver signature Driver Telematics Analysis Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Use telematic data to identify a driver signature Update: this dataset has been removed at the request of the host. You are provided a directory containing a number of folders. Each folder represents a driver. Within each folder are 200 .csv files. Each file represents a driving trip. The trips are recordings of the car's position (in meters) every second and look like the following: In order to protect the privacy of the drivers' location, the trips were centered to start at the origin (0,0), randomly rotated, and short lengths of trip data were removed from the start/end of the trip. A small and random number of false trips (trips that were not driven by the driver of interest) are planted in each driver's folder. These false trips are sourced from drivers not included in the competition data, in order to prevent similarity analysis between the included drivers. You are not given the number of false trips (it varies), nor a labeled training set of true positive trips. You can safely make the assumption that the majority of the trips in each folder do belong to the same driver. The challenge of this competition is to identify trips which are not from the driver of interest , based on their telematic features. You must predict a probability that each trip was taken by the driver of interest. Too many requests",
    "data_description": "Driver Telematics Analysis | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz Driver Telematics Analysis Use telematic data to identify a driver signature Driver Telematics Analysis Overview Data Code Models Discussion Leaderboard Rules Overview Start Dec 15, 2014 Close Mar 17, 2015 Merger & Entry Description link keyboard_arrow_up For automobile insurers, telematics represents a growing and valuable way to quantify driver risk. Instead of pricing decisions on vehicle and driver characteristics, telematics gives the opportunity to measure the quantity and quality of a driver's behavior. This can lead to savings for safe or infrequent drivers, and transition the burden to policies that represent increased liability. AXA has provided a dataset of over 50,000 anonymized driver trips. The intent of this competition is to develop an algorithmic signature of driving type. Does a driver drive long trips? Short trips? Highway trips? Back roads? Do they accelerate hard from stops? Do they take turns at high speed? The answers to these questions combine to form an aggregate profile that potentially makes each driver unique. For this competition, Kaggle participants must come up with a \"telematic fingerprint\" capable of distinguishing when a trip was driven by a given driver. The features of this driver fingerprint could help assess risk and form a crucial piece of a larger telematics puzzle. Evaluation link keyboard_arrow_up Submissions are judged on area under the ROC curve . The ROC area is calculated in a global manner (all predictions together). You should therefore aim to submit calibrated probabilities between the drivers. Submission File You must submit a predicted probability for all possible driver_trip pairs. The resulting submission format looks like like the following, where \"prob\" represents the predicted probability that the trip belongs to the associated driver (1 = the trip belongs to the driver of interest, 0 = the trip does not belong): driver_trip,prob 1_1,1 1_2,1 1_3,1 ... Prizes link keyboard_arrow_up 1st place - $15,000 2nd place - $10,000 3rd place - $5,000 Timeline link keyboard_arrow_up March 9, 2015 - First Submission deadline. Your team must make its first submission by this deadline. March 9, 2015 - Team Merger deadline. This is the last day you may merge with another team March 16, 2015 - Final submission deadline All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary. Citation link keyboard_arrow_up Guillaume BS and Will Cukierski. Driver Telematics Analysis. https://kaggle.com/competitions/axa-driver-telematics-analysis, 2014. Kaggle. Cite Prizes & Awards $30,000 Awards Points & Medals Participation 2,326 Entrants 1,861 Participants 1,524 Teams 35,962 Submissions Tags Multiclass Classification Tabular Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Prizes Timeline Citation Too many requests error Too many requests error Too many requests"
  },
  {
    "competition_slug": "stumbleupon",
    "discussion_links": [
      "/competitions/stumbleupon/discussion/6197",
      "/competitions/stumbleupon/discussion/6184",
      "/competitions/stumbleupon/discussion/6207"
    ],
    "discussion_texts": [
      "StumbleUpon Evergreen Classification Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz StumbleUpon Evergreen Classification Challenge Build a classifier to categorize webpages as evergreen or non-evergreen StumbleUpon Evergreen Classification Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "StumbleUpon Evergreen Classification Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz StumbleUpon Evergreen Classification Challenge Build a classifier to categorize webpages as evergreen or non-evergreen StumbleUpon Evergreen Classification Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests",
      "StumbleUpon Evergreen Classification Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz StumbleUpon Evergreen Classification Challenge Build a classifier to categorize webpages as evergreen or non-evergreen StumbleUpon Evergreen Classification Challenge Overview Data Code Models Discussion Leaderboard Rules Too many requests error Too many requests"
    ],
    "competition_overview": "Home Competitions Datasets Models Code Discussions Learn More View Active Events Build a classifier to categorize webpages as evergreen or non-evergreen Note: researchers who wish to use this data outside the competition should download and read the data access agreement. There are two components to the data provided for this challenge: The first component is two files: train.tsv and test.tsv . Each is a tab-delimited text file containing the fields outlined below for 10,566 urls total. Fields for which no data is available are indicated with a question mark. The second component is raw_content.zip , a zip file containing the raw content for each url, as seen by StumbleUpon's crawler. Each url's raw content is stored in a tab-delimited text file, named with the urlid as indicated in train.tsv and test.tsv. The following table includes field descriptions for train.tsv and test.tsv: 4 files 196.18 MB tsv, zip, csv Subject to Competition Rules Competition Rules To see this data you need to agree to the competition rules . Please sign in or register to accept the rules. 196.18 MB raw_content.zip sampleSubmission.csv test.tsv train.tsv 4 files 55 columns  Too many requests",
    "data_description": "StumbleUpon Evergreen Classification Challenge | Kaggle menu Skip to content Create search ​ explore Home emoji_events Competitions table_chart Datasets tenancy Models code Code comment Discussions school Learn expand_more More auto_awesome_motion View Active Events menu Skip to content search ​ Sign In Register Kaggle uses cookies from Google to deliver and enhance the quality of its services and to analyze traffic. Learn more OK, Got it. Late Submission more_horiz StumbleUpon Evergreen Classification Challenge Build a classifier to categorize webpages as evergreen or non-evergreen StumbleUpon Evergreen Classification Challenge Overview Data Code Models Discussion Leaderboard Rules Overview Start Aug 16, 2013 Close Nov 1, 2013 Description link keyboard_arrow_up StumbleUpon is a user-curated web content discovery engine that recommends relevant, high quality pages and media to its users, based on their interests. While some pages we recommend, such as news articles or seasonal recipes, are only relevant for a short period of time, others maintain a timeless quality and can be recommended to users long after they are discovered. In other words, pages can either be classified as \"ephemeral\" or \"evergreen\". The ratings we get from our community give us strong signals that a page may no longer be relevant - but what if we could make this distinction ahead of time? A high quality prediction of \"ephemeral\" or \"evergreen\" would greatly improve a recommendation system like ours. Many people know evergreen content when they see it, but can an algorithm make the same determination without human intuition? Your mission is to build a classifier which will evaluate a large set of URLs and label them as either evergreen or ephemeral. Can you out-class(ify) StumbleUpon? As an added incentive to the prize, a strong performance in this competition may lead to a career-launching internship at one of the best places to work in San Francisco. Evaluation link keyboard_arrow_up Submissions are judged on area under the ROC curve . In Matlab (using the stats toolbox): [~, ~, ~, auc ] = perfcurve(true_labels, predictions, 1); In R (using the verification package): auc = roc.area(true_labels, predictions) In python (using the metrics module of scikit-learn): fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions, pos_label=1)\nauc = metrics.auc(fpr,tpr) Submission Format Each line of your submission should contain an urlid and a label. Note that you may submit any real-valued number as a prediction, since AUC is only sensitive to the ranking. sampleSubmission.csv shows a representative valid submission. The format looks like this: urlid,label 5865,0 782,0 6962,0\netc... Timeline link keyboard_arrow_up Competition Friday, August 16, 2013, 7:00 PM UTC - Competition begins Thursday, October 31, 2013 - Final submission deadline Winning Model Selection and Notification Friday, November 15, 2013 - Preliminary winner notified Friday, November 22, 2013 - Deadline for preliminary winner to open source models. Friday, November 29, 2013 - Winner finalized All deadlines are at 11:59:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary as the contest progresses. Prizes link keyboard_arrow_up The Winner will receive $5,000.  Top performers may also have the opportunity to interview remotely for an internship at the StumbleUpon office in San Francisco. Winners link keyboard_arrow_up First Place: François C. - Paris, France Citation link keyboard_arrow_up Collin Chuck, Debora Donato, timabe, Vishal Vaingankar, and Will Cukierski. StumbleUpon Evergreen Classification Challenge. https://kaggle.com/competitions/stumbleupon, 2013. Kaggle. Cite Prizes & Awards $5,000 Awards Points & Medals Participation 819 Entrants 624 Participants 624 Teams 7,486 Submissions Tags Text Tabular Internet Area Under Receiver Operating Characteristic Curve Table of Contents collapse_all Description Evaluation Timeline Prizes Winners Citation Too many requests error Too many requests error Too many requests"
  }
]